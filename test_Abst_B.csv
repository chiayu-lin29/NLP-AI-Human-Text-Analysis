clean_abstract
style transfer aims to synthesize an image which inherits the content of one image while preserving a similar style of the other one the style of an image usually refers to its unique feeling conveyed from visual features which is highly related to the aesthetic effect of the image aesthetic effect can be mainly decomposed as two factors colour and texture previous methods like neural style transfer and colour transfer have shown strong abilities in transferring colour and texture features however such approaches neglect to further disentangle colour and texture which makes some of unique aesthetic effects designed by human artists hard to express in this paper we propose a novel problem called aestheticaware image style transfer task which aims to transfer colour and texture separately and independently to manipulate the aesthetic effect of an image we propose a novel aestheticaware modeloptimisationbased style transfer aamobst model to solve this problem specifically aamobst is a multireference twopath model it uses different reference images to decide desired colour and texture features it can segregate colour and texture into two distinct paths and transfer them independently qualitative and quantitative experiments show that our model can decide colour and texture features separately and is able to keep one of them fixed while changing the other one which is not applicable for previous methods furthermore on tasks that are applicable for previous methods such as style transfer colourpreserved transfer and colouronly transfer our model shows comparable abilities with other baseline methods
this paper formulates a chanceconstrained optimal distribution network partitioning odnp problem addressing uncertainties in load and renewable energy generation and presents a solution methodology using sample average approximation saa the objective is to identify potential subnetworks in the existing distribution grid that are likely to survive as selfadequate islands if supply from the main grid is lost this constitutes a planning problem practical constraints like ensuring network radiality and availability of gridforming generators are considered quality of the obtained solution is evaluated by comparison with a an upper bound on the probability that the identified islands are supplydeficient and b a lower bound on the optimal value of the true problem performance of the odnp formulation is illustrated on a modified ieee 37bus feeder it is shown that the network flexibility is well utilized the partitioning changes with risk budget and that the saa method is able to yield good quality solutions with modest computation cost
this paper introduces a design scheme of intelligent gas valve management and service system based on internet this scheme adds sensor and general packet radio service gprs modules to the traditional gas valve and establishes communication connection between gas valve and the server through wireless packet communication technology which makes the traditional gas valve have the networking ability compared with the traditional gas valve management and service business the method proposed in this paper is more convenient and efficient
background artificial intelligence ai has become a powerful tool and is attracting more attention in the field of medicine there are a number of ai studies focusing on skin diseases and there are many ai products that have been applied in dermatology however the attitudes of dermatologists specifically those from china towards ai is not clear as few if any studies have focused on this issue methods a webbased questionnaire was designed by experts from the chinese skin image database csid and published on the umer doctor platform an online learning platform for dermatologists developed by the shanghai wheat color intelligent technology company china a total of 1228 chinese dermatologists were recruited and provided answers to the questionnaire online the differences of dermatologists attitudes towards ai among the different groups stratified by age gender hospital level education degree professional title and hospital ownership were compared by using the mannwhitney u test and the kruskalwallis h test the correlations between stratified factors and dermatologists’ attitudes towards ai were calculated by using the spearman’s rank correlation test spss version 220 was utilized for all analyses a twosided p value 005 was considered statistically significant in all analyses results a total of 1228 chinese dermatologists from 30 provinces autonomous regions municipalities and other regions including hong kong macau and taiwan participated in this survey the dermatologists who participated acquired airelated information mainly through the internet meetings or forums and 7051 of participated dermatologists acquired airelated information by two or more approaches in total 9951 of participated dermatologists pay attention general passiveactive and active attention to information pertaining to ai stratified analyses revealed statistically significant differences in their attention levels unconcerned general passiveactive and active attention to airelated information by gender hospital level education degree and professional title p values ≤179e−02 in total 9536 of the participated dermatologists thought the role of ai to be in “assisting the daily diagnosis and treatment activities for dermatologists” stratified analyses about the thought of ai roles unconcerned useless assist and replace showed that there was no statistically significant difference except for the hospital level p value 409e−03 the correlations between stratified factors with attention levels and the opinions of ai roles showed extremely weak correlations furthermore 6417 of participated dermatologists thought secondary hospitals in china are in most need of the application ai and 9178 of participated dermatologists thought the priority implementation of ai should be in skin tumors conclusions the majority of chinese dermatologists are interested in ai information and acquired information about ai through a variety of approaches nearly all dermatologists are attentive to information on ai and think the role of ai is in “assisting the daily diagnosis and treatment activities for dermatologists” future ai implementation should be primarily focused on skin tumors and utilized in in secondary hospitals
based on the group heuristic model and the model of intuitive cooperation we hypothesized that ingroup favoritism would be conspicuously shown through an intuitive process to test this hypothesis we utilized a minimal group paradigm which is traditionally used in social psychological studies and manipulated decision time in a oneshot prisoner’s dilemma game to compare the cooperative contribution level toward ingroup and outgroup members under three conditions intuitive empathic deliberation and rational deliberation our findings confirmed that ingroup favoritism was clearly shown in the intuitive condition only suggesting that the intuitive cooperation model may only be valid in the context of social exchange with ingroup members additional analysis also showed that ingroup favoritism disappeared for participants who had been forced into empathic or rational deliberation for decision making the theoretical implications of the findings are discussed
game theoretic methods and simulations based on reinforcement learning rl are often used to analyze electricity market equilibrium however the former is limited to a simple market environment with complete information and difficult to visually reflect the tacit collusion while the conventional rl algorithm is limited to lowdimensional discrete state and action spaces and the convergence is unstable to address the aforementioned problems this paper adopts deep deterministic policy gradient ddpg algorithm to model the bidding strategies of generation companies gencos simulation experiments including different settings of genco load and network demonstrate that the proposed method is more accurate than conventional rl algorithm and can converge to the nash equilibrium of complete information even in the incomplete information environment moreover the proposed method can intuitively reflect the different tacit collusion level by quantitatively adjusting gencos’ patience parameter which can be an effective means to analyze market strategies
glutamic oxaloacetic transaminase and lactate dehydrogenase are present in brain the present study was undertaken to investigate whether there was any change in cerebrospinal fluid glutamic oxaloacetic transaminase andor lactate dehydrogenase levels in alzheimers disease in the present case control study cerebrospinal fluid glutamic oxaloacetic transaminase and lactate dehydrogenase levels were assayed in 162 alzheimers disease cases and 149 healthy controls glutamic oxaloacetic transaminase levels of cases were highly significantly increased with respect to the levels of controls lactate dehydrogenase levels of cases were not significantly increased with respect to the levels of controls data from the present study might help in understanding the biochemical basis of alzheimer’s disease it is stressed that these findings should be supplemented by further prospective research
this paper consists of the development of a system to help patients with different disabilities affected by rare or chronic diseases or any kind of dependence through tele assistance virtual interaction and intelligent monitoring the main goal is to increase the quality of life of the minorities who cannot take full advantage of the healthcare system by providing an alternative way of monitoring them with the technology embedded in this paper the result of the paper is not intended to be a single solution but a modular system that allows the construction of an application that is able to measure the needs of a health administration and the patients the paper also pursues an educational training to the facultative trainees in a new way to approach patient treatments it can improve the quality of life of the patients by saving them time and other resources in moving to the health center and the professionals can also save time as they can take advantage of the online treatments by using the proposed system
we analyse the darkweb and find its structure is unusual for example  sim 87  of darkweb sites emphnever link to another site to call the darkweb a web is thus a misnomer  its better described as a set of largely isolated dark silos as we show through a detailed comparison to the world wide web www this siloed structure is highly dissimilar to other social networks and indicates the social behavior of darkweb users is much different to that of www users we show a generalized preferential attachment model can partially explain the strange topology of the darkweb but an understanding of the anomalous behavior of its users remains out of reach our results are relevant to network scientists social scientists and other researchers interested in the social interactions of large numbers of agents
in this brief a 400w gallium nitride device based dcdc boost converter with a four phase matrix inductor is proposed it is wellknown that there is a significant influence of the dc bias on inductor core losses and the simplest solution is to employ multiphase structures however the size of the converter will also increase as the number of phases increases therefore in order to achieve both high efficiency and high power density a matrix inductor is proposed for four phases boost converter the proposed matrix inductor retains the flux sharing advantages of conventional inductors with ecores and thus greatly increases the power density and utilization rate by integrating the inductors through flux cancellation therefore both the size and the core loss are reduced when compared with four conventional inductors moreover since the four phases are operated in parallel the phase error which is a critical problem in the interleaved structure may not affect the proposed converter removing the current balance issue critical operation mode is utilized to achieve zerovoltage switching zvs for all switches finally the proposed four phase dcdc boost converter and matrix inductor is built and tested to verify its feasibility the peak and cec efficiency is tested to be 993 and 991 respectively
in this note we introduce the notion of polynions and discuss their mathematical relevance we note that the wellknown mathematical structures like quaternions octonions and sedenions etc are special cases of polynions
computational fluid dynamics has become an important tool for studying blood flow dynamics as an insilico collection of methods computational fluid dynamics is noninvasive and provides numerical values for the most important parameters of blood flow such as velocity and pressure that are crucial in hemodynamic studies in this primer we briefly explain the basic theory and workflow of the two most commonly applied computational fluid dynamics techniques used in the congenital heart disease literature the finite element method and the finite volume method we define important terminology and include specific examples of how using these methods can answer important clinical questions in congenital cardiac surgery planning and perioperative patient management
fireinduced forest loss has substantially increased worldwide over the last decade in china the connection between forest loss and frequent fires on a national scale remains largely unexplored in this study we used a data set for a timeseries of forest loss from the global forest watch and for a modisderived burned area for 2003–2015 to ascertain variations in forest loss and to explore its relationship with forest fires represented by burned areas at the country and forestzone levels we quantified trends in forest loss during 2003–2015 using linear regression analysis and assessed the relation between forest loss and burned areas using spearman’s correlation forest loss increased significantly 2648 km 2  a −1  r 2   054 p   001 throughout china with an average annual increase of 114 during 2003–2015 however the forest loss trend had extensive spatial heterogeneity forest loss increased mainly in the subtropical evergreen broadleaf forest zone 3150 km 2  a −1  r 2   069 p   001 and tropical rainforest zone 388 km 2  a −1  r 2   066 p   001 but the loss of forest decreased in the cold temperate deciduous coniferous forest zone − 708 km 2 year −1  r 2   075 p   001 and the temperate deciduous mixed broadleaf and coniferous forest zone − 144 km 2  a −1  r 2   045 p   005 we found that 10 of china’s area had a significant positive correlation  r  ≥ 055 p   005 with burned areas and 03 had a significant negative correlation  r  ≤ − 055 p   005 in particular forest loss had a significant positive relationship with the burned area in the cold temperate deciduous coniferous forest zone 169 of the lands and the subtropical evergreen broadleaf forest zone 78 these results provide a basis for future predictions of fireinduced forest loss in china
assistant professor american university of paris dep of computer science mathematics and environmental science 102 rue st dominique 75007 paris fr tlibalaupedu httptomerlibalinfo postdoctoral researcher universität luxemburg fakultät für naturwissenschaften technologie und kommunikation 6 avenue de la fonte l4364 eschsuralzette lu alexandersteenunilu httpsalexandersteende
environmental hazard is very challenging when it comes to providing support to the affected areas due to the lack of effective communication communication infrastructures such as cellular networks might be destroyed disabled partially accessible or deliberately shut down during environmental hazards as a result hundreds of people die before they have a chance to be saved this paper proposes an emergency management system using a network of mobile devices that can be implemented in areas affected by disasters both stranded survivors and rescue teams can use this technology the proposed system provides an emergency communication network when all modes of communication infrastructure are broken down for path discovery and message transmission a wireless selforganized opportunistic routing protocol is also proposed
refleksi dan scattering pada sinyal informasi dimana akan mengurangi kualitas sinyal informasi diterima pada sisi penerima hal tersebut mengakibatkan adanya sifat konstruktif maupun destruktif pada sinyal di sisi penerima contoh sifat destruktif yang sering terjadi yakni fading dan delay  sistem penerima correlation receiver diimplementasikan guna mengatasi masalah yang timbul akibat efek dari multipath fading  untuk memperoleh sinyal dengan nilai error minimum maka diperlukan analisis terkait proses transmisi pada masingmasing komponen multipath  hasil analisis diperoleh bahwa unjuk kerja dari kode pn  pseudo noise  pada kanal frequency selective fading yang lebih baik adalah variasi komponen multipath  3 dibandingkan dengan variasi komponen multipath  6 dan yang paling buruk adalah variasi komponen multipath  7 hal ini dibuktikan dengan melihat nilai bit error rate ber saat energy per bit to noise power spectral density ratio ebno  10 dengan komponen multipath  3 memiliki nilai ber abstract barriers to the line of sight  los  can cause diffraction reflection and scattering of information signals which will reduce the quality of information received on the receiving side this results in a constructive and destructive nature of the signal at the receiving end examples of destructive properties that often occur are fading and delay the receiver correlation system is implemented to overcome the problems that arise due to the effects of multipath fading to obtain a signal with a minimum error value an analysis is needed related to the transmission process for each multipath component the analysis results show that the performance of the pn code pseudo noise on the frequency selective fading channel is best when the multipath component variation  3 compared to multipath component variation  6 and the worst is multipath component variation  7 this is proved by the value of ber when ebno  10 with multipath component  3 has ber value of 000368 while multipath component  6 and multipath component  7 have ber values of 000386 and 000507 respectively based on these results it can be concluded that the number of multipath components have an effect on the quality of the received signal
public attitudes towards learning disabilities lds are generally reported as positive inclusive and empathetic however these findings do not reflect the lived experiences of people with lds to shed light on this disparity a team of coresearchers with lds created the first online survey to challenge public understanding of lds asking questions in ways that are important to them and represent how they see themselves here we describe and evaluate the process of creating an accessible survey platform and an online survey in a research team consisting of academic and nonacademic professionals with and without lds or autism through this inclusive research process the codesigned survey met the expectations of the coresearchers and was wellreceived by the initial survey respondents we reflect on the coresearchers perspectives following the study completion and consider the difficulties and advantages we encountered deploying such approaches and their potential implications on future survey data analysis
traffic sign detection is an important task in assisted safety and autonomous driving it is important to continuously detect the traffic signs emerged on the road currently most object detection methods make independent detections based on single images when we apply these methods directly to a video clip to detect traffic signs without taking into account temporal correlations among adjacent frames missed detections or incorrect detections can frequently occur due to motion blur size change partial occlusion andor bad pose in this paper we fully exploit the temporal consistency of traffic sign detection in videos more specifically we incorporate information of adjacent frames with high confidence scores to enhance the discovery of potential objects in the missed or incorrect detected frames by “recovering” the missed roi proposals or by “improving” the incorrect roi proposals with low confidence scores our method can be regarded as a “detectionbytracking” strategy which results in a more robust detection performance in videos
growing interest in the impact of gender on technology includes concerns with regard to gender bias and its implication in the user experience past explorations of gender and design revealed significant differences between female and male users with regard to the perception and the acceptance of dating apps a novel study was designed to investigate whether gender differences in the user experience could be caused by gender bias in mobile app development significant differences in the definition of design priorities emerged with regard to the participants’ gender and the persona’s perceived gender
this article deals with the control problem of 7degrees of freedom fullcar suspension system which takes into account the springdamper nonlinearities unmodeled dynamics and external disturbances the existing active disturbance rejection control uses an extended state observer to estimate the “total disturbance” and eliminate it with state error feedback in this article a new type of active disturbance rejection control is developed to improve the ride comfort of full car suspension systems taking into account the suspension nonlinearities and actuator saturation the proposed controller combines active disturbance rejection control and fuzzy sliding mode control and is called fuzzy sliding mode active disturbance rejection control to validate the system mathematical model and analyze the controller performance a virtual prototype is built in adams the simulation results demonstrate better performance of fuzzy sliding mode active disturbance rejection control compared to the existing active disturbance rejection control
industrial control systems ics monitor and control physical processes ics are found in among others critical infrastructures such as water treatment plants water distribution systems and the electric power grid while the existence of cybercomponents in an ics leads to ease of operations and maintenance it renders the system under control vulnerable to cyber and physical attacks an experimental study was conducted with replay attacks launched on an operational water distribution wadi plant to understand under what conditions an attackerattack can remain undetected while stealing water a detection method based on an inputoutput linear timeinvariant system model of the physical process was developed and implemented in wadi to detect such attacks the experiments reveal the strengths and limitations of the detection method and challenges faced by an attacker while attempting to steal water from a water distribution system
the work done in this paper has focused on the prediction of failures of a complex and highly stressed technical system for energy production namely the w18v50df engine powering an mw gasfired power plant the aim of this work is to highlight the prediction curves a priori and a posteriori of the evolution probabilistic of the state of these w18v50df engines in order to anticipate the appearance of failure and to put a humanmachine interface to facilitate the knowledge of a possible event and to allow a remote action to do this a hybrid method has been employed in the field of dataoriented modeling which highlights the neural network modeling used to determine the state of the components of the system studied by classification coupled with bayesian network modeling also known as probabilistic graphical models used to predict the state of the system the neural model and the hmi have been built respectively through the ntools library and via the guide library of the matlab software while the probabilistic graphical model has been built using the bayesianlab software the work carried out has shown that the w18v50df engine and its components are degraded as their lifetimes evolve in addition because of its complexity and the criticality of some of its components the degradation of the w18v50df engine will be accelerated as each of them will be in addition to this the financial evaluation revealed that this work beyond its multiple technical challenges would allow the user to make significant financial gains
abstract client preferences can have an impact on psychotherapy however little is known about client preferences for therapists’ personality characteristics the present study aimed to address this gap using data from psychotherapy clients recruited through 1 amazon’s mechanical turk n  235 and 2 a psychology department subject pool n  100 participants completed a brief personality measure about themselves their ideal therapist their actual therapist their mother their father a close friend and a romantic partner they also rated their satisfaction with each relationship number of sessions attended with their current therapist and the therapeutic alliance across both samples emotional stability and conscientiousness were the most preferred therapist personality traits the ideal therapist’s personality characteristics were most discrepant from ratings of mothers and fathers for both samples in most cases congruence in personality ratings between the ideal therapist and the other relationship was significantly correlated with relationship satisfaction the personality characteristics of an ideal therapist were highly similar to participants’ actual therapists for both samples congruence in actual and ideal therapist ratings was not predicted by number of sessions but was significantly predicted by ratings of the therapeutic alliance implications and future directions are discussed
objective early detection and timely management of bleeding is critical as failure to recognize physiologically significant bleeding is associated with significant morbidity and mortality many such instances are detected late even in highly monitored environments contributing to delay in recognition and intervention we propose a noninvasive early identification model to detect bleeding events using continuously collected photoplethysmography ppg and electrocardiography ecg waveforms approach fiftynine york pigs undergoing fixedrate controlled hemorrhage were involved in this study and a least absolute shrinkage and selection operator regressionbased early detection model was developed and tested using ppg and ecg derived features the output of the early detection model was a risk trajectory indicating the future probability of bleeding main results our proposed models were generally accurate in predicting bleeding with an area under the curve of 089 95 ci 087–092 and achieved an average time of 161 mins to detect 168 blood loss when a false alert rate of 1 was tolerated models developed on noninvasive data performed with similar discrimination and lead time to hemorrhage compared to models using invasive arterial blood pressure as monitoring data significance a bleed detection model using only noninvasive monitoring performs as well as those using invasive arterial pressure monitoring
the aim of this research was to test recent developments in the use of remotely piloted aircraft systems or unmanned aerial vehicles uavdrones to map both pasture quantity as biomass yield and pasture quality as the proportions of key pasture nutrients across a selected range of field sites throughout the rangelands of queensland improved pasture management begins with an understanding of the state of the resource base uav based methods can potentially achieve this at improved spatial and temporal scales this study developed machine learning based predictive models of both pasture measures uavbased structure from motion photogrammetry provided a measure of yield from overlapping high resolution visible colour imagery pasture nutrient composition was estimated from the spectral signatures of visible near infrared hyperspectral uav sensing an automated pasture height surface modelling technique was developed tested and used along with field site measurements to predict further estimates across each field site both prior knowledge and automated predictive modelling techniques were employed to predict yield and nutrition pasture height surface modelling was assessed against field measurements using a rising plate meter results reported correlation coefficients r2 ranging from 02 to 04 for both woodland and grassland field sites accuracy of the predictive modelling was determined from further field measurements of yield and on average indicated an error of 08 t ha−1 in grasslands and 13 t ha−1 in mixed woodlands across both modelling approaches correlation analyses between measures of pasture quality acid detergent fibre and crude protein adf cp and spectral reflectance data indicated the visible red 651 nm and rededge 759 nm regions were highly correlated adf r2  09 and cp r2  05 mean values these findings agreed with previous studies linking specific absorption features with grass chemical composition these results conclude that the practical application of such techniques to efficiently and accurately map pasture yield and quality is possible at the field site scale however further research is needed in particular further field sampling of both yield and nutrient elements across such a diverse landscape with the potential to scale up to a satellite platform for broader scale monitoring
we develop heuristic interpolation methods for the functions t↦logdetatbdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenttmapsto log det left textbfa  ttextbfb right enddocument and t↦traceatbpdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenttmapsto textrmtraceleft textbfa  ttextbfbp right enddocument where the matrices adocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenttextbfaenddocument and bdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenttextbfbenddocument are hermitian and positive semi definite and pdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentpenddocument and tdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenttenddocument are real variables these functions are featured in many applications in statistics machine learning and computational physics the presented interpolation functions are based on the modification of sharp bounds for these functions we demonstrate the accuracy and performance of the proposed method with numerical examples namely the marginal maximum likelihood estimation for gaussian process regression and the estimation of the regularization parameter of ridge regression with the generalized crossvalidation method
the maximum eigenvalue of the adjacency matrix am has been supposed to contain rich information about the corresponding network an experimental study focused on revealing the meaning and application of the maximum eigenvalue is missing to this end am was constructed using mutual information mi to determine the functional connectivity with electroencephalogram eeg data recorded with a mental fatigue model and then was converted into both binary and weighted brain functional network bfn and corresponding random networks rns both maximum eigenvalue and corresponding network characters in bfns and rns were considered to explore the changes during the formation of mental fatigue the results indicated that large maximum eigenvalue means more edges in the corresponding network along with a high degree and a short characteristic path length both in weighted and binary bfns interestingly the maximum eigenvalue of am was always a little larger than that of the corresponding random matrix rm and had an obvious linearity with the sum of the am elements indicating that the maximum eigenvalue can be able to distinguish the network structures which have the same mean degree what is more the maximum eigenvalue which increased with the deepening of mental fatigue can become a good indicator for mental fatigue estimation
we present a novel online endtoend neural diarization system bwedaeend that processes data incrementally for a variable number of speakers the system is based on the encoderdecoderattractor eda architecture of horiguchi et al but utilizes the incremental transformer encoder attending only to its left contexts and using blocklevel recurrence in the hidden states to carry information from block to block making the algorithm complexity linear in time we propose two variants for unlimitedlatency bwedaeend which processes inputs in linear time we show only moderate degradation for up to two speakers using a context size of 10 seconds compared to offline edaeend with more than two speakers the accuracy gap between online and offline grows but the algorithm still outperforms a baseline offline clustering diarization system for one to four speakers with unlimited context size and shows comparable accuracy with context size of 10 seconds for limitedlatency bwedaeend which produces diarization outputs blockbyblock as audio arrives we show accuracy comparable to the offline clusteringbased system
to accurately estimate freeway traffic carbon dioxide cosub2sub emissions this paper proposes a spatiotemporal cellbased model by taking traffic dynamics into account highfidelity vehicle trajectory data is used to construct a spatiotemporal traffic st diagram and to calculate the exact cosub2sub emissions of the traffic in the st diagram the factors impacting the cosub2sub emissions in the st diagram are selected and taken as model inputs first and secondorder regression models are employed to fit the exact cosub2sub emissions it is found that the relationship between complicated traffic dynamics and cosub2sub emissions can be simply described by using a linear or nearly linear function ie for larger cells such as inlineformula texmath notationlatex90cdot 150 texmathinlineformula secinlineformula texmath notationlatexcdot textm texmathinlineformula that are used to construct an st diagram a firstorder regression model is able to well reflect the relationship while for small cells such as inlineformula texmath notationlatex30cdot 50 texmathinlineformula secinlineformula texmath notationlatexcdot textm texmathinlineformula a secondorder model is more accurate to validate the proposed model another trajectory dataset that was collected in a different freeway segment is introduced and the transferability and predictability of the model are demonstrated the proposed spatiotemporal cellbased model allows us to accurately estimate cosub2sub emissions by inputting the prevailing st diagram it opens a gate for estimating cosub2sub emissions from widely available lowfidelity traffic data since the st diagram can be constructed by using various traffic flow data such as loop detector data and floating car data
due to their simplicity and flexibility the unsupervised statistical models such as gaussian mixture model gmm are powerful tools to address the brain magnetic resonance mr images segmentation problems however the gmm is based only on the intensity information which makes it sensitive to noise lots of gmmbased segmentation algorithms have the low segmentation accuracy because of the influence caused by noise and intensity inhomogeneity to further improve the segmentation accuracy a robust spatial informationtheoretic gmm algorithm is proposed in this paper to simultaneously estimate the intensity inhomogeneity and segment brain mr images first a novel spatial factor containing the nonlocal spatial information is incorporated to reduce the impact of noise the proposed spatial factor not only takes into account the local neighboring information but also considers the spatial structure information of pixels thus our algorithm can retain more image details while reducing the influence of noise second the mutual information mi maximization method is used to identify and eliminate outliers finally to overcome the impact of intensity inhomogeneity we use a linear combination of a set of orthogonal polynomials to approximate the bias field the objective function is integrated with the bias field estimation model to segment the images and estimate the bias field simultaneously the experimental results on both synthetic and clinical brain mr images show that the proposed algorithm can overcome the influence of noise and intensity inhomogeneity and achieve more accurate segmentation results
a 4‐year global precipitation measurement gpm precipitation feature pf data set is used to quantify the frequency and global distribution of overshooting convection in this study overshooting convection is defined as pfs with maximum 20 dbz echo top height maxht20 greater than the height of the lapse rate tropopause derived from era‐interim reanalysis data the geographical distribution of overshooting convection exhibits a strong preference for specific land regions such as over the central united states argentina central africa and colombia larger areas and greater occurrence of 20 dbz radar reflectivity at the tropopause are found in northern middle to high latitudes than in the tropics the occurrence of 20 dbz radar reflectivity reaching above the level of 380 k potential temperature z380k in middle and high latitudes is found to be comparable to that in the tropics furthermore a methodology is developed to detect overshooting convection using the gpm microwave imager measured brightness temperature at 18331 ± 3 and 18331 ± 7 ghz and polarization corrected temperature at 89 ghz the geographical distribution of overshooting convection can be closely reproduced using the combinations of these brightness temperatures with an average heidke skill score of 04 and probability of 038 this shows the possibility of identifying overshooting convection from microwave observations at high‐frequency channels near the water vapor absorption line centered at 18331 ghz from other satellite missions
in nature a biological invasion is a common phenomenon that often threatens the existence of local species getting rid of the invasive species is hard to achieve after its survival and reproduction at present killing some invasive ones and putting artificial local species are usual methods to prevent local species from extinction an ode model is constructed to simulate the invasive procedure and the protection policy is depicted as a series of impulses depending on the state of the variables both the odes and the impulses form a state feedback impulsive model which describes the invasion and protection together the existence of homoclinic cycle and bifurcation of order1 periodic solution of the impulsive model are discussed and the orbitally asymptotical stability of the order1 periodic solution is certificated with a novel method finally the numerical simulation result is listed to confirm the theoretical work
cancers are the leading cause of deaths worldwide in 2018 an estimated 181 million new cancer cases and 96 million cancerrelated deaths occurred globally several previous studies have shown that the enzyme leucine aminopeptidase is involved in pathological conditions such as cancer on the basis of the knowledge that isoquinoline alkaloids have antiproliferative activity and inhibitory activity towards leucine aminopeptidase the present study was conducted a study which involved database search virtual screening and design of new potential leucine aminopeptidase inhibitors with a scaffold based on 34dihydroisoquinoline these compounds were then filtered through lipinski’s “rule of five” and 25 081 of them were then subjected to molecular docking next threedimensional quantitative structureactivity relationship 3dqsar study was performed for the selected group of compounds with the best binding score results the developed model calculated by leaveoneout method showed acceptable predictive and descriptive capability as represented by standard statistical parameters r2 0997 and q2 0717 further 35 compounds were identified to have an excellent predictive reliability finally nine selected compounds were evaluated for druglikeness and different pharmacokinetics parameters such as absorption distribution metabolism excretion and toxicity our methodology suggested that compounds with 34dihydroisoquinoline moiety were potentially active in inhibiting leucine aminopeptidase and could be used for further indepth in vitro and in vivo studies
this research analyses and describes in detail how the digital biennale activities that are a part of the indonesian digital art community has become a form of criticism and silent resistance to the social hegemony it refers to the ideology norms rules and myths that exist in modern society in indonesia especially the reproduction of hoax content hoax refers to the logic people who live in a world of cyber media with all of its social implications this phenomenon is a problem and it is at the heart of the exploration of the art community in east java biennale the critical social theory perspective of gramsci’s theory forms the basis of this research analysis the qualitative research approach used a digital ethnomethodology research method focused on the online and offline social movements in the biennale art community the data collection techniques used were observation and nonactive participation in the process of reproductionrelated to the exhibition of indonesian biennale digital artworks it was then analyzed using gramsci’s hegemony theory the purpose of this study was to describe the process of social movements in a digital format conducted by the indonesian biennale when reproducing works of art to counteract the dominance and hegemony of the hoax phenomenon in indonesia the benefit of this research was that it obtained a preposition of gramsci’s hegemony theory in the world of digital art as created by contemporary indonesian biennale artists digital technology has had a tremendous effect on the media industry government trade informal industry sector human resources urban planning services disaster relief health education religion artistic and cultural expression in addition to various other fields the conclusion obtained from this research is that there is a formation of a new hegemony a digital hegemony this new hegemony is of particular concern for the digital artists in east java biennale through the digital format works the artists also try to communicate their art as a form of silent resistance protest and criticism of the hegemony that occurs in society referring to the ideology norms and myths it can be called a digital counterhegemonys
more than 1000 crystalline silicide materials have been screened for thermoelectric properties using firstprinciples atomistic calculations coupled with the semiclassical boltzmann transport equation compounds that contain radioactive toxic rare and expensive elements as well as oxides hydrides carbides nitrides and halides have been neglected in the study the already wellknown silicides with good thermoelectric properties such as sige mg2si and mnsix are successfully predicted to be promising compounds along with a number of other binary and ternary silicide compositions some of these materials have only been scarcely studied in the literature with no thermoelectric properties being reported in experimental papers these novel materials can be very interesting for thermoelectric applications provided that they can be heavily doped to give a sufficiently high charge carrier concentration and that they can be alloyed with isoelectronic elements to achieve adequately low phonon thermal conductivity the study concludes with a list of the most promising silicide compounds that are recommended for further experimental and theoretical investigations
facial sketches drawn by artists are widely used for visual identification applications and mostly by law enforcement agencies but the quality of these sketches depend on the ability of the artist to clearly replicate all the key facial features that could aid in capturing the true identity of a subject recent works have attempted to synthesize these sketches into plausible visual images to improve visual recognition and identification however synthesizing photorealistic images from sketches proves to be an even more challenging task especially for sensitive applications such as suspect identification in this work we propose a novel approach that adopts a generative adversarial network that synthesizes a single sketch into multiple synthetic images with unique attributes like hair color sex etc we incorporate a hybrid discriminator which performs attribute classification of multiple target attributes a quality guided encoder that minimizes the perceptual dissimilarity of the latent space embedding of the synthesized and real image at different layers in the network and an identity preserving network that maintains the identity of the synthesised image throughout the training process our approach is aimed at improving the visual appeal of the synthesised images while incorporating multiple attribute assignment to the generator without compromising the identity of the synthesised image we synthesised sketches using xdog filter for the celeba wvu multimodal and celebahq datasets and from an auxiliary generator trained on sketches from cuhk iitd and feret datasets our results are impressive compared to current state of the art
layered snse2 flakes and snse2sno2 heterojunctionbased chemiresistivetype sensors demonstrate good gas sensing properties at low temperatures the pure snse2based chemiresistivetype sensor exhibited good response and recovery to both no2 and nh3 while also showing a moderate response to hydrogen co c3h6 and several typical volatile organic vapors such as methanol ethanol acetone and formaldehyde near room temperature indicating that it possesses poor selectivity however compared to pristine snse2 a snse2sno2 heterojunctionbased chemiresistivetype sensor prepared by the thermal oxidation method showed greatly improved sensitivity and superior selectivity toward no2 at 120 °c such impressive improvements in the sensing performance can be attributed to the formation of snse2sno2 heterojunctions the density functional theory dft calculations revealed that there was a lower adsorption energy and greater bader charge value between the adsorbed no2 and snse2sno2 heterojunction materials
this paper presents the application of hierarchical model predictive control hmpc as an energy management framework for a multitimescale mixedenergy system ie powertox ptx the goal of the energy managing controller is to minimize an economic objective by determining the energy flows within the system hmpc enables a longterm scheduling solution to anticipate ahead of the seasonal energy mismatches occurring in ptx systems this paper presents a novel approach to couple the separate control layers in the hierarchy by heuristic assignment to fully employ the ptx fundamentals in the controller design simulation results based on historical data of the dutch energy sector show the suitability of hmpc and its superiority over a rulebased control approach the proposed controller may however also be used for any configuration of multitimescale mixedenergy systems dealing with temporal energy mismatches
a novel control scheme utilizing fractionalorder terminal sliding mode control with sliding perturbation observer fotsmcspo is proposed herein it was used to solve the trajectory tracking problem on a sevendegreeoffreedom robot manipulator the fractionalorder terminal sliding mode in fotsmcspo provides a finite convergence time for reaching the sliding surface in this article a sliding perturbation observer spo is used to estimate the disturbance from the environment and modeling uncertainties the new control scheme exhibits high precision fast convergence and robust performance against the lumped uncertainties these advantages are ensured through the newly designed fractionalorder terminal sliding surface and the spo the stability is analyzed based on the lyapunov functions for general and fractionalorder systems an implementation on a real robot manipulator demonstrated the effects and performance of the new control method
moss biomonitoring technique was used for a heavymetal pollution study in macedonia in the framework of the international cooperative program on effects of air pollution on natural vegetation and crops unece ipc vegetation moss samples n  72 were collected during the summers of 2002 2005 and 2010 the contents of 41 elements were determined by neutron activation analysis atomic absorption spectrometry and inductively coupled plasma atomic emission spectrometry using factor and cluster analyses three geogenic factors were determined factor 1 including al as co cs fe hf na rb sc ta th ti u v zr and rareearth elements–re factor 4 with ba k and sr and factor 5 with br and i one anthropogenic factor factor 2 including cd pb sb and zn and one geogenicanthropogenic factor factor 3 including cr and ni the highest anthropogenic impact of heavy metal to the air pollution in the country was from the ferronickel smelter near kavadraci ni and cr the lead and zinc mines in the vicinity of makedonska kamenica probištip and kriva palanka in the eastern part of the country cd pb and zn and the former lead and zinc smelter plant in veles beside the anthropogenic influences the lithology and the composition of the soil also play an important role in the distribution of the elements
this paper introduces bruce the csiro dynamic hexapod robot capable of autonomous dynamic locomotion over difficult terrain this robot is built around apptronik linear series elastic actuators and went from design to deployment in under a year by using approximately 80 3d printed structural joints and link parts the robot has so far demonstrated rough terrain traversal over grass rocks and rubble at 03ms and flatground speeds up to 05ms this was achieved with a simple controller inspired by rhex with a central pattern generator taskframe impedance control for individual legs and no foot contact detection the robot is designed to move at up to 10ms on flat ground with appropriate control and was deployed into the the darpa subt challenge tunnel circuit event in august 2019
abstract background the prediction of alcohol consumption in youths and particularly biomarkers of resilience is critical for early intervention to reduce the risk of subsequent harmful alcohol use methods at baseline the longitudinal relaxation rate r1 indexing grey matter myelination ie myeloarchitecture was assessed in 86 adolescentsyoung adults mean age  2176 range 1575–2667 years the alcohol use disorder identification test audit was assessed at baseline 1 and 2year followups 12 and 24months postbaseline we used a whole brain datadriven approach controlled for age gender impulsivity and other substance and behavioural addiction measures such as problematic cannabis use drug userelated problems internet gaming pornography use binge eating and levels of externalization to predict the change in audit scores from r1 results greater baseline bilateral anterior insular and subcallosal cingulate r1 clustercorrected familywise error p  005 predict a lower risk for harmful alcohol use measured as a reduction in audit scores at 2year followup control analyses show that other grey matter measures local volume or fractional anisotropy did not reveal such an association an atlasbased machine learning approach further confirms the findings conclusions the insula is critically involved in predictive coding of autonomic function relevant to subjective alcohol cuecraving states and risky decisionmaking processes the subcallosal cingulate is an essential node underlying emotion regulation and involved in negative emotionality addiction theories our findings highlight insular and cingulate myeloarchitecture as a potential protective biomarker that predicts resilience to alcohol misuse in youths providing novel identifiers for early intervention
providing personalized recommendations that are also accompanied by explanations as to why an item is recommended is a research area of growing importance at the same time progress is limited by the availability of open evaluation resources in this work we address the task of scientific literature recommendation we present arxivdigest which is an online service providing personalized arxiv recommendations to end users and operates as a living lab for researchers wishing to work on explainable scientific literature recommendations
in this work we develop and compare two innovative strategies for parameter estimation and radar detection of multiple pointlike targets the first strategy which appears here for the first time jointly exploits the maximum likelihood approach and bayesian learning to estimate targets’ parameters including their positions in terms of range bins the second strategy relies on the intuition that for high signaltointerferenceplusnoise ratio values the energy of data containing target components projected onto the nominal steering direction should be higher than the energy of data affected by interference only the adaptivity with respect to the interference covariance matrix is also considered exploiting a training data set collected in the proximity of the window under test finally another important innovation aspect concerns the adaptive estimation of the unknown number of targets by means of the model order selection rules
cette étude vise à mesurer l’acquisition des relations phonographémiques chez des apprenants de français langue étrangère fle en contexte scolaire et à identifier d’éventuelles interférences entre la langue des apprenants allemand et la langue en cours d’acquisition français en effet la recherche a montré que l’activation des systèmes phonologiques relatifs aux langues de l’apprenant n’était pas spécifique cette coactivation peut produire des interférences pouvant limiter l’apprentissage des correspondances phonographémiques de la langue étrangère 45 adolescents francophones et 45 germanophones ont participé à cette étude ils devaient identifier la forme graphémique d’un stimulus oral dans une tâche de dictée composée de 47 pseudomots susceptibles de générer des interférences crosslinguistiques les résultats analysés par une étude descriptive et des modèles statistiques multiniveaux ont montré qu’après 330 heures de leçons de français les apprenants germanophones identifient correctement 50 des formes écrites que leur probabilité de se tromper est toujours significativement supérieure à celle d’un francophone que les voyelles nasales du français sont les phonèmes dont les cpg sont les plus difficiles à identifier pour des germanophones et les plus faciles pour des francophones et que les erreurs commises lors de l’écriture des voyelles nasales sont principalement des erreurs phonologiques ces résultats semblent indiquer qu’il existe des interférences fortes entre les deux langues des apprenants qui limitent l’acquisition implicite des correspondances phonographémiques les implications de ces résultats sont mises en perspective pour la didactique du fle en milieu scolaire
in this paper a theoretical scheme for bidirectional quantum teleportation by using a fourqubit ghz state and two bell states as a quantum channel is proposed in this scheme two arbitrary singlequbit quantum states and an unknown threequbit state can be mutually transmitted between the two communication participants alice and bob the quantum state information can be transmitted if alice performs four singlequbit measurement operations on her qubits and bob carries out a fourqubit joint measurement on his qubits respectively then both alice and bob can reconstruct the target quantum state by means of appropriate unitary operations then we take comparisons with the other bidirectional quantum teleportation protocols in five aspects of quantum information bits transmitted necessary operations consumption of quantum resource consumption of classical resource and intrinsic efficiency furthermore we use channel authentication method to ensure the channel security and prove that our protocol can withstand four attack scenarios it is concluded that our scheme has significant advantages of more qubits number transmitted higher intrinsic efficiency and high security
despite acceptance in our ﬁeld many sophisticated visualization projects suffer from failing acceptance by the targeted audience though the reasons for this circumstance might be manifold we argue that they align with the typical pitfalls of software development on the one hand stakeholders are often not or only marginally integrated in the visualization design process on the other hand the goals we follow as visualization scholars do often not align with the goals of the stakeholders reducing them to data deliverers we provide case studies reporting on ﬁnished and ongoing projects following a participatory design approach those projects are initiated by the needs from users in digital humanities biodiversity research sports analysis and data science and our results indicate that participatory visualization design leads to mutual beneﬁts reducing the gap between research and application in the targeted domain
in this study we investigate how drugs systemically affect genes via pathways by integrating information from interactions between chemical compounds and molecular expression datasets and from pathway information such as gene sets using mathematical models first we adopt druginduced gene expression datasets then employ gene set enrichment analysis tools for selecting candidate enrichment pathways and lastly implement the inverse algorithm package for identifying gene–gene regulatory networks in a pathway we tested ly294002induced datasets of the mcf7 breast cancer cell lines and found a cell cycle pathway with 101 genes erbb signaling pathway consisting of 82 genes and mtor pathway consisting of 45 genes we consider two interactions quantity strength depending on number of interactions and quality strength depending on weight of interaction as positive  and negative − interactions our methods revealed anapc1cdk6 −0412 and orc2l chek10951 for the cell cycle pathway insrps6 −3125 and prkaa2prkaa2 1319 for the mtor pathway and cblbrps6kb1 −0141 rps6kb1cblc 0238 for the erbb signaling pathway to be top quality interactions top quantity interactions discovered include 12 the cdc − gene family for the cell cycle pathway 20 pik3 − 23 pik3cg  for the mtor pathway 11 pak − 10 pik3  for the erbb signaling pathway
this paper considers decentralized periodic eventtriggered control petc for a linear timeinvariant plant composed of spatially distributed sensor and actuator nodes that are connected by a shared communication channel the proposed petc mechanism is based on local trigger rules at the sensor nodes that use the locally available part of the state error since the last transmission and in addition state information from other sensor nodes that has been transmitted earlier over the communication channel it is shown how the received information from other sensor nodes can be exploited to reduce the amount of transmissions and to guarantee asymptotic stability of the origin for the closedloop system with the proposed petc mechanism to achieve this the trigger rules are used to overapproximate a lyapunovlike function moreover it is demonstrated how the information exploiting petc mechanism can be modified to deal with network induced transmission delays and bounded state disturbances
minmax optimization of an objective function f ℝd × ℝd → ℝ is an important model for robustness in an adversarial setting with applications to many areas including optimization economics and deep learning in many applications f may be nonconvexnonconcave and finding a global minmax point may be computationally intractable there is a long line of work that seeks computationally tractable algorithms for alternatives to the minmax optimization model however many of the alternative models have solution points which are only guaranteed to exist under strong assumptions on f such as convexity monotonicity or special properties of the starting point we propose an optimization model the εgreedy adversarial equilibrium and show that it can serve as a computationally tractable alternative to the minmax optimization model roughly we say that a point x⋆ y⋆ is an εgreedy adversarial equilibrium if y⋆ is an εapproximate local maximum for fx⋆· and x⋆ is an εapproximate local minimum for a “greedy approximation” to the function maxz fx z which can be efficiently estimated using secondorder optimization algorithms we prove the existence of such a point for any smooth function which is bounded and has lipschitz hessian to prove existence we introduce an algorithm that converges from any starting point to an εgreedy adversarial equilibrium in a number of evaluations of the function f the maxplayer’s gradient ∇y fxy and its hessian ∇y2 fxy that is polynomial in the dimension d 1ε and the bounds on f and its lipschitz constant
a multiobjective framework that optimises the uprating of the line’s realtime thermal rating and capacity of battery storage against wind curtailment network ageing and reliability is proposed the two enhancements are limited to the accumulated expected amount and duration of wind power losses of each wind farm bus in the framework actual conductor properties line failures due to thermal effects weather data battery operation policy and wind farm model are considered the tradeoff of the optimisation criteria the pareto front is solved using the nonsorting genetic algorithm and fuzzy decisionmaking method results show that the conductor maximum allowable temperature affects all the three optimisation parameters but battery efficiency only affects wind curtailment level
"
 using the moore–penrose pseudoinverse this work generalizes the gradient approximation technique called the centred simplex gradient to allow sample sets containing any number of points this approximation technique is called the generalized centred simplex gradient we develop error bounds and under a fullrank condition show that the error bounds have mathcal ovardelta 2 where vardelta  is the radius of the sample set of points used we establish calculus rules for generalized centred simplex gradients introduce a calculusbased generalized centred simplex gradient and confirm that error bounds for this new approach are also mathcal ovardelta 2 we provide several examples to illustrate the results and some benefits of these new methods"
"multiple wireless sensing tasks eg radar detection for driver safety involve estimating the channel or relationship between signal transmitted and received in this work we focus on a certain channel model known as the delaydoppler channel this model begins to be useful in the high frequency carrier setting which is increasingly common with developments in millimeterwave technology moreover the delaydoppler model then continues to be applicable even when using signals of large bandwidth which is a standard approach to achieving high resolution channel estimation however when high resolution is desirable this standard approach results in a tension with the desire for efficiency because in particular it immediately implies that the signals in play live in a space of very high dimension n eg 106 in some applications as per the shannonnyquist sampling theorem 
to address this difficulty we propose a novel randomized estimation scheme called sparse channel estimation or sce for short for channel estimation in the ksparse setting eg k objects in radar detection this scheme involves an estimation procedure with sampling and space complexity both on the order of klogn3 and arithmetic complexity on the order of klog n3  k2 for n sufficiently large 
to the best of our knowledge sparse channel estimation sce is the first of its kind to achieve these complexities simultaneously  it seems to be extremely efficient as an added advantage it is a simple combination of three ingredients two of which are wellknown and widely used namely digital chirp signals and discrete gaussian filter functions and the third being recent developments in sparse fast fourier transform algorithms"
as a biometric modality palmprints have been largely underutilized but they offer some advantages over fingerprints and facial biometrics recent improvements in imaging capabilities on handheld and wearable consumer devices have reawakened interest in the use of palmprints the aim of this paper is to provide a comprehensive review of stateoftheart methods for palmprint recognition including region of interest extraction methods feature extraction approaches and matching algorithms along with an overview of available palmprint datasets in order to understand the latest trends and research dynamics in the palmprint recognition field
covid19 is rapidly spreading and there are currently no specific clinical treatments available the absence of an immediate available vaccine against sarscov2 made it hard for health professionals to tackle the problem thus the need of ready to use prescription drugs or herbal remedies is urgent sarscov2 main protease mpro and angiotensin converting enzyme2 ace2 protein structure are made available to facilitate finding solutions to the present problem in this brief research we compare the efficacy of some natural compounds against covid19 mpro and ace2 to that of hydroxychloroquine in silicomolecular docking investigations were carried out using autodock virtual screening was performed using autodock vina and the best ligand  protein mode was identified based on the binding energy amino acids residues of ligands interactions were identified using pymol according to present research results quercetin hispidulin cirsimaritin sulfasalazine artemisin and curcumin exhibited better potential inhibition than hydroxychloroquine against covid19 main protease active site and ace2 our provided docking data of these compounds may help pave a way for further advanced research to the synthesis of novel drug candidate for covid19
while classical approaches to autonomous robot navigation currently enable operation in certain environments they break down in tightly constrained spaces eg where the robot needs to engage in agile maneuvers to squeeze between obstacles recent machine learning techniques have the potential to address this shortcoming but existing approaches require vast amounts of navigation experience for training during which the robot must operate in close proximity to obstacles and risk collision in this letter we propose to sidestep this requirement by introducing a new machine learning paradigm for autonomous navigation called learning from hallucination lfh which can use training data collected in completely safe environments to compute navigation controllers that result in fast smooth and safe navigation in highly constrained environments our experimental results show that the proposed lfh system outperforms three autonomous navigation baselines on a real robot and generalizes well to unseen environments including those based on both classical and machine learning techniques
background the manifesto for agile development has already inspired many software development methods such as scrum xp and crystal reports however being “agile” is not trivial and only a few companies are capable of mastering socalled agile practices failure to apply the agile approach properly can do more harm than good and may jeopardize the benefits of an agile method thus evaluating an organization’s ability to apply agile practices using an agility assessment tool is critical aims in this paper we extend the metaphor of code smell and introduce the term agile smell to denote the issues and practices that may impair the adoption of the agile approach the focus of the paper is defining and validating a catalogue of agile smells that can support agility assessment method a literature review and a survey were conducted to identify and confirm the characterization of agile smells once identified the agile smells were organized in a structured catalogue results the literature review found 2376 references published between 2001 and 2018 we selected 55 papers for full consideration and identified 20 agile smells the survey consulted 20 participants to determine the relevance of the selected agile smells conclusion we have identified a set of 20 agile smells that were ranked according to their relevance for each smell we proposed at least one strategy to identify the smell’s presence in real projects the catalogue can be used by companies to support the assessment of their agility ability
understanding the interaction between land surface and atmosphere processes is fundamental for predicting the effects of future climate change on ecosystem functioning and carbon dynamics the objectives of this work were to analyze the trends in land surface phenology lsp metrics from remote sensing data and to reveal their relationship with precipitation and enso phenomenon in the argentina pampas using a time series of modis normalized difference vegetation index ndvi data from 2000 to 2014 the start of the growing season sos the annual integral of ndvi indvi linear estimator of annual productivity the timing of the annual maximum ndvi tmax and the annual relative range of ndvi rrel estimator of seasonality were obtained for the argentina pampas then spatial and temporal relationships with the multivariate enso index mei and precipitation were analyzed results showed a negative trend in annual productivity over a 536 of the study area associated to natural and seminatural grassland under cattle grazing whereas a 403 of argentina pampas showed a significant positive trend in seasonality of carbon gains the study also reveals that climate variability has a significant impact on land surface phenology in argentina pampas although the impact is heterogeneous sos and tmax showed a significant negative correlation with the precipitation indicating an earlier occurrence 236 and 284 of the study area showed a positive correlation of the annual productivity with mei and precipitation respectively associated to rangelands in the first case and to both rangeland and croplands in the second case climate variability did not explain the seasonal variability of phenology the relationships found between lsp metrics and climate variability could be important for implementation of strategies for natural resource management
abstract visual cortex functional deficits can be observed in schizophrenia patients and in individuals at high risk of schizophrenia however to date few studies have investigated methods to improve these functional deficits this study aimed to investigate the pathological change in the primary visual cortex of a prenatal mk801induced highrisk mouse model of schizophrenia hrmms and to test the effect of paroxetine on visual cortex activity pregnant mice were given a systemic injection of mk801 and male offspring that did not present schizophrenialike behaviors in early adulthood were defined as hrmms some of the hrmms mice were treated with pharmacological agents beginning at 4 weeks of age after 4 weeks of treatment with risperidone andor paroxetine twophoton calcium imaging was performed to analyze the primary visual cortex activity the sucrose preference test and the prepulse inhibition ppi apparatus test were used to assess the cognitive and behavioral performance hrmms mice with or without risperidone treatment had impairments in the primary visual cortex as observed by reduced neuronal calcium activity risperidone plus paroxetine and paroxetine alone treatments increased the neuronal calcium activity in the primary visual cortex notably the neuronal calcium activity was higher in mice treated with paroxetine alone treatment with paroxetine alone also improved the cognitive and behavioral performance better than treatment with risperidone plus paroxetine our pioneering animal model showed that treatment with paroxetine alone improves visual cortex impairments in hrmms mice better than treatment with risperidone plus paroxetine indicating that antipsychotics cannot normalize visual cortex impairments
rgbinfrared ir crossmodality person reidentification reid which aims to search an ir image in rgb gallery or vice versa is a challenging task due to the large discrepancy between ir and rgb modalities existing methods address this challenge typically by aligning feature distributions or image styles across modalities whereas the very useful similarities among gallery samples of the same modality ie intramodality sample similarities are largely neglected this paper presents a novel similarity inference metric sim that exploits the intramodality sample similarities to circumvent the crossmodality discrepancy targeting optimal crossmodality image matching sim works by successive similarity graph reasoning and mutual nearestneighbor reasoning that mine crossmodality sample similarities by leveraging intramodality sample similarities from two different perspectives extensive experiments over two crossmodality reid datasets sysumm01 and regdb show that sim achieves significant accuracy improvement but with little extra training as compared with the stateoftheart
abstract engineering changes ecs and engineering change management ecm are crucial for successful product design processes pdp due to the increasing complexity of todays products like vehicles and the interaction of different engineering domains mechanics electricelectronics software involved in the pdp crossdomain ec impact assessments as well as processes are required to better support engineers in assessing change propagation across domains and products existing approaches for ecm product models are analyzed in this paper and an enhanced product model is derived using mbse
this paper presents the results of research concerning multilayered epoxy composites reinforced with different materials the strength of multilayered composites depends to a large extent on the reinforcing material the authors decided to compare the low velocity impact response and perform tensile strength tests on several composites to ascertain the mechanical properties of the prepared composites five different reinforcing materials were provided for the research two fabrics made from aramid fibers two fabrics made from carbon fibers and one fabric made from polyethylene fibers the composites were manufactured by the vacuum supported hand laminating method the low velocity impact response tests were conducted with the use of a pneumatic launcher three strikers with different geometry conical striker hemispherical striker and ogival striker were used a comparison of the resulting damage to the composites after the impact of the strikers was based on the images obtained using an optical microscope tensile tests were also performed the experimental investigation showed significant differences in the mechanical properties of the composites depending on the applied reinforcing material it was found that as a result of the impacts less damage occurred in the composites which were characterized by a lower young’s modulus and a higher tensile strength
energy consumption of block ciphers is critical in resource constrained devices unrolling has been explored in literature as a technique to increase efficiency by eliminating energy spent in loop control elements such as registers and multiplexers however these savings are minimal and are offset by the increase in glitching power that comes with unrolling we propose an efficient latchbased glitch filter for unrolled designs that reduces energy per encryption by an order of magnitude over a straightforward implementation and by 2845 percent over the best existing glitch filtering schemes we explore the optimal number of glitch filters that should be used in order to minimize total energy and provide estimates of the area cost partially unrolled designs also benefit from using our scheme with energies competitive to fully serialized implementations power gating to reduce leakage power and reuse of computed key enable unrolled designs to be more efficient than serialized ones without compromising latency advantages we demonstrate our approach on the simon128 and aes128 block ciphers
duality is one of the key techniques in the categorical treatment of modal logics from the duality between modal algebras and descriptive frames one derives eg completeness via a syntactic characterisation of algebras or definability using a suitable version of the goldblattthomason theorem this is by now well understood for classical modal logics and modal logics based on distributive lattices via extensions of stone and priestley duality respectively what is conspicuously absent is a comprehensive treatment of modal intuitionistic logic this is the gap we are closing in this paper our main conceptual insight is that modal intuitionistic logics do not appear as algebracoalgebra dualities but instead arise naturally as dialgebras our technical contribution is the development of dualities for dialgebras together with their logics that instantiate to large class of modal intuitionistic logics and their frames as special cases we derive completeness and expressiveness results in this general case for modal intuitionistic logic this systematises the existing treatment in the literature
dengue fever is the most hurriedly diffused mosquitoborne viral disease in the world more than 33 of the total population in the world is under risk currently the prediction of dengue can save a person ‘s life by alerting them to take proper diagnosis and care the objectives of this research is 1 to predict the area with the most potential to suspend dengue fever in indonesia and 2 to predict dengue fever cases 3 to analyz how many percent the effect factor of dengue fever there are many ways to predict one of them is regression and deep learning approach reseacher tried to analyz the most accurate such as regression multyplied neural network and sensitivity analysis set of data have been used is timeseries from 1997—2017 the variable has been used for this research is humadity temperature wind airpressure rainfall index income sunlight population density and output is cases the result of this research is first the area with the most potential to suspend dengue fever in indonesia 2019 is jambi lampung bangka belitung west sumatera central java with average accuracy 8716 the prediction dengue fever cases 2019 is 80233 cases with accuracy 8716 the third all variable  x1 sd x8 have been to effect to partial and simultaneous to y in the amount of 016 16   with a significance level of 0001 99 while the remaining 100  16  84 is influenced by other variables outside of this research
we examine the formula see textary linear codes from incidence matrix of the bouwer graph formula see text with vertex set formula see text and two vertices are adjacent if they can be written as formula see text and formula see text where either formula see text or formula see text differs from formula see text in exactly one position say the formula see textth position where formula see text all the main parameters of the codes are obtained as formula see text also we determine linear codes from incidence matrices of bouwer graphs formula see text and all the main parameters of the codes are obtained as formula see text all the above codes can be used for full error correction by permutation decoding
this paper considers the problem of conflict resolution involving two constantspeed aircraft moving in a horizontal plane a planar deconflicting maneuver that resolves the conflict with the desir
"
purpose
this study aims to explore the relationship between different categories of motivation and the intention to engage in collaborative consumption cc using attitude as a mediator


designmethodologyapproach
the authors extend an existing measurement scale focused on sustainability enjoyment reputation and economic benefits as factors relevant in shaping how people perceive cc the extension includes the role of personal beliefs and social relationships the authors conduct a mediation analysis using partial least squares path modelling


findings
this study partially confirms existing literature sustainability and enjoyment are positively related and statistically significant in predicting attitude towards cc while only enjoyment impacts behavioural intention attitude further impacts behavioural intention further reputation and economic benefits positively and significantly impact attitude economic benefits are not significant for behavioural intention in this study’s romanian sample but reputation is neither beliefs nor relationships are significantly associated with behavioural intention


originalityvalue
the authors investigate cc determinants in a postcommunist economy a novel setting for the development of sharing economy as most studies focus on traditionally developed economies
"
in this paper a phasefield approach for structural topology optimization for a 3dprinting process which includes stress constraints and potentially multiple materials or multiscales is analyzed firstorder necessary optimality conditions are rigorously derived and a numerical algorithm which implements the method is presented a sensitivity study with respect to some parameters is conducted for a twodimensional cantilever beam problem finally a possible workflow to obtain a 3dprinted object from the numerical solutions is described and the final structure is printed using a fused deposition modeling fdm 3d printer
through a largescale national survey this study provided the first comprehensive examination of podcast users in the united states from the perspectives of motivation and usage it deepened our understanding of this new ondemand audio platform in the context of consumption drivers behaviors and competing media options the results showed that entertainment information and audio platform superiority were the most important motivators for podcast consumption in addition motives were found to affect listening behaviors including listening settings width depth and routine of listening and usage of competing audio media such as regular radio online radio and streaming music the findings revealed that podcasting is a distinct medium with unique characteristics rather than a mobile ondemand extension of existing audio platforms like radio podcast consumption especially on today’s complex media platforms is multidimensional and should be measured from multiple aspects and examined in various settings
the coronavirus 2019 covid19 disease has affected millions of lives forcing most of us to stay at home and work however there is an immediate need to conduct research on potential drugs against covid19 in this article the extent to which major publishers have provided access for the public to read research articles relevant to potential drug candidates for the covid19 disease are presented a systematic search of five electronic databases elsevier’s sciencedirect taylor  francis springerlink wiley and new england journal of medicine nejm was conducted on april 1217 2020 the total number of research articles containing terms ‘ribavirin’ ‘remdesivir’ ‘hydroxychloroquine or chloroquine’ ‘favipiravir’ ‘lopinavir or ritonavir’ ‘sarilumab’ and ‘tocilizumab’ available for the public to read for free were determined in this study there was a lack of full access to research articles related to potential drugs of covid19 in commercial academic databases except for ‘remdesivir’ and ‘favipiravir’ from nejm
a bstract  the conjugate gradient method is among the efﬁcient method for solving unconstrained optimization problems in this paper we propose a new formula for the conjugate gradient method based on the modiﬁcation of the nprp formula zhang 2009 the proposed method satisﬁes the sufﬁcient descent condition and global convergence proof was established under some assumptions and strong wolfe line search numerical results based on 98 test problems show that the new method very efﬁcient as compared with the classical conjugate gradient method
this paper proposes a grouping design method based on a combination of spatial ray tracing and aberration correction to construct the initial structure for an offaxis multireflective aspheric optical system this method establishes a mathematical parameter model of the optical system to satisfy the aberration balance and multiconstraint control requirements the simulated annealing particle swarm algorithm is applied to calculate the initial optical system structure finally an extreme ultraviolet euv lithography projection objective with sixreflective aspheric mirrors is used as an example to verify the reliability and effectiveness of this method a 033 numerical aperture euv lithographic objective with wavefront error better than 170λλ135nm root mean square rms is obtained
reward modulation is represented in the motor cortex m1 and could be used to implement more accurate decoding models to improve brain computer interfaces bcis zhao et al 2018 analyzing trialtotrial noisecorrelations between neural units in the presence of rewarding r and nonrewarding nr stimuli adds to our understanding of cortical network dynamics we utilized pearson’s correlation coefficient to measure shared variability between simultaneously recorded units 32 – 112 and found significantly higher noisecorrelation and positive correlation between the populations’ signal and noisecorrelation during nr trials as compared to r trials this pattern is evident in data from two nonhuman primates nhps during singletarget center out reaching tasks both manual and action observation versions we conducted mean matched noisecorrelation analysis in order to decouple known interactions between event triggered firing rate changes and neural correlations isolated reward discriminatory units demonstrated stronger correlational changes than units unresponsive to reward firing rate modulation however the qualitative response was similar indicating correlational changes within the network as a whole can serve as another information channel to be exploited by bcis that track the underlying cortical state such as reward expectation or attentional modulation reward expectation and attention in return can be utilized with reinforcement learning towards autonomous bci updating
"with the rapid development of covid19 into a global pandemic there is an ever more urgent need for cheap fast and reliable tools that can assist physicians in diagnosing covid19 medical imaging such as ct can take a key role in complementing conventional diagnostic tools from molecular biology and using deep learning techniques several automatic systems were demonstrated promising performances using ct or xray data here we advocate a more prominent role of pointofcare ultrasound imaging to guide covid19 detection ultrasound is noninvasive and ubiquitous in medical facilities around the globe our contribution is threefold first we gather a lung ultrasound pocus dataset consisting of currently 1103 images 654 covid19 277 bacterial pneumonia and 172 healthy controls sampled from 64 videos while this dataset was assembled from various online sources and is by no means exhaustive it was processed specifically to feed deep learning models and is intended to serve as a starting point for an openaccess initiative second we train a deep convolutional neural network pocovidnet on this 3class dataset and achieve an accuracy of 89 and by a majority vote a video accuracy of 92  for detecting covid19 in particular the model performs with a sensitivity of 096 a specificity of 079 and f1score of 092 in a 5fold cross validation third we provide an openaccess web service pocovidscreen that is available at this https url the website deploys the predictive model allowing to perform predictions on ultrasound lung images in addition it grants medical staff the option to bulk upload their own screenings in order to contribute to the growing public database of pathological lung ultrasound images 
dataset and code are available from this https url"
to implement a modular flexible open‐source hardware configuration for parallel transmission ptx experiments on medical implant safety and to demonstrate real‐time mitigation strategies for radio frequency rf induced implant heating based on sensor measurements
in the information age it is becoming crucial to understand the sociotechnological factors and their possible outcomes so as to fulfill the upcoming spatial needs of the society thus it is aimed to put forward how sociospatiality is changing along with the developments in new information and communication technologies nicts in the twentyfirst century and how sociotechnological factors are affecting urban space in terms of the formation of new urban functionsuses or spaces that will habitsuithouse these dynamics within the city the outcome of the study shows that there are three phases by which urban public space is expected to change according to the impact of sociotechnological factors
fog computing is an emerging computing paradigm that uses processing and storage capabilities located at the edge in the cloud and possibly in between testing and benchmarking fog applications however is hard since runtime infrastructure will typically be in use or may not exist yet while approaches for the emulation of infrastructure testbeds do exist their focus is typically the emulation of edge devices other approaches also emulate infrastructure within the core network or the cloud but they miss support for automated experiment orchestration in this article we propose to evaluate fog applications on an emulated infrastructure testbed created in the cloud which can be manipulated based on a predefined orchestration schedule developers can freely design the infrastructure configure performance characteristics manage application components and orchestrate their experiments we also present our proofofconcept implementation mockfog 20 we use mockfog 20 to evaluate a fogbased smart factory application and showcase how its features can be used to study the impact of infrastructure changes and workload variations with these experiments we also show that mockfog can achieve good experiment reproducibility even in a public cloud environment
abstract the human species is combining an increased understanding of our cognitive machinery with the development of a technology that can profoundly influence our lives and our ways of living together our sciences enable us to see our strengths and weaknesses and build technology accordingly what would future historians think of our current attempts to build increasingly smart systems the purposes for which we employ them the almost unstoppable goldrush toward ever more commercially relevant implementations and the risk of superintelligence we need a more profound reflection on what our science shows us about ourselves what our technology allows us to do with that and what apparently we aim to do with those insights and applications as the smartest species on the planet we don’t need more intelligence since we appear to possess an underdeveloped capacity to act ethically and empathically we rather require the kind of technology that enables us to act more consistently upon ethical principles the problem is not to formulate ethical rules it’s to put them into practice cognitive neuroscience and ai provide the knowledge and the tools to develop the moral crutches we so clearly require why aren’t we building them we don’t need superintelligence we need superethics
cirrhosis is associated with diffuse brain manganese deposition which results in increased signal intensity si in the brain on t1‐weighted images most often visualized in the globus pallidus the purpose of this study was to determine if automated image intensity measurements can detect si differences in the basal ganglia and other regions reported to have manganese deposition in patients with cirrhosis compared with controls
voltage fluctuations caused output variation in the distributed energy system such as solar photovoltaic and wind produce flickers the extraction of voltage flicker envelope parameters and accurate calculation of voltage flicker values are important for voltage flicker detection traditional digital filters need to be calculated according to the sampling frequency before they can be designed so that the flicker meter can only sample at a fixed frequency to overcome the shortcomings of digital filters a new method for measuring flickers is presented in this study the proposed method uses analytical mode decomposition amd based on spectral correction and a novel constructed mutual convolution window the voltage flicker envelope can be extracted using an amd method which can replace the filter used in traditional methods it can be analysed to obtain the corrected spectrum which is based on an improved fast fourier transform the proposed method simplifies the voltage flicker detection process and exhibits high detection accuracy which with lower detection relative error and stronger antiinterference ability the simulation analysis and experimental results show that this approach can effectively overcome problems associated with voltage flicker measurements such as single modulation wave frequency change and multifrequency modulation wave fluctuation
in minimally invasive surgical robot system the implementation of the force feedback function can increase the flexibility of the surgeon during surgery and reduce the risk of damage to the tissues and organs of the patient in order to achieve the force detection during surgical process this paper designs a 3 axis force sensor based on fiber bragg grating fbg when decoupling the sensor a decoupling result superior to least square ls is achieved by combining extreme learning machine elm and particle swarm optimization pso by using psoelm for decoupling the average error rates in three mutually perpendicular directions are 135 107 570 respectively
field programmable gate arrays fpgas provide complex embedded blocks to ease the development of highperformance computing systems for diverse area applications including among others space avionics and health although the rich set of features is ever expanding there is one significant shortcoming of the srambased fpgas which concerns system designers for applications demanding high reliability their vulnerability to single event upsets seus which can cause system malfunction in this work we propose a placement approach to improve system reliability by reducing the execution time of configuration memory scrubbing which can be used in conjunction with other reliability mechanisms proposed in the literature the proposed placement approach is based on i an automated floorplanning process to shape and locate the design regions and ii a modified version of the simulated annealing placement algorithm aiming to reduce the scrubbing time first we performed a set of experiments with three quip benchmarks to demonstrate the efficiency of the proposed approach at different device utilization levels moreover we illustrated its effectiveness for three different fault tolerance schemes where scrubbing plays a different role in each one i a tmr microcontroller combined with scrubbing ii a soft processor protected by a lowcost mitigation scheme including scrubbing and checkpointing iii a jpeg encoder protected by a prioritized scrubbing scheduling scheme based on module criticality levels the experimental results showed that the proposed approach improves system reliability in all the above schemes by reducing critical timing parameters such as meantimetodetect and meantimetorepair this reduction leads to a modest or high reliability improvement depending on the role of scrubbing in the adopted fault tolerance ft scheme
abstract purpose we present photoncounting computed tomography pcct imaging of contrast agent triplets similar in atomic number z achieved with a highflux cadmium zinc telluride czt detector approach the tabletop pcct imaging system included a 330μmpitch czt detector of size 8  mm  ×  24  mm2 capable of using six energy bins four 3dprinted 3cmdiameter phantoms each contained seven 6mmdiameter vials with water and low and high concentration solutions of various contrast agents lanthanum z    57 gadolinium gd z    64 and lutetium z    71 were imaged together and so were iodine z    53 gd and holmium z    67 each phantom was imaged with 1mm aluminumfiltered 120kvp cone beam x rays to produce six energybinned computed tomography ct images results kedge images were reconstructed using a weighted sum of six ct images which distinguished each contrast agent with a rootmeansquare error rmse of 029     and 051 for the 05 and 5 concentrations respectively minimal crosscontamination in each kedge image was seen with rmse values 027     in vials with no contrast conclusion this is the first preliminary demonstration of simultaneously imaging three similar z contrast agents with a difference in z as low as 3
excavating valuable outlier information of gray privacy products the purpose of this study takes the online reviews of women’s underwear as an example explores the outlier characteristics of online commentary data and analyzes the online consumer behavior of consumers’ gray privacy productsthis research adopts the social network analysis method to analyze online reviews based on the online reviews collected from women’s underwear flagship store victoria’s secret at tmall this study performs word segmentation and word frequency analysis using the fuzzy query method the research builds the corresponding coword matrix and conducts cooccurrence analysis to summarize the factors affecting consumers’ purchase behavior of female underwearestablishing a formal framework of gray privacy products this paper confirms the commonalities among consumers with respect to their perceptions of gray privacy products shows that consumers have high privacy concerns about the disclosure or secondary use of personal private information when shopping gray privacy products and demonstrates the big difference between online reviews of gray privacy products and their consumer descriptionsthe research lays a solid foundation for future research in gray privacy products the factors identified in this study provide a practical reference for the continuous improvement of gray privacy products and services
the interaction between light and metal nanoparticles enables investigations of microscopic phenomena on nanometer length and ultrashort timescales benefiting from strong confinement and enhancement of the optical field however the ultrafast dynamics of these nanoparticles are primarily investigated by multiphoton photoluminescence on picoseconds or photoemission on femtoseconds independently here we presented twophoton photoluminescence tppl measurements on individual au nanobipyramids aunp to reveal their ultrafast dynamics by doublepulse excitation on a global timescale ranging from subfemtosecond to tens of picoseconds two orders of magnitude photoluminescence enhancement namely coherent interference fringes has been demonstrated powerdependent measurements uncovered the transform of the nonlinearity from 1 to 2 when the interpulse delay varied from tens of femtoseconds to tens of picoseconds we proved that the real intermediate state plays a critical role in the observed phenomena supported by numerical simulations with a threestate model our results provide insight into the role of intermediate states in the ultrafast dynamics of noble metal nanoparticles the presence of the intermediate states in aunp and the coherent control of state populations offer interesting perspectives for imaging sensing nanophotonics and in particular for preparing macroscopic superposition states at room temperature and lowpower superresolution stimulated emission depletion microscopy
the withingrowingseason correlations wgsc and the intergrowingseason correlations igsc are widely used linear correlation analysis methods between vegetation index and climatic factors such as temperature precipitation and so on the wgsc method usually calculates the linear correlation coefficient between vegetation index and climatic factors of each month in all the growing seasons for instance whether vegetation index or temperature had data of 204 months 12 months × 17 years during 2000–2016 to get the wgsc the igsc calculates the linear correlation coefficient between the vegetation index and climatic factors in the same month of each growing season among all the years for example only 17 couples’ data of vegetation index and temperature during 2000–2016 were used to get the linear correlation of igsc what is the difference between the results of the two methods and why do the results show that difference which is the more suitable method for the analysis of the relationship between the vegetation index and climatic conditions to clarify the difference of the two methods and to explore more about the relationship between the vegetation index and climatic factors we collected the data of 2000–2016 moderate resolution imaging spectroradiometer modis 13a1 normalized difference vegetation index ndvi and the meteorological datatemperature and precipitation then calculated wgsc and igsc between ndvi and the climatic factor in three riverheadwater regions of china the results showed that 1 as for wgsc the more of the years included the higher the correlation coefficient between ndvi and the temperatureprecipitation the correlation coefficient of wgsc is dependent on how many years’ the data were included and it was increased with the more year’s data included while the correlation coefficients of igsc are relatively independent on the amount of the data 2 the wgsc showed a pseudo linear correlation between ndvi and climatic conditions caused by the accumulation of data amount while the igsc can more accurately indicate the impact of climatic factors on vegetation since it did not rely on the data amount
recently sentimental analysis sa attains most important attention while making a decision when it used for extracting and classifying the sentiments existing in webbased reviews which are written by a customer here sa is used as a sentiment classification sc issue where the reviews are classified as positive and negative factors which are based on internet reviews this work projects an efficient sa technique for online reviews by the combination of feature selection fs as well as classification the firefly ff and levy flights ffl models are used to extract features from webbased review and multilayer perceptron mlp is employed to classify the sentiments for experimentation a benchmark dvd dataset is employed to test the effectiveness of the presented model the final outcome reveals that the proposed ffmlp method attains effective classifying operation from every dataset on behalf of various performance metrics
for automated evaluation of changes on uterine cervix the external os here simply os is a primary anatomical landmark in locating the transformation zone tzone any abnormal tissue changes typically occur at or within the tzone this makes localizing the os on cervical images of great interest for detecting and classifying changes however there has been very limited work reported on segmentation of the os region in digitized cervix images and to our knowledge no work has been done on sets of cervix images acquired from independent data collections exhibiting variabilities due to collection devices environments and procedures in this paper we present a process pipeline which consists of deep learning os region segmentation over such multiple datasets followed by comprehensive evaluation of the performance first we evaluate of two stateoftheart deep learningbased localization and classification algorithms viz mask rcnn and maskx rcnn on multiple datasets second in consideration of the os being small and irregularlyshaped and of the variabilities in image quality we use performance measurements beyond the commonly used diceiou scores we obtain higher performance on a larger dataset as compared with the work reported in the literature and achieve a highest detection rate of 991 and an average minimal distance of 102 pixels furthermore the network models we obtained in this study show potential use of quality control for data acquisition
the globalization of the ic supply chain has raised many security threats especially when untrusted parties are involved this has created a demand for a dependable logic obfuscation solution to combat these threats amongst a wide range of threats and countermeasures on logic obfuscation in the 2010s decade the boolean satisfiability sat attack or one of its derivatives could break almost all stateoftheart logic obfuscation countermeasures however in some cases particularly when the logic locked circuits contain complex structures such as big multipliers large routing networks or big tree structures the logic locked circuit is hardtobesolved for the sat attack usage of these structures for obfuscation may lead a strong defense as many sat solvers fail to handle such complexity however in this paper we propose a neuralnetworkguided sat attack nngsat in which we examine the capability and effectiveness of a messagepassing neural network mpnn for solving these complex structures sathard instances in nngsat after being trained as a classifier to predict satunsat on a sat problem nn serves as a sat solver the neural network is used to guidehelp the actual sat solver for finding the sat assignments by training nn on conjunctive normal forms cnfs corresponded to a dataset of logic locked circuits as well as finetuning the confidence rate of the nn prediction our experiments show that nngsat could solve 935 of the logic locked circuits containing complex structures within a reasonable time while the existing sat attack cannot proceed the attack flow in them
immune checkpoint inhibitors icis have revolutionized the field of cancer immunotherapy most commonly inhibitors of pd1 and ctla4 are used having received approval for the treatment of many cancers like melanoma nonsmallcell lung carcinoma and leukemia in contrast to date clinical studies conducted in patients with cns malignancies have not demonstrated promising results however patients with cns malignancies have several underlying factors such as treatment with supportive medications like corticosteroids and cancer therapies including radiation and chemotherapy that may negatively impact response to icis although many clinical trials have been conducted with icis measures that reproducibly and reliably indicate that treatment has evoked an effective immune response have not been fully developed in this article we will review the history of ici therapy and the correlative biology that has been performed in the clinical trials testing these therapies in different cancers it is our aim to help provide an overview of the assays that may be used to gauge immunologic response this may be particularly germane for cns tumors where there is currently a great need for predictive biomarkers that will allow for the selection of patients with the highest likelihood of responding
due to several unique features electrical discharge machining edm has proved itself as one of the efficient nontraditional machining processes for generating intricate shape geometries on various advanced engineering materials in order to fulfill the requirement of the present day manufacturing industries in this paper the machining capability of an edm process is studied during standard hole making operation on pearlitic sg iron 45012 grade material while considering gap voltage peak current cycle time and tool rotation as input parameters on the other hand material removal rate surface roughness tool wear rate overcut and circularity error are treated as responses based on single and multiobjective optimization models this process is optimized using the teachinglearningbased optimization tlbo algorithm and its performance is contrasted against firefly algorithm differential evolution algorithm and cuckoo search algorithm it is revealed that the tlbo algorithm supersedes the others with respect to accuracy and consistency of the derived optimal solutions and computational efforts
people can be motivated when the right factor occurs in their work as a result they will work harder and deliver better performance being able to achieve optimum work performance is one of the ways to ensure professionals will remain competitive and survive in the construction industry thus this study aims to analyze the most important factors that motivate architects to work in the project delivery process the assessment of architects’ important work motivators was carried out using selfevaluation through a questionnaire survey there were 131 completed and usable questionnaires received and used for data analysis a relative importance index rii was adopted to rank the relative importance of the architect’s work motivation factors the results revealed that having adequate resources in the process of delivering a project is the most important factor which motivates architects to work the results can be used as a valuable feedback for the employers of consultant firms as well as the clients that seek to enhance the performance of their key design contributor by providing the appropriate work motivators which can motivate architects to achieve higher performance in building project delivery process
accurate and fast prediction of the remaining useful life of lithium li ion batteries is an important requirement for successful electrification of automobiles consequently there is a growing interest in the development of reduced‐order models the existing reduced‐order electrochemical models can be used to predict battery performance state of charge soc terminal voltage when the current through the battery is known a priori therefore these models cannot be used for studying the constant voltage cv mode of the constant current‐cv cc‐cv charging protocol which is a common battery charging mechanism in this work we propose a reduced‐order electrochemical model to estimate the battery soc under cc‐cv charging conditions along with an analytical expression to approximate the cv mode charging time we further propose a framework that accounts for the influence of the battery state of health soh on the battery soc during an operating cycle and vice‐versa the proposed framework for estimating the battery soc and soh in a coupled manner shows good comparison with a first principles electrochemical model for cc‐cv charging conditions this model can be used to study battery ageing and it can find applications in real‐time state estimation charge protocol optimization and battery design
kriging is a statistical approach for analyzing computer experiments kriging models can be used as fast running surrogate models for computationally expensive computer codes kriging models can be built using different methods the maximum likelihood estimation method and the leaveoneout cross validation method the objective of this paper is to evaluate and compare these different methods for building kriging models these evaluation and comparison are achieved via some measures that test the assumptions that are used in building kriging models we apply kriging models that are built based on the two different methods on a real high dimensional example of a computer code we demonstrate our evaluation and comparison through some measures on this real computer code
in this attempt we introduce a new technique to solve main generalized abel’s integral equations and generalized weakly singular volterra integral equations analytically this technique is based on the adomian decomposition method laplace transform method and  riemann–liouville fractional integrals finally some examples are proposed and they illustrate the rapidness of our new technical method
image enhancement of a preprocessed and colored ˆngerprint with complex background patterns has high demand in appraisal evaluation of forensic evidence this paper focuses on image enhancement of ninhydrinprocessed ˆngerprints on periodicpatterned backgrounds contrast adjustment usually does not work well for separation between ˆngerprints and periodicpatterned backgrounds because of similarity in their color scales and  or superposing patterns as alternative methods of contrast more enhanced ˆngerprints than the contrast adjustment does if some periodic patterns remain after the processing of each approach these patterns can be removed by using frequency ˆltering based on 2dimensional fast fourier transformation it is ešective to apply the huebased approach ˆrst and if the enhancement of ˆngerprints is not su‹cient then it is recommended to use the pcabased approach both in rgb and cielab color space proposed methods are easy to be conducted when an rgb image including a colored ˆngerprint is given by a common digital camera or ‰atbed scanner our methods can also be used even when an image for background subtraction is not available proposed methods expand the capability of utilizing colored ˆngerprints processed by ninhydrin for the forensic evidence
the rapid progress of micronanoelectronic systems and miniaturized portable devices has tremendously increased the urgent demands for miniaturized and integrated power supplies miniaturized energy storage devices mesds with their excellent properties and additional intelligent functions are considered to be the preferable energy supplies for uninterrupted powering of microsystems in this review we aim to provide a comprehensive overview of the background fundamentals device configurations manufacturing processes and typical applications of mesds including their recent advances particular attention is paid to advanced device configurations such as twodimensional 2d stacked 2d planar interdigital 2d arbitraryshaped threedimensional planar and wireshaped structures and their corresponding manufacturing strategies such as printing scribing and masking techniques additionally recent developments in mesds including microbatteries and microsupercapacitors as well as microhybrid metal ion capacitors are systematically summarized a series of onchip microsystems created by integrating functional mesds are also highlighted finally the remaining challenges and future research scope on mesds are discussed
dense flow visualization is a popular visualization paradigm traditionally the various models and methods in this area use a continuous formulation resting upon the solid foundation of functional analysis in this work we examine a discrete formulation of dense flow visualization from probability theory we derive a similarity matrix that measures the similarity between different points in the flow domain leading to the discovery of a whole new class of visualization models using this matrix we propose a novel visualization approach consisting of the computation of spectral embeddings ie characteristic domain maps defined by particle mixture probabilities these embeddings are scalar fields that give insight into the mixing processes of the flow on different scales the approach of spectral embeddings is already well studied in image segmentation and we see that spectral embeddings are connected to fourier expansions and frequencies we showcase the utility of our method using different 2d and 3d flows
in recent years softwaredefined networking sdn has gained increased popularity by reducing the complexity of management operations and increasing the performance of networks as a result the number of purchasable sdn devices and their deployment in real world network constantly rises making a thorough understanding of their behaviors and interactions even more important in this work we analyze distributed controller architectures via simulation identify key performance indicators kpis classify various network topologies with respect to their impact on sdn as well as create a prediction model to estimate the overall performance of the overall sdn ecosystem which can be used for network planning
cd38 genetic variation has been associated with autism spectrum disorders and social anxiety disorder which may result from cd38’s regulation of oxytocin secretion converging evidence has found that the rs3796863 aallele contributes to increased social sensitivity compared to the cc genotype the current study examined the moderating role of cd38 genetic variants rs3796863 and rs6449182 that have been associated with enhanced or reduced social sensitivity on neural activation related to neuroticism which is commonly elevated in individuals with social anxiety and depression adults n  72 with varying levels of social anxiety and depression provided biological samples for dna extraction completed a measure of neuroticism and participated in a standardized emotion processing task affect matching while undergoing fmri a significant interaction effect was found for rs3796863 x neuroticism that predicted right amygdalasubgenual anterior cingulate cortex sgacc functional connectivity simple slopes analyses showed a positive association between neuroticism and right amygdalasgacc connectivity among rs3796863 aallele carriers findings suggest that the more socially sensitive rs3796863 aallele may partially explain the relationship between a known risk factor ie neuroticism and promising biomarker ie amygdalasgacc connectivity in the development and maintenance of social anxiety and depression
in a referendum conducted in the united kingdom uk on june 23 2016 516 of the participants voted to leave the european union eu the outcome of this referendum had major policy and financial impact for both uk and eu and was seen as a surprise because the predictions consistently indicate that the remain would get a majority in this paper we investigate whether the outcome of the brexit referendum could have been predictable by polls data the data consists of 233 polls which have been conducted between january 2014 and june 2016 by yougov populus comres opinion and others the sample size range from 500 to 20058 we used singular spectrum analysis ssa which is an increasingly popular and widely adopted filtering technique for both short and long time series we found that the real outcome of the referendum is very close to our point estimate and within our prediction interval which reinforces the usefulness of ssa to predict polls data
an important issue in a synthetic aperture radar sar system employing multidimensional waveform encoding mwe is the fulfillments of digital beamforming dbf on receive in elevation for a reliable separation of the mutually overlapped echoes from multiple transmit waveforms in this paper the performance of a separation approach employing hybrid dbf in elevation by combining the onboard realtime beamsteering and a posteriori nullsteering dbf on the ground is elaborately investigated as a cascaded structure which comprises two subsequent dbf networks the onboard part effectuates the steering of the mainlobes within multiple partitioned groups of antenna elements to ensure sufficient signal receive gain over the whole swath the a posteriori adaptive dbf network on the ground mainly performs the task of placing nulls to cancel the range interference from other transmit waveforms which enables adaptive beamforming to avoid the topographic height variation problem two type of onboard realtime beamformers are investigated depending on the utilization of the transmit waveform structure information or not the performance of the hybrid dbf approach is theoretically analyzed and evaluated in simulation experiment it is shown that the hybrid dbf approach can provide additional dimensions of the tradespace to optimize the performance on range ambiguity suppression and signaltonoise ratio improvement as well as the onboard data volume reduction in comparison with the a posteriori dbf on the ground employing the hybrid dbf networks can get satisfactory performance while remarkably reducing the output data volume in the presented example the corresponding output channel number is decreased from 10 to 6
auxetic materials are characterized by a negative poissons ratio that they expand laterally in the directions perpendicular to the applied stretching stress and vice versa piezoelectrics will change their dimensions when exposed to an external electric field here we introduce the concept of the electric auxetic effect electric auxetic materials will contract or expand in all dimensions in response to an electric field such unusual piezoelectric response driven by an electric field is a close analogy to the auxetic effect driven by a stress field a key feature of electric auxetic materials is that their longitudinal and transverse piezoelectric coefficients are of the same sign we demonstrate using firstprinciples calculations that the pca21 orthorhombic phase of ferroelectric hfo2 exhibits both the negative longitudinal piezoelectric effect and the electric auxetic effect the unusual negative longitudinal piezoelectric effect arises unexpectedly from the domination of the negative internalstrain contribution over the positive clampedion contribution a character often found in van der waals solids we confirm a few more electric auxetic materials with finite electric field calculations by screening through a firstprinciplesbased database of piezoelectrics
a fundamental challenge for rapidly growing cities of nowadays is to offer public transport services to fulfill the increasing demands for mobility in our country a widely adaptable and convenient mode of public transport is by bus the buses in the running and bus arrival time are considered as the essential data for almost every open transport travelers excessively longer waiting periods at bus stations frequently deter travelers and the nonavailability of information about the buses running makes them reluctant to take buses so to make passengers more informative we present a system for the manifestation of the onroad buses and its’ arrival time based upon the bus passengers’ participatory detecting the bus passengers’ encompassing environmental framework is successfully gathered with the help of commodity mobile phones and employed to present the bus traveling pathways and to reckon the estimated time of arrival of the bus at several bus stations through the mobile application the suggested framework entirely depends upon the cooperative attempt of the partaking users and the driver in the designated route therefore it can be facilely adopted to fortify bus accommodation framework excluding the demanding assistance through the specific bus operating organizations we develop an archetype system with variants of androidpredicated mobile phones the suggested framework is usually more accessible and energy cordial
behavioral modeling and identification of nonlinear time invariant systems in the frequency domain represents an extremely interesting and up to date topic in widespread application fields the frequencydomain volterrawiener or polynomial approach is one of the most widely employed since it can be derived as the straightforward extension of the usual frequency response function to the nonlinear case its main drawback is that its complexity rapidly grows with the number of input harmonic components and nonlinearity order the purpose of this work is presenting a method to reduce the number of coefficients defining the volterra models by exploiting a priori knowledge about the input signal spectral content similarly to the spectral linearization approximation which is commonly used in radiofrequency and microwave applications input components are classified into large and small according to their expected amplitudes the output spectrum is computed by considering all the possible interactions between large components according to the volterra theory on the contrary interactions between small components are neglected the proposed modeling approach has been tested in numerical simulations on a hammerstein system results clearly show the advantages with respect to a conventional polynomial model
spatial recordings allow viewers to move within them and freely choose their viewpoint however such recordings make it easy to miss events and difficult to follow moving objects when skipping through the recording to alleviate these problems we present the who put that there system that allows users to navigate through time by directly manipulating objects in the scene by selecting an object the user can navigate to moments where the object changed users can also view trajectories of objects that changed location and directly manipulate them to navigate we evaluated the system with a set of sensemaking questions in a thinkaloud study participants understood the system and found it useful for finding events of interest while being present and engaged in the recording
for vehicle safety the intime monitoring of the driver and assessing hisher state is a demanding issue frustration can lead to aggressive driving behaviours which play a decisive role in up to onethird of fatal road accidents consequently the authors present the automatic analysis of the emotional driver states of frustration anxiety positive and neutral based on experiments with normal drivers within cars in realworld low expressivity situations they use speech data as speech can be recorded with zero invasiveness and comes naturally in driving situations a careful selection of speech features subject data identification hyperparameter optimisation and machine learning algorithms was applied for this difficult 4emotionclass detection problem where the literature hardly reports results above chance level incar assistance demands realtime computing a very detailed analysis yields best results with relatively small random forests and with an optimal feature set containing only 65 features 651 of the standard emobase feature set which outperformed all other feature sets producing 3538 unweighted average recall 5326 precision with low computational effort and also reducing the inevitably high confusion of ‘neutral’ with lowexpressed emotions this result is comparable to and even outperforming other reported studies of emotion recognition in the wild their work therefore triggers adaptive automotive safety applications
this study reports a numerical hybrid cluster simulation of a downer reactor unit using cpfd barracuda vr software this model includes an experimentally determined particle cluster size distributi
"
 drawing on domestication theory and intersectionality theory this study explores the multiple roles dating apps play in chinese gay men’s lives amid changing personal and social circumstances we present indepth narratives of three blued users from different generations and classes with unique relationship statuses the app’s geolocative features strengthened the gay capital of our younger participant but threatened our middleaged closeted participant although coming from a homophobic generation our senior participant had no issue becoming an online celebrity on the app because his wife had passed away pointing out the intersectional influence of generational and relational backgrounds our participants’ socioeconomic positions also shaped whom they would interact with on blued and how these interactions took place these observations illustrate the relationship between users’ intersectional positions and their domestication of blued complementing existing dating app studies that skew toward younger users and focus only on certain elements of app use"
this paper examines the participation of various healthcare specialists and representatives in three design sprints aimed to codesign healthcare services through service design approaches the design sprints were executed during the spring of 2019 in gothenburg sweden tallinn estonia and rovaniemi finland each lasting four to five days this paper discusses the varied roles of healthcare representatives as participants in healthrelated design sprints and how these different participatory roles can optimise support and catalyse design sprint processes to develop health services the findings show that by being part of a team healthcare representatives can learn to use design methods and design thinking which have an impact on future development work for healthcare services the value of the paper lies in presenting a practical framework for use in design sprints by healthcare representatives towards the development of healthrelated services
unmanned aerial vehicle uavenabled communication system provides flexibility and reliability compared to conventional ones millimeter wave mmwave and massive multipleinput–multipleoutput mimo have widely been researched since recent years which are promising techniques for the next and even the later generation communication system hybrid precoding as a method to reduce the high cost in hardware and power brought by massive antenna array develops fiercely and is often combined to deep learning a kind of popular optimization tool which brings an overwhelming performance on the other hand there are not so many attentions about the hybrid precoding in timevarying mmwave massive mimo which is necessary to be considered in a uavenabled communication scenario because the performance will degrade seriously if the channel changed while the transmitter and receiver use the precoding matrix corresponding to the expired channel yet in this paper we propose a doublepilotbased hybrid precoding system which completes analog precoding and digital precoding separately—predicting the previous one using deep learning structure and updating equivalent channel frequently for the post one by enhancing the frequency of equivalent channel estimation
in this paper we present three methods that reduce the computational time of training deep neural networks with multispectral images optimize the resource occupation of the dataset and obtain high performance for reduced datasets in the first two methods we reduce the dimension of the input data with either histograms of pixel intensity or bagofwords then we train a convolutional neural network with either histograms or bagofwords and we achieve an accelerated training moreover storing the image patches from the dataset in the form of histograms or bagofwords reduced the memory storage significantly in the last method we subsample the training dataset randomly to 50 20 and 10 of the original dataset thus training a convolutional neural network on a smaller number of samples in the form of histograms or bagofwords and the classification performance is almost unaffected this is an important achievement as there are few labelled datasets for earth observation and the number of images in these datasets is small our results show that the training time is reduced by a maximum of 387 times and the datasets with histograms or bagofwords occupy 633 times less space
lifetimeoptimal speculative partial redundancy elimination lospre is the most advanced currently known redundancy elimination technique it subsumes many previous approaches such as common subexpression elimination global common subexpression elimination and loopinvariant code motion previously known lospre algorithms have high time complexity faster but less powerful approaches have been used and developed further instead we present a simple lineartime algorithm for lospre for structured programs that can also handle some more general scenarios compared to previous approaches we prove that our approach is optimal and that the runtime is linear in the number of nodes in the controlflow graph the condition on programs of being structured is true for many programming languages and for others such as c is equivalent to a bound on the number of goto labels per function an implementation in a mainstream c compiler demonstrates the practical feasibility of our approach which is based on graphstructure theory and uses treedecompositions for structured programs we also improve the runtime bound of deterministic implementations of the previously known mcpre and mcssapre algorithms from on3 to on25
we present a simple threedimensional model to describe the autonomous expansion of a substrate whose growth is driven by the local mean curvature of its surface the model aims to reproduce the nest construction process in arboreal nasutitermes termites whose cooperation may similarly be mediated by the shape of the structure they are walking on for example focusing the building activity of termites where local mean curvature is high we adopt a phasefield model where the nest is described by one continuous scalar field and its growth is governed by a single nonlinear equation with one adjustable parameter d when d is large enough the equation is linearly unstable and fairly reproduces a growth process in which the initial walls expand branch and merge while progressively invading all the available space which is consistent with the intricate structures of real nests interestingly the linear problem associated with our growth equation is analogous to the buckling of a thin elastic plate under symmetric inplane compression which is also known to produce rich patterns through nonlinear and secondary instabilities we validated our model by collecting nests of two species of arboreal nasutitermes from the field and imaging their structure with a microcomputed tomography scanner we found a strong resemblance between real and simulated nests characterized by the emergence of a characteristic length scale and by the abundance of saddleshaped surfaces with zeromean curvature which validates the choice of the driving mechanism of our growth model
multicast communication is characterized by the multiplicity of streams defining different groups where each stream has multiple sources a multicast communication tends to flood the network with a large number of flows that can overload some nodes and unload others this imbalance in the load distribution weakens network performance and could produce bottlenecks around overloaded nodes we propose in this article an approach based on a combination of a flow approach and a multiagent optimization to resolve the load balancing issue of multicast communication we use ordered weighted average owa a multicriteria optimization method to balance the degree of the nodes ensuring a balanced load distribution across the network the experiments conducted on a series of networks show that our approach provides a better equitable load assignment
model compression has drawn great attention in deep learning community a core problem in model compression is to determine the layerwise optimal compression policy eg the layerwise bitwidth in network quantization conventional handcrafted heuristics rely on human experts and are usually suboptimal while recent reinforcement learning based approaches can be inefficient during the exploration of the policy space in this article we propose bayesian automatic model compression bamc which leverages nonparametric bayesian methods to learn the optimal quantization bitwidth for each layer of the network bamc is trained in a oneshot manner avoiding the back and forth retraining in reinforcement learning based approaches experimental results on various datasets validate that our proposed methods can find reasonable quantization policies efficiently with little accuracy drop for the quantized network
the rapid development of technology leads to more demands and innovation that must be conducted by learners especially teacher educators one of the innovation that can be applied in higher education is elearning elearning enables students to study the materials without space and time constraint this article discussed the impact of elearning on the activities of bachelor of primary education students in the teacher training and education faculty of syiah kuala university this qualitative study employed action research approach based on lesson study conducted in three cycles with three main stages of lesson study in each cycle plan do and see the research subjects were 30 bachelor of primary education students enrolled in the course of learning plan data collection was conducted by observation and questionnaire data analysis involved descriptive analysis and the percentage formula for the questionnaire the results showed that student activities steadily increased in each cycle as indicated by the average score in cycle 1 mean  4 cycle 2 mean 425 and cycle 3 mean 441 therefore it can be concluded that elearning has a positive impact on the learning this finding indicates that elearning is advantageous for student learning
for many years primary weather forecasting services global forecast system gfs and the european centre for mediumrange weather forecasts ecmwf have been made available to the public through global numerical weather prediction nwp models estimating a multitude of general weather variables in a variety of resolutions secondary services such as weather experts meteomatics ag use data and improve the forecasts through various methods they tailor for the specific needs of customers in the wind and solar power generation sector as well as data scientists analysts and meteorologists in all areas of business these auxiliary services have improved performance and provide reliable data however this work extended these auxiliary services to socalled tertiary services in which the weather forecasts were further conditioned for the very niche application environment of mobile solar technology in solar car energy management the gridded model output statistics gmos global horizontal irradiance ghi model developed in this work utilizes historical data from various ground station locations in south africa to reduce the mean forecast error of the ghi component an average root mean square error rmse improvement of 1128 was shown across all locations and weather conditions it was also shown how the incorporation of the gmos model could have increased the accuracy in regard to the state of charge soc energy simulation of a solar car during the sasol solar challenge 2018 and the possible range benefits thereof
telemedicine is one of the primary health policies in thailand to improve life quality of people by accessing medical service from anywhere anytime many rural areas still have difficulties to get medical services such as primary diagnosis we propose a webbased application for chief complaint analysis that is publicly available our system takes input of chief complaint text then makes predictions of primary diagnosis using machine learning algorithm our experiment shows that the model has promising results at 077 of accuracy our system could be used as a patient screening tool that makes impact to operational excellence for thailand healthcare
the fate of water and watersoluble toxic wastes in the subsurface is of high importance for many scientific and practical applications although solute transport is proportional to water flow rates theoretical and experimental studies show that heavytailed powerlaw solute transport distribution can cause chemical transport retardation prolonging cleanup timescales greatly however no consensus exists as to the physical basis of such transport laws in percolation theory the scaling behavior of such transport rarely relates to specific medium characteristics but strongly to the dimensionality of the connectivity of the flow paths for example two or threedimensional as in fracturedporous media or heterogeneous sediments as well as to the saturation characteristics ie wetting drying and entrapped air in accordance with the proposed relevance of percolation models of solute transport to environmental cleanup these predictions also prove relevant to transportlimited chemical weathering and soil formation where the heavytailed distributions slow chemical weathering over time the predictions of percolation theory have been tested in laboratory and field experiments on reactive solute transport chemical weathering and soil formation and found accurate recently this theoretical framework has also been applied to the water partitioning at the earth’s surface between evapotranspiration et and runoff q known as the water balance a wellknown phenomenological model by budyko addressed the relationship between the ratio of the actual evapotranspiration et and precipitation etp versus the aridity index et0p with p being the precipitation and et0 being the potential evapotranspiration existing work was able to predict the global fractions of p represented by q and et through an optimization of plant productivity in which downward water fluxes affect soil depth and upward fluxes plant growth in the present work based likewise on the concepts of percolation theory we extend budyko’s model and address the partitioning of runoff q into its surface and subsurface components as well as the contribution of interception to et using various published data sources on the magnitudes of interception and information regarding the partitioning of q we address the variability in et resulting from these processes the global success of this prediction demonstrated here provides additional support for the universal applicability of percolation theory for solute transport as well as guidance in predicting the component of subsurface runoff important for predicting natural flow rates through contaminated aquifers
cleanliness control is one of the most important processes for final optics assembly foa in highpower laser systems in this paper an “ultra clean manufacturing ucm” concept is put forward fo
the presence of harmonics in solar photo voltaic pv energy conversion system results in deterioration of power quality to address such issue this paper aims to investigate the elimination of harmonics in a solar fed cascaded fifteen level inverter with aid of proportional integral pi artificial neural network ann and fuzzy logic fl based controllers unlike other techniques the proposed flc based approach helps in obtaining reduced harmonic distortions that intend to an enhancement in power quality in addition to the power quality improvement this paper also proposed to provide output voltage regulation in terms of maintaining voltage and frequency at the inverter output end in compatible with the grid connection requirements the simulations are performed in the matlab  simulink environment for solar fed cascaded 15 level inverter incorporating pi ann and fl based controllers to exhibit the proposed technique a 3 kwp photovoltaic plant coupled to multilevel inverter is designed and hardware is demonstrated all the three techniques are experimentally investigated with the measurement of power quality metrics along with establishing output voltage regulation
according to the statistics the disease with which most of the women die is breast cancer lot of new cases and deaths because of this cancer places this disease as a major public health issue the diagnosis of this disease at the starting stages will help the treatment by which the mortality can be reduced this leads to extensive research in the diagnosis and classification of patients based on its malignancy lot of machine learning algorithms was used to diagnose this disease this work analyses the various works done in this area also it shows the comparative study of those algorithms
encapsulant discoloration is a wellknown field degradation mode of crystallinesilicon photovoltaic modules particularly in the hot climate zones the discoloration rate is influenced by several weathering factors such as uv light module temperature and humidity as well as the permeability of oxygen into the module in this article a rate dependence model employing the modified arrhenius equations to predict the degradation rate for encapsulant discoloration in different climates is presented two modeling approaches are introduced which utilize the field and accelerated uv testing degradation data in conjunction with the field meteorological data to determine the acceleration factor for encapsulant browning a novel method of accelerated uv stress testing at three simultaneous module temperatures in a single environmental chamber test run is implemented to estimate the activation energy for browning the test was performed on three fieldretrieved modules to capture the wearout failure mechanism the degradation in shortcircuit current isc rather than maximum power is used as a decisive parameter for the discoloration analysis furthermore the developed model has been used to predict the isc degradation rate for the arizona field characterized by a hot and dry climate and is validated against the fieldmeasured value it has also been applied to other climate types eg the cold and dry climate of new york
this work focuses on achieving cooperative simultaneous interception against moving targets using the concepts of deviated pursuit guidance strategy unlike most existing salvo guidance strategies which use estimates of timetogo based on proportional navigation guidance the present strategy uses exact expression for timetogo to ensure simultaneous interception the guidance command is derived considering nonlinear engagement kinematics weighted consensus in timetogo over a pseudoundirected graph is used to intercept a moving target cooperatively it has been shown that through a judicious choice of these weights the achievable set of interception times expands simulations are provided to vindicate the effectiveness of the proposed strategy
in this paper we examine min matrix llkminij1ij1 where lnr denotes the nth hyperlucas number of order r we mainly focus on characteristic polynomial of l also we compute determinants inverses of l and its hadamard inverse moreover we give a numerical example to illustrate our results
with the advent of powerful convolutional neural networks cnns recent studies have extended early applications of neural networks to imaging tasks thus making cnns a potential new tool for assessing medical image quality here we compare a cnn to model observers in a search task for two possible signals a simulated mass and a smaller simulated microcalcification embedded in filtered noise and single slices of digital breast tomosynthesis dbt virtual phantoms for the case of the filtered noise we show how a cnn can approximate the ideal observer for a search task achieving a statistical efficiency of 077 for the microcalcification and 078 for the mass for search in single slices of dbt phantoms we show that a channelized hotelling observer cho performance is affected detrimentally by false positives related to anatomic variations and results in detection accuracy below human observer performance in contrast the cnn learns to identify and discount the backgrounds and achieves performance comparable to that of human observer and superior to model observers proportion correct for the microcalcification cnn  096 humans  098 cho  084 proportion correct for the mass cnn  098 humans  083 cho  051 together our results provide an important evaluation of cnn methods by benchmarking their performance against human and model observers in complex search tasks
linear multivariable systems which have physical parameters changing their values from the nominal ones in an arbitrary and nonstationary way are considered a control plant is subject to action of polyharmonic external disturbances containing an arbitrary number of frequencies with unknown amplitudes their sum is bounded the following problem of controller synthesis is formulated design a controller that guarantees robust stability of the closed loop system and in addition provides given controlled variables errors of the nominal plant for the steadystate process the problem solution is based on a representation of the system equations in w λ kform and is formulated as a standard h∞ optimization procedure desired accuracy is provided through analytic selection of a weighting matrix for the controlled variables of the plant the proposed approach is illustrated with a solution of a wellknown benchmark problem
sequencetosequence models have lead to significant progress in keyphrase generation but it remains unknown whether they are reliable enough to be beneficial for document retrieval this study provides empirical evidence that such models can significantly improve retrieval performance and introduces a new extrinsic evaluation framework that allows for a better understanding of the limitations of keyphrase generation models using this framework we point out and discuss the difficulties encountered with supplementing documents with not present in text keyphrases and generalizing models across domains our code is available at httpsgithubcomboudinflirusingkg
the paper considers the relevance of the problem of measuring the deformation of aircraft structures in connection with the increasing volume of use of materials similar to an increasing number of parts and structures of aircraft and helicopters as well as unmanned aerial vehicles the necessity of monitoring the state of materials and structures in real time with the analysis of the current state and operating modes of aircraft supplementing the pilot assistance system is shown an overview of fiberoptic sensors and measurement technologies used in various types is presented as promising for use in diagnostic systems with the possibility of embedding into the structure of composite materials a method is proposed for choosing the type of fiberoptic sensors for monitoring various parts of aircraft depending on the characteristics
swarm robots have been used successfully in many studies to solve complex tasks the common task of swarm robots depends on the communication between them in this study differently swarm robots were used in the motion planning study with the help of swarm robots a path was found that would allow the main mission robot to travel the shortest distance from the starting point to the end point without hitting the obstacles this study was named traffic police algorithm tpa according to the algorithm robotic individuals belonging to the swarm are provided to spread in a certain order within the boundaries of the environment almost everywhere in the configuration space the swarm robot members with this random propagation pattern are positioned as close to each other as far as communication distance if there is no member closest to the target point the members move randomly at a predetermined distance when a robot sees the target point it transmits the distance and orientation angle by notifying other neighboring robots nearby all robots transmit distance and orientation information to neighboring robots that they can see and this information is finally transmitted to the main task robot at the starting point the main task robot at the starting point found the shortest distance using the dijkstra algorithm one of the search methods to find the shortest path among the nodes transmitted to it the developed algorithm was initially tested in a virtual environment and its implementation will be done in future studies
as part of a new snowpack monitoring framework this study evaluated the feasibility of using an led lidar leddar time of flight sensor for snowpack depth measurement the leddar sensor has two additional features over simple sonic ranging sensors i the return signal is divided into 16 segments across a 48° field of view each recording individual distancetotarget dtt measurements ii an index of reflectance or intensity signal is recorded for each segment these two features provide information describing snowpack morphology and surface condition the accuracy of leddar sensor dtt measurements for snow depth monitoring was found to be  20 mm which was better than the 50 mm quoted by the manufacturer and the precision was  5 mm leddar and independent sonic ranger snow depth measurement showed strong linear agreement r2  098 there was also a strong linear relationship r2  098 between leddar and manual field snow depth measurements the intensity signal response was found to correlate with snow surface albedo and inversely with air temperature r  077 and −077 respectively
aerialground interference is the main obstacle to achieving high spectral efficiency in cellular networks with both traditional terrestrial users and new unmanned aerial vehicle uav users due to their strong lineofsight los channels with the ground uavs could causesuffer severe interference tofrom a large number of nonassociated but cochannel base stations bss in their uplinkdownlink communications in this letter we propose a new cooperative interference cancellation cic scheme for the uav’s uplink communication to mitigate its strong interference to cochannel bss which requires only local cooperation between each cochannel bs and its adjacent helping bss specifically the helping bss without serving any users in the uav’s communication channel quantize and forward their received signals from the uav to their aided cochannel bs which then processes the quantized signals jointly with its own received signal to decode its served terrestrial user’s message via canceling the uav’s signal by linearnonlinear interference cancellation techniques icts we derive the achievable rates of the proposed cic scheme with different icts as a function of the uav’s transmit power and rate and thereby unveil the conditions under which the proposed cic scheme outperforms the existing cic scheme based on the decodeandforward df operation of the helping bss
received 29 july 2019 accepted 28 november 2019 a problem that is common in agriculture but not very publicized thanks to the absence of victims is the rollover of centre pivot and lateral move irrigation systems these accidents are due to particularlystrong winds acting on the spans and they are potentially very destructive for the installations also the restoration phase of the installations requires always an intervention of lifting of the machinery on the field with a potential further damage to crops setting and land compaction given the basic inevitability of the phenomenon due to atmospheric events these rollovers could be however limited eg by proposing a system design granting a higher stability therefore we have firstly modelled the rollover dynamics of these systems considering the geometry the masses the forces acting on them wind gravity the position of the centre of gravity then thanks to morphometry we have investigated booms’ stability as a consequence of a proportional or notproportional alteration of the system sizes in particular the upscaling of supports done by some manufacturers and the lengthening of spans often required by customers morphometry is a method born in biology typically used to describe and analyse statistically the shape variations within and among samples of organisms as a result of growth experimental treatments or evolution as the idea of evolutionary adaptation is intrinsic in the technical evolution of humanmade systems models variants operated by manufacturers also artificial systems can be studied or improved via the morphometry as operated here the output of this study is a physical model of rollover and a sensitivity analysis of a reference configuration for an irrigation boom thanks to these analyses we were able to demonstrate for example how a scalingup of boom supports respectful of geometric ratios can increase the system stability despite the elevation of the pressure point of the wind on the frame
the identification of sensitive biomarkers is essential to validate therapeutics for huntington disease hd we directly compare structural imaging markers across the largest collective imaging hd dataset to identify a set of imaging markers robust to multicenter variation and to derive upper estimates on sample sizes for clinical trials in hd
objectives to evaluate a newgeneration noninvasive wireless axillary thermometer with artificial intelligence ithermonitor wt705 raiing medical beijing china and to ascertain its feasibility for perioperative continuous body temperature monitoring in surgical patients setting departments of biliary surgery and operating room and the postanaesthesia care unit of a university teaching hospital in chengdu china participants a total of 526 adult surgical patients were consecutively enrolled design this was a prospective observational study axillary temperatures were continuously recorded with ithermonitor throughout the whole perioperative period the temperatures of the contralateral armpit were measured with mercury thermometers at 800 1200 1600 and 2000 every day and were used as references outcome measures the outcomes were the accuracy and precision of the temperatures measured with ithermonitor the validity to detect fever and the feasibility of continuous wear pairs of temperatures were evaluated with student’s ttest pearson’s correlation and repeatedmeasures blandaltman plot results a total of 3621 pairs of body temperatures were obtained the temperatures measured with ithermonitor agreed with those measured with the mercury thermometers overall with a mean difference of 003°c±035°c and a moderate correlation r0755 p0001 the 95 limits of agreement loa ranged from −063°c to 073°c with 511 of the differences outside the 95 loa the intraclass correlation coefficient was 0753 continuous temperature monitoring captured more fevers than intermittent observation 117526 vs 91526 p0001 detected fever up to 435 hours earlier and captured a higher peak temperature 029°c±027°c 95 ci 026–031 all subjects felt that wearing ithermonitor was more or less comfortable and did not affect their daily activities conclusions ithermonitor is promising for continuous remote temperature monitoring in surgical patients however further developments are still needed to improve the precision of this device especially for temperature detection in underweight patients and those with lower body temperature trial registration number chictr1900024549 results registered on 5 july 2019
assessment of the nitrogen status of grapevines with high spatial temporal resolution offers benefits in fertilizer use efficiency crop yield and quality and vineyard uniformity the primary objective of this study was to develop a robust predictive model for grapevine nitrogen estimation at bloom stage using highresolution multispectral images captured by an unmanned aerial vehicle uav aerial imagery and leaf tissue sampling were conducted from 150 grapevines subjected to five rates of nitrogen applications subsequent to appropriate preprocessing steps pixels representing the canopy were segmented from the background per each vine first we defined a binary classification problem using pixels of three vines with the minimum lown class and two vines with the maximum highn class nitrogen concentration following optimized hyperparameters configuration we trained five machine learning classifiers including support vector machine svm random forest xgboost quadratic discriminant analysis qda and deep neural network dnn with fullyconnected layers among the classifiers svm offered the highest f1score 8224 on the test dataset at the cost of a very long training time compared to the other classifiers alternatively qda and xgboost required the minimum training time with promising f1score of 8085 and 8027 respectively second we transformed the classification into a regression problem by averaging the posterior probability of highn class for all pixels within each of 150 vines xgboost exhibited a slightly larger coefficient of determination r2  056 and lower root mean square error rmse 023 compared to other learning methods in the prediction of nitrogen concentration of all vines the proposed approach provides values in i leveraging highresolution imagery ii investigating spatial distribution of nitrogen across a vine’s canopy and iii defining spatial zones for nitrogen application and smart sampling
the textilebased flexible electronic device has attracted considerable attention due to its excellent conformability skinaffinity and compatibility with clothing industry however the machinewashing process may damage the electronic components further resulting in the failure of the device herein parafilm® a commercially available cohesive thermoplastic is introduced as both substrate and encapsulating material to fabricate an allsolidstate supercapacitor which could be tightly stuck on and easily peeled off from a fabric the supercapacitor possesses excellent capacitive behavior 737 fg at a current density of 1 ag long cycle life capacitance retention  90 after 5000 cycles and great flexibility capacitance retention 98 after 100 times of bendingtwisting after water flushing and soaking the capacitance of the supercapacitor could be retained at about 98 of its original level a parafilm®based piezoresistive sensor with good pressuresensing performance has also been fabricated via the same approach to demonstrate the universality of the proposed strategy for textile restickable electronics this work may not only fabricate novel flexible electronic systems for wearable applications but also provide a universal strategy to address the machinewashing issues in textile electronics
due to the severe limitation of the power of load platforms such as satellites drones and the internet of things in order to be able to use ofdm to improve spectrum utilization efficiency and access to largescale users how to control ofdm’s high paprpeaktoaverage ratio has become a communication industry research hotspots the signal distortion technology suppression method uses a filter to suppress or truncate parts with peak ratios that are too high resulting in an increase in the signal evm and an increase in ber bit error rate this paper first builds a multicarrier signal transmission model and analyzes the peaktoaverage ratio and the degree of signal quality distortion of the multicarrier signal on this basis a multicarrier modulation method based on a multifilter bank is designed to specifically suppress the power of each subcarrier and then the subcarriers are superimposed on this basis so that the information of each subcarrier is saved and suppress the excessively high peaktoaverage ratio after subcarrier superposition build a multicarrier communication environment in matlab and verify the performance advantages of the modulation method in this paper on papr and evm
deep learning has attracted the attention of many researchers for structural health monitoring however it is difficult to use most of the deep learningbased techniques to detect damage throughout the life cycle of a large or inaccessible structure especially a ship few studies have focused on hull stiffened plate crack damage detection we propose such a method based on deep learning using a convolutional neural network cnn the model is trained on acceleration data which are calculated by the abaqus scripting interface five crack locations and four crack lengths are considered as well as the intact condition the effects of damping ratio loading area and load level on the proposed method are considered the robustness of the proposed approach to noise and stiffener slenderness ratio are also discussed the proposed method is compared to the multilayer perceptron method by wavelet packet transformation using the same data so as to quantify its performance the results show that the proposed method performs better at single and doublecrack detection and is less sensitive to noise damping ratio loading area and load level
power management informed consumers to lightsout nine minutes as a challenge for the darkness of the coronavirus across india the absence of commercial operations and a substantial difference between demand and supply could contribute to a power outage accurate and reliable measures are taken to overcome these differences in demand and supply this phenomenal rapid decrease in load and quick restoration must be done with water and gas resources realtime widearea monitoring and smart automation wamsa technologies measure and control these irregular instances and power grid variables wide area monitoring system wams is an integrated pmubased frequency and voltage stabilization system it helps to assess the reactive power deficit of the weaker regions and estimate the critical situations it identifies voltage stability conditions for specific power generation the novelty of this paper is to present the new realtime load shedding and grid balancing strategy by using pmus potential functions and practices this case study creates awareness among power engineers to avoid future blackouts worldwide
locating ability to decide target position is exceptionally valuable in numerous application fields multilateration is one of the method to get the position prediction using hyperbolic algorithm this method exploits the time difference of arrival tdoa information from least four receivers to get 3d position of target this paper centers around the passive multilateration by using target’s communication burst signal received burst signals are being correlated to get the tdoa at that point by utilizing tdoa and receiver positions information we can calculate the multilateration algorithm this paper additionally shows the accuracy of position prediction in some example places of target
natural or synthetic polycations are used as biocides or as druggene carriers understanding the interactions between these macromolecules and cell membranes at the molecular level is therefore of great importance for the design of effective polymer biocides or biocompatible polycationbased delivery systems until now details of the processes at the interface between polycations and biological systems have not been fully recognized in this study we consider the effect of strong polycations with quaternary ammonium groups on the properties of anionic lipid membranes that we use as a model system for proteinfree cell membranes for this purpose we employed experimental measurements and atomicscale molecular dynamics md simulations md simulations reveal that the polycations are strongly hydrated in the aqueous phase and do not lose the water shell after adsorption at the bilayer surface as a result of strong hydration the polymer chains reside at the phospholipid headgroup and do not penetrate to the acyl chain region the polycation adsorption involves the formation of anionic lipidrich domains and the density of anionic lipids in these domains depends on the length of the polycation chain we observed the accumulation of anionic lipids only in the leaflet interacting with the polymer which leads to the formation of compositionally asymmetric domains asymmetric adsorption of the polycation on only one leaflet of the anionic membrane strongly affects the membrane properties in the polycation–membrane contact areas i anionic lipid accumulates in the region near the adsorbed polymer ii acyl chain ordering and lipid packing are reduced which results in a decrease in the thickness of the bilayer and iii polycation–anionic membrane interactions are strongly influenced by the presence and concentration of salt our results provide an atomicscale description of the interactions of polycations with anionic lipid bilayers and are fully supported by the experimental data the outcomes are important for understanding the correlation of the structure of polycations with their activity on biomembranes
we propose a novel decentralized feature extraction approach in federated learning to address privacypreservation issues for speech recognition it is built upon a quantum convolutional neural network qcnn composed of a quantum circuit encoder for feature extraction and a recurrent neural network rnn based endtoend acoustic model am to enhance model parameter protection in a decentralized architecture an input speech is first upstreamed to a quantum computing server to extract melspectrogram and the corresponding convolutional features are encoded using a quantum circuit algorithm with random parameters the encoded features are then downstreamed to the local rnn model for the final recognition the proposed decentralized framework takes advantage of the quantum learning progress to secure models and to avoid privacy leakage attacks testing on the google speech commands dataset the proposed qcnn encoder attains a competitive accuracy of 9512 in a decentralized model which is better than the previous architectures using centralized rnn models with convolutional features we conduct an indepth study of different quantum circuit encoder architectures to provide insights into designing qcnnbased feature extractors neural saliency analyses demonstrate a high correlation between the proposed qcnn features class activation maps and the input melspectrogram we provide an implementation1 for future studies
chemical vapor deposition is an important method for the preparation of boron carbide knowledge of the correlation between the phase composition of the deposit and the deposition conditions temperature inlet gas composition total pressure reactor configuration and total flow rate has not been completely determined in this work a novel approach to identify the kinetic mechanisms for the deposit composition is presented machine leaning ml and computational fluid dynamic cfd techniques are utilized to identify core factors that influence the deposit composition it has been shown that ml combined with cfd can reduce the prediction error from about 25 to 7 compared with the ml approach alone the sensitivity coefficient study shows that bhcl2 and bcl3 produce the most boron atoms while c2h4 and ch4 are the main sources of carbon atoms the new approach can accurately predict the deposited boron–carbon ratio and provide a new design solution for other multielement systems
bioelectricity plays an important role in cell behavior and tissue modulation but is understudied in tissue engineering research endogenous electrical signaling arises from the transmembrane potential inherent to all cells and contributes to many cell behaviors including migration adhesion proliferation and differentiation electrical signals are also involved in tissue development and repair synthetic and natural conductive materials are under investigation for leveraging endogenous electrical signaling cues in tissue engineering applications due to their ability to direct cell differentiation aid in maturing electroactive cell types and promote tissue functionality in this review we provide a brief overview of bioelectricity and its impact on cell behavior report recent literature using conductive materials for tissue engineering and discuss opportunities within the field to improve experimental design when using conductive substrates
"

random black hole bh attack significantly degrades manet’s
performance for strategic applications the performance parameters like packet delivery ratio
routing overheads etc are important the objectives are a to model random bh attack b
to propose a routing strategy for the protocol to mitigate random bh attack c to evaluate and
compare the network performance of modified protocol with the standard protocol



the random bh attack is modelled probabilistically the analysis is carried out by varying
black hole attack bha time as early median late occurrences and mix of these three categories
the blocking performance is also analysed by varying the percentages of malicious presence in the
network normal optimized link state routing olsr protocol is used to simulate the manet
performance using a typical medium size network the protocol has then been modified using trust
confidence aware routing strategy named as tcaolsr with a view to combat the degradations
due to the random bh attack



the random behavior of black hole attack is analyzed with all the possible random parameters
like deployment of mobile nodes number of malicious nodes and timing instances at which these
nodes change their state from the results of individual type early median and late it is observed
that the tcaolsr protocol gives stable performance for packet delivery ratio pdr and
routing overheads ro whereas for olsr protocol pdr gradually reduces and ro increases for
individual and mix type average energy consumption aec per node increases marginally for
tcaolsr protocol for the mix type pdr for tcaolsr is 4060 better whereas ro for
tcaolsr is very less compared to olsr protocol the efficacy of the tcaolsr protocol remains
stable for different categories of bh attack with various percentages of malicious nodes compared
to olsr with the same environment



simulations reveal that the modified protocol tcaolsr effectively mitigates the network
degradation for packet delivery ratio and routing overheads considerably at the cost of a slight increase
in average energy consumption per node of the network efficacy of the olsr and tcaolsr
protocols has also been defined and compared to prove robustness of the tcaolsr protocol
"
if robots can merge the appearancebased place knowledge of other robots with their own they can relate to these places even if they have not previously visited them we have investigated this problem using robots with compatible visual sensing capabilities and with each robot having its individual longterm place memory here each place refers to a spatial region as defined by a collection of appearances and in the place memory the knowledge is organized in a tree hierarchy in the proposed merging approach the hierarchical organization plays a key role—as it corresponds to a nested sequence of hyperspheres in the appearance space the merging proceeds by considering the extent of overlap of the respective nested hyperspheres—starting with the largest covering hypersphere thus differing from related work knowledge is merged in as large chunks as possible while the hierarchical structure is preserved accordingly as such the merging scales better as the extent of knowledge to be merged increases this is demonstrated in an extensive set of multirobot experiments where robots share their knowledge and then use their merged knowledge when visiting these places
in the midst of the current covid19 pandemic telehealth or the remote delivery of mental health services via videoconferencing technology is experiencing exponential growth in utilization telehealth services provide mental healthcare providers the ability to deliver timely assessments facilitate and leverage scarce resources and maintain client connections in a time where social distancing is endorsed the delivery of culturally appropriate psychiatric telehealth services is particularly relevant for diverse ethnic populations along with best practices to promote clientprovider engagement and client satisfaction the aim of this article is to provide an overview of psychiatric telehealth services and its functions and deliver insights into culturally appropriate practice strategies
chloride is a wellknown chemical compound that is very useful in industry and agricultural chloride can be transformed to hypochlorite chlorite chlorate and perchlorate chloride and their substances are not dangerous if we used in the optimal level groundwater that contaminated chloride and their substances impacts human health for an example if we drink water that contaminated chloride exceed 250 mgl it can cause heart problems and contribute to high blood pressure to avoid this problem we used mathematical models to explain groundwater contamination with chloride and their substances transient groundwater flow model provides the hydraulic head of groundwater in this model we will get the level of groundwater next we need to find its velocity and direction by using the result in first model put into second model groundwater velocity model provides x and zdirection vector in groundwater after computation we will plugin the result into the last model to approximated the chloride concentration in groundwater groundwater contamination dispersion model provides chloride hypochlorite chlorite chlorate and perchlorate concentration the proposed explicit finite difference techniques are used to approximate the model solution explicit method was used to solved hydraulic head model forward space described groundwater velocity model forward time and central space used to predict transient groundwater contaminated models the simulations can be used to indicate when each simulated zone becomes a hazardous zone or a protection zone
background premature cervical softening and shortening may be considered an early mechanical failure that predisposes to preterm birth preliminary clinical studies demonstrate that cervical elastography may be able to quantify this phenomenon and predict spontaneous preterm delivery objective to explore a new approach for cervix elasticity and length measurements with tactileultrasound probe methods cervix probe has tactile array and ultrasound transducer designed to apply controllable load to cervix and acquire stressstrain data for calculation of cervical elasticity young’s modulus and cervical length for four cervix sectors average values standard deviations intraclass correlation coefficients and the 95 limits of agreement blandaltman plots were estimated results ten nonpregnant and ten pregnant women were examined with the probe the study with nonpregnant women demonstrated a reliable acquisition of the tactile signals the ultrasound signals had a prolonged appearance identification of the internal os of the cervix in these signals was not reliable the study with pregnant women with the gestational age of 254 ± 23 weeks demonstrated reliable data acquisition with realtime visualization of the ultrasound signals average values for cervical elasticity and standard deviations of 197 ± 154 kpa and length of 307 ± 66 mm were calculated based on two measurements per 4 sectors measurement repeatability calculated as intraclass correlation coefficients between two measurements at the same cervix sector on pregnant women was found to be 097 for cervical elasticity and 093 for the cervical length the 95 limits of agreement of 1 cervical elasticity were from −224 to 149 and 2 cervical length from −133 to 165 conclusions this study demonstrated clinically acceptable measurement performance and reproducibility the availability of stressstrain data allowed the computation of cervical elasticity and length this approach has the potential to provide cervical markers to predict spontaneous preterm delivery
die digitalisierung von produktionsprozessen schreitet mit einer hohen intensität voran weiterbildung hat eine hohe relevanz für betriebliche transformationsprozesse die betriebliche weiterbildungspraxis ist den aktuellen herausforderungen der digitalisierung jedoch nicht gewachsen herausforderungen sind kompetenzlücken der mitarbeiter ungewisse anforderungsprofile und tätigkeitstypen demographischer wandel sowie veraltete didaktische ansätze zudem wird bestehender inhaltlicher und pädagogischer freiraum bei der gestaltung von weiterbildung oftmals nur unzureichend ausgenutzt die skizzierte situation führt dazu dass der mehrwert gegenwärtiger qualifizierungsangebote sowohl für unternehmen als auch beschäftigte nicht ausgeschöpft wird ausgehend von veränderungen durch digitalisierung in der produktion und deren auswirkungen auf die kompetenzentwicklung diskutiert dieser beitrag herausforderungen gegenwärtiger betrieblicher weiterbildung er leitet handlungsempfehlungen ab die mithilfe von beispielen gewerkschaftlich unterstützter weiterbildungspraxis illustriert werden im ergebnis erhalten interessierte einen überblick über gegenwärtige herausforderungen und handlungsempfehlungen für die gestaltung und durchführung von weiterbildung in zeiten der digitalisierung
in this article we introduce the generalized multiquadratic mappings and then describe them as a equation as a special case of such mappings we study the hyersulam stability of multiquadratic mappings in nonarchimedean spaces by applying a fixed point theorem moreover we prove that such mappings can be hyperstable
this article analyzes gender parity in the computer science degrees that are registered in the chilean single admission system sas the database analysis is descriptive the results show that 5381 192 female students enrolled in the computer science programs in 2018 there is a 27 female enrollment in 2018 in the civil engineering degrees offered by the universidad de chile the pontificia universidad católica the universidad adolfo ibáñez and the universidad de los andes however gender parity fell from 17 in 2014 to 11 in 2018 in the civil engineering degrees computer science offered by other chilean universities it is concluded that proequity gender programs such as those implemented by the universidad de chile for engineering degrees seem to be appropriate to increase female enrollment
"a blend of two taylor series for the same smooth real or complexvalued function of a single variable can be useful for approximation we use an explicit formula for a twopoint hermite interpolational polynomial to construct such blends we show a robust maple implementation that can stably and efficiently evaluate blends using linearcost horner form evaluate their derivatives to arbitrary order at the same time or integrate a blend exactly the implementation is suited for use with evalhf we provide a toplevel user interface and efficient module exports for programmatic use 
this work intended for presentation at the maple conference 2020 see this http url"
building facial analysis systems that generalize to extreme variations in lighting and facial expressions is a challenging problem that can potentially be alleviated using naturallooking synthetic data towards that we propose legan a novel synthesis framework that leverages perceptual quality judgments for jointly manipulating lighting and expressions in face images without requiring paired training data legan disentangles the lighting and expression subspaces and performs transformations in the feature space before upscaling to the desired output image the fidelity of the synthetic image is further refined by integrating a perceptual quality estimation model trained with face images rendered using multiple synthesis methods and their crowdsourced naturalness ratings into the legan framework as an auxiliary discriminator using objective metrics like fid and lpips legan is shown to generate higher quality face images when compared with popular gan models like stargan and starganv2 for lighting and expression synthesis we also conduct a perceptual study using images synthesized by legan and other gan models and show the correlation between our quality estimation and visual fidelity finally we demonstrate the effectiveness of legan as training data augmenter for expression recognition and face verification tasks
the internet of things iot is currently one of the most trending research topic many wireless network technologies called lpwan low power wide area network have been developed with lowpower longrange and lowthroughput in mind it is estimated that there will be over 50 billion connected devices in these lpwan networks this paper focuses on medium access control mac protocol of lpwan networks to evaluate the scalability and energy consumption performances more precisely we compare the two wellknown mac protocols namely pure aloha and slotted aloha first we define two performances indicators such as the data extraction rate der and the network energy consumption nec then we simulate a typical lorawan networks based on the lorasim simulator the results show that the slotted aloha protocol is more efficient than the pure aloha protocol in terms of energy consumption
in the paper ‘on the dirac–frenkel variational principle on tensor banach spaces’  we provided a geometrical description of manifolds of tensors in tucker format with fixed multilinear or tucker rank in tensor banach spaces that allowed to extend the dirac–frenkel variational principle in the framework of topological tensor spaces the purpose of this note is to extend these results to more general tensor formats more precisely we provide a new geometrical description of manifolds of tensors in treebased or hierarchical format also known as tree tensor networks which are intersections of manifolds of tensors in tucker format associated with different partitions of the set of dimensions the proposed geometrical description of tensors in treebased format is compatible with the one of manifolds of tensors in tucker format
abstractthe structure of vortex rossby waves vrws and their role in the development of a secondary eyewall in hurricane matthew 2016 is examined from observations taken during the noaa sensing 
in this paper droplet mobility and penetration into a fibrous porous medium are studied considering different physical and geometrical properties for the fibers an indepth insight into the droplet imbibition into the fibrous medium is beneficial for improving membrane products in different applications herein a multiphase lattice boltzmann method is employed as an efficient numerical algorithm for predicting the multiphase flow characteristics and the interfacial dynamics affected by the interaction between the droplet and fibrous substrates considered this computational technique is validated by comparison of the present results obtained for different benchmark twophase flow problems with those reported in the literature which shows good agreement and confirms its accuracy and efficiency droplet spreading and penetration into the fibrous porous geometries are then studied considering various porous topologies intrinsic contact angles and fiber sizes this study shows that the intrinsic contact angle has a great influence on the capillary pressure and consequently on the droplet imbibition into the porous medium the droplet easily penetrates the porous substrate by decreasing the intrinsic contact angle of the fibers and vice versa it is also concluded that by coating the fibrous porous medium with a narrow layer of thin fibers the surface resistance to liquid penetration significantly increases the present results illustrate that the droplet size impacts the directional wicking ability of the fibrous porous structure used in this study this property should be considered to produce appropriate twolayer membranes for different applications
steel moment frames are widely used in multistory buildings due to their advantages such as robust as well as flexibility in architectural planning this paper proposes a design method of steel moment frames based on an optimization algorithm called differential evolution in the optimization problem the design variables are the crosssections of the steel members while the objective function is the total weight of the frame the frame is designed based on the direct analysis method which is specified in the ansiaisc 36010 “specification for structural steel buildings” a program called frameopt is developed using visual basic for application language to implement the optimization algorithm this program connects to the sap2000 analysis program for the steel design through the open application programming interface feature an example is conducted to demonstrate the applicability of the proposed method
a novel approach of multiband real permittivity and permeability estimation of the magnetic composites using a modified under coupled multimodal te10p substrate integrated waveguide siw cavity is presented the proposed schemes allow the accurate wideband microwave characterization of the magnetodielectric composites by using the higherorder modes of the designed siw cavity the analytical closedform modified formulation is developed to calculate the permittivity and permeability over the wide frequency range 10–20 ghz consequently a unified empirical formulation is also developed which can be used to generate the approximate prior values of multimodal permittivity and permeability for the synthesized magnetic composites which is usually not known in advance the proposed analytical and empirical approaches are validated numerically and experimentally corresponding to various test samples the estimated parameters using the proposed analytical approach are found to be in quite close agreement error 5 with the corresponding reference values and empirically generated values
due to the high demands for computing the available resources always lack the approximate computing technique is the key to lowering hardware complexity and improving energy efficiency and performance however it is a challenge to properly design approximate multipliers since input data are unseen to users this challenge can be overcome by machine learning ml classifiers ml classifiers can predict the detailed feature of upcoming input data previous approximate multipliers are designed using simple adders based on ml classifiers but by using a simple adderbased approximate multiplier the level of approximation cannot change at runtime to overcome this drawback using an accumulator and reconfigurable adders instead of simple adders are proposed in this paper also the rounding technique is applied to approximate floatingpoint multipliers for further improvement our experimental results show that when the error tolerance of our target application is less than 5 the proposed approximate multiplier can save area by 7098 and when the error tolerance is less than 3 a rounding enhanced simple addersbased approximate multiplier can save area by 659 and a reconfigurable adderbased approximate multiplier with rounding can reduce the average delay and energy by 5495 and 4667 respectively compared to an exact multiplier
undertaking an interactive evaluation of goaloriented conversational agents cas is challenging it requires the search task to be realistic and relatable while accounting for the users cognitive limitations in the current paper we discuss findings of two wizard of oz studies and provide our reflections regarding the impact of different interactive search task designs on participants performance satisfaction and cognitive workload in the first study we tasked participants with finding a cheapest flight that met a certain departure time in the second study we added an additional criterion travel time and asked participants to find a fight option that offered a good tradeoff between price and travel time we found that using search tasks where participants need to decide between several competing search criteria price vs time led to a higher search involvement and lower variance in usability and cognitive workload ratings between different cas we hope that our results will provoke discussion on how to make the evaluation of voiceonly goaloriented cas more reliable and ecologically valid
this paper introduces a novel neural networkbased speech coding system that can process noisy speech effectively the proposed sourceaware neural audio coding sanac system harmonizes a deep autoencoderbased source separation model and a neural coding system so that it can explicitly perform source separation and coding in the latent space an added benefit of this system is that the codec can allocate a different amount of bits to the underlying sources so that the more important source sounds better in the decoded signal we target a new use case where the user on the receiver side cares about the quality of the nonspeech components in the speech communication while the speech source still carries the most important information both objective and subjective evaluation tests show that sanac can recover the original noisy speech better than the baseline neural audio coding system which is with no sourceaware coding mechanism and two conventional codecs
every year cerebrovascular diseases are diagnosed in 6 million people in the world actual clinical data indicate that cerebrocardial disorders are inherent not only in adults but also in children so this issue requires careful study one of the most pressing problems of modern medicine are cerebrovascular accident cerebral stroke in a child and its causes a special problem in modern medicine is the diagnosis and monitoring of the condition of children who have had a stroke the causes of the onset and development of diseases are individual the creation of specialized medical software for processing neuroimages mrict images of a child’s head was proposed as a biotechnical decision support system for a neurologist in the treatment and rehabilitation of a child who has had an ischemic stroke in order to determine the area of the focus of ischemia the dynamics of the results of treatment and rehabilitation as well as predicting the characteristic neurological symptoms resulting from a child stroke
abstract we present a quantum algorithm which computes group action inverses of the complex multiplication group action on isogenous ordinary elliptic curves using subexponential time but only polynomial quantum space one application of this algorithm is that it can be used to find the private key from the public key in the isogenybased crs and csidh cryptosystems prior claims by childs jao and soukharev of such a polynomial quantum space algorithm for this problem are false our algorithm along with contemporaneous independent work by biasse iezzi and jacobson is the first such result
automated theorem proving in firstorder logic is an active research area which is successfully supported by machine learning while there have been various proposals for encoding logical formulas into numerical vectors  from simple strings to more involved graphbased embeddings  little is known about how these different encodings compare in this paper we study and experimentally compare patternbased embeddings that are applied in current systems with popular graphbased encodings most of which have not been considered in the theorem proving context before our experiments show that the advantages of simpler encoding schemes in terms of runtime are outdone by more complex graphbased embeddings which yield more efficient search strategies and simpler proofs to support this we present a detailed analysis across several dimensions of theorem prover performance beyond just proof completion rate thus providing empirical evidence to help guide future research on neuralguided theorem proving towards the most promising directions
global oil reserves are facing great stress because of massive fuel consumption worldwide and the booming world population these fossil fuels when processed affect the environment by releasing greenhouse gases the outstanding usage of oil inspired many countries to develop alternative fuels biodiesel is the future prospect to fulfil the energy needs it is compatible ready to use fuel and can easily be deployed in the existing engines by combining it with conventional fuel diesel biodiesel emits a lower amount of greenhouse gases in this experiment we first synthesized biodiesel using methanol naoh and vegetable oil through transestrification process after that we prepared a sample for 14 biodiesel which was then used to run a 4stroke compression ignition ci engine that was attached to the engine testbed after that we rerun the same 4stroke ci engine with purediesel from the data obtained and subsequent dataanalysis the 14 biodiesel came out to be less efficient avg 3 lower brake power and 89 lower brake thermal efficiency and more expensive avg 128 higher fuel consumption as compared to purediesel
cancer is a disease that can affect all organs of humans based on data from the world health organization who fact sheet in 2018 cancer deaths have reached 96 million one known way to detect cancer that is with microarray technique but the microarray data have large dimensions due to the number of features that are very much compared to the number of samples therefore dimension reduction should be made to produce optimum accuracy in this paper we compare minimum redundancy maximum relevance mrmr and least absolute shrinkage and selection operator lasso to reduce the dimension of microarray data moreover by using random forest rf classifier the performance of classification cancer detection is compared based on the simulation it can be concluded that lasso is better than mrmr because it can produce an evaluation of 100 in lung and ovarian cancer 92 colon cancer 93 prostate tumor and 83 central nervous system
in modern cities smart irrigation systems are designed to operate via internet of things iot based sensor units having precise measurements of irrigation requirements such as amount of water crop temperature and humidity to build a robust supply chain ecosystem the usage of sensors and networking units enable the optimal usage of irrigation resources and is termed as precision irrigation pi thus pi leverage an efficient solution to handle the scarcity of essential resources such as food water land units and crop yields thus farmers gets better returns in the market due to high production however in pi the exchange of crop readings from sensor units to actuators are processed through open channels that is internet thus it open the doors for malicious intruders to deploy network and sensor‐based attacks on pi‐sensors to drain the available resources and battery power of sensor nodes in the network this reduces the optimum and precise utilization of irrigation resources low‐yield crops and damaged crops in supply chain systems this leads to dissatisfaction among agriculture stakeholders such as quality control units logistics suppliers and customers motivated from the above discussions the survey presents the advantages of integrating blockchain bc with pi to handle issues pertaining to security trust and transactional payments among agriculture stakeholders the survey is directed to achieve threefold objective‐ attack models and countermeasures in pi systems integration of bc in pi to mitigate attack models and research challenges in deploying bc in pi to address the first objective the survey proposes an in‐depth comparative analysis of traditional irrigation systems with pi with discussions on attack models to address the second objective the survey proposes an integration model of bc with pi to secure iot sensors and maintain trust and transparency among stakeholders finally the survey addresses the open research challenges of deploying bc in pi‐based irrigation systems and presents a case‐study of agrichain as an industry ready‐solution that envisions bc with pi ecosystem thus the proposed survey acts as a roadmap for agriculture industry stakeholders researchers to deploy bc in iot‐based pi that leverages an efficient robust trust‐worthy and secure ecosystem
the constitutive androstane receptor car is the essential regulator of genes involved both in xenobiotic and endobiotic metabolism diazepam has been shown as a potent stimulator of car nuclear translocation and is assumed as an indirect car activator not interacting with the car cavity in this study we sought to determine if diazepam is a ligand directly interacting with the car ligand binding domain lbd and if it regulates its target genes in a therapeutically relevant concentration we used different car constructs in translocation and luciferase reporter assays recombinant carlbd in a trfret assay and target genes induction studied in primary human hepatocytes phhs heparg cells and in car humanized mice we also used in silico docking and carlbd mutants to characterize the interaction of diazepam and its metabolites with the car cavity diazepam and its metabolites such as nordazepam temazepam and oxazepam are activators of carala in translocation and twohybrid assays and fit the car cavity in docking experiments in gene reporter assays with car3 and in the trfret assay only diazepam significantly interacts with carlbd diazepam also promotes upregulation of cyp2b6 in phhs and in heparg cells however in humanized car mice diazepam significantly induces neither cyp2b6 nor cyp2b10 genes nor does it regulate critical genes involved in glucose and lipids metabolism and liver proliferation thus we demonstrate that diazepam interacts with human carlbd as a weak ligand but it does not significantly affect expression of tested car target genes in car humanized mice
italicobjectiveitalic this study describes the development and offline validation of a heuristic algorithm for accurate prediction of ground terrain in a lower limb prosthesis this method is based on inference of the ground terrain geometry using estimation of prosthetic limb kinematics during gait with a single integrated inertial measurement unit italicmethodsitalic we asked five subjects with belowknee amputations to traverse level ground stairs and ramps using a highrangeofmotion powered prosthesis while internal sensor data were remotely logged we used these data to develop three terrain prediction algorithms the first two employed stateoftheart machine learning approaches while the third was a directly tuned heuristic using thresholds on estimated prosthetic ankle joint translations and ground slope we compared the performance of these algorithms using resubstitution error for the machine learning algorithms and overall error for the heuristic algorithm italicresultsitalic our optimal machine learning algorithm attained a resubstitution error of inlineformulatexmath notationlatex34texmathinlineformula using 45 features while our heuristic method attained an overall prediction error of inlineformulatexmath notationlatex28texmathinlineformula using only 5 features derived from estimation of ground slope and horizontal and vertical ankle joint displacement compared with pattern recognition the heuristic performed better on each individual subject and across both level and nonlevel strides italicconclusion and significanceitalic these results demonstrate a method for heuristic prediction of ground terrain in a powered prosthesis the method is more accurate more interpretable and less computationally expensive than machine learning methods considered stateoftheart for intent recognition and relies only on integrated prosthesis sensors finally the method provides intuitively tunable thresholds to improve performance for specific walking conditions
knowledge distillation has been used to transfer knowledge learned by a sophisticated model teacher to a simpler model student this technique is widely used to compress model complexity however in most applications the compressed student model suffers from an accuracy gap with its teacher we propose extracurricular learning a novel knowledge distillation method that bridges this gap by 1 modeling student and teacher output distributions 2 sampling examples from an approximation to the underlying data distribution and 3 matching student and teacher output distributions over this extended set including uncertain samples we conduct rigorous evaluations on regression and classification tasks and show that compared to the standard knowledge distillation extracurricular learning reduces the gap by 46 to 68 this leads to major accuracy improvements compared to the empirical risk minimizationbased training for various recent neural network architectures 16 regression error reduction on the mpiigaze dataset 34 to 91 improvement in top1 classification accuracy on the cifar100 dataset and 29 top1 improvement on the imagenet dataset
in hospitality and tourism research pvalues continue to be the most common approach to hypothesis testing in this article we elaborate on some of the misconceptions associated with pvalues we discuss the advantages of the bayesian approach and provide several important practical recommendations and considerations for bayesian hypothesis testing with the main challenge of bayesian hypothesis testing being the sensitivity of the results to prior distributions we present in this article several priors that can be used for that purpose and illustrate their performance in a regression context
although a cohesive society is difficult to maintain social cohesion is seen as ideal to unite a heterogeneous community studies in the past have focused on social cohesion within real life context involving facetoface encounters nowadays it was noted that social networking sites sns have a role in determining the level of social cohesion this study aims to examine the role of sns in determining the level of social cohesiveness among its virtual youth communities’ members a total of 600 multiethnic youths in urban and rural areas in malaysia were selected based on a multistage cluster sampling technique the respondents were asked to state their level of agreement on sense of togetherness sense of belonging social trust and social interaction through a distributed selfadministered questionnaire the results showed that the overall level of social cohesion among virtual youth communities was only moderate and the dimension sense of belonging yielded the highest score this study implied that sns are capable of enhancing the level of social cohesion among youths in malaysia
"
purpose
this paper aims to examine the effect of inventory information sharing on inventory efficiency and its intervening effect of information technology it capability in manufacturing firms


designmethodologyapproach
stratified random sampling and filter questions selected targeted respondents and an online survey collected 124 completed questionnaires from malaysian manufacturing firms partial least squares structural equation modeling plssem examined the structural model and hypothesis statement an analysis of importanceperformance map analysis ipma test identified the relative importance drivers of inventory efficiency


findings
the findings showed that enhanced it capabilities in manufacturing firms mediate a positive relationship between inventory sharing and inventory efficiency


research limitationsimplications
this study portrays the relationship between inventory level demand and information sharing the research was carried out only within malaysian manufacturing firms


practical implications
these findings will enable the management of manufacturing firms to design and visualise their inventory levels and share best practices across supply chain networks to achieve effective and optimised inventory planning


social implications
this study illustrates an intervention model that offers a direct and indirect impact of it capabilities that allow scholars to close inventories productivity gaps in research


originalityvalue
this paper extends the limited literature on the sharing of inventory information and inventory productivity notably from a strategic management perspective the findings help scholars clearly understand the information systems capability and its mediating impact on information sharing and inventory efficiency’s relationship in the manufacturing sector moreover demand information sharing affected the dynamic supply chain
"
novel advanced driver assistance systems are being increasingly designed also for twowheeled vehicles both to increase safety and to improve the vehicle usability one of the most challenging problems is ensuring roll stability at lowspeed or standstill useful to face dangerous conditions for instance those related to impending rider’s sickness in this work we develop an active gyroscopic controller to achieve stabilization and make the motorcycle sustain itself while at standstill the parametric uncertainties that affect the system the errors in the roll angle estimation the unbalanced loads and the coupling between the roll dynamics and the rider’s posture make the control problem non trivial thus a 3rd order sliding mode controller is designed and experimentally tested simulations allow to understand the tuning principle of the controller while experimental results on an instrumented motorcycle equipped with a gyroscopic actuator prove that the proposed approach allows to achieve the desired vertical stabilization
herein we study the problem of recovering a density operator from a set of compatible marginals motivated by limitations of physical observations given that the set of compatible density operators is not singular we adopt jaynes’ principle and wish to characterize a compatible density operator with maximum entropy we first show that comparing the entropy of compatible density operators is complete for the quantum computational complexity class qszk even for the simplest case of 3chains then we focus on the particular case of quantum markov chains and trees and establish that for these cases there exists a procedure polynomial in the number of subsystems that constructs the maximum entropy compatible density operator moreover we extend the chow–liu algorithm to the same subclass of quantum states
approximate computing ac is an emerging computation paradigm that utilizes many applications’ intrinsic error resilience to improve power and energy efficiency several approaches have been proposed to identify the noncritical computations by analyzing the output sensitivity to the accuracy of the results and then perform approximation on these computations however these static approaches only use the prior knowledge eg input ranges for analysis and fail to consider the runtime information which limits the energy saving and incurs large computation error in this paper we propose a runtime ac framework to solve this problem the basic idea is to use a low cost method to estimate the impact of each immediate input value to the accuracy of computation at every node in the data flow graph dfg and then decide whether we should simply use the estimated value or perform an accurate computation our novel runtime estimation method is based on converting data to the logarithmic representation we propose two algorithms to make the decision at certain nodes whether an accurate computation will be needed to balance energy saving and computation error compared to the static dfg node cutting approach our approach’s estimation accuracy is inlineformula texmath notationlatex32 times  texmathinlineformula better to achieve the same amount of energy saving furthermore we combine our dynamic design methodology with the traditional static approach and propose a new algorithm to improve the runtime efficiency we apply this algorithm to two machine learning algorithms inlineformula texmath notationlatexk texmathinlineformulameans and perceptron the results show that we can save up to 5722 computations with only 027 error in inlineformula texmath notationlatexk texmathinlineformulameans and 9141 with 072 error in perceptron
building envelopes separate the confined interior world engineered for human comfort and indoor activity from the exterior world with its uncontainable climatic forces and manmade immission in the future active sustainable and lightweight building skins are needed to serve as an adaptive interface to govern the buildingphysical interactions between these two worlds this article provides conceptual and experimental results regarding the integration of ionic electroactive polymer sensors and actuators into fabric membranes the ultimate goal is to use this technology for adaptive membrane building skins these devices have attracted high interest from industry and academia due to their small actuation voltages relatively large actuation and sensing responses and their flexible and soft mechanical characteristics however their complex manufacturing process sophisticated material compositions and their environmental sensitivity have limited the application range until now the article describes the potentials and limitations of employing such devices for two different adaptive building functionalities first as a means of ventilation control and humidity regulation by embedding small actuated apertures into a fabric membrane and second as flexible energy and costefficient distributed sensors for external load monitoring of such structures the article focusses on designing building and testing two experimental membrane demonstrators with integrated polymer actuators and sensors it addresses the challenges encountered and draws conclusions for potential future optimization at the device and system level
as businesses grow trust among participating stakeholders assumes prime importance the transparency and efficiency in the transactions that occur in these businesses are equally valuable as the profits digitization of the economy helps easeout the conduct of business however the increase in vulnerability to cyberattacks is also on the rise blockchain technology revolutionizes the way digital transactions can occur and holds much promise in securing the flow of information that primarily drives them such a distributed ledger framework allows the traceability of a transaction through its immutable chain of blocks each block registers a timestamped information set verified by all the stakeholders involved in the business these sets of features and much more make the blockchain technology an immensely powerful force to bring in transparency efficiency and trust in various industries the paper’s primary focus is given the features that empower a blockchain to facilitate various tasks securely efficiently and smoothly in a power sector it also gives impressions of pilot projects in india with blockchain applications in different sectors glimpses of a prototype developed by the authors for managing the trading of rooftop solar energy among a set of consumers have been provided as one of the applications in power distribution
in this paper a calibration approach based on transfer function extraction for the cartesian vector modulator vm is presented three kinds of vm models—the ideal vm model the frequencydependent vm model and the modified frequencydependent vm model are introduced in the proposed calibration approach the calibration approach starts with an initialization of the transfer function of the modified frequencydependent vm model then the parameters of the transfer function are modified and extracted from the data of the measured transmission state transmission amplitude and phase of the actual vm by iteration until the transmission state predicted by the extracted transfer function agrees well with the measured transmission state subsequently the extracted transfer function of the modified frequencydependent vm model is capable of describing the transmission characteristics of the actual vm and the calibrated baseband control voltages for the desired transmission amplitude and phase of the actual vm are able to be obtained by using the extracted transfer function an actual vm is used as an example to verify this method by adopting the proposed method the maximum amplitude and phase errors at different complex gain setpoints are reduced to 005 db and 03° respectively after only two iteration steps since the actual vm is able to be accurately calibrated in only a few iteration steps the results reveal that high accuracy and efficiency can be obtained in this calibration technique which is well suited for applications involving highaccuracy calibration realtime calibration and multichannel vm system calibration
many university campuses have been decreasing the environmental impacts and costs associated with electricity consumption through the implementation of energy efficiency programs as well as the installation of renewable energy generation as a next step such buildings must be equipped with technologies able to provide the flexibility required to increase the matching between renewable generation and demand in order to enable largescale integration of renewable generation with such objective this paper presents a microgrid constituted by photovoltaic generation lithiumion battery storage unidirectional and bidirectional charging of electric vehicles and controllers for the space conditioning systems such microgrid was implemented in the department of electrical and computer engineering  university of coimbra for supporting sustainable energy systems operation ensuring the optimized integration of renewable generation in large buildings simultaneously the microgrid enables the assessment of new smart grid solutions acting as a testbed for the research developed on flexibility options for future power systems
vigor identification in sweet corn seeds is important for seed germination crop yield and quality in this study hyperspectral image hsi technology integrated with germination tests was applied for feature association analysis and germination performance prediction of sweet corn seeds in this study 89 sweet corn seeds 73 for training and the other 16 for testing were studied and hyperspectral imaging at the spectral range of 400–1000 nm was applied as a nondestructive and accurate technique to identify seed vigor the root length and seedling length which represent the seed vigor were measured and principal component regression pcr partial least squares pls and kernel principal component regression kpcr were used to establish the regression relationship between the hyperspectral feature of seeds and the germination results specifically the relevant characteristic band associated with seed vigor based on the highest correlation coefficient hcc was constructed for optimal wavelength selection the hyperspectral data features were selected by genetic algorithm ga successive projections algorithm spa and hcc the results indicated that the hyperspectral data features obtained based on the hcc method have better prediction results on the seedling length and root length than spa and ga by comparing the regression results of kpcr pcr and pls it can be concluded that the hyperspectral method can predict the root length with a correlation coefficient of 07805 the prediction results of different feature selection and regression algorithms for the seedling length were up to 06074 the results indicated that based on hyperspectral technology the prediction of seedling root length was better than that of seed length
this paper proposes a simple locally active memristor whose state equation only consists of linear terms and an easily implementable function and design for its circuit emulator the effectiveness of the circuit emulator is validated using breadboard experiments and numerical simulations the proposed circuit emulator has a simple structure which not only reduces costs but also increases its application value the poweroff plot and dc vi loci verify that the memristor is nonvolatile and locally active respectively this locally active memristor exhibits low cost easy physical implementation and wide locally active region characteristics furthermore a neural model composed of two 2d hr neurons based on the proposed locally active memristor is established it is found that complicated firing behaviors occur only within the locally active region a new phenomenon is also discovered that shows coexisting position symmetry for different attractors the firing pattern transition is then observed via bifurcation analysis the results of matlab simulations are verified from the hardware circuits
medical errors are a major public health concern and a leading cause of death worldwide many healthcare centers and hospitals use reporting systems where medical practitioners write a preliminary medical report and the report is later reviewed revised and finalized by a more experienced physician the revisions range from stylistic to corrections of critical errors or misinterpretations of the case due to the large quantity of reports written daily it is often difficult to manually and thoroughly review all the finalized reports to find such errors and learn from them to address this challenge we propose a novel ranking approach consisting of textual and ontological overlaps between the preliminary and final versions of reports the approach learns to rank the reports based on the degree of discrepancy between the versions this allows medical practitioners to easily identify and learn from the reports in which their interpretation most substantially differed from that of the attending physician who finalized the report this is a crucial step towards uncovering potential errors and helping medical practitioners to learn from such errors thus improving patientcare in the long run we evaluate our model on a dataset of radiology reports and show that our approach outperforms both previouslyproposed approaches and more recent language models by 45 to 154
abstract rapid advances in artificial intelligence ai and machine learning are creating products and services with the potential not only to change the environment in which actuaries operate but also to provide new opportunities within actuarial science these advances are based on a modern approach to designing fitting and applying neural networks generally referred to as “deep learning” this paper investigates how actuarial science may adapt and evolve in the coming years to incorporate these new techniques and methodologies part 1 of this paper provides background on machine learning and deep learning as well as an heuristic for where actuaries might benefit from applying these techniques part 2 of the paper then surveys emerging applications of ai in actuarial science with examples from mortality modelling claims reserving nonlife pricing and telematics for some of the examples code has been provided on github so that the interested reader can experiment with these techniques for themselves part 2 concludes with an outlook on the potential for actuaries to integrate deep learning into their activities finally a supplementary appendix discusses further resources providing more indepth background on machine learning and deep learning
heart rate hr is an essential clinical measure for the assessment of cardiorespiratory instability since communities of color are disproportionately affected by both covid19 and cardiovascular disease there is a pressing need to deploy contactless hr sensing solutions for highquality telemedicine evaluations existing computer vision methods that estimate hr from facial videos exhibit biased performance against dark skin tones we present a novel physicsdriven algorithm that boosts performance on darker skin tones in our reported data we assess the performance of our method through the creation of the first telemedicinefocused remote vital signs dataset the vital dataset 432 videos 864 minutes of 54 subjects with diverse skin tones are recorded under realistic scene conditions with corresponding vital sign data our method reduces errors due to lighting changes shadows and specular highlights and imparts unbiased performance gains across skin tones setting the stage for making medically inclusive noncontact hr sensing technologies a viable reality for patients of all skin tones
traumatic brain injury tbi is often characterized by alterations in brain connectivity we explored connectivity alterations from a network perspective using graph theory and examined whether injury severity affected structural connectivity and modulated the association between brain connectivity and cognitive deficits posttbi we performed diffusion imaging network analysis on chronic tbi patients with different injury severities and healthy subjects from both global and local perspectives we found an effect of injury severity on network strength in addition regions which were considered as hubs differed between groups further exploration of graph measures in the determined hub regions showed that efficiency of six regions differed between groups an association between reduced efficiency in the precuneus and nonverbal abstract reasoning deficits calculated using actual preinjury scores was found in the controls but was lost in tbi patients our results suggest that disconnection of network hubs led to a less efficient network which in turn may have contributed to the cognitive impairments manifested in tbi patients we conclude that injury severity modulates the disruption of network organization reflecting a “dose response” relationship and emphasize the role of efficiency as an important diagnostic tool to detect subtle brain injury specifically in mild tbi patients
internet of things iot is becoming a widespread reality as interconnected smart devices and sensors have overtaken the it market and invaded every aspect of the human life this kind of development while already foreseen by it experts implies additional stress to already congested networks and may require further investments in computational power when considering centralized and cloud based solutions that is why a common trend is to rely on local resources provided by smart devices themselves or by aggregators to deal with part of the required computations this is the base concept behind fog computing which is becoming increasingly adopted as a distributed calculation solution in this paper a methodology initially developed within the toreador european project for the distribution of big data computations over cloud platforms will be described and applied to an algorithm for the prediction of energy consumption on the basis of data coming from home sensors already employed within the cossmic european project the objective is to demonstrate that by applying such a methodology it is possible to improve the calculation performances and reduce communication with centralized resources
word embeddings have rapidly become an allpurpose tool for a diverse range of real world applications this development is nurtured by the availability and applicability of pretrained models however their usage faces the risk of being inaccurate when used in domains different from the ones they were trained on in this paper we formulate the adaptation of word embeddings as a vector multiplication problem which enables us to apply search methods to explore potential word embedding adaptations with respect to their semantic correctness to assess the effectiveness of our proposal we empirically investigate the use of both local and global searchbased approaches ie hill climbing tabu search and genetic algorithm in order to maximise the semantic correctness of a popular word2vec pretrained model namely googlenews when applied to another domain ie the men dataset the results of our study reveal that hill climbing tabu search and genetic algorithm perform equally well and all outperform the original googlenews model as well as a baseline model based on random search this shows that optimising word embeddings with searchbased approaches is possible and effective
functional magnetic resonance imaging fmri is one of the most popular methods for studying the human brain it measures brain activity by detecting local changes of blood oxygen level dependent bold signal in the brain over time and can be used in both taskrelated and restingstate studies in taskrelated studies our aim is to determine which brain areas are activated when a specific task is performed various unsupervised multivariate statistical methods are being increasingly employed in fmri data analysis their main goal is to extract information from a dataset often with no prior knowledge of the experimental conditions generalized canonical correlation analysis gcca is a well known statistical method that can be considered as a way to estimate a linear subspace which is common to multiple random linear subspaces we propose a new fmri data generating model which takes into consideration the existence of common taskrelated and restingstate components we estimate the common spatial taskrelated component via a twostage gcca we test our theoretical results using realworld fmri data our experimental findings corroborate our theoretical results rendering our approach a very good candidate for multisubject taskrelated fmri processingclinical relevance—this work provides a set of methods for amplifying and recovering commonalities across subjects that appear in data from multisubject taskrelated fmri experiments
abstract biomimetics is the imitation of the systems models and nature elements for the motive of elucidating complex biological problems in the current century the use of nanoparticles in the treatment of biological systems such as cancer and atherosclerosis treatment is common in worldwide due to many properties of nanoparticles in general the size of nanoparticles are very small so that they can pierce extensively throughout the body keeping this in mind the current article focusses the blood treated as phanthientanner nanofluid transport in the intrauterine cavity walls under the various effects such as induced magnetic field and solar radiation for the formulation the cartesian coordinate is used utilizing appropriate transformations and dimensionless quantities the dimensional laboratory frame system converted to dimensionless wave frame system the resulting system of equations are highly nonlinear so we have used the regular perturbation technique to find the solution streamlines velocity pressure rise temperature and concentration have been plotted to see the blood transport in various situations it is noticed from our analysis that the external effect of radiation enhances the peristaltic pumping the nanoparticle temperature and concentration are enhanced with the increase of brownian motion parameter the sharp reductions in the induced magnetic field are seen with stronger hartmann number
the process of acquiring largescale complex systems is usually characterized by cost and schedule overruns we develop and evaluate a model of the acquisition process that accounts for the strategic behavior of different parties specifically we cast our model in terms of governmentfunded projects and assume the following steps first the government publishes a request for bids then private firms offer their proposals in a bidding process and the winner bidder enters in a contract with the government the contract describes the system requirements and the corresponding monetary transfers for meeting them the winner firm devotes effort to deliver a system that fulfills the requirements this can be assumed as a game that the government plays with the bidder firms the objective of this paper is to study how different parameters in the acquisition procedure affect the bidders’ behaviors and therefore the utility of the government using reinforcement learning we seek to learn the optimal policies of involved actors in this game in particular we study how the requirements contract types such as costplus and incentivebased contracts number of bidders problem complexity etc affect the acquisition procedure furthermore we study the bidding strategy of the private firms and how the contract types affect their strategic behavior also we study the effects of different contract types on the winner’s optimal effort level necessary to meet the system requirements we run exhaustive numerical simulations which show that costplus contracts are particularly prone to strategic misrepresentation this analysis can be expanded to help the government select procedures that achieve specific goals such us minimizing cost overruns
the orchestration of applications and their components over heterogeneous clouds is recognized as being critical in solving the problem of vendor lockin with regards to distributed and cloud computing there have been recent strides made in the area of cloud application orchestration with emergence of the tosca standard being a definitive one although orchestration by itself provides a considerable amount of benefit to consumers of cloud computing services it remains impractical without a compelling reason to ensure its utilization by cloud computing consumers if there is no measurable benefit in using orchestration then it is likely that clients may opt out of using it altogether in this paper we present an approach to cloud orchestration that aims to combine an orchestration model with a cost and policy model in order to allow for costaware application orchestration across heterogeneous clouds our approach takes into consideration the operating cost of the application on each provider while performing a forward projection of the operating cost over a period of time to ensure that cost constraints remain unviolated this allows us to leverage the existing state of the art with regards to orchestration and modeldriven approaches as well as tie it to the operations of cloud clients in order to improve utility through this study we were able to show that our approach was capable of providing not only scaling features but also orchestration features of application components distributed across heterogeneous cloud platforms
as a result of an increasingly automatized and digitized industry processes are becoming more complex augmented reality has shown considerable potential in assisting workers with complex tasks by enhancing user understanding and experience with spatial information however the acceptance and integration of ar into industrial processes is still limited due to the lack of established methods and tedious integration efforts meanwhile deep neural networks have achieved remarkable results in computer vision tasks and bear great prospects to enrich augmented reality applications in this paper we propose an augmentedrealitybased human assistance system to assist workers in complex manual tasks where we incorporate deep neural networks for computer vision tasks more specifically we combine augmented reality with object and action detectors to make workflows more intuitive and flexible to evaluate our system in terms of user acceptance and efficiency we conducted several user studies we found a significant reduction in time to task completion in untrained workers and a decrease in error rate furthermore we investigated the users learning curve with our assistance system
computing with words and perceptions is a technique that has been proven to be incredibly powerful to understand and compute with entities with imprecise nature such as natural language and human perceptions to be able to correctly handle these elements however a high degree of humangenerated data is needed for the precisiation of meaning the first step of computing with words and perceptions systems this can be achieved via direct user intervention or with a  not existing yet  data set representing the meaning of words for several people this aspect reduces the practical usability of computing with words and perceptions to cases where only an extremely reduced set of words needs to be understood precisiated in the current article an algorithm able to fully automate the precisiation of the meaning of quantitative and qualitative adjectives using already available data  from a thesaurus  is proposed and analyzed in an exploratory study with six experts as participants results indicate an accuracy of the algorithm close to humanlevel accuracy for the task of precisiating another persons perceptions and help to understand some improvements to be taken into account for future developments of the algorithm the presented algorithm represents a promising step toward humanlike information processing
multiple hypothesis testing requires a control procedure the error probabilities in statistical testing compound when several tests are performed for the same conclusion a common type of multiple hypothesis testing error rates is the familywise error rate fwer which measures the probability that any one of the performed tests rejects its null hypothesis erroneously these are often controlled using bonferroni’s method or later more sophisticated approaches all of which involve replacing the test level α with αk reducing it by a factor of the number of simultaneous tests performed common paradigms for hypothesis testing in persistent homology are often based on permutation testing however increasing the number of permutations to meet a bonferronistyle threshold can be prohibitively expensive in this paper we propose a null model based approach to testing for acyclicity ie trivial homology coupled with a familywise error rate fwer control method that does not suffer from these computational costs
with the rise in popularity of machine and deep learning models there is an increased focus on their vulnerability to malicious inputs these adversarial examples drift model predictions away from the original intent of the network and are a growing concern in practical security in order to combat these attacks neural networks can leverage traditional image processing approaches or stateoftheart defensive models to reduce perturbations in the data defensive approaches that take a global approach to noise reduction are effective against adversarial attacks however their lossy approach often distorts important data within the image in this work we propose a visual saliency based approach to cleaning data affected by an adversarial attack our model leverages the salient regions of an adversarial image in order to provide a targeted countermeasure while comparatively reducing loss within the cleaned images we measure the accuracy of our model by evaluating the effectiveness of stateoftheart saliency methods prior to attack under attack and after application of cleaning methods we demonstrate the effectiveness of our proposed approach in comparison with related defenses and against established adversarial attack methods across two saliency datasets our targeted approach shows significant improvements in a range of standard statistical and distance saliency metrics in comparison with both traditional and stateoftheart approaches
this paper analyzes the role of liquidity regulation and its interaction with capital requirements we first introduce costly capital in a bank run model with endogenous bank portfolio choice and run probability and show that capital regulation is the only way to restore the efficient allocation we then enrich the model to include re sales and show that capital and liquidity regulation are complements the key implications of our analysis are that the optimal regulatory mix should be designed considering both sides of banks balance sheet and that its effectiveness depend on the costs of both capital and liquidity
silicon carbide sic mosfets are widely acknowledged for low loss fast switching and remarkable thermal conductivity compared to silicon si counterparts however reliability remains a significant hindrance for the wide adoption in power electronics condition monitoring of sic devices in sicbased power converters addresses the reliability issues by providing an early sign of potential failure one of the most consistent failure precursors of degraded sic mosfets is an increase of gate leakage current this paper presents a method of condition monitoring the sic mosfets with an external circuit that indirectly estimates the gate leakage current during operation experimental results obtained with a prototype circuit validate the analysis and method reliable information can be obtained for condition monitoring at various duty ratios dclink voltages and load currents in the test setup with proper calibration this method opens opportunities to perform prognostic and health monitoring of sic devices using the estimated onstate gate leakage current
we discuss the teleportation of particles in an environment of an nbody system in this case we can change a manybody system into an arbitrary shape in space by teleporting some or all the constituent particles and thus we call the quantum teleportation under this circumstance as quantum teletransformation qtt the particular feature of qtt is that the wave function of the internal degrees of freedom remains the same while the spatial wave function experiences a drastic change the notion of qtt provides conceptual and pedagogical convenience for quantum information processing in view of qtt teleportation is the change of a single particle in space while entanglement swapping is the change of one particle of an entangled pair
adaptive curriculum sequencing acs problem has been treated in the literature with metaheuristic approaches and with techniques that reduce it to a monoobjective optimization problem the objectives used are mostly conflicting that is an improvement in one of these objectives does not necessarily result in the improvement of the others thus the objective of this article is to propose a new approach to acs based on many objectives optimization using the nsgaiii algorithm although it is still an unexplored solution it proved to be adequate for the problem according to experiments carried out in the laboratory resumo o problema do sequenciamento curricular adaptativo sca vem sendo tratado na literatura com técnicas de inteligência computacional e com abordagens que o reduzem a um problema de otimização monoobjetivo em geral os objetivos tratados são em sua maioria conflitantes ou seja uma melhora em um desses objetivos não resulta necessariamente na melhora dos demais este trabalho propõe uma nova abordagem para o sca baseada em otimização de problema multiobjetivo utilizando o algoritmo nsgaiii embora seja uma solução ainda não explorada os experimentos realizados mostraram que a abordagem é adequada ao problema
abstract this study aims to improve the existing problems of intellectual capital ic measurement methods including those with unclear and unspecific calculation details and the inability to valuate the influences between ic measures and between ic components which is a fundamental characteristic of ic to address the fundamental issues of past methods this study integrates two multicriteria decisionmaking mcdm methods the analytic network process anp and the simple additive weighting saw method both of which provide specific and clear calculation procedures anp is adopted to manage the valuation of influences between ic elements while saw is used to solve incommensurable units of ic performance indicators and different concentrations on ic measures and components implementation of the method revealed a clear and systematic measurement procedure the proposed method could consider the relationships among ic measures as well as among ic components and presented calculated results in the form of the weights of ic measurement compositions furthermore the method maintained the crucial characteristics of ic measurement the ability to standardise units of measure and the capability to valuate the performance of ic for the appropriate measure component and holism level
abstract the recent growth in digital technologies are enabling cities to undergo transformations for streamlining smart services and offering new products digitization has changed the way citizens and stakeholders live work collaborate and communicate this disruptive change interconnects with all information systems and processes that are important for providing services although digital transformation present opportunities for achieving smart cities municipalities still struggle with managing data integration and complexity accordingly this study systematically reviews 70 research articles from 1999 to 2020 and discusses on development and stateoftheart of enterprise architecture ea and digital transformation of cities into smart cities
the present study analyzes the extent to which verbal mimicry contributes to improving outgroup perceptions in virtual reality vr interactions particularly this study examined the interplay between avatar customization the salience of a common ingroup identity and verbal mimicry in 54 vr dyads comprising users from different ethnic backgrounds participants were asked to customize their avatars to look either like themselves or someone completely different participants interacted wearing either similar avatar uniforms salient common identity or different clothes nonsalient identity the linguistic style matching lsm algorithm was employed to calculate verbal mimicry in the communication exchanged during a joint task the results suggested that verbal mimicry significantly predicted lesser social distance and greater social attraction towards the outgroup member these results are discussed in terms of their contribution for potential intergroup models of avatar communication in immersive virtual environments ives
in this work aligned discontinuous fibre composite adfrc tapes were developed and investigated as precursors for a novel 3d printing filament adfrcs have the potential to achieve mechanical performance comparable to continuous fibre reinforced composites given sufficient fibre length and high level of alignment and avoid many of the manufacturing difficulties associated with continuous fibres eg wrinkling bridging and corner radii constraints their potential use for fused filament fabrication fff techniques was investigated here an extensive downselection process of thermoplastic matrices was performed as matrix properties significantly impact both the processing and performance of the filament this resulted in four candidate polymers abs pla nylon petg which were used to manufacture adfrc tapes with a vf of 125 using the high performance discontinuous fibre hiperdif technology and an inhouse developed continuous consolidation module tensile stiffness and strength up to 30 gpa and 400 mpa respectively were recorded showing that a discontinuous fibre filament has the potential to compete with continuous fibre filaments
adversarial examples are inputs subtly perturbed to produce a wrong prediction in machine learning models while remaining perceptually similar to the original input to ﬁnd adversarial examples some attack strategies rely on linear approximations of di ﬀ erent properties of the models this opens a number of questions related to the accuracy of such approximations in this paper we focus on deepfool a stateoftheart attack algorithm which is based on e ﬃ ciently approximating the decision space of the target classiﬁer to ﬁnd the minimal perturbation needed to fool the model the objective of this paper is to analyze the feasibility of ﬁnding inaccuracies in the linear approximation of deepfool with the aim of studying whether they can be used to increase the e ﬀ ectiveness of the attack we introduce two strategies to e ﬃ ciently explore gaps in the approximation of the decision boundaries and evaluate our approach in a speech command classiﬁcation task
this letter proposes a lowcomplexity suboptimal twodimensional crest factor reduction 2dcfr scheme for dualband signals based on noise segment cancellation which is approximated by combining the pulse kernel signals of two bands with the dualband combining effect being taken into account the proposed method reduces the calculation of fast fourier transform fft and inverse fast fourier transform ifft due to requiring no filtering at the end of the letter the computational complexity and performance of the algorithm are analysed and compared the numerical results show that the computational complexity of the proposed solution is significantly reduced with similar error vector magnitudeevm performance as compared to traditional methods
the wettest 2018 long rainy season march to may mam resulted in daily intensive rainfall events in east africa that have seriously affected the environment and economy in many countries land subsidence is one of the environmental disasters that has occurred due to the long rainy season in kenya for many years however it has received limited scientific attention in this paper we incorporate hydrological soil moisture active–passive smap and loading models and geodetic data global positioning system gps and interferometric synthetic aperture radar insar to study hydrological changes and their associated subsidence potential in kenya results show that widespread subsidence of more than 20 mm was associated with the mam season in kenya during 2018 based on sbas insar measurements the high values of land subsidence were well correlated with the areas of intense flooding during the mam season the widespread subsidence during the wet season has implications for the stability of the earth’s surface during the season rather than creating the possibility of potential stresses along active faults these stresses may trigger seismicity that is expected to pose risks to urban features the results of the current study can help governmental authorities to adopt proper urban planning that avoids or minimizes the risks of land subsidence in the areas of sinkholes
abstract based on the technology of rs  gis and the remote sensing images of jixi city in 1993 2003 and 2015 this study analyzes quantitatively the temporal and spatial evolution characters of landscape pattern in the downtown area of jixi city in china in the past 22 years the research results show that the area of mine land farmland and construction land increased greatly from 1993 to 2015 as an important part of the urban natural ecosystem the area of water area forest and grassland are also decreasing year by year in the past 22 years due to the impact of human activities such as coal resources development the coal mine land in the central urban area is distributed in the agricultural farming area and the edge zone with different landscape types on the whole the fragmentation degree and diversity of regional landscape are increasing and human activities have a significant impact on the urban landscape pattern of jixi city this paper attempts to explore the general law of landscape pattern change under the influence of human activities in mining cities and to provide a reference for urban land use planning and scientific decisionmaking of relevant government departments
detecting manipulated images and videos is an important aspect of digital media forensics due to severe discriminative information loss caused by resolution degradation the performance of most existing methods is significantly reduced on low resolution manipulated images to address this issue we propose an artifactsfocus superresolution afsr module and a twostream feature extractor tfe the afsr recovers facial cues and manipulation artifact details using an autoencoder learned with an artifacts focus training loss the tfe adopts a twostream feature extractor with key pointsbased fusion pooling to learn discriminative facial representations these two complementary modules are jointly trained to recover and capture distinctive manipulation artifacts in low resolution images extensive experiments on two benchmarks including faceforensics and deepfaketimit evidence the favorable performance of our method against other stateoftheart methods
engagement is a fuzzy concept in the present work we operationalize engagement mechanistically by linking it directly to human behaviour and show that the construct of engagement can be used for shaping and interpreting datadriven methods first we outline a formal framework for engagement modelling second we expanded on our previous work on theoryinspired datadriven approaches to better model the engagement process by proposing a new modelling technique the melchoir model third we illustrate how through model comparison and inspection we can link machinelearned models and underlying theoretical frameworks finally we discuss our results in light of a theorydriven hypothesis and highlight potential application of our work in industry
a spacepolarization domain multibeam parameter estimation algorithm is proposed for the conformal arrays composed of dual polarized antennas the polarization signal can be broken down into two orthogonal polarization components the dual polarized conformal array has strong polarization processing ability which can form a multibeam covering spacepolarization domain the multibeam contains angle and polarization information and each beam only receives signal of a certain angle and polarization state therefore the signals of different angles and polarization states have different response distributions to these beams and this characteristic can be used for the direction of arrival doa and polarization parameter estimation in addition the process between the polarization parameter and the angle estimation does not affect each other and can be estimated simultaneously finally simulation results demonstrate the effectiveness of the proposed algorithm
massive multipleinput multipleoutput communications can achieve highlevel security by concentrating radio frequency signals towards the legitimate users however this system is vulnerable in a rician fading environment if the eavesdropper positions itself such that its channel is highly “similar” to the channel of a legitimate user to address this problem this paper proposes an angle aware user cooperation aauc scheme which avoids direct transmission to the attacked user and relies on other users for cooperative relaying the proposed scheme only requires the eavesdropper’s angle information and adopts an angular secrecy model to represent the average secrecy rate of the attacked system with this angular model the aauc problem turns out to be nonconvex and a successive convex optimization algorithm which converges to a karushkuhntucker solution is proposed furthermore a closedform solution and a bregman firstorder method are derived for the cases of largescale antennas and largescale users respectively extension to the intelligent reflecting surfaces based scheme is also discussed simulation results demonstrate the effectiveness of the proposed successive convex optimization based aauc scheme and also validate the lowcomplexity nature of the proposed largescale optimization algorithms
abstract
abstract this article responds to recent calls for social justiceoriented work in technical and professional communication detailing moments from a participatory photovoice project with community organizers working toward a more just regional economy by juxtaposing participatory action research methods and the rhetorical concept of metis or embodied rhetorical cunning this article highlights how reversals of power might transform research projects for all parties involved and how disenfranchised groups might challenge extractive practices draining their communities
in this tutorial we introduce basic conceptual elements to understand and build a gatebased superconducting quantum computing system
in this paper closed forms of the sum formulas ∑n k0 kx w 2 k and ∑n k1 kx w 2 −k for the squares of generalized fibonacci numbers are presented as special cases we give sum formulas of fibonacci lucas pell pelllucas jacobsthal jacobsthallucas numbers we present the proofs to indicate how these formulas in general were discovered of course all the listed formulas may be proved by induction but that method of proof gives no clue about their discovery our work generalize second order recurrence relations
we address the problem of incrementally modeling and forecasting longterm goals of a firstperson camera wearer what the user will do where they will go and what goal they seek in contrast to prior work in trajectory forecasting our algorithm darko goes further to reason about semantic states will i pick up an object and future goal states that are far in terms of both space and time darko learns and forecasts from firstperson visual observations of the users daily behaviors via an online inverse reinforcement learning irl approach classical irl discovers only the rewards in a batch setting whereas darko discovers the transitions rewards and goals of a user from streaming data among other results we show darko forecasts goals better than competing methods in both noisy and ideal settings and our approach is theoretically and empirically noregret
the use of agile methodologies in software development has grown steadily over recent years one of the main emphases of these methods is employing crossfunctional and selforganized teams and highly skilled developers in software projects in such a condition project management would be a serious concern indeed it would be confusing whether agile teams are really in need of the role of the project manager while agile methodologies do not explicitly define the role of the project manager many reports mention the existence of this role in agile projects in real environments so it seems that the existence of this role is debated conducting a systematic literature review this study tried to find out answers to the ambiguities and questions regarding the role of agile project management the role of the project manager and related issues focusing on the primary studies the results show that there is no independent job called project manager in agile methodologies however there is a need for it moreover in the absence of this role in agile methodologies and the need for it it seems that this role would be structurally different from the traditional role of the project manager in terms of responsibilities and duties finally the results show that predefined roles in agile methodologies are often responsible for the project manager duties in software teams with no project manager
youth unemployment rates are still in alerting levels for many countries among which italy direct consequences include poverty social exclusion and criminal behaviours while negative impact on the future employability and wage cannot be obscured in this study we employ survey data together with social media data and in particular likes on facebook pages to analyse personality moral values but also cultural elements of the young unemployed population in italy our findings show that there are small but significant differences in personality and moral values with the unemployed males to be less agreeable while females more open to new experiences at the same time unemployed have a more collectivist point of view valuing more ingroup loyalty authority and purity foundations interestingly topic modelling analysis did not reveal major differences in interests and cultural elements of the unemployed utilisation patterns emerged though the employed seem to use facebook to connect with local activities while the unemployed use it mostly as for entertainment purposes and as a source of news making them susceptible to misdisinformation we believe these findings can help policymakers get a deeper understanding of this population and initiatives that improve both the hard and the soft skills of this fragile population
polymers with lightresponsive groups have gained increased attention in the design of functional materials as they allow changes in polymers properties on demand and simply by light exposure for the synthesis of polymers and polymer networks with photolabile properties the introduction onitrobenzyl alcohol onb derivatives as lightresponsive chromophores has become a convenient and powerful route although onb groups were successfully exploited in numerous applications this review pays particular attention to the studies in which they were included as photoresponsive moieties in thin polymer films and functional polymer coatings the review is divided into four different sections according to the chemical structure of the polymer networks i acrylate and methacrylate ii thiolclick iii epoxy and iv polydimethylsiloxane we conclude with an outlook of the present challenges and future perspectives of the versatile and unique features of onb chemistry
in this article a novel sensing approach is presented for glucose level monitoring where a robust lowpower millimetermmwave radar system is used to differentiate between blood samples of disparate glucose concentrations in the range 05 to 35 mgml the proposed radar sensing mechanism shows greater capabilities for remote detection of blood glucose inside test tubes through detecting minute changes in their dielectric properties in particular the reflected mmwaves that represent unique signatures for the internal synthesis and composition of the tested blood samples are collected from the multichannels of the radar and analyzed using signal processing techniques to identify different glucose concentrations and correlate them to the reflected mmwave readings the mmwave spectrum is chosen for glucose sensing in this study after a set of preliminary experiments that investigated the dielectric permittivity behavior of glucoseloaded solutions across different frequency bands in this regard a newlydeveloped commercial coaxial probe kit daktl is used to characterize the electromagnetic properties of glucoseloaded samples in a broad range of frequencies from 300 mhz to 67 ghz using two different 50 ω opencoaxial probes this would help to determine the portion of the frequency spectrum that is more sensitive to slight variations in glucose concentrations as indicated by the amount of change in the dielectric constant and loss tangent parameters due to the different concentrations under test the mmwave frequency range 50 to 67 ghz has shown to be promising for acquiring both high sensitivity and sufficient penetration depth for the most interaction between the glucose molecules and electromagnetic waves the processed results have indicated the reliability of using mmwave radars in identifying changes in blood glucose levels while monitoring trends among those variations particularly blood samples of higher glucose concentrations are correlated with reflected mmwave signals of greater energy the proposed system could likely be adapted in the future as a portable noninvasive continuous blood glucose level monitoring for daily use by diabetics
a novel approach to realize broadband microwave spectrum sensing based on photonic rf channelization and compressive sampling cs is proposed the photonic rf channelization system is used to slice the input broadband signal into multiple subchannel signals with narrow bandwidth in parallel and thus the rate of pseudorandom binary sequence prbs and the bandwidth of the mzm for cs can be largely decreased it is shown that a spectrally sparse signal within a wide bandwidth can be captured with a sampling rate far lower than the nyquist rate thanks to both photonic rf channelization and cs in addition the influence of the nonideal filtering of the photonic channelizer is evaluated and a novel approach based on measuring twice is proposed to overcome the problem of frequency aliasing induced by the nonideal filtering it is demonstrated that a system with 20 gbits prbs and 25 gss digitizer can be used to capture a signal with multiple tones within a 40 ghz bandwidth which means a sampling rate 32 times lower than the nyquist rate
we compute the floquet hamiltonian hf for weakly interacting fermions subjected to a continuous periodic drive using a floquet perturbation theory fpt with the interaction amplitude being the perturbation parameter this allows us to address the dynamics of the system at intermediate drive frequencies ensuremathhbarensuremathomegadensuremathgev0ensuremathllmathcalj0 where mathcalj0 is the amplitude of the kinetic term ensuremathomegad is the drive frequency and v0 is the typical interaction strength between the fermions we compute for random initial states the fidelity f between wave functions after a drive cycle obtained using hf and that obtained using exact diagonalization ed we find that fpt yields a substantially larger value of f compared to its magnus counterpart for v0ensuremathleensuremathhbarensuremathomegad and v0ensuremathllmathcalj0 we use the hf obtained to study the nature of the steady state of an weakly interacting fermion chain we find a wide range of ensuremathomegad which leads to subthermal or superthermal steady states for finite chains the driven fermionic chain displays perfect dynamical localization for v00 we address the fate of this dynamical localization in the steady state of a finite interacting chain and show that there is a crossover between localized and delocalized steady states we discuss the implication of our results for thermodynamically large chains and chart out experiments which can test our theory
cascading failures between interdependent multilayer networks are being widely studied especially the trend of robustness caused by the interlinks between networks however few researchers pay attention to the effect of the interlink topology on the robustness of coupled networks which is a critical interlink factor of multilayer networks in this study the method frame of multilayer network experiment simulation is given through numerical simulation and actual network simulation the exhaustive method is used to enumerate all the patterns of interlink topological relations of multilayer networks threelayer or more the research verifies that the interlink topology affects the global robustness and that there exists a fragile interlink pattern in the patterns of interlink topologies the starlike interlink pattern with the most uneven interlinkdegree distribution leads to the weakest robustness the pattern with average interlinkdegree distribution reveals good global stability as a looplike pattern or entire interlink pattern in addition the influence of interlink topology is independent the simulation results are not affected by the network layer number and intraparameters including the networkgenerated form each layer of network node number and average degree of each layer of network thus ignoring the interlink topology may result in the actual system suddenly becoming vulnerable before the theoretical calculation point interlink topology as an independent factor affecting the robustness of multilayer networks should be paid more attention
"aimpurpose the purpose of this study is to discover usage differences in task performance by students of different cultures by examining procrastination patterns from a national cultural perspective and exploring the effect of multicultural virtual teamwork on students’ individual procrastination

background this study aims to examine highereducation entrepreneurial learning in the context of multicultural virtual teamwork as performed during participation on a global entrepreneurship course

methodology the methodology consists of quantitative comparative data analytics preceding and subsequent to intercultural team activities this research is based on analyses of objective data collected by moodle the lms used in the in2it project in its builtin log system from the global entrepreneurship course website which offers students diverse entities of information and tasks in the examined course there were 177 participants from three different countries united kingdom france and israel the students were grouped into 40 multicultural virtual not facetoface teams each one comprised of participants from at least two countries the primary methodology of this study is analytics of the extracted data which was transferred into excel for cleaning purposes and then to spss for analysis 

contribution this study aims to discover the effects of multicultural teamwork on individual procrastination while comparing the differences between cultures as there are only a few studies exploring this relation the uniqueness of this study is using and analyzing actual data of student procrastination from logs whereas other studies of procrastination in multicultural student teams have measured perceived procrastination collected using surveys

findings the results show statistical differences between countries in procrastination of individual assignments before team working students from uk were the most procrastinators and israeli students were the least procrastinators but almost all students procrastinated however the outcome of the teamwork was submitted almost without procrastination moreover procrastination in individual assignments performed after finishing the multicultural teamwork dramatically decreased to 10 of the students’ prior individual procrastination 

recommendations for practitioners the results from this study namely the decline of the procrastination after the multicultural virtual teamwork can be used by global firms with employees all over the world working in virtual multicultural teams such firms do not need to avoid multicultural teams working virtually as they can benefit from this kind of collaboration

recommendation for researchers these results can be also beneficial for academic researchers from different cultures and countries working together in virtual multicultural teams

impact on society understanding the positive effect of virtual multicultural teamwork in mitigating the negative tendency of students from diverse cultures to procrastinate as concluded in this study can provide a useful tool for higher education or businesses to mitigate procrastination in teamwork processes it can also be used as an experiential learning tool for improving task performance and teamwork process 

future research the relation between procrastination and motivation should be further examined in relation to multicultural virtual teams further research is needed to explore the effect of multicultural virtual teamwork during the teamwork process and the reasoning for this effect

"
the process of developingadaptive interfaces is a nontrivial task in the field of rapidly developing information technologies an intelligent systembased onan ontological model can provide assistance in the development of applications for people with special needs
leaf area index lai is an important biophysical indicator of forest health that is linearly related to productivity serving as a key criterion for potential nutrient management a single equation was produced to model surface reflectance values captured from the sentinel2 multispectral instrument msi with a robust dataset of field observations of loblolly pine pinus taeda l lai collected with a lai2200c plant canopy analyzer support vector machine svmsupervised classification was used to improve the model fit by removing plots saturated with aberrant radiometric signatures that would not be captured in the association between sentinel2 and lai2200c the resulting equation lai  0310sr − 0098 where sr  the simple ratio between nearinfrared nir and red bands displayed good performance  r 2  081 rmse  036 at estimating the lai for loblolly pine within the analyzed region at a 10 m spatial resolution our model incorporated a high number of validation plots n  292 spanning from southern virginia to northern florida across a range of soil textures sandy to clayey drainage classes well drained to very poorly drained and site characteristics common to pine forest plantations in the southeastern united states the training dataset included plotlevel treatment metrics—silviculture intensity genetics and density—on which sensitivity analysis was performed to inform model fit behavior plot density particularly when there were ≤618 trees per hectare was shown to impact model performance causing lai estimates to be overpredicted to a maximum of x i  016 silviculture intensity competition control and fertilization rates and genetics did not markedly impact the relationship between sr and lai results indicate that sentinel2’s improved spatial resolution and temporal revisit interval provide new opportunities for managers to detect withinstand variance and improve accuracy for lai estimation over current industry standard models
legislation can be viewed as a body of prescriptive rules expressed in natural language the application of legislation to facts of a case we refer to as statutory reasoning where those facts are also expressed in natural language computational statutory reasoning is distinct from most existing work in machine reading in that much of the information needed for deciding a case is declared exactly once a law while the information needed in much of machine reading tends to be learned through distributional language statistics to investigate the performance of natural language understanding approaches on statutory reasoning we introduce a dataset together with a legaldomain text corpus straightforward application of machine reading models exhibits low outofthebox performance on our questions whether or not they have been finetuned to the legal domain we contrast this with a handconstructed prologbased system designed to fully solve the task these experiments support a discussion of the challenges facing statutory reasoning moving forward which we argue is an interesting realworld task that can motivate the development of models able to utilize prescriptive rules specified in natural language
this paper proposes a method to improve the quality of omnidirectional freeviewpoint images using generative adversarial networks gan by estimating the 3d information of the capturing space while integrating the omnidirectional images taken from multiple viewpoints it is possible to generate an arbitrary omnidirectional appearance however the image quality of freeviewpoint images deteriorates due to artifacts caused by 3d estimation errors and occlusion we solve this problem by using gan and moreover by focusing on projective geometry during training we further improve image quality by converting the omnidirectional image into perspectiveprojection images copyright © 2020 by scitepress – science and technology publications lda all rights reserved
as the performance of supervised classification using convolutional neural networks cnns are affected significantly by training patches it is necessary to analyze the effects of the information content of training patches in patchbased classification the objective of this study is to quantitatively investigate the effects of class purity of a training patch on performance of crop classification here class purity that refers to a degree of compositional homogeneity of classes within a training patch is considered as a primary factor for the quantification of information conveyed by training patches new quantitative indices for class homogeneity and variations of local class homogeneity over the study area are presented to characterize the spatial homogeneity of the study area crop classification using 2dcnn was conducted in two regions anbandegi in korea and illinois in united states with distinctive spatial distributions of crops and class homogeneity over the area to highlight the effect of class purity of a training patch in the anbandegi region with high class homogeneity superior classification accuracy was obtained when using large size training patches with high class purity 71p improvement in overall accuracy over classification with the smallest patch size and the lowest class purity training patches with high class purity could yield a better identification of homogenous crop parcels in contrast using small size training patches with low class purity yielded the highest classification accuracy in the illinois region with low class homogeneity 198p improvement in overall accuracy over classification with the largest patch size and the highest class purity training patches with low class purity could provide useful information for the identification of diverse crop parcels the results indicate that training samples in patchbased classification should be selected based on the class purity that reflects the local class homogeneity of the study area
nowadays energy efficiency is recognized as a core quality attribute of applications apps running on androidpowered devices constrained by their battery indeed energy hogging apps are a liability to both the enduser and software developer yet there are very few tools available to help developers increase the quality of their native code by ridding it of energyrelated bugs android studio is the official ide for millions of developers worldwide and theres no better place to enforce green coding rules in everyday projects indeed android studio provides a code scanning tool called android lint that can be extended with lacking green checks in order to foster the design of more ecoresponsible apps
this study presents the results of a documentary research on recent approaches related to the causes and solutions of university student plagiarism it contains a classification of causes according to the level on which they occur thus based on the results of a direct observation the study shows that in romania where projectbased learning is promoted the students’ habits to unethically use internet based sources is developed rather than academic education with the requirement for pupils to elaborate school projects this is proposed as an additional cause of plagiarism  by part of the university students the habit of copypasting available materials online is difficult to counter efficiently but it can be corrected and directed towards the acquisition of new knowledge plagiarism is no longer efficient for students when they are explicitly asked to make comments about materials which are already available online
the article discusses the approach to building an underwater robot control system based on the “programming by demonstration” approach the basic elements of the implementation of the approach are considered the participants in the learning process are indicated input variables are defined and a training model is selected as a result a training methodology for the underwater robot control system is formulated for the system to perform specified manipulations
reconstructing the topography of shallow underwater environments using structurefrommotion—multi view stereo sfmmvs techniques applied to aerial imagery from unmanned aerial vehicles uavs is challenging as it involves nonlinear distortions caused by water refraction this study presents an experiment with aerial photographs collected with a consumergrade uav on the shallowwater reef of fuvahmulah the maldives under conditions of rising tide we surveyed the same portion of the reef in ten successive flights for each flight we used sfmmvs to reconstruct the digital elevation model dem of the reef and used the flight at low tide where the reef is almost entirely dry to compare the performance of dem reconstruction under increasing water levels our results show that differences with the reference dem increase with increasing depth but are substantially larger if no underwater ground control points are taken into account in the processing correcting our imagery with algorithms that account for refraction did not improve the overall accuracy of reconstruction we conclude that reconstructing shallowwater reefs less than 1 m depth with consumergrade uavs and sfmmvs is possible but its precision is limited and strongly correlated with water depth in our case the best results are achieved when ground control points were placed underwater and no refraction correction is used
this article presents a part of the ongoing economic and social research council esrcfunded project “floraguard tackling the illegal trade in endangered plants” that relies on crossdisciplinary approaches to analyze online marketplaces for the illegal trade in endangered plants and explores strategies to develop digital resources to assist law enforcement in countering and disrupting this criminal market this contribution focuses on how the project brought together computer science criminology conservation science and law enforcement expertise to create a tool for the automatic gathering of relevant online information to be used for research intelligence and investigative purposes the article also discusses the ethical standards applied and proposes the concept of “artificial intelligence ai review” to provide a sociotechnical solution that builds trustworthiness in the ai approaches used for this type of crossdisciplinary information and communications technology ictenabled methodology
background theoretical models help to explain or predict the adoption of electronic health ehealth technology and illustrate the complexity of the adoption process these models provide insights into general factors that influence the use of ehealth technology however they do not give hospitals much actionable knowledge on how to facilitate the adoption process objective our study aims to provide insights into patient portal adoption processes among patients and hospital staff including health care professionals hcps managers and administrative clerks studying the experiences and views of stakeholders answers the following question how can hospitals encourage patients and hcps to adopt a patient portal methods we conducted 22 semistructured individual and group interviews n69 in 12 hospitals and four focus groups with members of national and seminational organizations and patient portal suppliers n53 results the effort hospitals put into adopting patient portals can be split into three themes first inform patients and hcps about the portal this communication strategy has four objectives users should 1 know about the portal 2 know how the portal works 3 know that action on the portal is required and 4 know where to find help with the portal second embed the patient portal in the daily routine of hcps and management this involves three forms of support 1 hospital policy 2 management by monitoring the numbers and 3 a structured implementation strategy that includes all staff of one department third try to adjust the portal to meet patients’ needs to optimize userfriendliness in two ways 1 use patients’ feedback and 2 focus on optimizing for patients with special needs eg low literacy and low digital skills conclusions asking stakeholders what they have learned from their efforts to stimulate patient portal use in hospitals elicited rich insights into the adoption process these insights are missing in the theoretical models therefore our findings help to translate the relatively abstract factors one finds in theoretical models to the everyday pragmatics of ehealth projects in hospitals
— in a mobile ad hoc network manet while getting the information about intruder from this method the intruders are identified with some extra security related transmission the drawback of this method that is identifies once the intruder is detected the direction goes on transmitting some extra packets known as intruder check packets which ultimately increase the routing overhead and degrades the performance of network as it hops from the network node so there is a need to work upon and to reduce the routing overhead we are going to propose a protocol for reducing routing overhead in secure mobile ad hoc network in this work the overview of proposed research direction and the objectives of the work are demonstrated
hydrostatic equilibrium is an excellent approximation for the dense layers of planetary atmospheres where it has been canonically used to interpret transmission spectra of exoplanets here we exploit the ability of highresolution spectrographs to probe tenuous layers of sodium and potassium gas due to their formidable absorption crosssections we present an atmosphereexosphere degeneracy between optically thick and optically thin mediums raising the question of whether hydrostatic equilibrium is appropriate for na i lines observed at exoplanets to this end we simulate three nonhydrostatic evaporative density profiles i escaping ii exomoon and iii torus to examine their imprint on an alkaline exosphere in transmission by analyzing an evaporative curve of growth we find that equivalent widths of wmathrmna d2 sim 1 10 ma are naturally driven by evaporation rates sim 103  105 kgs of pure atomic na to break the degeneracy between atmospheric and exospheric absorption we suggest that if the line ratio is mathrmd2d1 gtrsim 12 the gas is optically thin on average and roughly indicating a nonhydrostatic structure of the atmosphereexosphere we show this is the case for na i observations at hot jupiters wasp49b and hd189733b and also simulate their k i spectra lastly motivated by the slew of metal detections at ultrahot jupiters we suggest a toroidal atmosphere at wasp76b and wasp121b is consistent with the na i data at present
oxidebased photoelectrodes recently have been at the forefront of research for photoelectrochemical water splitting while most oxidebased photoanodes suffer from severe electron–hole recombinati
automation has become an inseparable part of today’s world and with the help of iot providing a platform for connectivity between various sensors controllers and the internet that enables remote monitoring and controlling different environments that need automation india grows enough food to meet the needs of its entire population which is 13392 crores as of 2017 yet is unable to provide proper and consistent meals to millions of them especially women and children india ranks 100 in the global hunger index ghi — 2017 of 119 countries where it has been consistently ranked poor despite this the country manages to waste food quantities worth a total of rs 58000 crores in a year — about seven per cent of its total food production food waste occurs during production harvesting processing transportation retailing and consumption in order to reduce the amount of food wasted proper warehousing techniques are needed to safeguard the produce and ensure that it reaches the people who need it iot can help improve this method by continuously monitoring and tracking the food products that enter the warehouse this paper describes a system which consists of a microcontroller and various sensors that can collect information such as temperature humidity food quality and post this information to the client while also taking appropriate steps to ensure that the produce is kept at optimum environmental conditions
this paper aims to evaluate the potential solutions to address negative outcomes of hiv care and treatment that were proposed by hiv care providers researchers and hiv programme managers in southwest ethiopia
accurate crop disease prediction and continuous monitoring is essential in agriculture to improve the crop yield internet of things iot are being used in developing decision support systems for traditional farming methods to optimize disease estimation and yield estimation traditionally a large numbers of statistical and scientific models have been implemented to monitor and predict the crop yield estimation in the agriculture fields however most of these models are limited to small and fixed number of agriculture characteristics image processing and machine learning approaches are used to predict the disease on agricultural crops using iot devices as the size of the training images and features increases these approaches are incorporate to find and analyze the crop yield in agriculture field in this paper we have studied and analyzed various agricultural crop monitoring approaches using iot based image and machine learning techniques iot based image processing and machine learning approaches are necessary for the process of agricultural crop monitoring furthermore we have also studied the advantages and limitations of these approaches on complex crop data
the important reasons for the rapid spread of the novel coronavirus 2019ncov was the lack of awareness of infected individuals about their health due to the 14 day incubation period of the virus and the subsequent unintentional transmission large scale ignorance of social distancing guidelines and improper sanitary precautions and health in this paper we describe the development process of a system based solution for individuals to do covid19 susceptibility test using the spo2 oxygen level detection test to detect symptoms with higher levels of accuracy we also discuss various other implementation features to prevent the unaware spread of the virus including providing details to the user regarding government guidelines related to containing transmission of the virus alerts to the user about periodic maintenance of sanitary guidelines social distancing and notifying the user on noncompliance of above features finally we discuss possible future extensions to stricter the measures taken up by the user to prevent any mistakes like a frontcamera based mask authentication approach and alerting the user on entering crowded areas based on bluetooth crowd sensing also this system will be further developed into a mobile application
due to the fears from the corona pandemic and its consequences from curfews spacing and the difficulty to reach work centers including the greenhouse in the agricultural sector the idea arose to use lora one of the latest wireless communication technologies which has the ability to connect and use the internet of things iot to ensure monitoring management and control in any workplace lora technology will serve as the backbone of our design for the wireless network system to monitor and control greenhouse sensors the design consists of two parts the first is to prepare a group of sensors that support the iot and its mission is to measure and control inside the greenhouse and send data using lora technology to the next section the second section is to monitor and control the readings data remotely using lora technology to receiving first section data and connecting it to the internet or using the iot network to provide access monitoring and control from anywhere in the world the readings can have obtained after uploading it to the cloud computing the data deal with a website which allows monitoring the sensors wherever the internet service is available the system also gives the ability to send a feedback signal to the greenhouse the project provides integrated greenhouse control at a distance of 2 to 15 km and it represents as a fundamental solution to the current situation that is forced to remotely control our business to stay in homes for as long as possible to avoid corona disease
in this paper a patch antenna is designed for spatial modulation and ambient backscattering application at 35ghz its goal is to pave the way for numerical communications towards 5g and iot internet of things for those it is essential to design new antennas that are compact energy saving and radiation efficient the characteristic modes analysis cma is an interesting method for understanding an antenna properties and so being able to design antennas as compact as possible and keeping a good efficiency
the high time needed to reconfigure cloud resources in network function virtualization network environments has led to the proposal of solutions in which a prediction basedresource allocation is performed all of them are based on traffic or needed resource prediction with the minimization of symmetric loss functions like mean squared error when inevitable prediction errors are made the prediction methodologies are not able to differently weigh positive and negative prediction errors that could impact the total network cost in fact if the predicted traffic is higher than the real one then an over allocation cost referred to as overprovisioning cost will be paid by the network operator conversely in the opposite case quality of service degradation cost referred to as underprovisioning cost will be due to compensate the users because of the resource under allocation in this paper we propose and investigate a resource allocation strategy based on a long short term memory algorithm in which the training operation is based on the minimization of an asymmetric cost function that differently weighs the positive and negative prediction errors and the corresponding overprovisioning and underprovisioning costs in a typical traffic and network scenario the proposed solution allows for a cost saving by 30 with respect to the case of solution with symmetric cost function
recent advances in flexible electronics soft sensors and soft actuators are paving the way towards replacing hard printed circuit boards for soft counterparts in various applications eg soft robotics wearable devices etc the need to achieve robust electrical connections between both soft and traditional rigid components poses many challenges in particular the inextensiblility of commercially available interconnects eg singlemultistrand conductive wires conductive metallic tapes etc can affect the structural properties of soft components herein we present the design and demonstrate the fabrication method for making flexible fiber interconnects ffi by printing flexible guidepaths and simultaneously layering and embedding conductive yarns within the effectiveness and robustness of the flexible interconnects for use within soft structures is characterized simple ffi designs can be used within structures undergoing up to 200 strains without interfering with the substrate stressstrain behavior electrical conductivity is also shown to be stable even during cyclic loading
background to identify the main determinants of intraocular lens iol tilt and decentration after cataract surgery using a novel anterior segment optical coherence tomography asoct method methods fiftysix patients who underwent phacoemulsification with iol implantation in one eye were continuously enrolled in this cohort study axial length al was measured with iol master 700 the tilt and decentration of patients’ preoperative crystalline lenses and postoperative iols as well as crystalline lens thickness lt were measured using asoct before surgery and 1 week after surgery results the mean tilt and decentration of the patients’ preoperative crystalline lenses were 490°±181° and 021±002 mm and the mean tilt and decentration of iols were 475°±166° and 021±002 mm respectively there were no significant differences in magnitude direction of tilt or decentration between crystalline lenses and iols the strongest determinant of iol tilt was preoperative crystalline lens tilt r20512 p0001 followed by al r20154 p0003 additionally crystalline lens decentration and al explained 546 of the variability in iol decentration al was the factor most highly associated with iol decentration r20332 p0001 rather than crystalline lens decentration r20214 p0001 conclusions the position of the preoperative crystalline lens and al were the critical determinants of iol tilt and decentration the tilt and decentration of iols will be greater in patients with larger tilt and decentration of crystalline lenses or shorter and longer al
we consider the dynamic lot size problem for perishable inventory under minimum order quantities the stock deterioration rates and inventory costs depend on both the age of the stocks and their periods of order based on two structural properties of the optimal solution we develop a dynamic programming algorithm to solve the problem without backlogging we also extend the model by considering backlogging by establishing the regeneration set we give a sufficient condition for obtaining forecast horizon under without and with backlogging finally based on a detailed test bed of instance we obtain useful managerial insights on the impact of minimum order quantities and perishability of product and the costs on the length of forecast horizon
emerging evidence suggests that alterations in the development of the gastrointestinal gi tract during the early postnatal period can influence brain development and viceversa it is increasingly recognized that communication between the gi tract and brain is mainly driven by neural endocrine immune and metabolic mediators collectively called the gutbrain axis gba changes in the gba mediators occur in response to the developmental changes in the body during this period this review provides an overview of major developmental events in the gi tract and brain in the early postnatal period and their parallel developmental trajectories under physiological conditions current knowledge of gba mediators in context to brain function and behavioral outcomes and their synthesis and metabolism site timing etc is discussed this review also presents hypotheses on the role of the gba mediators in response to the parallel development of the gi tract and brain in infants
the preparation of functional fabrics with enhanced electrical properties requires a suitable selection of surface modification and particulate systems as fillers the best choice would be to use m
this paper outfits artificial intelligence based real time ldr data which is implemented in various applications like indoor lightning and places where enormous amount of heat is produced agriculture to increase the crop yield solar plant for solar irradiance tracking for forecasting the ldr information the system uses a sensor that can measure the light intensity by means of ldr the data acquired from sensors are posted in an adafruit cloud for every two seconds time interval using node mcu esp8266 module the data is also presented on adafruit dashboard for observing sensor variables a long shortterm memory is used for setting up the deep learning lstm module uses the recorded historical data from adafruit cloud which is paired with node mcu in order to obtain the realtime longterm time series sensor variables that is measured in terms of light intensity data is extracted from the cloud for processing the data analytics later the deep learning model is implemented in order to predict future light intensity values
the present research work covenants the preparation characterisation and optimisation of mucoadhesive microcapsules containing paclitaxel through ionic gelation method using 32 statistical factorial designs the effect of mixing proportion of primary polymer sodium alginate to copolymer x1 and speed of magnetic stirrer x2 on the microcapsules size y1 efficiency of paclitaxel encapsulation y2 and percentage yield y3 was optimised the morphology of microcapsules was characterised and evaluated by in vitro and in vivo tests to study the swelling characteristics mucoadhesion and drug release characteristics followed by mtt assay on human ht29 colon cancer cell lines the size of prepared microcapsules was within the range of 361 ± 450 to 931 ± 2241 encapsulation efficiency  was within the range of 4272 ± 043 to 9812 ± 043  the in vitro paclitaxel released over 24 hours were in a range of 8215 ± 343  to 9675 ± 241  the controlled release pattern of paclitaxel was observed from the in vitro drug release study of microcapsules the prepared microcapsules that showed better mucoadhesion were in the range of 7366 ± 142 to 9785 ± 108  for a period of 6 h the in vivo pharmacokinetic study conducted in rats resulted in high tmax the area under the curve and mean residence time for microcapsules as compared to that of the marketed formulation it could be concluded that the microcapsules containing povidone polymer showed superior results
we develop a new sequential rate distortion functionto compute lower bounds on the average length of all causal prefix free codes for partially observable multivariate markov processes with meansquared error distortion constraint our information measure is characterized by a variant of causally conditioned directed information and is utilized in various application examples first it is used to optimally characterize a finite dimensional optimization problem for jointly gaussian processes and to obtain the corresponding optimal linear encoding and decoding policiesunder the assumption that all matrices commute by pairswe show that our problem can be cast as a convex programwhich achieves its global minimum we also derive sufficientconditions which ensure that our assumption holds we thensolve the kkt conditions and derive a new reversewaterfilling algorithm that we implement if our assumption is violated one can still use our approach to derive suboptimal upper bound waterfilling solutions for scalarvalued gaussmarkov processes with additional observation noise we derive a new closed form solution and we compare it with known results in the literature for partially observable timeinvariant markov processes driven by additive iid system noise only we recover using an alternative approach and thus strengthening a recent result by kostina and hassibi in 1 theorem 9 whereas for timeinvariant and spatially iid markov processes driven by additive noise process we also derive new analytical lower bounds
as the result of regularization of the objective function of linear regression lasso regression is a classical algorithm of supervised learning in machine learning and it has a wide range of applications however its objective function has the defect of poor derivability so it does not use the coordinate descent method but the traditional solution method is the coordinate descent method but even if the poor derivability is avoided the method of descending along the coordinate axis also has some defects for example when the lasso is more complex it will obviously reduce the speed it is easy to fall into the local optimization in order to solve these defects this paper chooses a nonconvex quantum whale optimization algorithm which is processed by quantum algorithm and has good parallelism on the basis of whale optimization algorithm
innovative aesthetic product appearances can create buffering effects in the preadoption phase that lead consumers to base their expectations on the aesthetics rather than on objective information about innovative products how do innovative aesthetics influence product experience in the postadoption phase using longitudinal postadoption data from early adopters of an electric car model this study shows that consumers perceptions of innovative aesthetic value buffer the effect of productrelated hedonic experience on attitudes towards the product the more value consumers derive from innovative product aesthetics postadoption the less they ground their attitudes on actual hedonic experience product managers thus should opt for designs that grant aesthetic utility over time innate consumer innovativeness levels moderate the buffering effect such that the effect of aesthetic innovative product evaluations on the relationship between experienced hedonic utility and attitude is stronger for more innovative consumers innovative consumers are especially sensitive to innovative aesthetic value so in their cocreation efforts managers should seek out these customers because doing so can increase early adopters longterm product satisfaction and word of mouth both of which expand the breadth of diffusion for the innovations
the renewable energy resources such as photovoltaic systems have major role in the development of sustainable energy systems an efficient controller for standalone photovoltaic system based on fractional calculus is proposed in this work the power extracted from the pv systems are continuously varies with respect to the dynamic changes of solar irradiance value and climate temperature here we introduce a maximum power point tracking mppt algorithm is which realized with the support of fractional order pid fopid controllers to extract the maximum possible power from the photo voltaicpvmodules the pid controller and fopid controllers are also ensures the constant dc bus voltage and hence the ac output voltage at become stable the effectiveness of the controller are compared using various performance time domain specifications like overshoot percentage risetime settlingtime etc the various analysis were done under the variation of irradiance and temperature in matlab simulink platform the fopid controller give improved performance compare with conventional pid controllers
recently semantic segmentation has been widely used in text detection tasks and many excellent text detection methods have been proposed they usually adopt deep convolutional neural network with consecutive striding or pooling operations to obtain a larger receptive field however that would lead to the lack of context which is crucial for text detection in this paper we propose an endtoend trainable neural network to directly detect text regions without redundant stage other than a localityaware nonmaximum suppression is involved we introduce atrous convolution in the backbone network to enlarge the receptive field retaining more context information while controlling the spatial resolution of feature maps the atrous spatial pyramid pooling aspp module is attached on top of the feature maps to effectively detect texts of multiple scales we have benchmarked our algorithm on three public datasets it achieves highly competitive results in terms of text localization precision more specifically on the msratd500 datasets the proposed algorithm achieves an fscore of 0813 outperforming the previous best by a large margin
—current computer vision tasks based on deep learning require a huge amount of data with annotations for model training or testing especially in some dense estimation tasks such as optical ﬂow segmentation and depth estimation in practice manual labeling for dense estimation tasks is very difﬁcult or even impossible and the scenes of the dataset are often restricted to a small range which dramatically limits the development of the community to overcome this deﬁciency we propose a synthetic dataset generation method to obtain the expandable dataset without burdensome manual workforce by this method we construct a dataset called minenavi containing video footages from ﬁrstperspectiveview of the aircraft matched with accurate ground truth for depth estimation in aircraft navigation application we also provide quantitative experiments to prove that pretraining via our minenavi dataset can improve the performance of depth estimation model and speed up the convergence of the model on real scene data since the synthetic dataset has a similar effect to the realworld dataset in the training process of deep model we also provide additional experiments with monocular depth estimation method to demonstrate the impact of various factors in our dataset such as lighting conditions and motion mode
geologic fractures such as joints and faults are central to many problems in energy geotechnics notable examples include hydraulic fracturing injectioninduced earthquakes and geologic carbon storage nevertheless our current capabilities for simulating the development and evolution of geologic fractures in these problems are still insufficient in terms of efficiency and accuracy recently phasefield modeling has emerged as an efficient numerical method for fracture simulation which does not require any algorithm for tracking the geometry of fracture however existing phasefield models of fracture neglected two distinct characteristics of geologic fractures namely the pressuredependence and frictional contact to overcome these limitations new phasefield models have been developed and described in this paper the new phasefield models are demonstrably capable of simulating pressuredependent frictional fractures propagating in arbitrary directions which is a notoriously challenging task
we present results on the isovector momentum fraction langle x rangleud helicity moment langle x rangledelta udelta d and the transversity moment langle x rangledelta udelta d of the nucleon obtained using nine ensembles of gauge configurations generated by the milc collaboration using 211flavors of dynamical highly improved staggered quarks hisq the correlation functions are calculated using the wilsonclover action and the renormalization of the three operators is carried out nonperturbatively on the lattice in the riprimemom scheme the data have been collected at lattice spacings a approx 015 012 009 and 006 fm and mpi approx 310 220 and 135 mev which are used to obtain the physical values using a simultaneous chiralcontinuumfinitevolume fit the final results in the overlinems scheme at 2 gev are langle x rangleud  01731407 langle x rangledelta udelta d  02131522 and langle x rangledelta udelta d  02081924 where the first error is the overall analysis uncertainty and the second is an additional systematic uncertainty due to possible residual excitedstate contributions these are consistent with other recent lattice calculations and phenomenological global fit values
charisma is an essential component of spoken language production and has been used for centuries to engage audiences and obtain followers understanding charisma in speech is important not only for texttospeech synthesis but also for broader issues of explaining social events as well as helping speakers to improve their own charismatic speech production in this paper we present the ﬁrst genderbalanced study of charismatic speech including speakers and raters from diverse backgrounds we describe how raters deﬁne charisma by an alyzing its positive or negative relationship with other speaker traits such as enthusiasm persuasiveness boringness and un certainty using the features extracted from the voice clips we analyze the acoustic and textual correlates of charisma we also extend prior work to examine individual differences in the perception and production of charisma in speech we discuss how a speaker’s gender and how a rater’s gender level of education personality and own speaking style inﬂuence the rater’s percep tion of charismatic speech
emotion regulation difficulties precipitate and exacerbate acute mood symptoms in individuals with bipolar disorder bd and contribute to suicidal behavior however few studies have examined regional brain responses in explicit emotion regulation during acute bd mood states or hopelessness a major suicide risk factor we assessed brain responses during explicit emotion regulation and their relationship with hopelessness in acutely symptomatic and euthymic individuals with bd
the purpose of the multiview stereo is to restore the target 3d geometric model from multiperspective images there are several problems with the existing approaches based on deep learning such as missing the detailed information in the predicted depth map the low surface accuracy and the incomplete reconstructed 3d point cloud model in order to overcome these problems we propose the attentionguided multiview stereo network for 3d depth estimationagmvsnet we combine the camera geometry with the deep neural network and we adopt the coarsetofine deep learning framework to restore the target 3d geometry model highquality detailed feature information has an important influence on multiview 3d reconstruction and reference images in the natural environment contain detailed feature information which is needed in the reconstruction process therefore we use the detailed feature information from different scales of reference images to restore the lost details of the highlevel features the quantitative and qualitative experimental results show that the proposed algorithm is more complete than the common multiview 3d reconstruction algorithms
the present paper reviews for the first time to the best of our knowledge the most recent advances in research concerning two popular devices used for foot motion analysis and health monitoring smart socks and inshoe systems the first one is representative of textilebased systems whereas the second one is one of the most used pressure sensitive insole psi systems that is used as an alternative to smart socks the proposed methods are reviewed for smart sock use in special medical applications for gait and foot pressure analysis the pedar system is also shown together with studies of validation and repeatability for pedar and other inshoe systems then the applications of pedar are presented mainly in medicine and sports our purpose was to offer the researchers in this field a useful means to overview and select relevant information moreover our review can be a starting point for new relevant research towards improving the design and functionality of the systems as well as extending the research towards other areas of applications using sensors in smart textiles and inshoe systems
the novel ternary carbides and nitrides known as max phase materials with remarkable combined metallic and ceramic properties offer various engineering and technological applications using ab initio calculations based on generalized gradient approximation gga local density approximation lda and the quasiharmonic debye model the electronic structural elastic mechanical and thermodynamic properties of the m2gac m  zr hf max phase were investigated the optimized lattice parameters give the first reference to the upcoming theocratical and experimental studies while the calculated elastic constants are in excellent agreement with the available data moreover obtained elastic constants revealed that both the zr2gac and hf2gac max phases are brittle the band structure and density of states analysis showed that these max phases are electrical conductors having strong directional bonding between mc m  zr hf atoms due to md and cp hybridization formation and cohesive energies and phonon calculations showed that zr2gac and hf2gac max phases’ compounds are thermodynamically and dynamically stable and can be synthesized experimentally finally the effect of temperature and pressure on volume heat capacity debye temperature grüneisen parameter and thermal expansion coefficient of m2gac m  zr hf are evaluated using the quasiharmonic debye model from the nonequilibrium gibbs function in the temperature and pressure range 0–1600 k and 0–50 gpa respectively
we study how adding unknown linear safety constraints affects the performance of thompson sampling in the linear stochastic bandit problem the additional constraints must be met at each round in spite of uncertainty about the environment requiring that the learner acts conservatively in choosing her actions in this setting we propose safelts the first safe thompson sampling based algorithm and we prove that it achieves noregret learning we obtain regrets that have the same dependence on the total number of rounds modulo logarithmic factors as safeucb a recently proposed safe algorithm that uses the upper confidence bound principle finally we provide numerical simulations that demonstrate the efficacy of our algorithm
introduction maximising efficiency of resources is critical to progressing towards universal health coverage uhc and the sustainable development goal sdg for health this study estimates the technical efficiency of national health spending in progressing towards uhc and the environmental factors associated with efficient uhc service provision methods a twostage efficiency analysis using simar and wilson’s double bootstrap data envelopment analysis investigates how efficiently countries convert health spending into uhc outputs measured by service coverage and financial risk protection for 172 countries we use world bank and who data from 2015 thereafter the environmental factors associated with efficient progress towards uhc goals are identified results the mean biascorrected technical efficiency score across 172 countries is 857 689 for lowincome and 955 for highincome countries highachieving middleincome and lowincome countries such as el salvador colombia rwanda and malawi demonstrate that peerrelative efficiency can be attained at all incomes governance capacity income and education are significantly associated with efficiency sensitivity analysis suggests that results are robust to changes conclusion we provide a 2015 baseline for crosscountry uhc technical efficiency scores if countries wish to improve their uhc outputs within existing budgets they should identify their current efficiency and try to emulate more efficient peers policymakers should focus on strengthening institutions and implementing known best practices to replicate efficient systems using resources more efficiently is likely to positively impact uhc coverage goals and health outcomes and without addressing gaps in efficiency progress towards achieving the sdgs will be impeded
modern smart homes are being equipped with certain renewable energy resources that can produce their own electric energy from time to time these smart homes or microgrids are also capable of supplying energy to other houses buildings or energy grid in the time of available selfproduced renewable energy therefore researches have been carried out to develop optimal trading strategies and many recent technologies are also being used in combination with microgrids one such technology is blockchain which works over decentralized distributed ledger in this paper we develop a blockchain based approach for microgrid energy auction to make this auction more secure and private we use differential privacy technique which ensures that no adversary will be able to infer private information of any participant with confidence furthermore to reduce computational complexity at every trading node we use consortium blockchain in which selected nodes are given authority to add a new block in the blockchain finally we develop differentially private energy auction for blockchainbased microgrid systems deal we compare deal with vickrey–clarke–groves vcg auction scenario and experimental results demonstrates that deal outperforms vcg mechanism by maximizing sellers’ revenue along with maintaining overall network benefit and social welfare
dancing robot control system for children’s entertainment is introduced in order to achieve better interaction this robot use the arm core chip stm32 as the control core bluetooth module as the means of wireless communication and smart phones as the face display screen and motion controller of dance robots experimental results show that the robot is stable it can work given dance actions the robot is good operable and maintainable
background problematic instagram use pigu a specific type of internet addiction is prevalent among adolescents and young adults in certain instances instagram acts as a platform for exhibiting photos of risktaking behavior that the subjects with pigu upload to gain likes as a surrogate for gaining peer acceptance and popularity aims the primary objective was to evaluate whether addictionspecific cues compared with neutral cues ie negative emotional valence cues vs positive emotional valence cues would elicit activation of the dopaminergic reward network ie precuneus nucleus accumbens and amygdala and consecutive deactivation of the executive control network ie medial prefrontal cortex mpfc and dorsolateral prefrontal cortex dlpfc in the pigu subjects method an fmri cueinduced reactivity study was performed using negative emotional valence positive emotional valence and truly neutral cues using instagram themes thirty subjects were divided into pigu and healthy control hc groups based on a set of diagnostic criteria using behavioral tests including the modified instagram addiction test igat to assess the severity of pigu inscanner recordings of the subjects’ responses to the images and regional activity of the neural addiction pathways were recorded results negative emotional valence  positive emotional valence cues elicited increased activations in the precuneus in the pigu group a negative and moderate correlation was observed between psc at the right mpfc with the igat scores of the pigu subjects when corrected for multiple comparisons r  −0777 p  0004 twotailed conclusion addictionspecific instagramthemed cues identify the neurobiological underpinnings of instagram addiction activations of the dopaminergic reward system and deactivation of the executive control network indicate converging neuropathological pathways between instagram addiction and other types of addictions
abstract an initialboundary value problem whose differential equation contains a sum of fractional time derivatives with orders between 0 and 1 is considered its spatial domain is  0  1  d 01d for some d ∈  1  2  3  din123  this problem is a generalisation of the problem considered by stynes o’riordan and gracia in siam j numer anal 55 2017 pp 1057–1079 where d  1 d1 and only one fractional time derivative was present a priori bounds on the derivatives of the unknown solution are derived a finite difference method using the wellknown l1 scheme for the discretisation of each temporal fractional derivative and classical finite differences for the spatial discretisation is constructed on a mesh that is uniform in space and arbitrarily graded in time stability and consistency of the method and a sharp convergence result are proved hence it is clear how to choose the temporal mesh grading in a optimal way numerical results supporting our theoretical results are provided
highperformance computing researchers are trying to find new options tools to satisfy the performance criteria of a hardware design fpga field programmable gate array is one of the accelerators which is widely used for powerefficient applications due to its reconfigurability and high performance traditionally fpga can be programmed using hardware description language hdl using hdl for any fpga hardware architecture design the designer needs to be very knowledgeable about the hardware and the register transfer level language rtl programming while designing hardware architecture it was always desired to reduce design complexity and developing time fpga can be programmed by a software programmer using highlevel synthesis hls tools like opencl vivado while avoiding design complexity and reducing the developing time opencl is an hls tool where a designer can write the code like the software and design the hardware an opencl design can be done using many data partitions and task parallelism techniques the saxpy a level 1 basic linear algebra blas routine is widely used for many scientific applications this level 1 blas routine which involves vectorvector operations can be implemented in various ways like using the ndimensional range ndrange or single work item using global memory or local memory choosing the option to reuse the local storage and tuning the design knobs such as block size work item bank width number of memory banks and loop unrolling factor this paper is presenting a design space exploration dse of openclbased implementation for saxpy kernel on fpgas from our investigation we have found that the ndrange kernel is more throughput efficient for saxpy kernel operations
the divisor class group of a hyperelliptic curve defined over a finite field is a finite abelian group at the center of a number of important open questions in algebraic geometry number theory and cryptography many of these problems lend themselves to numerical investigation and as emphasized by sutherland 14 13 fast arithmetic in the divisor class group is crucial for their efficiency besides implementations of these fundamental operations are at the core of the algebraic geometry packages of widelyused computer algebra systems such as magma and sage
fast sc decoding overcomes the latency caused by the serial nature of the sc decoding by identifying new nodes in the upper levels of the sc decoding tree and implementing their fast parallel decoders in this work we first present a novel sequence repetition node corresponding to a particular class of bit sequences most existing special node types are special cases of the proposed sequence repetition node then a fast parallel decoder is proposed for this class of node to further speed up the decoding process of general nodes outside this class a thresholdbased harddecisionaided scheme is introduced the threshold value that guarantees a given errorcorrection performance in the proposed scheme is derived theoretically analysis and hardware implementation results on a polar code of length 1024 with code rates 14 12 and 34 show that our proposed algorithm reduces the required clock cycles by up to 8 and leads to a 10 improvement in the maximum operating frequency compared to stateoftheart decoders without tangibly altering the errorcorrection performance in addition using the proposed thresholdbased harddecisionaided scheme the decoding latency can be further reduced by 57 at mathrm ebmathrm n0  50 db
as a basic research topic of computer vision object detection is facing the challenges of being extended for video object detection for deformation anomaly lowquality candidate regions feature can be caused by motion blur rare poses and video defocus resulting to poor detection performance in videos in this paper we propose a video object detector which consists of an imagebased detector using deep reinforcement learning and a correction module using objects tracking algorithms we treat videos as continuous images the agent will search on the feature map of each image by a policy network to generate highquality candidate regions these candidate regions will be corrected by a correction module the bounding box at the current image from the tracklet proposed by the object tracking algorithms will correct the candidate regions experiments are conducted on the mot 15 dataset our detector achieves better detection performance exactly 6828 with 501 points improvement compared to the fasterr cnn as the singleframe baseline
magnetic fields are widely used in shortrange wireless applications such as sensor systems and communication systems to further exploit the potential of such systems that use magnetic fields we investigated their applicability to position sensing of a mobile device that generates these fields the principle involves estimating the position of the device via an analysis of the data detected by multiple magneticfield sensors located around the target space in this study we used machine learning to analyze the sensor data which were obtained by numerical calculations the results indicated that machine learning effectively estimated the position of the mobile devices based on our simulations the error of the position estimated with the machinelearning approach was within 10 cm in a 2times 2 times 2  textm3 cubic space for 73 of all the cases of mobiledevice states the estimation accuracy exceeded that obtained with a conventional optimizing approach furthermore the estimation accuracy obtained with the machine learning approach was maintained for the signaltonoiseratio higher than 30 db it was also shown that the degradation of the estimation accuracy caused by a sensorlocation shift can be restored by learning with training data for the shifted sensor location the computational speed of the machine learning approach is 30 times faster than that of the conventional one the results significantly support the applicability of magneticfieldbased systems for realtime tracking of moving persons and objects
this paper aims to examine the students acceptance on using selfarchive in open access repositories guiding by the unified theory of acceptance and use of technology utaut model this paper utilized convenience sampling method with a total of 204 sets of selfadministered questionnaire collected among the students of public higher learning institution in federal territory of labuan malaysia multiple regressions were performed to examine the relationships between performance expectancy effort expectancy social influence and facilitating conditions towards the intention to use selfarchive in institutional repository the findings revealed that the students behavioural attention on the acceptance of using selfarchive in repository were positively influence by social influence and facilitating condition the repository administrators shall increase the efforts to enhance the quality of the repository as well as encouraging students interest to use the service
the smooth design of selfsupporting topologies has attracted great attention in the design for additive manufacturing dfam field as it cannot only enhance the manufacturability of optimized designs but can obtain lightweight designs that satisfy specific performance requirements this paper integrates langelaar’s am filter into the smoothedged material distribution for optimizing topology semdot algorithm—a new elementbased topology optimization method capable of forming smooth boundaries—to obtain printready designs without introducing postprocessing methods for smoothing boundaries before fabrication and adding extra support structures during fabrication the effects of different build orientations and critical overhang angles on selfsupporting topologies are demonstrated by solving several compliance minimization stiffness maximization problems in addition a typical compliant mechanism design problem—the force inverter design—is solved to further demonstrate the effectiveness of the combination between semdot and langelaar’s am filter
we describe systems and methods for the deployment of global quantum key distribution qkd networks covering transoceanic longhaul metro and access segments of the network a comparative study of the stateoftheart qkd technologies is carried out including both terrestrial qkd via optical fibers and freespace optics as well as spaceborne solutions via satellites we compare the pros and cons of various existing qkd technologies including channel loss potential interference distance connection topology deployment cost and requirements as well as application scenarios technical selection criteria and deployment requirements are developed for various different qkd solutions in each segment of networks for example optical fiberbased qkd is suitable for access networks due to its limited distance and compatibility with pointtomultipoint p2mp topology with the help of trusted relays it can be extended to longhaul and metro networks spaceborne qkd on the other hand has much smaller channel loss and extended transmission distance which can be used for transoceanic and longhaul networks exploiting satellitebased trusted relays
"objective
most studies linking physical victimization and substance use have focused on concurrent or temporally proximal associations making it unclear whether physical victimization has a sustained impact on substance use problems we examined the longterm associations between adolescent physical victimization and symptoms of substance use disorders in adulthood controlling for intermediating victimization during young adulthood and several control variables


method
data were obtained from the monitoring the future study n  5291 women and men were recruited around age 18 and surveyed biennially through age 30 and again at 35 pastyear physical victimization threatened physical assaults injurious assaults was measured regularly from age 18 to 30 alcohol and cannabis use symptoms eg withdrawal tolerance were assessed at age 35 controls were measured in adolescence eg prior substance use and young adulthood eg marriage interactions examined whether associations varied by sex


results
when we controlled for adolescent substance use adolescents who were threatened with injury or who sustained physical injuries as a result of violence had more alcohol use symptoms at age 35 than nonvictims however when victimization during young adulthood was statistically accounted for only victimization during young adulthood was associated with age35 alcohol use symptoms the effects of young adult victimization but not adolescent victimization were stronger for women victimization was mostly unrelated to age35 cannabis use symptoms


conclusions
adolescents who are threatened with physical assaults or injured by physical assaults have significantly more alcohol use symptoms in their mid30s than nonvictimized adolescents but these associations are completely explained by subsequent victimization during young adulthood"
purposethis position paper urges a drive towards clarity in the key definitions terminologies and habits of speech associated with digital engineering and building information modelling bim the ultimate goal of the paper is to facilitate the move towards arriving at an ideal definition for both conceptsdesignmethodologyapproachthis paper takes the “explanation building” review approach in providing prescriptive guidelines to researchers and industry practitioners the aim of the review is to draw upon existing studies to identify describe and find application of principles in a realworld contextfindingsthe paper highlights the definitional challenges surrounding digital engineering and bim in australia to evoke a debate on bim and digital engineering boundaries how and why these two concepts may be linked and how they relate to emerging conceptsoriginalityvaluethis is the first scholarly attempt to clarify the definition of digital engineering and address the confusion between the concepts of bim and digital engineering
softwaredefined networks sdns offer unique and attractive solutions to solve challenging management issues in internet of things iotbased largescale multitechnological networks sdniot network collaboration is innovative and attractive but expected to be extremely heterogeneous in future generation iot systems for example multitechnology network network externality and nodes heterogeneity in sdniot may seriously affect the flow or applicationspecific qualityofservice qos requirements furthermore it highly influences security adoption in a network of interconnected iot nodes we observe that both qos and security are interdependent and nonnegligible factors thus we emphasize that in order to alleviate heterogeneity it is inevitable to study both these factors hand to hand or vice versa with this aim first we discuss significant and reasonable cases to encourage researchers to study qos and security integrally in order to alleviate heterogeneity at sdniot control plane second we propose a framework which successfully transforms the m heterogeneous controllers to n homogeneous controller groups the key metric of our observation and analysis is the sdn controller’s response time following this to validate our approach we use the mathematical model and a proof of concept poc in a virtual sdn ecosystem is demonstrated from performance evaluation we observe that the proposed framework significantly alleviates heterogeneity which helps to maintain qos and enhance security this fundamental analysis will enable network security individuals to deal heterogeneity qos and security of sdniot in more successful and promising ways
existing visualization recommendation approaches introduced an overwhelmingly large number of utility functions ufs to rank the visualizations ie views in order to discover the ideal uf and their tunable parameters that are most suited for a particular analysis context recent works have proposed interactive view recommendation ivr ivr identifies the ideal uf by learning from the interactions with the user during the analysis process we claim that the user is usually unable to provide an accurate real number between 0 and 1 indicating the view interestingness and in this work propose three alternative feedback types binary likert scale and pairwise comparison we further developed ranking algorithms for the three feedback types finally we propose different example selection strategies and experimentally evaluate them on the three feedback types we found that uncertaintybased and hybridbased strategies usually outperforms the random strategy in terms of recommendation accuracy across all feedback types while the interestingnessbased strategy usually performs worse than the random strategy
patients are often required to follow a medical treatment after discharge eg for a chronic condition rehabilitation after surgery or for cancer survivor therapies the need to adapt to new lifestyles medication and treatment routines can produce an individual burden to the patient who is often at home without the full support of healthcare professionals although technological solutions –in the form of mobile apps and wearables– have been proposed to mitigate these issues it is essential to consider individual characteristics preferences and the context of a patient in order to offer personalized and effective support the specific events and circumstances linked to an individual profile can be abstracted as a patient trajectory which can contribute to a better understanding of the patient her needs and the most appropriate personalized support although patient trajectories have been studied for different illnesses and conditions it remains challenging to effectively use them as the basis for data analytics methodologies in decentralized ehealth systems in this work we present a novel approach based on the multiagent paradigm considering patient trajectories as the cornerstone of a methodology for modelling ehealth support systems in this design semantic representations of individual treatment pathways are used in order to exchange patientrelevant information potentially fed to ai systems for prediction and classification tasks this paper describes the major challenges in this scope as well as the design principles of the proposed agentbased architecture including an example of its use through a case scenario for cancer survivors support
big data and technological change have enabled loyalty programs to become more prevalent and complex how these developments influence society has been overlooked both in academic research and in practice we argue why this issue is important and propose a framework to refocus loyalty programs in the era of big data through a societal lens we focus on three aspects of the societal lens—inequality privacy and sustainability we discuss how loyalty programs in the big data era impact each of these societal factors and then illustrate how by adopting this societal lens paradigm researchers and practitioners can generate insights and ideas that address the challenges and opportunities that arise from the interaction between loyalty programs and society our goal is to broaden the perspectives of researchers and managers so they can enhance loyalty programs to address evolving societal needs
in december 2019 rising pneumonia cases caused by a novel βcoronavirus sarscov2 occurred in wuhan china which has rapidly spread worldwide causing thousands of deaths the who declared the sarscov2 outbreak as a public health emergency of international concern therefore several scientists are dedicated to the study of the new virus since human viruses have codon usage biases that match highly expressed proteins in the tissues they infect and depend on host cell machinery for replication and coevolution we selected the genes that are highly expressed in the tissue of human lungs to perform computational studies that permit to compare their molecular features with sars sarscov2 and mers genes in our studies we analysed 91 molecular features for 339 viral genes and 463 human genes that consisted of 677873 codon positions hereby we found that at bias in viral genes could propitiate the viral infection favoured by a host dependant specialization using the host cell machinery of only some genes the envelope protein e the membrane glycoprotein m and orf7 could have been further benefited by a high rate of at in the third codon position thereby the mistranslation or deregulation of protein synthesis could produce collateral effects as a consequence of viral occupancy of the host translation machinery due tomolecular similarities with viral genes furthermore we provided a list of candidate human genes whose molecular features match those of sarscov2 sarsand mers genes which should be considered to be incorporated into genetic population studies to evaluate thesusceptibility to respiratory viral infections caused by these viruses the results presented here settle the basis for further research in the field of human genetics associated with the new viral infection covid19 caused by sarscov2 and for the development of antiviral preventive methods
genes with correlated expression across individuals in multiple tissues are potentially informative for systemic genetic activity spanning these tissues in this context the tissuelevel gene expression data across multiple subjects from the genotype tissue expression gtex project is a valuable analytical resource unfortunately the gtex data is fraught with missing entries owing to subjects often contributing only a subset of tissues in such a scenario standard techniques of correlation matrix estimation with or without data imputation do not perform well here we propose robocov a novel convex optimizationbased framework for robustly learning sparse covariance or inverse covariance matrices for missing data problems robocov produces more interpretable and less cluttered visual representation of correlation and causal structure in both simulation settings and gtex data analysis simulation experiments also show that robocov estimators have a lower false positive rate than competing approaches for missing data problems genes prioritized based on the average value of robocov correlations or partial correlations across tissues are enriched for pathways related to systemic activities such as signaling pathways heat stress factor immune function and circadian clock furthermore snps linked to these prioritized genes provide unique signal for bloodrelated traits in comparison no disease signal is observed for snps linked to genes prioritized by the standard correlation estimator robocov is an important standalone statistical tool for sparse correlation and causal network estimation for data with missing entries and when applied to gtex data it provides insights into both genetic and autoimmune disease architectures
"
 
 parkinson disease pd is a common neurodegenerative disorder that affects between 7 and 10 million people worldwide no objective test for pd currently exists and studies suggest misdiagnosis rates of up to 34 machine learning ml presents an opportunity to improve diagnosis however the size and nature of data sets make it difficult to generalize the performance of ml models to realworld applications
 
 
 
 this study aims to consolidate prior work and introduce new techniques in feature engineering and ml for diagnosis based on vowel phonation additional features and ml techniques were introduced showing major performance improvements on the large mpower vocal phonation data set
 
 
 
 we used 1600 randomly selected aa phonation samples from the entire data set to derive rules for filtering out faulty samples from the data set the application of these rules along with a joint agegender balancing filter results in a data set of 511 pd patients and 511 controls we calculated features on a 15second window of audio beginning at the 1second mark for a support vector machine this was evaluated with 10fold crossvalidation cv with stratification for balancing the number of patients and controls for each cv fold
 
 
 
 we showed that the features used in prior literature do not perform well when extrapolated to the much larger mpower data set owing to the natural variation in speech the separation of patients and controls is not as simple as previously believed we presented significant performance improvements using additional novel features with 886 certainty derived from a bayesian correlated t test in separating patients and controls with accuracy exceeding 58
 
 
 
 the results are promising showing the potential for ml in detecting symptoms imperceptible to a neurologist
"
it can be seen from the literature that nonhomogeneous wavelet frames are much simpler to characterize and construct than homogeneous ones in this work we address such problems in reducing subspaces of l2ℝd a characterization of nonhomogeneous wavelet dual frames is obtained and by using the characterization an moep and an mep are derived under general assumptions for such wavelet dual frames
hyperbolic metamaterials hmms attract increasing attentions due to their unique optical properties and offer new approaches for realizing novel functionalities in emerging photonic metadevices tunable is one of the most attractive optical properties since multifunction optical devices are one of the important research directions so far most active hmms working in the visible region are based on the combination of metal and phasechange chalcogenides and the performance is limited by the optical losses of phasechange chalcogenides and interdiffusion of the metals with phasechange chalcogenides in this work incorporating αphase molybdenum trioxide αmoo3 and au an active and low loss hmm device is proposed in the visible region and can effectively overcome the shortcoming a tunable plasmonic biosensor based on prism coupled αmoo3au hmm is further designed by enhancing goos–hänchen gh shift since gh shift is highly sensitive to the refractive index of the substrate the calculated refractive index sensitivity of this proposed biosensor is of the order of 106 nmrefractive index unit the proposed approach offers new direction for potential application in the development of the active ultrasensitive biosensor operating at visible range
this work presents a kinetic monte carlo algorithm to solve the gasphase chemistry in lowtemperature plasmas as a first effort to achieve a unified formulation of the electron and heavyparticle kinetics based on monte carlo techniques the implemented algorithm is successfully validated in the thermodynamic limit from the comparison with the traditional deterministic description using ratebalance equations the accuracy of the monte carlo description of the rare species strongly depends on the number of particles used in the simulation to surpass this limitation two novel variance reduction techniques that significantly reduce the statistical fluctuations on the concentrations of the minor species are proposed and evaluated these techniques lead to significant gains in computational time up to factors of the order of 104 times in the cases studied while ensuring the same quality of the solution
the traditional emotion classification framework usually fits all the features segments of the same trial to a fixed annotation considering the fact that emotion is a reaction to stimuli that lasts for varied periods we argue that the indiscriminate annotation is equivalent to taking the emotional state as fixed within the whole trial leading to a decrease of the classification accuracy in this study we attempt to alleviate this issue by developing a thresholding scheme converting the continuous emotional trace into a threeclass annotation temporally the features within a trial are therefore assigned to varied emotional states resulting in an improvement in the accuracy a long short term memory lstm networksbased emotion classification framework is implemented to which the proposed thresholding scheme is applied a subset of mahnobhci dataset with continuous emotional annotation is used the eeg signal and frontal facial video are used for feature extraction the experiment results demonstrate that the proposed scheme provides statistically significant improvement to the threeclass classification accuracy of the eeg featurebased lstm network pvalue  00329
most contemporary greek cities face the lack of adequate public space this lack has a direct effect on parking space issues turning cities’ open spaces into congested ones with parked cars although according to greek buildings’ regulations parking space must be provided within the building itself especially in the county of attica where athens the capital is located the law offers the possibility of exemption from this obligation in small building lots thus in many cases for small apartment buildings no parking space is provided within the building forcing drivers to resort to either offstreet parking or parking lots offstreet parking may cause more traffic jams noise and loss of time among other negative effects while parking lots have turned into environmental catastrophes in many cases soup 2017 davis et al 2010 on the other hand the construction of a basement parking space is linked with high initial costs while parking spaces on other floors deprive vital area from living spaces let alone the embodied energy of constructing these spaces in this paper life cycle analysis and multicriteria analysis is made so as to compare which the most sustainable option is the creation of underground and ground floor level parking spaces within a small apartment building offstreet parking or the creation of a parking lot to host these vehicles the environmental social and economic effects of these three cases are analysed taking into consideration both their constructional and operational effects a comparison is made between these three options with criteria for environmental protection social equity and economic growth the three pillars of sustainability selected from relevant indicators of sustainable development hierarchy of the three examined options offstreet in a parking lot or within the building parking is made with multicriteria analysis through this research conclusions are drawn on the policy that cities should follow so as to face in the most sustainable way the demand for parking space and whether legislation should change in regards to the provision of parking spaces within small building lots
abstract shortly after the successful launch of esa’s wind mission aeolus carried out by the european space agency collocated airborne wind lidar observations were performed in central europe employing the prototype of the satellite instrument the aladin airborne demonstrator a2d like the directdetection doppler wind lidar onboard aeolus the a2d is composed of a frequencystabilised ultraviolet laser a cassegrain telescope and a dualchannel receiver to measure lineofsight los wind speeds by analysing both mie and rayleigh backscatter signals in the frame of the first airborne validation campaign after the launch still during the commissioning phase of the mission four coordinated flights along the satellite swath were conducted in late autumn of 2018 yielding wind data in the troposphere with high coverage of the rayleigh channel owing to the different measurement grids and viewing directions of the satellite and airborne instrument intercomparison with the aeolus wind product requires adequate averaging as well as conversion of the measured a2d los wind speeds to the satellite los the statistical comparison of the two instruments with model wind data from the ecmwf shows biases of the a2d and aeolus los wind speeds of −09 m s−1 and 16 m s−1 respectively while the random errors are around 25 m s−1 the paper also discusses the influence of different threshold parameters implemented in the comparison algorithm as well as optimization of the a2d vertical sampling to be used in forthcoming validation campaigns
 this article analyzes the way public servants understand the necessity for information security in the digitalisation of public administration and sets out to describe a series of technical solutions with a view to protecting digital data this paper focuses on a study which was carried out on the basis of a questionnaire submitted to three entities from the romanian public administration the objective of this questionnaire was to identify the vulnerable points in certain areas such as managing accessibility in user interface design password management preventing cybersecurity incidents response capacity to cybersecurity incidents personal data protection data backup and recovery and personal evaluation the analysis of the results emerging from this questionnaire provides certain conceptual solutions which can be adopted so that information security in the romanian digital public administration can be ensured the solutions presented in this paper are based on the analysis of international documents in this field this article can also be a practical guide both for the romanian decision makers and civil servants enabling them to ensure data security and protection in the organisational structures of public administration preventing cybersecurity incidents protecting personal data data security solutions
since the uncertainty about an observable of a system prepared in a quantum state is usually described by its variance when the state is mixed the variance is a hybrid of quantum and classical uncertainties besides that complementarity relations are saturated only for pure singlequanton quantum states for mixed states the wave–particle quantifiers never saturate the complementarity relation and can even reach zero for a maximally mixed state so to fully characterize a quanton it is not sufficient to consider its wave–particle aspect one has also to regard its correlations with other systems in this paper we discuss the relation between quantum correlations and local classical uncertainty measures as well as the relation between quantum coherence and quantum uncertainty quantifiers we obtain a complete complementarity relation for quantum uncertainty classical uncertainty and predictability the total quantum uncertainty of a dpaths interferometer is shown to be equivalent to the wigner–yanase coherence and the corresponding classical uncertainty is shown to be an entanglement monotone the duality between complementarity and uncertainty is used to derive quantum correlations measures that complete the complementarity relations for l1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentl1enddocumentnorm and l2documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentl2enddocumentnorm coherences besides we show that brukner–zeilinger’s invariant information quantifies both the wave and particle characters of a quanton and we obtain a sum uncertainty relation for the generalized gellmann’s matrices
physical exercise is a powerful modulator of learning and memory mechanisms underlying the cognitive benefits of exercise are well documented in adult rodents exercise studies targeting postnatal periods of hippocampal maturation specifically targeting periods of synaptic reorganization and plasticity are lacking we characterize a model of earlylife exercise ele in male and female mice designed with the goal of identifying critical periods by which exercise may have a lasting impact on hippocampal memory and synaptic plasticity mice freely accessed a running wheel during three postnatal periods the 4 th postnatal week juvenile ele p21–27 6 th postnatal week adolescent ele p35–41 or 4 th 6 th postnatal weeks juvenileadolescent ele p21–41 all exercise groups increased their running distances during ele when exposed to a subthreshold learning stimulus juv ele and juvadol ele formed lasting longterm memory for an object location memory task whereas sedentary and adol ele mice did not electrophysiological experiments revealed enhanced longterm potentiation in hippocampal ca1 in the juvenileadolescent ele group io curves were also significantly modulated in all mice that underwent ele our results suggest that earlylife exercise specifically during the 4 th postnatal week can enable hippocampal memory synaptic plasticity and alter hippocampal excitability when occurring during postnatal periods of hippocampal maturation
wind turbines are often plagued by premature component failures with drivetrain bearings being particularly subjected to these failures to identify failing components vibration condition monitor 
when human movement is assisted or controlled with a muscle actuator such as electrical muscle stimulation a critical issue is the integration of such induced movement with the person’s motion intention and how this movement then affects their motor control towards achieving optimal integration and reducing feelings of artificiality and enforcement we explored perceptual simultaneity through electrical muscle stimulation which involved changing the interval between intentional and induced movements we report on two experiments in which we evaluated the ranges between detection and stimulus for perceptual simultaneity achievable with an electromyographytriggered electrical muscle stimulation system we found that the peak range was approximately 80160 ms with the timing of perceptual simultaneity shifting according to different adaptation states our results indicate that perceptual simultaneity is controllable using this adaptation strategy
gravity migration is a fast imaging technique based on the migration concept to obtain subsurface density distribution for higher resolution of migration imaging results we propose a 3d regularized focusing migration method that implements migration imaging of an entire gravity survey with a focusing stabilizer based on regularization theory when determining the model parameters the iterative direction is chosen as the conjugate migration direction and the step size is selected on the basis of the wolfe–powell conditions the model tests demonstrate that the proposed method can improve the resolution and precision of imaging results especially for blocky structures at the same time the method has high computational efficiency which allows rapid imaging for largescale gravity data it also has high stability in noisy conditions the developed novel method is applied to interpret gravity data collected from the skarntype iron deposits in yucheng shandong province migration results show that the depth of the buried iron ore in this area is 750–1500 m which is consistent with the drilling data we also provide recommendations for further mineral exploration in the survey area this method can be used to complete rapid global imaging of large mining areas and it provides important technical support for exploration of deep concealed deposits
election algorithm ea is a powerful metaheuristics model motivated by phenomena of the sociopolitical mechanism of the presidential election conducted in many countries ea is selected as a topic of discussion due to its capability and robustness to carry out complex problems in the random2sat logic program this paper utilizes a hybridized ea assimilated with the hopfield neural network hnn in carrying out random logic program hnnr2satea the efficiency of the proposed method was compared with the existing traditional exhaustive search hnnr2sates model and the recently introduced hnnr2satica model from the result obtained clearly proven that based on our proposed hybrid model outperformed other existing model based on the global minima ratio zm mean absolute error mae bayesian information criterion bic and execution time et the expected outcome portrays that the ea algorithm outperformed the other two algorithms in doing randomksat logic program the results proved the robustness effectiveness and compatibility of the hnnr2satea model
face masks provide effective easytouse and lowcost protection against airborne pathogens or infectious agents including sarscov2 there is a wide variety of face masks available on the market for various applications but they are all passive in nature ie simply act as air filters for the nasal passage andor mouth in this paper we present a new active mask paradigm in which the wearable device is equipped with smart sensors and actuators to both detect the presence of airborne pathogens in real time and take appropriate action to mitigate the threat the proposed approach is based on a closedloop control system that senses airborne particles of different sizes close to the mask and then makes intelligent decisions to reduce their concentrations this paper presents a specific implementation of this concept in which the onboard controller determines ambient air quality via a commercial particulate matter sensor and if necessary activates a piezoelectric actuator that generates a mist spray to load these particles thus causing them to fall to the ground the proposed system communicates with the user via a smart phone application that provides various alerts including notification of the need to recharge andor decontaminate the mask prior to reuse the application also enables a user to override the onboard control system and manually control the mist generator if necessary experimental results from a functional prototype demonstrate significant reduction in airborne particulate counts near the mask when the active protection system is enabled
automatic supervised classification with complex modelling such as deep neural networks requires the availability of representative training data sets while there exists a plethora of data sets that can be used for this purpose they are usually very heterogeneous and not interoperable in this context the present work has a twofold objective i to describe procedures of opensource training data management integration and data retrieval and ii to demonstrate the practical use of varying source training data for remote sensing image classification for the former we propose satimnet a collection of open training data structured and harmonized according to specific rules for the latter two modelling approaches based on convolutional neural networks have been designed and configured to deal with satellite image classification and segmentation
"purpose the purpose of this study was to determine the extent to which teachers and principals are prepared and capacitated to implement icts in the nepad eschools kenya 
methodology this study adopted a descriptive survey design both quantitative and qualitative data were collected the target population for the study were all principals and 256 teachers from 6 model eschools sampling was done employing a mixture of methods stratified sampling was used to pick schools while purposive sampling was used to pick principals and teachers five principals and 110 teachers were sampled data was collected using questionnaire for teachers and structured interview for principals the collected data was coded run for descriptive analysis including frequencies percentages measures of central tendency and measures of variabilityspread and presented with the aid of appropriate notes frequency tables percentages charts and figures 
findings the study established that despite most teachers and principals having been trained they had not been sufficiently empowered towards effectively and successfully applying icts moreover majority of the untrained teachers and principal had not trained because no training had occurred since they joined the schools however despite their status most teachers believed they could still perform some tasks involving icts most trained teachers and principals needed to be trained retrained andor trained further on several skills especially in the preparation and use of multimedia instructional tools and online research and communication the study concluded that most teachers and principals may be willing to apply icts but are limited by the nature and extent of their empowerment moreover icts application programmes are likely to succeed better if they employ regular ongoing training 
unique contribution to theory practice and policy the study could inform education planners and trainers about necessary adjustments to future preservice and inservice teacher training programmes involving application of icts it would also contribute to the body of knowledge in educational technology which might inform theory and practice in icts integration it could also inform the development of best practices in the application and integration of icts in instruction"
several analytical methodologies help estimate the shear strength of rock discontinuities whose main limitations are the difficulty to obtain all necessary parameters to satisfactorily represent the boundary conditions and influence of infill materials the objective of this study is to present a predictive model of peak shear strength for soft rock discontinuities developed making use of an artificial neural network known as multilayer perceptron the model’s input variables are normal stiffness initial normal stress acting on the discontinuity joint roughness coefficient jrc ratio ta fill thicknessasperity height uniaxial compressive strength and the basic friction angle of the intact rock and finally the internal friction angle of infill material to do so results from 115 direct shear tests with different soft rock discontinuities conditions were used the herein proposed ann predictive model with an architecture 7201 have shown coefficient of correlation in training and validation of 998  and 99  respectively the results from the model satisfactorily fit the experimental data and were also able to represent the influence of the input variables on the peak shear strength of soft rock discontinuities for different infill and boundary conditions
we solve the boundary value problem for einstein’s gravitational field equations in the presence of matter in the form of an incompressible perfect fluid of density ρ and pressure field pr located in a ball r≤r0 we find a 1parameter family of timeindependent and radially symmetric solutions gaρapa−2maa1 satisfying the boundary conditions ggs and p0 on rr0 where gs is the exterior schwarzschild solution solving the gravitational field equations for a point mass m concentrated at r0 and containing for a0 the interior schwarzschild solution ie the classical perfect fluid star model we show that schwarzschild’s requirement r09κm4c2 identifies the “physical” ie such that par≥0 and par is bounded in 0≤r≤r0 solutions paa∈u0 for some neighbourhood u0⊂−2m∞ of a0 for every star model gaa0aa1 we compute the volume va of the region r≤r0 in terms of abelian integrals of the first second and third kind in legendre form
histopathologic distinction between keratoacanthoma ka and squamous cell carcinoma scc is challenging we surmised that a discriminatory immunostain would be clinically meaningful previous investigators have found cd123‐positive plasmacytoid dendritic cells pdcs are more prominent in ka than scc we sought to determine if cd123 immunostaining might have value as a diagnostic test for distinguishing ka from scc
many existing approaches for estimating feature importance are problematic because they ignore or hide dependencies among features a causal graph which encodes the relationships among input variables can aid in assigning feature importance however current approaches that assign credit to nodes in the causal graph fail to explain the entire graph in light of these limitations we propose shapley flow a novel approach to interpreting machine learning models it considers the entire causal graph and assigns credit to textitedges instead of treating nodes as the fundamental unit of credit assignment shapley flow is the unique solution to a generalization of the shapley value axioms to directed acyclic graphs we demonstrate the benefit of using shapley flow to reason about the impact of a models input on its output in addition to maintaining insights from existing approaches shapley flow extends the flat setbased view prevalent in game theory based explanation methods to a deeper textitgraphbased view this graphbased view enables users to understand the flow of importance through a system and reason about potential interventions
in this paper the impact of temperature and sourcedrain overlap on dc characteristics and analogrf performance of the negative capacitance silicon nanotube fet nc si ntfet have been presented the performance of the device is examined by coupling the 3d tcad numerical simulation with 1d landau–khalatnikov lk equation the effect of negative capacitance has been investigated for different sourcedrain overlap lengths inlineformulatexmath notationlatexboldsymbollboldsymbolovtexmathinlineformula at 300 k and 380 k the optimal values of sourcedrain overlap lengths and effective ferroelectric thickness inlineformulatexmath notationlatexboldsymboltboldsymbolftexmathinlineformula have been found to attain significant improvement in device performance in terms of drain current inlineformulatexmath notationlatexboldsymboliboldsymboldtexmathinlineformula subthreshold swing inlineformulatexmath notationlatexboldsymbolsstexmathinlineformula gate capacitance inlineformulatexmath notationlatexboldsymbolcboldsymbolgtexmathinlineformula and cutoff frequency inlineformulatexmath notationlatexboldsymbolfboldsymbolttexmathinlineformula it is reported that a minimum inlineformulatexmath notationlatexboldsymbolss texmathinlineformulaof 307 mvdecade and maximum inlineformulatexmath notationlatexboldsymbolfboldsymbolttexmathinlineformula of 235 thz are achieved for nc si ntfet at 300 k for an operating frequency of 1 ghz it is observed that the core inner gate radius inlineformulatexmath notationlatextctexmathinlineformula can be used to increase the minimum italicssitalic and italicisubonsubitalic to delay the hysteresis and to achieve a better drive current
the current human activity recognition har methods need training data from users the data collection causes discomfort to the users and most of the studies ignore the realtime performance of classification this paper presents a realtime human activity recognition approach with strong generalization performance it uses existing dataset to avoid longterm data collection of subjects so that the machine can be quickly applied to each specific individual also it takes advantage of both combined accuracy and limited feature selection proposed by this paper to implement featureselectionbased transfer learning which improves har in both realtime and generalization performance in view of the recognition time and accuracy the depth neural network is selected changeable structure of which is more suitable for feature selection this approach utilizes four inertial measurement units placed on the outside of human thighs and shanks a total of seven activities are taken into account that includes levelwalking upstairs downstairs uphill downhill standing and sitting the experiments are performed on six healthy male subjects in freeliving settings to evaluate the efficacy of the algorithm this approach achieved a notable activity recognition accuracy of 9889 and reported a fast average activity classification time of 286 ms
smallscale mixing or actively known as micromixing had an utmost importance in the biological and chemical applications using micro total analysis systems tas or labonchips micromixing is achieved by stirring or agitating liquid or particles however in microfluidic technology it is very difficult to do mixing for a very small amount of liquid including microliter volume chemical reactions are often involved in most microfluidic applications hence making the fluid diffusivity to become very low thus chaotic advection was introduced in this study to shorten the reaction time conveniently a new method was proposed to actively mix micro volume liquid in this study we developed focusedsurface acoustic wave fsaw fabricated on a piezoelectric substrate to manipulate or mix two different color dyes with low concentration operation frequency of 50 mhz was used to generate acoustic waves along the substrate surface and we varied the droplet volume and input power on the fsaw mixing method comparison between passive and active mixing was done by studying the time taken for diffusion mixing designing fabrication and test on yjunction channel micromixers were done to compare and examine the mixing performance and time experimental results showed that mixing by fsaw device has achieved high efficiency of 91 – 84 in droplet volume of 1 – 5 µl therefore fsaw mixing has been proven more efficient and fast response compared to diffusion mixing
stenosis is the primary complication of current tissueengineered vascular grafts used in pediatric congenital cardiac surgery murine models provide considerable insight into the possible mechanisms underlying this situation but they are not efficient for identifying optimal changes in scaffold design or therapeutic strategies to prevent narrowing in contrast computational modeling promises to enable time and costefficient examinations of factors leading to narrowing whereas past models have been limited by their phenomenological basis we present a new mechanistic model that integrates molecular and cellulardriven immuno and mechanomediated contributions to in vivo neotissue development within implanted polymeric scaffolds model parameters are inferred directly from in vivo measurements for an inferior vena cava interposition graft model in the mouse that are augmented by data from the literature by complementing bayesian estimation with identifiability analysis and simplex optimization we found optimal parameter values that match model outputs with experimental targets and quantify variability due to measurement uncertainty utility is illustrated by parametrically exploring possible graft narrowing as a function of scaffold pore size macrophage activity and the immunomodulatory cytokine transforming growth factor beta 1 tgfβ1 the model captures salient temporal profiles of infiltrating immune and synthetic cells and associated secretion of cytokines proteases and matrix constituents throughout neovessel evolution and parametric studies suggest that modulating scaffold immunogenicity with early immunomodulatory therapies may reduce graft narrowing without compromising compliance
analysis on microdoppler features using timefrequency analysis and superresolution imaging has been a vital direction in radar target identification however because of heavy ground clutter it’s difficult to use timefrequency analysis in sargmti1 especially for lowvelocity target besides the traditional range model approximated by secondorder polynomials is not suitable for micromotion for the above bottleneck a regularity of interferometric phase of the micromotion target in multiple spatial channel in image domain is derived based on building the geometrical model in this paper besides the correlation coefficient of the spatial steering vectors between each ghost point and clutter is introduced the result of simulation also proves the founded principle has great improvement factor on detecting micromotion targets in strong clutters
introduction some surgical site infections ssi could be prevented by following adequate infection prevention and control ipc measures poor compliance with ipc measures often occurs due to knowledge gaps and insufficient education of healthcare professionals the education and training of ssi preventive measures does not usually take place in the operating room or due to safety and organisational and logistic issues the proposed study aims to compare virtual reality vr as a tool for medical students to learn the ssi prevention measures and adequate behaviours eg limit movements… in the or to conventional teaching methods and analysis this protocol describes a randomised controlled multicentre trial comparing an educational intervention based on vr simulation to routine education this multicentre study will be performed in three universities grenoble alpes university france imperial college london uk and university of heidelberg germany thirdyear medical students of each university will be randomised in two groups the students randomised in the intervention group will follow vr teaching the students randomised in the control group will follow a conventional education programme primary outcome will be the difference between scores obtained at the ipc exam at the end of the year between the two groups the written exam will be the same in the three countries secondary outcomes will be satisfaction and students’ progression for the vr group the data will be analysed with intentiontotreat and per protocol ethics and dissemination this study has been approved by the medical education ethics committee of the london imperial college meec1920172 by the ethical committee for the research of grenoble alpes university cer grenoble alpesavis2019099242 and by the ethics committee of the medical faculty of heidelberg university s7652019 results will be published in peerreviewed medical journals communicated to participants general public and all relevant stakeholders
the article focuses on public engagement and recent modifications in citizen participation through a case study regarding the collaborative governance of urban commons in the city of bologna italy civic collaboration is an experimental partnership which is being implemented between public administrations and citizens in order to develop treat and reuse commons with a view to improving the quality of life in cities the goal of the project is to understand whether and how civic collaboration is also transforming citizen participation in local public policies this article presents the results of research which was performed by interviewing citizens who are involved in the collaborative governance of urban commons this contribution aims to connect the literature regarding open government and its impact on participation with the consolidated debate regarding the role of culture in the conception of citizenship and civic engagement and as a consequence in the effectiveness of collaborative governance particular attention was paid to citizen engagement and to the role performed by both public and private platforms and digital media
techniques for nonstationary signal analysis are important in understanding dynamical behaviour of complex systems timefrequency coherence is widely used to analyse timevarying characteristics in nonstationary signals this paper presents waveletbased methods using airy wavelet to estimate coherence we incorporate a novel technique for removal of low frequency components due to envelope modulation in nonstationary signals the technique is demonstrated on synthetic and real neurophysiological data results not only provide a clear description of desired features in nonstationary signals but also suppress low frequency components due to envelope modulation our novel technique shows an effectiveness in extracting features hidden within the signals it may lead to improved results in coherence analysis of medical biological physical and geophysical data containing low frequency envelope modulation besides nonstationarities
remote sensing systems based on unmanned aerial vehicles uavs are well suited for airborne monitoring of small to mediumsized farmland in agricultural applications an imaging system is often used in the form of a multispectral multicamera system to derive wellestablished vegetation indices vis efficiently this study investigates the potential of such a multicamera system with a novel approach to extend spectral sensitivity from visibletonearinfrared vnir to shortwave infrared swir 400–1700 nm for estimating forage mass from an aerial carrier platform the system test was performed in a grassland fertilizer trial in germany near cologne in late july 2019 within 37 min a spectral response in four different wavelength bands in the nir and swir range was acquired during two consecutive flights spectral image data were calibrated to reflectance using two different methods the resulting reflectance data sets were processed to orthomosaics for each wavelength band from these orthomosaics for both calibration methods the fourband nirswir gnyli vi and the twoband nirswir normalized ratio index nri were calculated during both uav flights spectral ground truth data were recorded with a spectroradiometer on 12 plots in total for validation of camerabased spectral data the camera and spectroradiometer data sets were directly compared in resulting reflectance and further analyzed with simple linear regression slr models to predict dry matter dm yield in the camerabased slrs the nri performed best with r2documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentr2enddocument of 073 and 075 rmse 018 and 017 before the gnyli with r2documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentr2enddocument of 071 and 073 rmse 019 and 018 these results clearly indicate the potential of the camera system for applications in forage mass monitoring
although transcranial photoacoustic imaging tcpai has been used in small animal brain imaging in animals with thicker skull bones or in humans both light illumination and ultrasound propagation paths are affected hence the pa image is largely degraded and in some cases completely distorted this study aims to investigate and determine the maximum thickness of the skull through which photoacoustic imaging is feasible in terms of retaining the imaging target structure without incorporating any post processing we identify the effect of the skull on both the illumination path and acoustic propagation path separately and combined in the experimental phase the distorting effect of ex vivo sheep skull bones with thicknesses in the range of 0713 mm are explored we believe that the findings in this study facilitate the clinical translation of tcpai
spam can have fake links and trick users into redirecting to fake pages and stealing users information on these pages machine learning methods are typically used to detect spam to distinguish the spam pattern from normal text a good way to detect spam is to use machine learning techniques that can detect the pattern of spam while large volumes of text or email need to be considered these methods take a long time to make using these methods impractical a good way to accurately and quickly detect large volumes of spam is to use big data processing platforms such as apache spark in this research an attempt has been made to use feature selection by metaheuristic methods to increase the speed and efficiency in spam detection in this method the data is entered into apache spark for final processing to be detected by spam with highspeed distributed learning experiments show that using the feature selection mechanism can increase learning accuracy by 9866 the processing speed in apache spark is about 5453 times higher than nonspark systems
the design of interactive visualization systems is challenging due to several aspects involving data interpretation and manipulation mechanisms designers need adequate support in the conception of these tools for facilitating the interaction of users with data in this article we propose a set of design guidelines on humandata interaction hdi with focus on information visualization iv our recommendations were elaborated based on analysis and summary of knowledge from literature combined with our own design experience with this kind of systems this study shows to which extent the organized guidelines were understood used and discussed by computer science students in the construction and evaluation of software prototypes for datadriven decision making participants were able to understand the guidelines and applied them with success for evaluation of hdi in iv systems
automotive radar plays an important role in advanced driver assistant systems to support level2 automated driving functions however the mutual interference between automotive radars increases due to the rising density of radars on the road therefore the radar signal will be distorted to some extent and the performance of radars will degrade if no countermeasures are taken in this paper an interference mitigation approach using compressive sensing cs and bayesian learning is introduced by utilizing the sparsity of the beat signal in the frequency domain the rangedoppler rd spectrum can be reconstructed with the help of undistorted samples in the beat signal the sparse bayesian learning method sbl is used to estimate the posterior of the signals sparse representation and to infer the maximally sparse representation by using the expectation maximization em algorithm it is shown that the sblbased method has its advantages in signaltointerferenceplusnoise ratio sinr and target peak detection in comparison to conventional cs or classical signal reconstruction algorithms like linear predictive coding lpc
our brains at rest spontaneously replay recently acquired information but how this process is orchestrated to avoid interference with ongoing cognition is an open question we investigated whether replay coincided with spontaneous patterns of whole brain activity we found in two separate datasets that replay sequences were packaged into transient bursts occurring selectively during activation of the default mode network dmn and parietal alpha network these networks were characterized by widespread synchronized oscillations coupled to increases in ripple band power mechanisms that coordinate information flow between disparate cortical areas our data show a tight correspondence between two widely studied phenomena of neural physiology and suggest the dmn may coordinate replay bursts in a manner that minimizes interference with ongoing cognition
sequential recommendation systems model dynamic preferences of users based on their historical interactions with platforms despite recent progress modeling shortterm and longterm behavior of users in such systems is nontrivial and challenging to address this we present a solution enhanced by a knowledge graph called katrec knowledge aware attentive sequential recommendations katrec learns the short and longterm interests of users by modeling their sequence of interacted items and leveraging preexisting side information through a knowledge graph attention network our novel knowledge graphenhanced sequential recommender contains item multirelations at the entitylevel and users dynamic sequences at the itemlevel katrec improves item representation learning by considering higherorder connections and incorporating them in user preference representation while recommending the next item experiments on three public datasets show that katrec outperforms stateoftheart recommendation models and demonstrates the importance of modeling both temporal and side information to achieve highquality recommendations
semakin majunya teknologi di indonesia membuat seluruh pelosok indonesia perlu memeratakan teknologi yang ada teknologi saat ini semakin tren karena banyaknya para startup bermunculan di indonesia sehingga saat ini teknologi semakin digunakan semakin sumber penghubung informasi dalam sistem infromasi yang akan digunakan teknologi pada pendidikan saat ini mulai diratakan salah satunya pada universitas dalam mempermudah segala sistem yang berjalan dan salah satunya sistem administrasi diperlukan yaitu teknologi teknologi untuk meringankan user dan mensinkronkan segala informasi yang ada menjadi suatu informasi yang utuh untuk digunakan tetapi dalam membangun suatu sistem diperlukan perancangan terlebih dahulu yaitu sdd software design description untuk menjadi acuan dalam perancangan sistem dan dokumen dalam merancang dokumen sdd software design description diperlukan dokumen srs software requirement specification sebagai acuan dalam merancang dokumen sdd software design description hasil akhir dari perancangan terkait adalah berupa dokumen sdd software design description yang berisikan reverse engineering software design description dari sistem informasi macis mahasiswa
this paper investigates the inventory policies of a multiechelon inventory network consisting of one central warehouse three secondechelon warehouses and six thirdechelon warehouses the operation process of the inventory network is translated into a computerized simulation model crossregion transshipment policy under the threshold of transshipment limitation and the dynamic reorder points in response to changing demand are considered in the simulation model genetic algorithm is used to minimize the total cost of the inventory network finally numerical examples are illustrated to prove the superiority of the optimal policy
the recent advances in the chronic implantation of electrodes have allowed the collection of extracellular activity from neurons over long periods of time to fully take advantage of these recordings it is necessary to track single neurons continuously particularly when their associated waveform changes over time multiple spike sorting algorithms can track drifting neurons but they do not perform well in conditions like a temporary increase in the noise level and changes in the number of detectable neurons in this work we present trackinggraph a general framework to track neurons under these conditions trackinggraph can be implemented with different spike sorting algorithms allowing the experimenter to use the algorithm best fitted for their setup the main idea behind trackinggraph is the blockwise analysis of the recording and application of a classification algorithm to match spikes to templates in different segments leading to a directional metric that can be used to link clusters across blocks moreover the algorithm can detect and fix sorting errors splits and merges in isolated blocks we compared an implementation of trackinggraph with other algorithms using longterm simulations and obtained superior performance in all the metrics
the minimum length bounded cut problem is a natural variant of minimum cut  given a graph terminal nodes s t and a parameter l  ﬁnd a minimum cardinality set of nodes other than s t  whose removal ensures that the distance from s to t is greater than l  we focus on the approximability of the problem for bounded values of the parameter l  the problem is solvable in polynomial time for l ≤ 4 and nphard for l ≥ 5 the best known algorithms have approximation factor cid100  l − 1  2 cid101  it is nphard to approximate the problem within a factor of 1  1715 and unique games hard to approximate it within ω l  for any l ≥ 5 moreover for l  5 the problem is 4  3 − ε unique games hard for any ε  0 our ﬁrst result matches the hardness for l  5 with a 4  3approximation algorithm for this case improving over the previous 2approximation for 6bounded cuts we give a 7  4approximation improving over the previous best 3approximation more generally we achieve approximation ratios that always outperform the previous cid100  l − 1  2 cid101 guarantee for any ﬁxed value of l  while for large values of l  we achieve a signiﬁcantly better 11  25 l  o 1 approximation allouralgorithms apply in the weighted setting in both directed and undirected graphs as well as for edgecuts which easily reduce to the nodecut variant moreover by rounding the natural linear programming relaxation our algorithms also bound the corresponding boundedlength ﬂowcut gaps
software design is a complex activity a successful designer requires knowledge and training in specific design techniques combined with practical experience designing a dimensional model is not an exception to this challenge we present dimensional design patterns ddps that can be used for designing dimensional models we present a metamodel of the ddp and show how to integrate them into kimball’s dimensional modeling design process
reinforcement learning agent learns how to perform a task by interacting with the environment the use of reinforcement learning in reallife applications has been limited because of the sample efficiency problem interactive reinforcement learning has been developed to speed up the agent’s learning and facilitate to learn from ordinary people by allowing them to provide social feedback eg evaluative feedback advice or instruction inspired by reallife biological learning scenarios there could be many ways to provide feedback for agent learning such as via hardware delivered natural interaction like facial expressions speech or gestures the agent can even learn from feedback via unimodal or multimodal sensory input this paper reviews methods for interactive reinforcement learning agent to learn from human social feedback and the ways of delivering feedback finally we discuss some open problems and possible future research directions
abstract advanced and accurate forecasting of covid‐19 cases plays a crucial role in planning and supplying resources effectively artificial intelligence ai techniques have proved their capability in time series forecasting non‐linear problems in the present study the relationship between weather factor and covid‐19 cases was assessed and also developed a forecasting model using long short‐term memory lstm a deep learning model the study found that the specific humidity has a strong positive correlation whereas there is a negative correlation with maximum temperature and a positive correlation with minimum temperature was observed in various geographic locations of india the weather data and covid‐19 confirmed case data 1 april to 30 june 2020 were used to optimize univariate and multivariate lstm time series forecast models the optimized models were utilized to forecast the daily covid‐19 cases for the period 1 july 2020 to 31 july 2020 with 1 to 14 days of lead time the results showed that the univariate lstm model was reasonably good for the short‐term 1 day lead forecast of covid‐19 cases relative error 20 moreover the multivariate lstm model improved the medium‐range forecast skill 1–7 days lead after including the weather factors the study observed that the specific humidity played a crucial role in improving the forecast skill majorly in the west and northwest region of india similarly the temperature played a significant role in model enhancement in the southern and eastern regions of india
the effect of zno nanorods on the humiditysensing performance of graphene foam gf has been studied the gf is synthesized by atmospheric pressure chemical vapor deposition apcvd using xylene as the hydrocarbon source hydrothermal rout is utilized for getting zno nanorods loaded on the surface of gf the obtained materials are characterized using scanning electron microscopy sem xray diffraction xrd raman spectroscopy furrier transform infrared spectroscopy ftir thermal gravimetric analysis tga bet surface area and bjh pore diameter distribution the results of this study showed that zno nanorods are densely and homogeneously loaded over gf skelton the peak values of surface areas are estimated to be 20111 cm2g and 4523 cm2g for gf and gfzno respectively the humidity sensing performance of znogf composite shows a linear relationship with the rh value from 20 to 95
the purpose of this study is to determine and analyze the preconditions as well as to provide an overview of the acceleration of integrated administrative service of subdistrict in karangpawitan subdistrict garut district west java indonesia this study is a descriptive study using qualitative approach the data are collected using interview and document study the respondents are determined using purposive sampling technique as informants and key persons the data are then analyzed using qualitative data analysis through reduction data presentation and conclusion the findings show that the preconditions of paten in karangpawitan subdistrict garut district are not fully prepared if observed from the substantive and technical requirements the acceleration of paten is prepared but from administrative requirements it is unprepared because there is no regulation of regent stipulating the service standards and job descriptions of the subdistrict personnel for the implementation of paten the efforts to be carried out by the government of karangpawitan subdistrict in order to accelerate paten in karangpawitan subdistrict garut district are 1 implementing paten with full commitment in accordance with the authority delegated by the regent to the head of subdistrict 2 developing and utilizing information technology with computerized system to facilitate access to public service 3 maximizing the performance of the technical team of paten in establishing cooperation and coordination with related regional working unit skpd to complete the implementation requirements of paten 4 providing information and socialization about paten to the public in order to increase public awareness to manage licensing and nonlicensing services in subdistrict 5 changing the mindset of subdistrict officials by promoting public demand for the quality of public services provided 6 applying reward and punishment to support bureaucratic reform in public service 7 capacity building of human resources sdm of subdistrict apparatus in conducting qualified public service
a large part of the current success of deep learning lies in the effectiveness of data  more precisely labelled data yet labelling a dataset with human annotation continues to carry high costs especially for videos while in the image domain recent methods have allowed to generate meaningful pseudo labels for unlabelled datasets without supervision this development is missing for the video domain where learning feature representations is the current focus in this work we a show that unsupervised labelling of a video dataset does not come for free from strong feature encoders and b propose a novel clustering method that allows pseudolabelling of a video dataset without any human annotations by leveraging the natural correspondence between the audio and visual modalities an extensive analysis shows that the resulting clusters have high semantic overlap to ground truth human labels we further introduce the first benchmarking results on unsupervised labelling of common video datasets kinetics kineticssound vggsound and ave
previous studies have demonstrated that evolutionarily threatening information and goalrelevant information can both capture attention however some studies have suggested that goalrelevant information is prioritized over evolutionarily threatening information while some studies have shown the opposite conclusion the aim of the present study was to investigate the attention advantage by presenting evolutionarily threatening information and goalrelevant information simultaneously three conditions were presented in this study evolutionarily threatening information  an irrelevant stimulus goalrelevant information  an irrelevant stimulus and evolutionarily threatening information  goalrelevant information the behavioral results showed no attentional bias toward evolutionarily threatening information in the two conditions including evolutionarily threatening information in the two conditions including goalrelevant information participants showed attentional bias toward goalrelevant information in both however the erp results showed that in the two conditions including evolutionarily threatening information a significantly stronger n2pc response was seen for evolutionarily threatening information than for the other types of pictures and goalrelevant information produced a significantly stronger n2pc response than that for an irrelevant stimulus the abovementioned results indicated that in the earlier stage of attention both evolutionarily threatening information and goalrelevant information have attention processing advantages over irrelevant stimuli furthermore attention was captured by evolutionarily threatening information faster than it was by goalrelevant information
entity and relation extraction for chinese texts are typical performed in a pipelined fashion in the sense that by ﬁrst segmenting sequence into words then recognizing entities and relations subsequently however this process often leads to the problem of error propagation and prevents crosstask information integration to address this issue we propose a novel transitionbased model that performs nested entity recognition and relation extraction jointly without the need of word segmentation beforehand which is achieved by leveraging the recent advance in using a lattice structure for chinese sentence encodings on standard ace benchmarks our model gives the best results in the literature further analyses show the effectiveness of the proposed architecture in capturing structural outputs
we propose a unified multitasking framework to represent the complex and uncertain causal process of financial market dynamics and then to predict the movement of any type of index with an application on the monthly direction of the sp500 index our solution is based on three main pillars i the use of transfer learning to share knowledge and feature representation learning between all financial markets increase the size of the training sample and preserve the stability between training validation and test sample ii the combination of multidisciplinary knowledge financial economics behavioral finance market microstructure and portfolio construction theories to represent a global topdown dynamics of any financial market through a graph iii the integration of forward looking unstructured data different types of contexts long medium and short term through latent variablesnodes and then use a unique vae network parameter sharing to learn simultaneously their distributional representation we obtain accuracy f1score and matthew correlation of 743  67  and 042 above the industry and other benchmark on 12 years test period which include three unstable and difficult subperiod to predict
a new class of stochastic processes called episodic processes is introduced to model the statistical regularity of data observed in several applications in cyberphysical systems neuroscience and medicine algorithms are proposed to detect a change in the distribution of episodic processes the algorithms can be computed recursively using finite memory and are shown to be asymptotically optimal for welldefined bayesian or minimax stochastic optimization formulations the application of the developed algorithms to detect a change in waveform patterns is also discussed
a code x is kcircular if any concatenation of at most k words from x when read on a circle admits exactly one partition into words from x it is circular if it is kcircular for every integer k while it is not a priori clear from the definition there exists for every pair nℓdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentnell enddocument an integer k such that every kcircular ℓdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentell enddocumentletter code over an alphabet of cardinality n is circular and we determine the least such integer k for all values of n and ℓdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentell enddocument the kcircular codes may represent an important evolutionary step between the circular codes such as the commafree codes and the genetic code
hiking is a popular recreational activity and to cater to public demand it is apt to increase the number of hiking trails various methodologies have been proposed to evaluate the suitability of forest trails to be constructed as hiking trails but they can be costly and require relevant knowledge in analyzing digital information through a highthroughput dataset therefore there is a need to come up with a simple method to obtain firsthand information on the trail condition particularly considering the aspects of safety and suitability to hikers using both onground and aerial observations in this study we introduce a new assessment approach to analyze and select old forest trails to be reconstructed as new hiking trails this is useful for park managers who prioritize safety comfort and aesthetic features of the recreation site for their visitors trail condition assessment was carried out along the trail whereby a 2×2 m sampling plot was constructed at every 100 m aerial drone survey was conducted to produce an orthomosaic that revealed the percentage of exposed trail from above potential phytotourism products and scenic spots were identified and recorded for their locations along the trail to promote the aesthetic value of the recreation site a strength distribution plot was prepared based on the trail condition canopy coverage and aesthetic features along the trail that were categorized using three altitude ranges n ≤ 150 m 150  n  250 m n ≥ 250 m asl this is to assess the tradeoffs in safety comfort and aesthetic features along the trail the development of this methodology offers a direct and costeffective yet informative approach to evaluate the quality of a potential hiking trail thus could effectively aid in the promotion of naturebased tourism
the informationbearing molecules are used for communication among nanomachines in a diffusionbased molecular communication dmc system the location of the nanomachine is needed for improving the performance of a dmc system in the literature only the distance between a transmitter nanomachine tn and a receiver nanomachine is estimated in contrast an iterative maximum likelihood estimate of the nanomachine location is computed herein notably both the signaldependent noise and intersymbol interference isi effects are considered for nanomachine localization further the nanomachine localization is performed when the locations of the receiver nanomachine are both known and unknown additionally the condition for the unique localizability of the tn and the bound of the proposed estimator are obtained the results illustrate the effectiveness of the proposed method
toric topology assigns to each ndimensional combinatorial simple convex polytope p with m facets an mndimensional momentangle manifold mathcalzp with an action of a compact torus tm such that mathcalzptm is a convex polytope of combinatorial type p a simple npolytope is called brigid if any isomorphism of graded rings hmathcalzpmathbb z hmathcalzqmathbb z for a simple npolytope q implies that p and q are combinatorially equivalent an ideal almost pogorelov polytope is a combinatorial 3polytope obtained by cutting off all the ideal vertices of an ideal rightangled polytope in the lobachevsky hyperbolic space mathbb l3 these polytopes are exactly the polytopes obtained from any not necessarily simple convex 3polytopes by cutting off all the vertices followed by cutting off all the old edges the boundary of the dual polytope is the barycentric subdivision of the boundary of the old polytope and also of its dual polytope we prove that any ideal almost pogorelov polytope is brigid this produces three cohomologically rigid families of manifolds over ideal almost pogorelov manifolds momentangle manifolds canonical 6dimensional quasitoric manifolds and canonical 3dimensional small covers which are pullbacks from the linear model
the field of network neuroscience provided unprecedented insights into how brain connectivity gets altered by autism spectrum disorder asd on functional structural and morphological levels however a few studies have looked to design a framework that captures the complex network structure of the brain and disentangles the heterogeneity of asd in this paper we leverage multikernel unsupervised learning in the construction of multiview hypergraph neural networks hgnn each capturing a particular view of the brain connectome to eventually distinguish between asd and normal control nc subjects additionally we tested and measured how our proposed framework compares to other variants based on previous baseline methods our classification results outperformed comparison methods and agreed with the literature in the sense that the right hemisphere connectivity was more discriminative in asd diagnosis than the left hemisphere
most of the literature on supply chain management assumes that the demand distributions and their parameters are known with certainty however this may not be the case in practice since decision makers may have access to limited amounts of historical demand data only in this case treating the demand distributions and their parameters as the true distributions is risky and it may lead to suboptimal decisions to demonstrate this this paper considers an inventoryrouting problem with stochastic demands in which the retailers have access to limited amounts of historical demand data we use simheuristic method to solve the optimisation problem and investigate the impact of the limited amount of demand data on the quality of the simheuristic solutions to the underlying optimisation problem our experiment illustrates the potential impact of input uncertainty on the quality of the solution provided by a simheuristic algorithm
sensors exploiting longrange surface plasmon polariton lrspp waveguides comprised of au stripes embedded in cytop with integrated and encapsulated microfluidic channels are fabricated and demonstrated a fabrication approach was devised where the lower cladding and recessed au stripes are fabricated on a si supporting substrate and the upper cladding and microfluidic channels are fabricated on a glass substrate followed by wafer bonding to assemble the wafers into complete sealed structures the bond is centered over the full length of the optical path yet no evidence of optical scattering or excess loss due to the bond could be observed and no evidence of a bonding interface could be discerned from highmagnification crosssectional images we also demonstrate waferscale fabrication of inplane microfluidic inlets and outlets along with a fixture for fluidic edge coupling that provides sealed interfaces to external fluidic tubing and components inplane microfluidic interfaces are automatically defined along chip facets upon wafer dicing precluding the need to drill through holes in lids the performance of the chips was assessed by measuring the attenuation of lrspps on fully cladded reference waveguides on waveguides passing through microfluidic channels and by measuring the response of sensors to changes in refractive index produced by injecting various sensing solutions our fabrication approach based on wafer bonding and inplane fluidic interfacing is compelling for lowcost highvolume manufacturing
in this paper we deal with the problem of testing statistical hypothesis under fuzzy environment particularly for trapezoidal fuzzy data in order to test the statistical hypothesis for trapezoidal fuzzy data we extend the signed distance with the suitable modiﬁcation of average and width of triangular fuzzy data in the new signed distance introduced by areﬁ to trapezoidal fuzzy data the method introduced in this paper is also to be employed for testing hypothesis of various statistical parameters that is mean and variance finally some numerical examples are given to demonstrate the feasibility of the revised signed distance for statistical hypothesis testing
submeter highresolution remote sensing image land cover classification could provide significant help for urban monitoring management and planning deep learning dlbased models have achieved remarkable performance in many land cover classification tasks through endtoend supervised learning however the excellent performance of dlbased models relies heavily on a large number of wellannotated samples which is impossible in practical land cover classification scenarios additionally the training set could contain all of the different land cover types to overcome these problems in this article a semisupervised multiplecnn ensemble learning method namely semimcnn is proposed to solve the land cover classification problem considering the lack of labeled samples a semisupervised learning strategy was adopted to leverage large amounts of unlabeled data in the proposed approach an automatic sample selection method called an ensembled teacher model dataset generation was adopted to select samples and generate a dataset from large amounts of unlabeled data automatically to tackle the error propagation problem an important strategy was adopted to correct the errors by pretraining on the selected unlabeled data and finetuning on the labeled data moreover the semisupervised idea together with the multicnn ensemble framework was integrated into an endtoend architecture this could significantly improve the generalization ability of the semisupervised model as well as the classification accuracy experiments were conducted on shenzhens land cover data and two other public remote sensing datasets these experiments confirmed the superior performance of the proposed semimcnn compared to the stateoftheart land cover classification models
since late december 2019 the coronavirus pandemic covid19 previously known as 2019ncov caused by the severe acute respiratory syndrome coronavirus 2 sarscov2 has been surging rapidly around the world with more than 1700000 confirmed cases the world faces an unprecedented economic social and health impact the early rapid sensitive and accurate diagnosis of viral infection provides rapid responses for public health surveillance prevention and control of contagious diffusion more than 30 of the confirmed cases are asymptomatic and the high falsenegative rate fnr of a single assay requires the development of novel diagnostic techniques combinative approaches sampling from different locations and consecutive detection the recurrence of discharged patients indicates the need for longterm monitoring and tracking diagnostic and therapeutic methods are evolving with a deeper understanding of virus pathology and the potential for relapse in this review a comprehensive summary and comparison of different sarscov2 diagnostic methods are provided for researchers and clinicians to develop appropriate strategies for the timely and effective detection of sarscov2 the survey of current biosensors and diagnostic devices for viral nucleic acids proteins and particles and chest tomography will provide insight into the development of novel perspective techniques for the diagnosis of covid19
intraoperative functional mapping with direct electrical stimulation during awake surgery for patients with diffuse lowgrade glioma has been used in recent years to optimize the balance between surgical resection and quality of life following surgery mapping of executive functions is particularly challenging because of their complex nature with only a handful of reports published so far here we propose the recording of neural activity directly from the surface of the brain using electrocorticography to map executive functions and demonstrate its feasibility and potential utility to track a neural signature of executive function we recorded neural activity using electrocorticography during awake surgery from the frontal cortex of three patients judged to have an appearance of diffuse lowgrade glioma based on existing functional magnetic resonance imaging fmri evidence from healthy participants for the recruitment of areas associated with executive function with increased task demands we employed a task difficulty manipulation in two counting tasks performed intraoperatively following surgery the data were extracted and analyzed offline to identify increases in broadband highgamma power with increased task difficulty equivalent to fmri findings as a signature of activity related to executive function all three patients performed the tasks well data were recorded from five electrode strips resulting in data from 15 channels overall eleven out of the 15 channels 733 showed significant increases in highgamma power with increased task difficulty 266 of the channels 415 showed no change in power and none of the channels showed power decrease highgamma power increases with increased task difficulty were more likely in areas that are within the canonical frontoparietal network template these results are the first step toward developing electrocorticography as a tool for mapping of executive function complementarily to direct electrical stimulation to guide resection further studies are required to establish this approach for clinical use
lateral flow assays lfas are lowcost testing tools widely used for home pointofcare or laboratory medical diagnostics these tests typically use colorimetry to report the presence and the concentration of a certain physical biological quantity showing the result as a color marker this work presents a computer vision algorithm for the digitalization of lfa readouts enabling precise and reliable results at lowcost the algorithm receives as input an image of a sample identifies the color marker and computes its average color intensity in contrast to existing algorithms the proposed one can detect color markers that are not characterized by a predetermined precise shape size and position since the topology is identified and analyzed by the algorithm itself the evaluation of the proposed algorithm on a set of lfa strips shows correct functionality and execution time of less than a second
this paper proposes two bertbased models for accurately rescoring reranking n best speech recognition hypothesis lists reranking the n best hypothesis lists decoded from the acoustic model has been proven to improve the performance in a twostage automatic speech
since kermack and mckendrick have introduced their famous epidemiological sir model in 1927 mathematical epidemiology has grown as an interdisciplinary research discipline including knowledge from biology computer science or mathematics due to current threatening epidemics such as covid19 this interest is continuously rising as our main goal we establish an implicit timediscrete sir susceptible people–infectious people–recovered people model for this purpose we first introduce its continuous variant with timevarying transmission and recovery rates and as our first contribution discuss thoroughly its properties with respect to these results we develop different possible timediscrete sir models we derive our implicit timediscrete sir model in contrast to many other works which mainly investigate explicit timediscrete schemes and as our main contribution show unique solvability and further desirable properties compared to its continuous version we thoroughly show that many of the desired properties of the timecontinuous case are still valid in the timediscrete implicit case especially we prove an upper error bound for our timediscrete implicit numerical scheme finally we apply our proposed timediscrete sir model to currently available data regarding the spread of covid19 in germany and iran
image fusion plays a critical role in a variety of vision and learning applications current fusion approaches are designed to characterize source images focusing on a certain type of fusion task while limited in a wide scenario moreover other fusion strategies ie weighted averaging choosemax cannot undertake the challenging fusion tasks which furthermore leads to undesirable artifacts facilely emerged in their fused results in this paper we propose a generic image fusion method with a bilevel optimization paradigm targeting on multimodality image fusion tasks corresponding alternation optimization is conducted on certain components decoupled from source images via adaptive integration weight maps we are able to get the flexible fusion strategy across multimodality images we successfully applied it to three types of image fusion tasks including infrared and visible computed tomography and magnetic resonance imaging and magnetic resonance imaging and singlephoton emission computed tomography image fusion results highlight the performance and versatility of our approach from both quantitative and qualitative aspects
a shortest path between two vertices and in a connected graph is a geodesic a vertex of performs the geodesic identification for the vertices in a pair if either belongs to a geodesic or belongs to a geodesic the minimum number of vertices performing the geodesic identification for each pair of vertices in is called the strong metric dimension of  in this paper we solve the strong metric dimension problem for three convex plane graphs by performing the geodesic identification of their vertices
"
 cognitive radio cr is an intelligent and adaptive radio technology that automatically detects the available channels in the wireless spectrum and sometimes changes the transmission parameters to enable effective communication spectrum sensing in cr prevents harmful interference with the licensed users and maximizes the spectrum utilization thus this paper proposes a technique for optimal channel estimation and spectrum sensing for mac layer protocol in cr networks such that the scheduling issues are addressed initially in the cr networks spectrum sensing is done using the proposed optimal naive bayes classifier onbc based on the signal statistics such as energy and likelihood ratio the onbc is developed by integrating the bat–bird swarm algorithm bbsa with the naive bayes classifier which works based on the bayesian concept the bbsa is newly developed by integrating the bird swarm algorithm bsa and bat algorithm finally the channel estimation is done using the pilotbased sequential procedure and least square estimation lse the analysis of the proposed method is done in the rayleigh and rician environments using 256 and 512 subcarriers from the results it is exposed that the proposed bbsa  lse pilotbased sequential method obtains the bit error rate normalized energy and probability detection pd of is 00126 08446 and 09355 respectively"
"several methods currently exist for solving fuzzy linear fractional
 programming problems under non negative fuzzy variables however due to the
 limitation of these methods they cannot be applied for solving fully fuzzy
 linear fractional programming fflfp problems where all the variables and
 parameters are fuzzy numbers so this paper is planning to fill in this gap
 and in order to obtain the fuzzy optimal solution we propose a new efficient
 method for fflfp problems utilized in daily life circumstances this
 proposed method is based on crisp linear fractional programming and has a
 simple structure to show the efficiency of our proposed method some numerical
 and real life problems have been illustrated"
lclfilter design for a buckboost inverter based on unfolding circuit is addressed our aim is to find the dependencies between the passive components of the smallest size and the stable mode of the system guidelines for the design of passive components are provided for both modes the simulation results confirmed the design and helped to estimate a stable option in different sets of passive elements in conclusion the lcltype can be replaced by the lcfilter based on the correct selection of the output capacitor
generating and checking proof certificates is important to increase the trust in automated reasoning tools in recent years formal verification using computer algebra became more important and is heavily used in automated circuit verification an existing proof format which covers algebraic reasoning and allows efficient proof checking is the practical algebraic calculus in this paper we present two independent proof checkers pacheckand pastèquethe checker pacheckchecks algebraic proofs more efficiently than pastèquebut the latter is formally verified using the proof assistant isabellehol furthermore we introduce extension rules to simulate essential rewriting techniques required in practice for efficiency we also make use of indices for existing polynomials and include deletion rules too
urban villages uvs are distinctive products formed in the process of rapid urbanization the finegrained mapping of uvs from satellite images has always been a considerable challenge because of the complex urban structures and the insufficiency of labeled samples in this letter we propose using the domain adaptation strategy to tackle the domain shift problem by employing adversarial learning to tune the semantic segmentation network so as to adaptively obtain similar outputs for input images from different domains the proposed method was coupled with several segmentation networks including unet refinenet and deeplab v3 and the results show that domain adaptation can significantly improve the pixellevel mapping of uvs
this letter presents a selfsupervised framework for learning depth from monocular videos in particular the main contributions of this letter include 1 we present a windowed bundle adjustment framework to train the network compared to most previous works that only consider constraints from consecutive frames our framework increases the camera baseline and introduces more constraints to avoid overfitting 2 we extend the widely used unet architecture by applying a spatial pyramid net spn and a super resolution net srn the spn fuses information from an image spatial pyramid for the depth estimation which addresses the context information attenuation problem of the original unet the srn learns to estimate a high resolution depth map from a low resolution image which can benefit the recovery of details 3 we adopt a clip loss function to handle moving objects and occlusions that were solved by designing complicated network or requiring extra information such as segmentation mask 1 in previous works experimental results show that our algorithm provides stateoftheart results on the kitti benchmark
object detection problems also expect fast and accurate location of target objects but most approaches following convolutional neural networks cnn framework generate a large number of redundant candidate regions wasting a large percent of computation time for candidate classification human visual system is able to quickly find the parts or features belonging to objects of interest hence it is believed that by detecting salient regions associated with key classification features for specific objects can quickly provide rough location of objects to be detected a feature mining method by adding an enforcement learning block on top of cnn framework is proposed experiment results showed that the proposed method is able to generate accurate task specific salient regions both for localization and classification this study attempts to make object localization and classification in one shot based on salient region detection and puts forward a new idea for key feature mining and utilization
over the last two decades digital journalism and interactive documentaries have produced works in which interactivity multimedia and participation articulate the access and consumption of information these are basically multimedia and dynamic texts that delve into twoway communication and hypertext and motivate active reading these are informational pieces typical of the digital ecosystem that often mutate via social networks and present significant transformations in their temporal evolution reading analyzing and understanding these texts requires specific tools and methodologies that consider a the dynamism of such pieces as well as their temporal modification b their multimodal dimension and c their transmedia development this article proposes a methodological reflection on the ways of reading interactive documentary audiovisual texts and proposes strategies and tools for their understanding and analysis based on detailed reading close reading and decoupage this research focuses on an analysis of the temporal evolution of these journalistic pieces the need to observe and analyze the temporal dimension of journalistic texts in the digital ecosystem has allowed the development of specific methodologies widholm 2016 karlsson sjøvaag 2016 buhl günther quandt 2018 focused on the immediacy and mutability of journalistic news its permanence in networks and its temporal evolution however these tools do not consider the study of largescale journalistic stories typical of interactive documentaries which require a specific multimodal approach hiippala 2017 vankrieken 2018 freixa et al 2014 freixa 2015 a detailed reading reveals how the interactive documentary considers the dimension both temporal and of content and form of the traditional documentary text by becoming part of a transmedia framework as part of a dialogue with the public resumen desde hace dos décadas el periodismo digital y el documental interactivo produce obras en las que la interactividad la multimedialidad y la participación articulan el acceso y consumo de la información básicamente se trata de textos multimediales y dinámicos que ahondan en la comunicación bidireccional y el hipertexto y que proponen lecturas activas se trata de piezas informacionales propias del ecosistema digital que a menudo mutan en las redes sociales y presentan significativas transformaciones en su evolución temporal la lectura el análisis y la comprensión de estos textos precisa de herramientas y metodologías específicas que contemplen a el dinamismo de las piezas así como su modificación temporal b su dimensión multimodal y c su desarrollo transmedia en este artículo se propone una reflexión metodológica sobre las formas de lectura de los textos audiovisuales interactivos documentales y se proponen estrategias y herramientas para su comprensión y análisis basadas en la lectura detallada close reading y el découpage la investigación focaliza su interés en el análisis de la evolución temporal de estas piezas periodísticas la necesidad de observar y analizar la dimensión temporal de los textos periodísticos en el ecosistema digital ha permitido el desarrollo de metodologías específicas widholm 2016 karlsson sjøvaag 2016 buhl günther quandt 2018 focalizadas en la inmediatez y mutabilidad de la noticia periodística su permanencia en red y evolución temporal estas herramientas sin embargo no contemplan el estudio de los relatos periodísticos de gran dimensión propios del documental interactivo que precisan de una aproximación multimodal específica hiippala 2017 vankrieken 2018 freixa et al 2014 freixa 2015 la lectura detallada permite observar cómo el documental interactivo cuestiona la dimensión tanto temporal como de contenido y forma del texto documental tradicional al pasar a formar parte de un entramado transmedia en diálogo con el público
purpose simulation of indirect damage originating from the attack of free radical species produced by ionizing radiation on biological molecules based on the independent pair approximation is investigated in this work in addition a new approach relying on the independent pair approximation that is at the origin of the independent reaction time irt method is proposed in the chemical stage of geant4‐dna methods this new approach has been designed to respect the current geant4‐dna chemistry framework while proposing a variant irt method based on the synchronous algorithm this implementation allows us to access the information concerning the position of radicals and may make it more convenient for biological damage simulations estimates of the evolution of free species as well as biological hits in a segment of dna chromatin fiber in geant4‐dna were compared for the dynamic time step approach of the step‐by‐step sbs method currently used in geant4‐dna and this newly implemented irt results results show a gain in computation time of a factor of 30 for high let particle tracks with a better than 10 agreement on the number of dna hits between the value obtained with the irt method as implemented in this work and the sbs method currently available in geant4‐dna conclusion offering in geant4‐dna more efficient methods for the chemical step based on the irt method is a task in progress for the calculation of biological damage information on the position of chemical species is a crucial point this can be achieved using the method presented in this paper
we propose oneflow – a flowbased oneclass classifier for anomaly outlier detection that finds a minimal volume bounding region contrary to densitybased methods oneflow is constructed in such a way that its result typically does not depend on the structure of outliers this is caused by the fact that during training the gradient of the cost function is propagated only over the points located near to the decision boundary behavior similar to the support vectors in svm the combination of flow models and a bernstein quantile estimator allows oneflow to find a parametric form of bounding region which can be useful in various applications including describing shapes from 3d point clouds experiments show that the proposed model outperforms related methods on realworld anomaly detection problems
the russian river hydrometeorological observing network rhonet is a unique suite of highresolution in situ and remote sensing observations deployed over 20 years to address both scientific and operational gaps in understanding monitoring and predicting weather and water extremes on the united states’ west coast it was created over many years by diverse organizations ranging from universities to federal state and local government agencies and utilities today rhonet is a hybrid network with diverse observation sets aimed at advancing scientific understanding of physical processes driving extreme precipitation and runoff in the region its development is described including the specific goals that led to a series of network enhancements as well as the key characteristics of its sensors the hydroclimatology of the russian river area is described including an overview of the hydrologic extremes and variability driving the scientific and operational needs in the region from atmospheric river behavior and orographic precipitation processes to hydrologic conditions related to water supply and flooding a case study of lake mendocino storage response to a landfalling atmospheric river in 2018 is presented to demonstrate the network’s performance and hydrologic applications during highimpact weather events finally a synopsis of key scientific findings and applications enabled by the network is provided from the first documentation of the role of landfalling atmospheric rivers in flooding to the occurrence of shallow nonbrightband rain to the buffering influence of extremely dry soils in autumn and to the development of forecastinformed reservoir operations for lake mendocino
classification and accurate detection of brain tumor using mri is essential for purpose of treatment and diagnosis of tumor in this paper we propose and developed system using four stages namely image normalization image binarization with morphological operation anisotropic diffusion filtering and feature extraction using glcm the system evaluated on two types of database clinical brain mri images and digital database for screening mammogram ddsm normalization is process of contrast stretching which changes value of pixel intensity and image binarization is processing of grey scale image into black and white image by fixing threshold level of pixel if value of pixel above the threshold level is white either black followed by steps of morphological operation ie erosion and dilation by processing mri images apart from that anisotropic diffusion adf is applied for detection and sharpen the edge detection features taken or extracted by using glcm from filtered mr images in the stage of classification two neural networks have been implemented the first neural network is adaboost nn is based on boosting method which yields classification accurately and the second neural network lvq is feed forward network which uses quantization machine learning algorithm and lossy compression techniques the extracted features hence given to train neural network for classification accuracy with success has been obtain 95 and 806 for clinical brain mri images with 793 and 699 for ddsm
the nearspace hypersonic aerodynamic glider has strong maneuverability in wide flight envelope the glide is generally achieved in a smooth manner with no or weak altitude oscillations in the altitude the maximum lifttodrag ratio glide is a typical trajectory that can approximate the maximum glide range which is a crucial indicator for a glider however another important indicator the minimum glide range which is used in some timesensitive missions and is expected to reduce the velocity to a specified threshold in a short longitudinal range is difficult to be realized in practice the excessive velocity or energy is usually dissipated during lateral manipulation wherein either the entire glide range or the glide time is not shortened an innovative guidance strategy is proposed for achieving the minimum glide range based on a typical maximum glide scheme and bangbang control scheme only on the longitudinal plane and the flight time can be reduced considerably then a practical extended state observer based pitch control is utilized to efficiently track the bangbang command within a wide velocity envelope to achieve the guidance objective extensive simulation results demonstrate the effectiveness of the proposed methods
purposethe purpose of this paper is to look into the mechanism in which customers involve themselves in omnichannel retail setting and use its advantagesdesignmethodologyapproachvia an empirical analysis through surveying customers this paper assesses and confirms the drivers of omnichannel shopping intention within the context of fashion retailing sector in danangfindingsthe findings highlight the significance of customer perception of research shopping including showrooming and webrooming behaviours compatibility and risk to their intention towards omnichannel shopping implying profound understanding of designing effective omnichannel retailing strategyoriginalityvaluefrom a theoretical perspective comprehending customer perception of the omnichannel concept has emerged as an important theme in recent literature as well as in practitioners reports hence the meaningful contribution of this study is the involvement in the attractive steam of study from a managerial perspective this study could offer guidance to retailers or managers about developing a successful omnichannel strategy from a customer point of view
intracellular chloride levels in the brain are regulated primarily through the opposing effects of two cationchloride cotransporters cccs namely kcl− cotransporter2 kcc2 and nakcl− cotransporter1 nkcc1 these cccs are differentially expressed throughout the course of development thereby determining the excitatorytoinhibitory γaminobutyric acid gaba switch gabaergic excitation depolarisation is important in controlling the healthy development of the nervous system as the brain matures gabaergic inhibition hyperpolarisation prevails this developmental switch in excitability is important as uncontrolled regulation of neuronal excitability can have implications for health huntington’s disease hd is an example of a genetic disorder whereby the expression levels of kcc2 are abnormal due to mutant protein interactions although hd is primarily considered a motor disease many other clinical manifestations exist these often present in advance of any movement abnormalities cognitive change in addition to sleep disorders is prevalent in the hd population the effect of uncontrolled kcc2 function on cognition and sleep has also been explored several mechanisms by which kcc2 expression is reduced have been proposed recently thereby suggesting extensive investigation of kcc2 as a possible therapeutic target for the development of pharmacological compounds that can effectively treat hd comorbidities hence this review summarizes the role of kcc2 in the healthy and hd brain and highlights recent advances that attest to kcc2 as a strong research and therapeutic target candidate
fragmentbased drug discovery fbdd is a powerful method to develop potent smallmolecule compounds starting from fragments binding weakly to targets as fbdd exhibits several advantages over highthroughput screening campaigns it becomes an attractive strategy in targetbased drug discovery many potent compoundsinhibitors of diverse targets have been developed using this approach methods used in fragment screening and understanding fragmentbinding modes are critical in fbdd this review elucidates fragment libraries methods utilized in fragment identificationconfirmation strategies applied in growing the identified fragments into druglike lead compounds and applications of fbdd to different targets as fbdd can be readily carried out through different biophysical and computerbased methods it will play more important roles in drug discovery
the paper presents the concept of mission planning for a shortrange tactical class unmanned aerial vehicle uav that recognizes targets using the sensors it has been equipped with tasks carried out by such systems are mainly associated with aerial reconnaissance employing electro optical eonear infrared nir heads synthetic aperture radar sar and electronic intelligence elint systems uavs of this class are most often used in nato armies to support artillery actions etc the key task carried out during their activities is to plan a reconnaissance mission in which the flight route will be determined that optimally uses the sensors’ capabilities the paper describes the scenario of determining the mission plan and in particular the uav flight routes to which the recognition targets are assigned the problem was decomposed into several subproblems assigning reconnaissance tasks to uavs with choosing the reconnaissance sensors and designating an initial uav flight plan the last step is planning a detailed flight route taking into account the time constraints imposed on recognition and the characteristics of the reconnaissance sensors the final step is to generate the real uav flight trajectory based on its technical parameters the algorithm for determining exact flight routes for the indicated reconnaissance purposes was also discussed taking into account the presence of enemy troops and available air corridors the task scheduling algorithm—vehicle route planning with time window vrptw—using time windows is formulated in the form of the mixed integer linear problem milp the milp formulation was used to solve the uav flight route planning task the algorithm can be used both when planning individual uav missions and uav groups cooperating together the approach presented is a practical way of establishing mission plans implemented in real unmanned systems
this paper proposes a new eventtriggered adaptive horizon model predictive control for discretetime nonlinear systems with additive disturbance with the eventtriggered control scheme the optimization problem is solved only at triggering instant and the event is triggered if the difference between the actual state and the predicted state exceeds the triggering threshold the triggering threshold depends on the prediction horizon and becomes larger as the state approaches the terminal constraint set therefore larger triggering intervals can then be obtained finally a numerical example shows the effectiveness of the proposed scheme
"abstract
 background rehabilitation in cerebral palsy cp seeks to harness neuroplasticity to improve movement including walking yet cortical activation underlying gait is not well understood methods we used electroencephalography eeg to compare motor related cortical activity measured by mu rhythm during quiet standing and treadmill walking in 10 children with unilateral cp and 10 age and sexmatched children with typical development td peak mu band frequency mu rhythm desynchronization mrd and gait related intra and interhemispheric coherence were examined results mrd during walking was observed bilaterally over motor cortex in both cohorts but peak mu band frequency showing mrd was significantly lower in cp compared to td coherence during quiet standing between motor and frontal regions was significantly higher in the nondominant compared to dominant hemisphere in cp with no hemispheric differences in td conclusions eegbased measures should be further investigated as clinical biomarkers for atypical motor development and to assess rehabilitation effectiveness"
nowadays commuting by personal vehicles and public road transport is widely common and mostly costeffective in this regard globally there is traffic congestion in megacities and inevitably road collisions take place which leads to people’s injuries and a rise in traffic jams many governments are scrambling to decrease the number of accidents and are trying to introduce the latest technologies in the road infrastructure it might be possible if these technologies are used correctly and reasonably this paper proposes two blockchainbased accident detection models aiming at improving the ease of law violation detection and related measures in particular authors introduce a new technology called offlinedetection relating to the detection of accidents in the absence of communication and internet access blockchain technology itself may become a solution to improve honesty openness and truthfulness in cases of roadrelated issues it allows restoring the road situation in details including location vehicles and surrounding infrastructure involvement due to blockchain property of immutability
a computationally efficient firstprinciples approach to predict intrinsic semiconductor charge transport properties is proposed by using a generalized eliashberg function for shortrange electron–phonon scattering and analytical expressions for longrange electron–phonon and electron–impurity scattering fast and reliable prediction of carrier mobility and electronic thermoelectric properties is realized without empirical parameters this method which is christened “energydependent phonon and impuritylimited carrier scattering time approximation epic star” approach is validated by comparing with experimental measurements and other theoretical approaches for several representative semiconductors from which quantitative agreement for both polar and nonpolar isotropic and anisotropic materials is achieved the efficiency and robustness of this approach facilitate automated and unsupervised predictions allowing highthroughput screening and materials discovery of semiconductor materials for conducting thermoelectric and other electronic applications
machine learning predictive models are being used in neuroimaging to predict information about the task or stimuli or to identify potentially clinically useful biomarkers however the predictions can be driven by confounding variables unrelated to the signal of interest such as scanner effect or head motion limiting the clinical usefulness and interpretation of machine learning models the most common method to control for confounding effects is regressing out the confounding variables separately from each input variable before machine learning modeling however we show that this method is insufficient because machine learning models can learn information from the data that cannot be regressed out instead of regressing out confounding effects from each input variable we propose controlling for confounds posthoc on the level of machine learning predictions this allows partitioning of the predictive performance into the performance that can be explained by confounds and performance independent of confounds this approach is flexible and allows for parametric and nonparametric confound adjustment we show in real and simulated data that this method correctly controls for confounding effects even when traditional input variable adjustment produces falsepositive findings
alzayat saleh developed a computer vision framework that can aid aquaculture experts in analyzing fish habitats in particular he developed a labelling efficient method of training a cnnbased fishdetector and also developed a model that estimates the fish weight directly from its image
the rapid growth in the use of portable systems has sparked research and development in the field of microelectronics especially for power consumption since battery technology does not match the speed of microelectronics lowpower technology has become an important technological factor mgdi is a minimumpower architecture design which is quite a gate diffusion input gdi change and is the lowest design method optimal for rapid low power circuitry model using a reduced number of a transistor here primitive cellsand or and xor gates full adders full subtractors and 4×4 multiplier have been proposedusing the 180nm technologydependent mgdi in cadence virtuoso device the main downside associated with gdi is that this strategy cannot resolve the bulk terminal which is not sufficiently biased so that the number transistor count and delay will be reduced
the present study focuses on the hydrodynamic hull form optimization of a zero emission battery driven fast catamaran vessel a twostage optimization procedure was implemented to identify in the first stage global optimization the optimum combination of a ship’s main dimensions and later on in the second stage local optimization the optimal ship hull form minimizing the required propulsion power for the set operational specifications and design constraints numerical results of speedpower performance for a prototype catamaran intended for operation in the stavanger area norway were verified by model experiments at hamburgische schiffbau versuchsanstalt hsva proving the feasibility of this innovative zero emissions waterborne urban transportation concept
multiomics analysis reveals the changes that occur in the extracellular matrix in the brains of zika virus–infected newborns how zika affects the extracellular matrix in some cases zika virus zikv infection during pregnancy leads to a series of severe defects in the fetus collectively known as congenital zika syndrome czs these include microcephaly defective neuronal migration and impaired cortical development aguiar et al combined genomic transcriptomic and proteomic analyses of blood and postmortem brains and demonstrated that zikvinfected neonates showed a reduction in collagen expression and an increase in adhesion factor expression alterations in the extracellular matrix consistent with the brain defects seen in czs together these datasets form a useful resource for those investigating the molecular mechanisms underlying czs in humans zika virus zikv infection during pregnancy can cause a set of severe abnormalities in the fetus known as congenital zika syndrome czs experiments with animal models and in vitro systems have substantially contributed to our understanding of the pathophysiology of zikv infection here to investigate the molecular basis of czs in humans we used a systems biology approach to integrate transcriptomic proteomic and genomic data from the postmortem brains of neonates with czs we observed that collagens were greatly reduced in expression in czs brains at both the rna and protein levels and that neonates with czs had several singlenucleotide polymorphisms in collagenencoding genes that are associated with osteogenesis imperfecta and arthrogryposis these findings were validated by immunohistochemistry and comparative analysis of collagen abundance in zikvinfected and uninfected samples in addition we showed a zikvdependent increase in the expression of cell adhesion factors that are essential for neurite outgrowth and axon guidance findings that are consistent with the neuronal migration defects observed in czs together these findings provide insights into the underlying molecular alterations in the zikvinfected brain and reveal host genes associated with czs susceptibility
purposethe purpose of this paper is to propose a strategic conflict analysis based on the graph model for conflict resolution gmcr that is applied to information technology outsourcing ito in a realworld software development and implementation process in brazildesignmethodologyapproachbecause the idea of this study is to answer “why” the ito conflicts occur and “how” they can be avoided the case study methodology was adopted the software gmcr ii was used to analyze the interactions between an it vendor and an it clientfindingsthe results suggest that a lack of relational governance is a critical issue that could be handled to improve the interaction between those involvedresearch limitationsimplicationsthe main results are restricted to the case study and cannot be generalized moreover a specific limitation of this paper pertains to the use of the gmcr and the consequent difficulty for it vendors and it clients to work with a large number of actions and to set preferences for several states of conflictpractical implicationsthe strategic analysis of outsourcing conflicts provides a holistic view of the current situation that may assist the client and vendor in future decisions and identify guidelines to ensure successful ito therefore this paper provides an effective guide for clients and vendors to better manage conflicts and establish a contingency vision to avoid such disputesoriginalityvaluethe ito conflict is analyzed using the gmcr considering both perspectives of the outsourcing process vendors and clients
abstractrecent advances in technology and workflows related to 3d geovisualization present numerous opportunities for development and evaluation of the usefulness of these tools for analysis and communication of environmental risks this article explores how cartographic tools currently used for understanding and managing flood risks could be improved through the use of emerging 3d visualization approaches the topological and dimensional realism enabled by these platforms has the potential both to improve the quality of representation and analysis and to reduce the knowledge barriers impeding understanding of flood risk by nonexpert audiences in risk communication furthermore emerging mixedreality interfaces offer multiple advantages over desktops for interaction with 3d content the significant recent growth in both the interface and visualization domains represents an opportunity for researchers and practitioners to evaluate the contributions of these approaches to realworld planning and risk management in this study we overview the recent trends in the realm of flood risk visualization and the contributions mixed reality can have for the field we then present a pragmatic workflow that enables integration of rigorous geospatial data related to flooding into a 3d visualization environment to illustrate how various interface platforms can easily be integrated and evaluatedrésuméles progrès récents de la technologie et des flux opérationnels liés à la géovisualisation 3d offrent de nombreuses possibilités de perfectionnement de ces instruments et dévaluation de leur utilité dans lanalyse et la communication des risques pour lenvironnement les auteurs se demandent comment les instruments cartographiques qui servent actuellement à comprendre et à gérer les risques dinondation pourraient être améliorés grâce à lutilisation des nouvelles approches de visualisation 3d le réalisme topologique et dimensionnel que ces plateformes permettent dobtenir a le potentiel daméliorer la qualité de la représentation et de lanalyse aussi bien que de réduire les obstacles à la connaissance qui nuisent à la compréhension des risques dinondation par les publics non experts lorsque ces risques sont communiqués en outre les nouvelles interfaces de réalité mixte offrent de multiples avantages par rapport aux ordinateurs de bureau pour ce qui est de linteraction avec les contenus 3d la croissance récente appréciable des domaines de linterface et de la visualisation fournit aux chercheurs et aux professionnels loccasion dévaluer lapport de ces approches à la planification de la réalité et à la gestion des risques les auteurs donnent un aperçu des dernières tendances dans le secteur de la visualisation des risques dinondation et du concours possible de la réalité mixte en ce domaine ils proposent un flux opérationnel pragmatique permettant lintégration de données géospatiales rigoureuses relatives aux inondations dans un environnement de visualisation 3d afin dillustrer la facilité avec laquelle diverses plateformes dinterface peuvent être intégrées et évaluées
"
 background
 telemedicine offers a unique opportunity to improve coordination and administration for urgent patient care remotely in an emergency setting it has been used to support first responders by providing telephone or video consultation with specialists at hospitals and through the exchange of prehospital patient information this technological solution is evolving rapidly yet there is a concern that it is being implemented without a demonstrated clinical need and effectiveness as well as without a thorough economic evaluation
 
 
 objective
 our objective is to systematically review whether the clinical outcomes achieved as reported in the literature favor telemedicine decision support for medical interventions during prehospital care
 
 
 methods
 this systematic review included peerreviewed journal articles searches of 7 databases and relevant reviews were conducted eligibility criteria consisted of studies that covered telemedicine as data and informationsharing and twoway teleconsultation platforms with the objective of supporting medical decisions eg diagnosis treatment and receiving hospital decision in a prehospital emergency setting simulation studies and studies that included pediatric populations were excluded the procedures in this review followed the prisma preferred reporting items for systematic reviews and metaanalyses statement the risk of bias in nonrandomised studies–of interventions robinsi tool was used for the assessment of risk of bias the results were synthesized based on predefined aspects of medical decisions that are made in a prehospital setting which include diagnostic decision support receiving facility decisions and medical directions for treatment all data extractions were done by at least two reviewers independently
 
 
 results
 out of 42 fulltext reviews 7 were found eligible diagnostic support and medical direction and decision for treatments were often reported a key finding of this review was the high agreement between prehospital diagnoses via telemedicine and final inhospital diagnoses as supported by quantitative evidence however a majority of the articles described the clinical value of having access to remote experts without robust quantitative data most telemedicine solutions were evaluated within a feasibility or shortterm preliminary study in general the results were positive for telemedicine use however biases due to preintervention confounding factors and a lack of documentation on quality assurance and protocol for telemedicine activation make it difficult to determine the direct effect on patient outcomes
 
 
 conclusions
 the informationsharing capacity of telemedicine enables access to remote experts to support medical decision making on scene or in prolonged field care the influence of human and technology factors on patient care is poorly understood and documented
"
abstract as an innovative technology robot services are now used in the tourism industry to enhance consumer experience despite its importance the impact of robot applications on the purchase intention of consumers has received limited if any attention considering that purchase intention can largely lead to actual purchase behaviour this study investigated the impact of robot hotel service on the purchase intention of consumers through an experiment findings revealed that the purchase intention of the group who watched a video about robot hotel service was significantly higher than those who watched traditional hotel service video implications are further discussed
 the prime focus of most entrepreneurial education is to develop some level of entrepreneurial competencies and skill levels this paper explores novel opportunities that web 20 and 30 tools created for extending teacherlearner learnerlearner and teacherteacher communications interactions and collaborations to infuse entrepreneurial competencies and skill level the research findings also prove that interaction with these tools is expected to contribute to the development of crosscurricular generic competences implications for setting up of technologybased skilloriented entrepreneurship education program are provided
fermentation is a crucial bioengineering process existentially important for modern society the most commonly used production unit for this process is the batch bioreactor its main advantage is unsophisticated construction which unfortunately results in its incapability of controlling the transient state of the fermentation process control of the fermentation can significantly improve the quality of the product and the economy of the process therefore it is useful for bioreactors to be equipped with a control system based on the experimental results we used an optimization method to identify a mathematical model that describes the impact of the bioreactor’s temperature on the fermentation’s transient process the obtained model was applied for the design and synthesis of the closedloop control system simulations and experiments confirmed the effectiveness of the proposed control system in this way we can ensure the consistent quality of the produced probiotic product increase the amount of the product and shorten the fermentation time the original results display the feasibility of the closedloop control of the batch bioreactor’s fermentation process by changing the temperature so far the process has been carried without a closedloop control system the problem is current and has not yet been solved sufficiently there are many attempts published one of the last shows the possibility of controlling the fermentation process by changing the oxygen supply which is more complex and expensive for realization than the solution from our study
the conserved omega ω subunit of rna polymerase rnap is the only nonessential subunit of bacterial rnap core the small ω subunit 7 kda–115 kda contains three conserved α helices and helices α2 and α3 contain five fully conserved amino acids of ω four conserved amino acids stabilize the correct folding of the ω subunit and one is located in the vicinity of the β′ subunit of rnap otherwise ω shows high variation between bacterial taxa and although the main interaction partner of ω is always β′ many interactions are taxon‐specific ω‐less strains show pleiotropic phenotypes and based on in vivo and in vitro results a few roles for the ω subunits have been described interactions of the ω subunit with the β′ subunit are important for the rnap core assembly and integrity in addition the ω subunit plays a role in promoter selection as ω‐less rnap cores recruit fewer primary σ factors and more alternative σ factors than intact rnap cores in many species furthermore the promoter selection of an ω‐less rnap holoenzyme bearing the primary σ factor seems to differ from that of an intact rnap holoenzyme
additive manufacturing am technology has attracted the interest of industrial professionals and researchers in the last years this interest lies primarily in understanding the trends benefits and implications of am technology on supply chain sc and logistics as it requires reconfiguring the supply chain based on a distributed manufacturing strategy closer to the consumer market with shorter lead times and less raw materials it still is an emerging field and needs further study therefore a better understanding of main trends will contribute to the dissemination of knowledge about am technology and its consolidation this article seeks to investigate the implications of am as an advanced manufacturing model on sc and logistics a fourstep research method was used to develop a systematic literature review and a bibliometric analysis on the am implications in sc and logistics the main implications of am on sc and logistics were classified in seven key issues gathered as result of the literature review additionally bibliometric study allowed understanding researches major trends in this field the key aspects highlighted and characterized as major implications of am on sc and logistic are supply chain complexity reduction more flexible logistics and inventory management better spreading and popularization of mass customization decentralization of manufacturing greater design freedom and rapid prototyping increasing of resource efficiency and sustainability and the need to have clearly defined legal and safety aspects
we propose a unique framework to study the topological properties of an optical bound state in the continuumbic we employ the interactions between proximity resonances undergoing avoided resonance crossing in a specialty optical microcavity we utilize a continuous system parameter tuning to induce destructive interference between the resonances with cancelling leakage losses similar to the physical insight of friedrichwingten typebic we demonstrate the evolution of an ultrahigh quality mode we report the formation of a specialbic line in the system parameter space connecting locations of multiple quasibics aiming to develop a novel scheme to enhance the performance of optical sensing in microcavity we study the sensitivity of transmission coefficients and quality factor to sense even ultrasmall perturbations in the system configuration
the covid19 pandemic has challenged researchers working in physical contact with research participants cognitive interviews examine whether study components most often questionnaire items are worded or structured in a manner that allows study participants to interpret the items in a way intended by the researcher we developed guidelines to conduct cognitive interviews virtually to accommodate interviewees who have limited access to the internet the guidelines describe the essential communication and safety equipment requirements and outline a procedure for collecting responses while maintaining the safety of the participants and researchers furthermore the guidelines provide suggestions regarding training of participants to use the technology encouraging them to respond aloud a potential challenge given that the researcher is not physically present with the participant and testing and deploying the equipment prior to the interview finally the guidelines emphasize the need to adapt the interview to the circumstances and anticipate potential problems that might arise
a large number of complex systems naturally emerging in various domains are well described by directed networks resulting in numerous interesting features that are absent from their undirected counterparts among these properties is a strong nonnormality inherited by a strong asymmetry that characterizes such systems and guides their underlying hierarchy in this work we consider an extensive collection of empirical networks and analyze their structural properties using information theoretic tools a ubiquitous feature is observed amongst such systems as the level of nonnormality increases when the nonnormality reaches a given threshold highly directed substructures aiming towards terminal sink or source nodes denoted here as leaders spontaneously emerge furthermore the relative number of leader nodes describe the level of anarchy that characterizes the networked systems based on the structural analysis we develop a null model to capture features such as the aforementioned transition in the networks ensemble we also demonstrate that the role of leader nodes at the pinnacle of the hierarchy is crucial in driving dynamical processes in these systems this work paves the way for a deeper understanding of the architecture of empirical complex systems and the processes taking place on them
abstract this paper proposes a new mathematical model for the reliabilityredundancy allocation problem rrap with a choice of redundancy strategies to maximize the reliability of a system this model chooses the best redundancy strategy from among both active and standby ones for each subsystem for those with a standby strategy a continuous time markov chain model is used to calculate the exact reliability values in order to solve the proposed mixedinteger nonlinear programing model a powerful evolutionary algorithm called water cycle algorithm wca is developed and implemented on three famous benchmark problems finally the results of different benchmark problems are compared with those previously reported to show the superiority of the proposed model and the efficiency of wca
trigonometric pfunction is defined as a special case of hconvex function in this article we used a general lemma that gives trapezoidal midpoint ostrowski and simpson type inequalities with the help of this lemma we have obtained many integral inequalities and generalisations for trigonometric pfunction we have shown that it goes down to the studies in special cases which are described in our study apart from that we got new results for the trapezoidal midpoint ostrowski and simpson type inequalities
one of the main topics in human resources management is the subject of informal organizations in the organization such that recognizing and managing such informal organizations play an important role in the organizations some managers are trying to recognize the relations between informal organizations and being a member of them by which they could assist the formal organization development methods of recognizing informal organizations are complicated and occasionally even impossible this study aims to provide a method for recognizing such organizations using data mining techniques this study classifies indices of human resources influencing the creation of informal organizations including individual social and work characteristics of an organizations employees then a questionnaire was designed and distributed among employees a database was created from obtained data applied data mining techniques in this study are factor analysis clustering by kmeans classification by decision trees and finally association rule mining by gri algorithm at the end a model is presented that is applicable for recognizing the similar characteristics between people for optimal recognition of informal organizations and usage of this information
recently a new strain rate map of italy and the surrounding areas has been obtained by processing data acquired by the persistent scatterers ps of the synthetic aperture radar interferometry insar satellites—ers and envisat—between 1990 and 2012 this map clearly shows that there is a link between the strain rate and all the shallow earthquakes less than 15 km deep that occurred from 1990 to today with their epicenters being placed only in high strain rate areas eg emilia plain nw tuscany central apennines however the map also presents various regions with high strain rates but in which no damaging earthquakes have occurred since 1990 one of these regions is the apennine sector formed by sannio and irpinia this area represents one of the most important seismic districts with a wellknown and recorded seismicity from roman times up to the present day in our study we merged historical records with new satellite techniques that allow for the precise determination of ground movements and then derived physical dimensions such as strain rate in this way we verified that in irpinia the occurrence of new strong shocks—forty years after one of the strongest known seismic events in the district that occurred on the 23 november 1980 measuring mw 68—is still a realistic possibility the reason for this is that from 1990 only areas characterized by high strain rates have hosted significant earthquakes this picture has been also confirmed by analyzing the historical catalog of events with seismic completeness for magnitude m ≥ 6 over the last four centuries it is easy to see that strong seismic events with magnitude m ≥ 6 generally occurred at a relatively short time distance between one another with a period of 200 years without strong earthquakes between the years 1732 and 1930 this aspect must be considered as very important from various points of view particularly for civil protection plans as well as civil engineering and urban planning development
shortterm traffic speed prediction has been an important research topic in the past decade and many approaches have been introduced however providing finegrained accurate and efficient trafficspeed prediction for largescale transportation networks where numerous traffic detectors are deployed has not been well studied in this paper we propose distpre which is a distributed finegrained traffic speed prediction scheme for largescale transportation networks to achieve finegrained and accurate trafficspeed prediction distpre customizes a long shortterm memory lstm model with an appropriate hyperparameter configuration for a detector to make such customization process efficient and applicable for largescale transportation networks distpre conducts lstm customization on a cluster of computation nodes and allows any trained lstm model to be shared between different detectors if a detector observes a similar traffic pattern to another one distpre directly shares the existing lstm model between the two detectors rather than customizing an lstm model per detector experiments based on traffic data collected from freeway i5n in california are conducted to evaluate the performance of distpre the results demonstrate that distpre provides timeefficient lstm customization and accurate finegrained trafficspeed prediction for largescale transportation networks
this article considers an mm1 online retailing queueing model with strategic customers and a monopoly retailer customers arrive according to a poisson process and upon arrival they decide whether to purchase the product based on a reward‐cost structure the valuation of the product is determined by its matching probability with the demand after receiving the purchased product each customer enters a “virtual” orbit to update the information about the product and then makes her decision on whether to replace or return the product based on the products assessed value accordingly a two‐dimensional decision among customers is formulated and we investigate the equilibrium customer strategies and the optimal pricing strategy for the retailer interestingly in equilibrium we find that the systems throughput can be nonmonotone in the products matching probability in addition under the equilibrium strategy of customers we reveal the fact that the retailers optimal price is not necessarily increasing in the matching probability from the perspective of the social planner the returning behavior can worsen the social welfare and therefore those unsatisfied customers are encouraged to replace the product rather than to return it finally some numerical experiments are conducted to verify our results and to examine the sensitivity of several key performance measures of the system
integrated elastomagnetic pumps power portable microfluidic devices for point of care testing
bastian jetten and ferris 2014 reported that shared pain enhances people’s bonding and cooperative behavior but that shared nopain has no such effect they concluded that shared pain is a type of social glue that can improve people’s cooperation however in real life both painful and painless experiences are often nonshared logically the most direct way to determine whether sharing is the important element or not is to compare shared conditions with nonshared conditions we conducted two experiments to investigate the relative effects of pain and sharing on enhancing people’s bonding and cooperative behavior by adding conditions of unshared pain and unshared nopain in experiment 1 we replicated bastian jetten and ferris’s 2014 findings and found that the effect of pain on bonding was mediated by empathy in experiment 2 we used a 2 painnopain × 2 sharedunshared design and found that while shared pain still induced more cooperative behavior than shared nopain unshared pain did not induce more cooperative behavior than unshared nopain moreover we found that empathy significantly mediated the relationship between pain and bonding when participants shared the experience these results suggest that sharing is a necessary component for pain to act as social glue
 methods of ranking and evaluation of the effectiveness of social media sm given and applied in this paper are the basis for the selection of online media that the public administration uses when communicating with citizens the methodology presented is based on multicircular decisionmaking using the fuzzy analytical hierarchical process fuzzy ahp  z number model  fuzzy multiattributive border approximation area comparison fuzzy mabac which eliminates the traditional intuitive ratings of pr services this resulted in poor use of available channels of communication or ineffective communication positive results of the application of presented methods are especially evident in increasing the number of channels of communication on the internet and the realization of communication goals for greater participation of citizens in public administration
the emergence of braininspired neuromorphic computing as a paradigm for edge ai is motivating the search for highperformance and efficient spiking neural networks to run on this hardware however compared to classical neural networks in deep learning current spiking neural networks lack competitive performance in compelling areas here for sequential and streaming tasks we demonstrate how a novel type of adaptive spiking recurrent neural network srnn is able to achieve stateoftheart performance compared to other spiking neural networks and almost reach or exceed the performance of classical recurrent neural networks rnns while exhibiting sparse activity from this we calculate a  100x energy improvement for our srnns over classical rnns on the harder tasks to achieve this we model standard and adaptive multipletimescale spiking neurons as selfrecurrent neural units and leverage surrogate gradients and autodifferentiation in the pytorch deep learning framework to efficiently implement backpropagationthroughtime including learning of the important spiking neuron parameters to adapt our spiking neurons to the tasks
clinical trials in the medical domain are constrained by budgets the number of patients that can be recruited is therefore limited when a patient population is heterogeneous this creates difficulties in learning subgroup specific responses to a particular drug and especially for a variety of dosages in addition patient recruitment can be difficult by the fact that clinical trials do not aim to provide a benefit to any given patient in the trial in this paper we propose c3tbudget a contextual constrained clinical trial algorithm for dosefinding under both budget and safety constraints the algorithm aims to maximize drug efficacy within the clinical trial while also learning about the drug being tested c3tbudget recruits patients with consideration of the remaining budget the remaining time and the characteristics of each group such as the population distribution estimated expected efficacy and estimation credibility in addition the algorithm aims to avoid unsafe dosages these characteristics are further illustrated in a simulated clinical trial study which corroborates the theoretical analysis and demonstrates an efficient budget usage as well as a balanced learningtreatment tradeoff
next generation sequencing techniques like whole genome exome etc generate rich variant annotation outputs from the highthroughput sequencing data these outputs may contain millions of gene variants depending on the sequencing techniques used and need to be filtered to a set of potential gene variants for a disease of interest many systems and models have addressed the problem of prioritising gene variants from annotated variant outputs there is a need to provide a flexible computational model that integrates the contextual knowledge from diverse sources and provides control of the filtering and prioritisation process to the bioinformaticians furthermore to the best of our knowledge there has been limited efforts to build an interoperable reusable and machinereadable knowledge framework to link identified potential variants for multiple known genetic conditions for the indian population that can be searched explored analysed and applied for future cases with increased accuracy in this paper we present sandhi gene variant analysis sandhi gva that combines individual gene variant analysis summaries of patients their clinical data and patient information into rich linked data fostering the application of semantic intelligence in effective filtering and prioritisation of potential gene variants in new cases
in automatic milking systems ams sensors can measure cow behavior and milk composition at every milking the aim of this observational study of previously collected data was to gain insight into the differences in dynamics of udder inflammation indicators between cows that recover and those that do not recover after detection of an initial inflammation milk diversion milk separated from the bulk tank and thus indicating farmer intervention conductivity and somatic cell count scc data from 4 wk before the initial inflammation to 12 wk after the initial inflammation were used to analyze 2584 cases of udder inflammation an udder inflammation case was defined as an initial observation of scc ≥200000 cellsml as well as 1 additional scc measurement 200000 cellsml within 10 d after the initial case among other requirements the data originated from 15 ams herds in 6 countries four subsets of cows were created based on whether milk was diverted after the initial inflammation and whether the udder inflammation cases recovered using a 10d rolling average scc threshold of 200000 cellsml and checking whether this rolling mean was below the threshold within 90 d after the initial inflammation as the indication of recovery this formed the following subsets of cow lactations milk divertedrecovered milk divertednot recovered no milk divertednot recovered no milk divertedrecovered thresholds of 100000 sccml and 300000 sccml for the definition of case and recovery were also applied in a sensitivity analysis but with no substantial difference in results linear mixed models were used for each subset to study the variation in scc natural logarithm of scc divided by 1000 and σconductivity natural logarithm of standard deviation of quarter conductivities when observing the fraction of cows with scc 200000 cellsml in the recovery subsets most cows recovered within 20 d after the initial inflammation in the recovery subsets both σconductivity and scc stabilized mostly within 3 to 4 wk after the initial inflammation σconductivity stabilized above the preonset level in all subsets and did not show a clear increase in the nomilkdiverted subgroups whereas scc stabilized closer to the preonset level overall this study indicated a cutoff point between nonchronic and chronic changes in indicators 3 to 4 wk after the initial inflammation for scc and σconductivity
abstract this paper analyses the turbulent energy cascade from the perspective of statistical mechanics and relates interscale energy fluxes to statistical irreversibility and information entropy production the microscopical reversibility of the energy cascade is tested by constructing a reversible threedimensional turbulent system using a dynamic model for the subgrid stresses this system when reversed in time develops a sustained inverse cascade towards the large scales evidencing that the characterisation of the inertial energy cascade must consider the possibility of an inverse regime this experiment is used to study the origin of statistical irreversibility and the prevalence of direct over inverse energy cascades in isotropic turbulence statistical irreversibility a property of statistical ensembles in phase space related to entropy production is connected to the dynamics of the energy cascade in physical space by considering the space locality of the energy fluxes and their relation to the local structure of the flow a mechanism to explain the probabilistic prevalence of direct energy transfer is proposed based on the dynamics of the rateofstrain tensor which is identified as the most important source of statistical irreversibility in the energy cascade
this article investigates the leader–follower formation control problem for the multiple underactuated autonomous underwater vehicles with model uncertainties and external disturbances sliding mode control multilayer neural network and adaptive robust techniques are employed to design a formation controller for underwater vehicles after that a lyapunovbased stability analysis is proposed to guarantee that all variables are uniformly ultimately bounded in the closedloop control system the following advantages of this approach are highlighted 1 the proposed control strategy only depends on the measurements of lineofsight and angle sensors no other information about the leader vehicle is required 2 a continuous function is designed to replace the signum term in the control system which greatly reduces the inherent chattering of sliding mode controller 3 the proposed controller does not rely on any prior knowledge about hydrodynamic damping and external disturbances which is easily implemented in practice finally the numerical simulations are provided to demonstrate the effectiveness of the proposed controller
predicting the future can significantly improve the safety of intelligent vehicles which is a key component in autonomous driving 3d point clouds can accurately model 3d information of surrounding environment and are crucial for intelligent vehicles to perceive the scene therefore prediction of 3d point clouds has great significance for intelligent vehicles which can be utilized for numerous further applications however due to point clouds are unordered and unstructured point cloud prediction is challenging and has not been deeply explored in current literature in this paper we propose a novel motionbased neural network named monet the key idea of the proposed monet is to integrate motion features between two consecutive point clouds into the prediction pipeline the introduction of motion features enables the model to more accurately capture the variations of motion information across frames and thus make better predictions for future motion in addition content features are introduced to model the spatial content of individual point clouds a recurrent neural network named motionrnn is proposed to capture the temporal correlations of both features moreover an attentionbased motion align module is proposed to address the problem of missing motion features in the inference pipeline extensive experiments on two largescale outdoor lidar point cloud datasets demonstrate the performance of the proposed monet moreover we perform experiments on applications using the predicted point clouds and the results indicate the great application potential of the proposed method
this article is concerned with the problem of the global mittag–leffler synchronization and stability for fractionalorder quaternionvalued neural networks foqvnns the systems of foqvnns which contain either general activation functions or linear threshold ones are successfully established meanwhile two distinct methods such as separation and nonseparation have been employed to solve the transformation of the studied systems of foqvnns which dissatisfy the commutativity of quaternion multiplication moreover two novel inequalities are deduced based on the general parameters compared with the existing inequalities the new inequalities have their unique superiorities because they can make full use of the additional parameters due to the lyapunov theory two novel lyapunov–krasovskii functionals lkfs can be easily constructed the novelty of lkfs comes from a wider range of parameters which can be involved in the construction of lkfs furthermore mainly based on the new inequalities and lkfs more multiple and more flexible criteria are efficiently obtained for the discussed problem finally four numerical examples are given to demonstrate the related effectiveness and availability of the derived criteria
background preeclampsia pe is a heterogeneous hypertensive disorder of pregnancy with no robust biomarkers or effective treatments pe increases the risk of poor outcomes for both the mother and the baby methylationmediated transcriptional dysregulation motifs methtdms could contribute the pe development however precise functional roles of methtdms in pe have not been globally described methods here we develop a comprehensive and computational pipeline to identify pespecific methtdms following tf gene methylation expression profile and experimentally verified tfgene interactions results the regulation patterns of methtdms are multiple and complex in pe and contain relax inhibition intensify inhibition relax activation intensify activation reverse activation and reverse inhibition a core module is extracted from global methtdm network to further depict the mechanism of methtdms in pe the common and specific features of any two kinds of regulation pattern are also analyzed in pe some key methylation sites tfs and genes such as il2rg are identified in pe functional analysis shows that methtdms are associated with immune insulin and nk cellrelated functions drugrelated network identifies some key drug repurposing candidates such as nadh conclusion collectively the study highlighted the effect of methylation on the transcription process in pe methtdms could contribute to identify specific biomarkers and drug repurposing candidates for pe
an instance of colorfulkcenter consists of points in a metric space that are colored red or blue along with an integer k and a coverage requirement for each color the goal is to find the smallest radius ρdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentrho enddocument such that there exist balls of radius ρdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentrho enddocument around k of the points that meet the coverage requirements the motivation behind this problem is twofold first from fairness considerations each colorgroup should receive a similar service guarantee and second from the algorithmic challenges it poses this problem combines the difficulties of clustering along with the subsetsum problem in particular we show that this combination results in strong integrality gap lower bounds for several natural linear programming relaxations our main result is an efficient approximation algorithm that overcomes these difficulties to achieve an approximation guarantee of 3 nearly matching the tight approximation guarantee of 2 for the classical kcenter problem which this problem generalizes algorithms either opened more than k centers or only worked in the special case when the input points are in the plane
"the growth of internet and its reachability to all sectors of people have never been greater internet has become the best marketplace the best library and may be the best guide for everything but this revolution comes with some bigger problems one of the most challenging problems among them would be copyright protection of digital data being transferred over internet digital images videos and audios undergo illegal reproduction and redistributions tampering and other acts of copyright violation this is proved to have leading the film and other prominent industries to loss of millions of dollars per year encrypting the data provides security to it in this case only people who pay to buy the secret key that should be used for decryption can use the data but the problem is that once decrypted the data can be reproduced into any number of copies and can be redistributed without any permission from the author watermarking is an intelligent solution for this problem where the presence of watermark can be checked to distinguish pirated copies from the actual ones a lot of methods have been developed for image and video watermarking but the research on audio watermarking started a little bit later the reason might be the fact that audio watermarking is tedious compared to image and video as human auditory system has is more sensitive compared to human visionary system so ensuring the imperceptibility of audio watermarks is a tougher task in this thesis various audio watermarking schemes introduced so far in the literature and their merits and demerits are studied 
"
a novel hydraulic fracture scattering simulation method in the hydrocarbon diagnosis is developed in this article the traditional thin dielectric structurebased surface integral equation tdssie is adopted for accurate and stable fracture scattering analysis to improve the low simulation efficiency of conventional tdssie a higher order tdssie hotdssie is developed in this article the junction vector bases are defined to extend tdssie for the branched fractures moreover the matrix normalization method is adopted to improve the conditioning of impedance matrices and accelerate the convergence rate the excellent performance of this new hotdssie solver for complex hydraulic fracture and network is validated by numerical experiments to best of our knowledge this is the first time to extend tdssie into higher order scheme for accurate fracture scattering simulation
terahertz timedomain spectroscopy thztds systems based on ultrahigh repetition rate modelocked laser diodes mllds and semiconductor photomixers show great potential in terms of a wide bandwidth fast acquisition speed compactness and robustness they come at a much lower total cost than systems using femtosecond fiber lasers however to date there is no adequate mathematical description of thztds using a mlld in this paper we provide a simple formula based on a systemtheoretical model that accurately describes the detected terahertz spectrum as a function of the optical amplitude and phase spectrum of the mlld and the transfer function of the terahertz system furthermore we give a simple yet exact relationship between the optical intensity autocorrelation and the detected terahertz spectrum we theoretically analyze these results for typical optical spectra of mllds to quantify the effect of pulse chirp on the terahertz spectrum finally we confirm the validity of the model with comprehensive experimental results using a singlesection and a twosection mlld in a conventional thztds system
in classical computer vision rectification is an integral part of multiview depth estimation it typically includes epipolar rectification and lens distortion correction this process simplifies the depth estimation significantly and thus it has been adopted in cnn approaches however rectification has several side effects including a reduced field of view fov resampling distortion and sensitivity to calibration errors the effects are particularly pronounced in case of significant distortion eg wideangle fisheye cameras in this paper we propose a generic scaleaware selfsupervised pipeline for estimating depth euclidean distance and visual odometry from unrectified monocular videos we demonstrate a similar level of precision on the unrectified kitti dataset with barrel distortion comparable to the rectified kitti dataset the intuition being that the rectification step can be implicitly absorbed within the cnn model which learns the distortion model without increasing complexity our approach does not suffer from a reduced field of view and avoids computational costs for rectification at inference time to further illustrate the general applicability of the proposed framework we apply it to wideangle fisheye cameras with 190° horizontal field of view the training framework unrectdepthnet takes in the camera distortion model as an argument and adapts projection and unprojection functions accordingly the proposed algorithm is evaluated further on the kitti rectified dataset and we achieve stateoftheart results that improve upon our previous work fisheyedistancenet 1 qualitative results on a distorted test scene video sequence indicate excellent performance1
this paper describes and discusses creating and evaluating a virtual reality simulation of avebury stone circle and henge complex as it might have appeared and sounded circa 2300 bce avebury is a neolithic heritage site in the uk which is part of the stonehenge avebury and associated sites unesco world heritage site the overall aim of the project was to better understand the sense of place and presence that visitors can experience in virtual simulations of heritage sites we investigated how virtual spaces might become experienced as places by visitors through their exploration active participation sensory stimulation and communication with other visitors in the simulation more than 1200 members of the public experienced the simulation both at avebury itself and at three public exhibitions the specific objectives of the project were to explore if and how the believability of a simulation was associated with feeling a sense of place in the virtual landscape and if some personal characteristics viz age disability sex immersive tendency familiarity with it and frequency of playing computer games were associated with levels of enjoyment in and learning from the simulation we analysed the data from a detailed questionnaire completed by 388 of the 702 visitors to avebury from june to september 2018 who experienced the simulation supported by observational data from all participants at all events we found that believability was associated with a sense of place in the simulation ie that the more believable the simulation appeared the greater the sense of place experienced by the participants we also found that personal characteristics had very little influence upon visitor reactions to the simulation suggesting that such simulations might have wide appeal for heritage and museum visitors regardless of age gender or familiarity with technology highlights more than 1200 members of the public experienced a 3d fully immersive simulation of avebury henge wiltshire uk over a ninemonth period we found patterns of use and familiarity with information technology it and using mobile technologies for gaming that did not follow age and gender stereotypes we found little correlation between age gender and it familiarity with reactions to virtual avebury suggesting that such simulations might have wide appeal for heritage site visitors
the current stateoftheart for testing and evaluation of autonomous surface vehicle asv decisionmaking is currently limited to oneversusone vessel interactions by determining compliance with the international regulations for prevention of collisions at sea referred to as colregs strict measurement of colregs compliance however loses value in multivessel encounters as there can be conflicting rules which make determining compliance extremely subjective this work proposes several performance metrics to evaluate asv decisionmaking based on the concept of good seamanship a practice which generalizes to multivessel encounters methodology for quantifying good seamanship is presented based on the criteria of reducing the overall collision risk of the situation and taking early appropriate actions case study simulation results are presented to showcase the seamanship performance criteria against different asv planning strategies
unsupervised domain adaptation uda for semantic segmentation has been favorably applied to realworld scenarios in which pixellevel labels are hard to be obtained in most of the existing uda methods all target data are assumed to be introduced simultaneously yet the data are usually presented sequentially in the real world moreover continual uda which deals with more practical scenarios with multiple target domains in the continual learning setting has not been actively explored in this light we propose continual uda for semantic segmentation based on a newly designed expanding targetspecific memory etm framework our novel etm framework contains targetspecific memory tm for each target domain to alleviate catastrophic forgetting furthermore a proposed double hinge adversarial dha loss leads the network to produce better uda performance overall our design of the tm and training objectives let the semantic segmentation network adapt to the current target domain while preserving the knowledge learned on previous target domains the model with the proposed framework outperforms other stateoftheart models in continual learning settings on standard benchmarks such as gta5 synthia cityscapes idd and crosscity datasets the source code is available at httpsgithubcomjoonhkimetm
the purpose of this paper is to demonstrate the relevance of developing literary tourism in coimbrathis exploratory qualitative research identifies existent resources and development potential of literary tourism the instruments of data collection were bibliographic research questionnaires interviews and participant observationthere are few literary tourism products in coimbra which contrasts with the number of literary places identified namely on the left bank of the river mondego tourism development stakeholders in coimbra have not paid enough attention to the emergence of literary tourism and the opportunities for the development of new sustainable cultural products related with itthis study is limited by the size and continual renewal of the corpus which implies a constant updating of data regarding authors and textsthis study will lead to the production of a database of coimbra’s literary resources and a digital literary map allowing any citizen or entity to design and implement literary tourism productsto the best of authors’ knowledge this is the first study reviewing the potential of coimbra as a literary tourism destination moreover it discusses literary heritage as a source of products and experiences to foster more balanced tourist flows throughout the city
given query access to an undirected graph g  we consider the problem of computing a 1 ± ε approximation of the number of k cliques in g  the standard query model for general graphs allows for degree queries neighbor queries and pair queries let n be the number of vertices m be the number of edges and n k be the number of k cliques previous work by eden ron and seshadhri stoc 2018 gives an o ∗  n n 1 kk  m k 2 n k time algorithm for this problem we use o ∗  ·  to suppress polylog n 1 ε k k  dependencies moreover this bound is nearly optimal when the expression is sublinear in the size of the graph our motivation is to circumvent this lower bound by parameterizing the complexity in terms of graph arboricity  the arboricity of g is a measure for the graph density “everywhere” there is a very rich family of graphs with bounded arboricity including all minorclosed graph classes such as planar graphs and graphs with bounded treewidth bounded degree graphs preferential attachment graphs and morewe design an algorithm for the class of graphs with arboricity at most α  whose running time is o ∗ min  nα k − 1 n k  n n 1 kk  mα k − 2 n k   we also prove a nearly matching lower bound for all graphs the arboricity is o  √ m  so this bound subsumes all previous results on sublinear clique approximation as a special case of interest consider minorclosed families of graphs which have constant arboricity our result implies that for any minorclosed family of graphs there is a 1 ± ε approximation algorithm for n k that has running time o ∗  nn k  such a bound was not known even for the special classic case of triangle counting in planar graphs
different from the traditional wireless communication systems where both amplitude and phase components are used to provide a form of modulation almost all of space timemodulated matesurface ms currently reported are modulated by phase components in this paper we propose a new architecture of timevarying modulated ms based on two features first inspired by the idea of beamforminglike techniques that different digital sequences are added to the ms in this way the amplitude can be changed rapidly for modulation by holding the constant phase here a pentagonal arraybased ms is designed to achieve amplitude modulation am by the use of onoff field programming gate array fpga hardware second a quasioptical measurable scheme in the nearfiled is used to show the phenomena and detect the measurements to verify the ideas mentioned above the simulations and measurements are carried out
wireless energy transfer wet is a promising technology to fundamentally settle energy and lifetime problems in a wireless sensor network wsn in this paper we study the operation of wsn based on wet using a mobile charging vehicle mcv and construct a periodic strategy to make the network operational permanently our goal is to decrease energy consumption of the entire system while maintaining the network operational forever based on the analysis of total energy consumption we propose an energyefficient renewable scheme ersvc to achieve energy saving compared to previous schemes where the mcv visits and charges all nodes in each cycle the mcv only needs to visit a portion of nodes in ersvc numerical results show that our scheme can significantly decrease the total energy consumption with no performance loss it is also validated that ersvc can maintain the network operational forever with lower complexity than other schemes making it more practical for real networks
we propose a novel approach to realize service chains with specific availability and latency requirements by dimensioning a nfv infrastructure nfvi our contribution leads to a theory that is based on extending sdn capacity computation to the domain of nfv we do so by expanding upon the requirements of availability of service chains scs projecting these onto a nfvi a key contribution of our theoretical model is the inclusion of delay bounds while traversing a sc the combination of availability delay bounds and computing sc granularity is achieved through modeling scs as directed acyclic graphs in a nfvi and acted upon using network calculus eventually leading to a sc realization theorem this sc realization theorem answers a question as to how many scs of a particular type and granularity can be provisioned on a particular dc a second fast approach towards computing scs realization on a nfvi is also presented the theorems are verified numerically for dcs of size upto 12228 servers nodes results for scalability and latency are presented
in twoway timeofarrival toa systems a user device ud obtains its position and timing information by roundtrip communications to a number of anchor nodes ans at known locations compared with the oneway toa technique the twoway toa scheme is easy to implement and has higher localization and synchronization accuracy existing twoway toa methods assume a stationary ud this will cause uncompensated position and timing errors in this article we propose an optimal maximum likelihood ml based twoway toa localization and synchronization method namely twlas different from the existing methods it takes the ud mobility into account to compensate the error caused by the ud motion we analyze its estimation error and derive the cramérrao lower bound crlb we show that the conventional twoway toa method is a special case of the twlas when the ud is stationary and the twlas has high estimation accuracy than the conventional oneway toa method we also derive the estimation error in the case of deviated ud velocity information numerical result demonstrates that the estimation accuracy of the new twlas for a moving ud reaches crlb better than that of the conventional oneway toa method and the estimation error caused by the deviated ud velocity information is consistent with the theoretical analysis
this work explores how sustainable citizen collaboration to foster open government can be achieved by means of blockchainbased solution the technical feasibility and economic viability of a set of extensions of ckan tool bringing together internet of people iop related technologies such as blockchain and crowdsourcing to address sustainability in open government portals is analysed for that a use case validation is performed and the costs of its deployment assessed the aim is to show how iop promoting technologies enhance public administration pa and citizen collaboration to meet common interest objectives
in winters china witnesses frequent mixedphase ice disasters which have a detrimental impact on the secure operation of transmission lines most of the studies are focused on the conventional overhead lines and little attention is paid to the novel carbon fiber composite core wire cfccw which were widely used in recent years besides the influence rule of frequent mixedphase icing on the corona onset characteristics for cfccw has not been extensively studied across extant literature thus this article addresses the aforementioned issues by conducting alternating current ac corona tests for four kinds of cfccw that would be coated by mixedphase ice in a lowtemperature laboratory the results showed that the impact of mixedphase ice on the wire corona onset voltage can be reduced by nearly 50 with more icing the corona onset voltage would further decrease but at a slower pace for the wires with a larger diameter higher corona onset voltage with low distortion in the electric field strength was observed for the same icing time for the freezingwater conductivity no significant impact on both the icing morphology and the corona onset voltage was observed moreover the validation for the simulation model was established by comparing the simulation results with the experimental results these inferences drawn could act as a theoretical reference for transmission lines designing and calculating the wire corona onset voltage in the mixedphase icing areas
the united nations have developed sustainable development goals sdg to guide countries’ development in the next decades in this paper we first propose a set of measurable indicators that define the degree of achievement of sdg secondly we use a microscopic integrated land use and transportation model to define future scenarios and measure sdg in the future with radical policies the model is implemented in munich and kagawa the results are not uniform across policies while the core cities scenario limits urban sprawl and consumption of greenfield land traffic conditions and ghg emissions worsened furthermore the scenarios also show the relevance of testing policies in different study areas the core city scenario and the draconic resettlement scenario showed some impact on vehiclekilometers traveled in munich while the impact in the kagawa region was almost negligible in general only strong and perhaps implausible relocation policies result in overall significant changes in the sdg indicators
in photovoltaic power generation ups and other applications dcac converters with a low input dc voltage are essentially used however this type of converter is commonly put to use the traditional twolevel converting topology which results problems of high harmonic content in the output voltage and large power loss therefore a single stage multilevel converter based on transformer multitap voltages control fed by a low dc voltage source mcmtvc is proposed in this paper the converter achieves multi levels by control different transformer tap voltages through the power switches connected to the taps and the number of output levels can be conveniently expanded through the expansion of the transformer taps during operation the power switch branches that each branch is connected in series by at most two power transistors including power diodes take turns to serve power exchange and the voltage stress upon the power switches active in pwm state is very low the principle of the converter is described in this paper in the first and then the corresponding control method is put forward furthermore the characteristics of the converter are discussed in final a 9level simulation platform with the mentioned topology and control strategy using the matlab  simulink software is established and then simulation research is carried out whose results have verified the proposed control strategy and conclusions
in this paper we investigate a predator–prey model with herd behavior and crossdiffusion subject to the zero flux boundary conditions first the temporal behavior of the model has been investiga
schizophrenia is a complex polygenic disorder associated with subtle distributed abnormalities in brain morphology here we report large genetic overlap between schizophrenia and brain morphology which enabled derivation of polygenic risk scores more predictive of schizophrenia diagnosis than the current stateoftheart our results illustrate the potential of exploiting genetic overlap in imaging genetics studies and how pleiotropyenriched risk scores may improve prediction of polygenic brain disorders
"in this article we study the natural nonparametric estimator of a wasserstein type cost between two distinct continuous distributions f
and g on r the estimator is based on the order statistics of a sample having marginals f g and any joint distribution we prove a central limit theorem under general conditions relating the tails and the cost function in particular these conditions are satisfied by wasserstein distances of order p1and compatible classical probability distributions"
selfsupervised visual representation learning has seen huge progress recently but no large scale evaluation has compared the many models now available we evaluate the transfer performance of 13 top selfsupervised models on 40 downstream tasks including manyshot and fewshot recognition object detection and dense prediction we compare their performance to a supervised baseline and show that on most tasks the best selfsupervised models outperform supervision confirming the recently observed trend in the literature we find imagenet top1 accuracy to be highly correlated with transfer to manyshot recognition but increasingly less so for fewshot object detection and dense prediction no single selfsupervised method dominates overall suggesting that universal pretraining is still unsolved our analysis of features suggests that top selfsupervised learners fail to preserve colour information as well as supervised alternatives but tend to induce better classifier calibration and less attentive overfitting than supervised learners
abstract the relaxed cq algorithm is a very efficient algorithm for solving the split feasibility problem sfp whenever the convex subsets involved are level subsets of given convex functions it approximates the original convex subset by a sequence of halfspaces that overcomes the difficulties for calculating the projection onto original convex subsets in this paper we propose a new inertial relaxed algorithm in which we approximate the original convex subset by a sequence of closed balls instead of half spaces moreover we construct a new variable stepsize that does not need any prior information of the norm we then establish the weak convergence of the proposed algorithm under two different assumptions experimental results in the lasso and elastic net methods show that our algorithm has a better performance than other relaxed algorithms
caregiving in stroke results in severe physical psychological and social impacts on the caregiver over the past few years researchers have explored the use of mhealth technologies to support healthcarerelated activities due to their ability to provide realtime care at any given place or time the purpose of this content review is to investigate mhealth apps in supporting stroke caregiving engagement based on three aspects motivation value and satisfaction we searched app stores and repositories for apps related to stroke caregiving published up to september 2020 extracted apps were reviewed and filtered using inclusion criteria and then downloaded onto compatible devices to determine eligibility results were compared with evidencebased frameworks to identify the ability of these apps in engaging and supporting the caregiver fortyseven apps were included in this review that enabled caregivers to support their needs such as adjustment to new roles and relationships involvement in care and caring for oneself using several different functionalities these functionalities include information resources risk assessment remote monitoring data sharing reminders and so on however no single app was identified that focuses on all aspects of stroke caregiving we also identified several challenges faced by users through their reviews and the factors associated with value and satisfaction our findings can add to the knowledge of existing mhealth technologies and their functionalities to support stroke caregiving needs and the importance of considering user engagement in the design they can be used by developers and researchers looking to design better mhealth apps for stroke caregiving
we propose a new colour transfer method with optimal transport ot to transfer the colour of a sourceimage to match the colour of a target image of the same scene that may exhibit large motion changes betweenimages by definition ot does not take into account any available information about correspondences whencomputing the optimal solution to tackle this problem we propose to encode overlapping neighborhoodsof pixels using both their colour and spatial correspondences estimated using motion estimation we solvethe high dimensional problem in 1d space using an iterative projection approach we further introducesmoothing as part of the iterative algorithms for solving optimal transport namely iterative distributiontransport idt and its variant the sliced wasserstein distance swd experiments show quantitative andqualitative improvements over previous state of the art colour transfer methods
let ag be the adjacency matrix and dg be the diagonal matrix of the vertex degrees of a simple connected graph g nikiforov defined the matrix aalphag of the convex combinations of dg and ag as aalphagalpha dg1alphaag for 0leq alphaleq 1 if  rho1geq rho2geq dots geq rhon are the eigenvalues of aalphag which we call alphaadjacency eigenvalues of g the  alpha adjacency energy of g is defined as eaalphagsumi1nleftrhoifrac2alpha mnright where n is the order and m is the size of g we obtain the upper and lower bounds for eaalphag  in terms of order n size m and zagreb index zgg associated to the structure of g further we characterize the extremal graphs attaining these bounds
electroencephalography eeg is noninvasive technology that is widely used to record brain signals in brain computer interfacing bci systems to control motor imagery in which movements signals occurring in limbs can control some services researchers have proposed numerous classification schemes of these motor imagery to incorporate it with various neurorehabilitation neuroprosthetics and gaming applications however the existing classification schemes face the performance degradation caused by motorimagery eeg signals with low signal to noise ratio the paper’s main objective is to use possible thick data analytics techniques to classify effectively the motor imagery eeg signals our attempt start with notable classifiers including decision trees extra trees naive bayes random forest and svm and move later to enhance classifications using variety of ensemble learning techniques including bagging adaboost and stacking more techniques has been applied on the results of the ensemble learring to eliminate classification noise and supply more relevant features such as substituting outliers with mean value and exercising bandpass filter and common spatial pattern csp the thick data methods has been validated on a public dataset rendered by bci competition ii dataset iii and was found to produce better classification performance metric which included performance metric parameters like accuracy specificity sensitivity precision and recall when confronted with the existing work thus projecting the usefulness of motor imagery bci the analytics is inclusive of area under the curve auc score and mathews correlation coefficient mcc score to display an impactful analysis the first operation step involved idle for 6 seconds and then thinking of moving or hand on the alert the signal composed of three were the hybrid
to achieve physical layer security pls with a flexible implementation a novel design named securityoriented trellis coded spatialmodulation sotcsm is proposed in this article where the transmitter does not know the channel state information csi of wiretapping channels and relies on the legitimate csi to vary the mapping patterns for the antenna information and the radiated information aiming at the optimisation of free euclidean distances between the resultant sotcsm symbols eavesdroppers cannot decode the confidential information delivered in the legitimate link as they do not know the legitimate csi and hence have no basis to acquire the transmitter’s mapping pattern of the moment moreover the sotcsm symbols are then compensated to maximise their euclidean distances which further improves the legitimate receiver’s decoding performance while enhancing the pls however the compensation results in high transmit power when the legitimate channels are in deep fades to boost the energy efficiency a constraint is set to limit the transmit power under this constraint the sotcsm performance is investigated in terms of bit error rate and transmit power consumption the outcome of these investigations substantiates that compared to conventional tcsm designs our sotcsm achieves better performance in the legitimate link with lower decoding complexity
virtual reality vr technology has been employed in a wide range of fields from entertainment to medicine and engineering advances in vr also provide new opportunities in art exhibitions this study discusses the experience of art appreciation through desktop virtual reality desktop vr or headmounted display virtual reality hmd vr and compares it with appreciating a physical painting seventyeight university students participated in the study according to the findings of this study painting evaluation and the emotions expressed during the appreciation show no significant difference under these three conditions indicating that the participants believe that paintings regardless of whether they are viewed through vr are similar owing to the limitation of the operation the participants considered hmd vr to be a tool that hinders free appreciation of paintings in addition attention should be paid to the proper projected size of words and paintings for better reading and viewing the above indicates that through digital technology we can shorten the gap between a virtual painting and a physical one however we must still improve the design of object size and the interaction in the vr context so that a virtual exhibition can be as impressive as a physical one
abstract rapid identification of postearthquake collapsed buildings can be used to conduct immediate damage assessments scope and extent which could potentially be conducive to the formulation of emergency response strategies up to the present the assessments of earthquake damage are mainly achieved through artificial field investigations which are timeconsuming and cannot meet the urgent requirements of quickresponse emergency relief allocation in this research study an intelligent assessment method based on deeplearning superpixel segmentation and mathematical morphology was proposed to evaluate the damage degrees of earthquakedamaged buildings this method firstly utilized the deeplab v2 neural network to obtain the initial damaged building areas then the simple linear iterative cluster slic method was employed to segment the test images so as to accurately extract the area boundaries of the earthquakedamaged buildings next the images subdivided by slic can be merged according to the initial damaged building areas identified by deeplab v2 neural network finally a mathematical morphological method was introduced to eliminate the background noise experimental results demonstrated that the proposed algorithm was superior to others in both convergent speed and accuracy besides its parameter selection was flexible and easily realized which was of great significance to earthquake damage assessments and provided valuable guidance for the formulation of future emergency response plans after earthquake events
although there have been many researches and corresponding degrees of freedom dof analysis on multiple pairwise users exchanging data by onehop relaying through one or more relays the twohop relay system still needs further exploring in this paper we design a communication scheme for multiple pairs of users in the twotier heterogeneous networks hetnets with wireless backhaul link which includes four phases two multiple access mac phases and two broadcast bc phases firstly a new transmission and codingdecoding scheme combining generalized signal alignment gsa zero forcing zf and physical layer network coding plnc is proposed which jointly takes account of users small basestations sbss and macro bs mbs furthermore we analyze the sum rate for users relayed by sbs or also helped by mbs the influence of the number of antenna configurations on the dof and the constraints that antenna should meet besides compared with the traditional ssa scheme the superiority of this scheme is highlighted meanwhile simulation results illustrate that the proposed scheme can achieve much higher system sum rate than zf scheme
abstractas the lowconsensus field of higher education grows it negotiates constraints and possibilities in the topics engaged and the scholars included to trace the shifting patterns of research topics in higher education studies during the past 20 years we examined networks of the topical structures of ashe conference papers and a subset of us higher education journals topical and cocitation networks indicated that the field may be at a point of reconfiguration we found shifts in terminology frameworks and methodologies related to increasingly disconnected topic clusters currently the field seems capable of encompassing these distinctions through more independent discourse subcommunities
mainstream boardlevel circuit design tools work at the lowest level of design  schematics and individual components while novel tools experiment with higher levels of design abstraction often comes at the expense of the finegrained control afforded by lowlevel tools in this work we propose a hardware description language hdl approach that supports users at multiple levels of abstraction from broad system architecture to subcircuits and component selection we extend the familiar hierarchical block diagram with polymorphism to include abstracttyped blocks eg generic resistor supertype and electronics modeling ie currents and voltages such an approach brings the advantages of reusability and encapsulation from objectoriented programming while addressing the unique needs of electronics designers such as physical correctness verification we discuss the system design including fundamental abstractions the block diagram construction hdl and user interfaces to inspect and finetune the design demonstrate example designs built with our system and present feedback from intermediatelevel engineers who have worked with our system
despite advances in the treatment of cervical cancer cc the prognosis of patients with cc remains to be improved this study aimed to explore candidate gene targets for cc cc datasets were downloaded from the gene expression omnibus database genes with similar expression trends in varying steps of cc development were clustered using short timeseries expression miner stem software gene functions were then analyzed using the gene ontology go database and kyoto encyclopedia of genes and genomes kegg enrichment analysis protein interactions among genes of interest were predicted followed by drugtarget genes and prognosisassociated genes the expressions of the predicted genes were determined using realtime quantitative polymerase chain reaction rtqpcr and western blotting red and green profiles with upward and downward gene expressions respectively were screened using stem software genes with increased expression were significantly enriched in dna replication cellcyclerelated biological processes and the p53 signaling pathway based on the predicted results of the druggene interaction database 17 druggene interaction pairs including 3 red profile genes top2a rrm2 and pola1 and 16 drugs were obtained the cancer genome atlas data analysis showed that high pola1 expression was significantly correlated with prolonged survival indicating that pola1 is protective against cc rtqpcr and western blotting showed that the expressions of top2a rrm2 and pola1 gradually increased in the multistep process of cc top2a rrm2 and pola1 may be targets for the treatment of cc however many studies are needed to validate our findings
shortterm traffic flow forecasting is a fundamental and challenging task due to the stochastic dynamics of the traffic flow which is often imbalanced and noisy this paper presents a samplerebalanced and outlierrejected k nearest neighbor regression model for shortterm traffic flow forecasting in this model we adopt a new metric for the evolutionary traffic flow patterns and reconstruct balanced training sets by relative transformation to tackle the imbalance issue then we design a hybrid model that considers both local and global information to address the limited size of the training samples we employ four realworld benchmark datasets often used in such tasks to evaluate our model experimental results show that our model outperforms stateoftheart parametric and nonparametric models
the facial attribute transfer refers to generating a face image with desired attributes while preserving other details in the existing methods some of the them consider the independence among attributes but neglect the integrity it may result in information loss and lead to distorted generation the others ensure the integrity at the cost of insufficient independence limits the generated images will be partially blurred in this paper gan and variational autoencoders vae structure are incorporated to preserve the details while attribute transfer occurs the concept of hierarchical latent representation is introduced to realize the attribute independence these methods work with each other forming an effective network for facial attribute transfer referred as higan experiments on the celeba dataset show that our model can output clearer and more realistic images on facial attribute transfer
this paper presents global navigation satellite system gnss ftype and bluetooth bt lshaped antennas printed on flexible low loss substrate materials for smartwatch applications the proposed printed antennas were designed along with the wristband of a smartwatch device with the main purpose of improving their electrical performance by using a new low loss polymer material and locating the antenna on the wrist strap the antenna performances were simulated using cst microwave studio and the prototypes were measured in a satimo starlab anechoic chamber silver printing and injection molding technologies were successfully utilized for fabricating new sebs materials styreneethylenebutylenestyrene in wearable devices the sebs materials improved the radiation efficiency of the antennas by 16 db for the gnss and 22 db for the bt over the previously used tpu thermoplastic polyurethane materials the overmolded printed and hybrid integrated discrete antennas produced addedvalue for electronics fabrication thanks to its flexible and seamless integration technique in addition it is a lowcost mass manufacturing method the research opens new perspectives for product definitions with a flexible low loss material that enables better antenna performance
minmax optimisation is a special instance of a bilevel problem it deals with the minimisation of the maximum output in all scenarios of a given problem in this paper numerical experiments are conducted to assess the accuracy and efficiency of three bilevel algorithms  known to perform well in general bilevel problems  on 13 unconstrained minmax testfunctions this study aims to bring the bilevel and minmax evolutionary community together and create a common ground for both optimisation problems
this paper proposes an efficient nondestructive testing technique for composite materials the proposed vibrothermal wave radar vtwr technique couples the thermal wave radar imaging approach to lowpower vibrothermography the vtwr is implemented by means of a binary phase modulation of the vibrational excitation using a 5 bit barker coded waveform followed by matched filtering of the thermal response a 1d analytical formulation framework demonstrates the high depth resolvability and increased sensitivity of the vtwr the obtained results reveal that the proposed vtwr technique outperforms the widely used classical lockin vibrothermography furthermore the vtwr technique is experimentally demonstrated on a 55 mm thick carbon fiber reinforced polymer coupon with barely visible impact damage a local defect resonance frequency of a backside delamination is selected as the vibrational carrier frequency this allows for implementing vtwr in the lowpower regime input power  1 w it is experimentally shown that the barker coded amplitude modulation and the resultant pulse compression efficiency lead to an increased probing depth and can fully resolve the deep backside delamination
applying a differenceindifferences framework to a census of residential property transactions in new york city 2003–17 we estimate the price effects of three flood risk signals 1 the biggertwaters flood insurance reform act which increased premiums 2 hurricane sandy and 3 new floodplain maps reflecting three decades of climate change estimates are negative for all three signals and some are large properties included in the new floodplain after escaping flooding by sandy experienced 11 price reductions we investigate possible mechanisms including selection of properties into the market and residential sorting finding no evidence for these we develop a parsimonious theoretical model that allows decomposition of our reducedform estimates into the effects of insurance premium changes and belief updating results suggest the new maps induced belief changes comparable to those from insurance reform
the increasing demand of past patient medical information at the point of care creates new data sharing and exchange demands on health information systems his however a number of existing his have data exchange challenges given that they are ordinarily designed as vertical silos without interoperability obligations yet to have data exchange within his and across health facilities participating systems ought to be interoperable however interoperability is usually not considered a key design requirement during his implementations therefore relying on exceptional existing practices to create benchmark design knowledge the author employs a sense making perspective to analyze how his implementers arrive at their interoperability design requirements through this approach an initial set of interoperability design prerequisites for purposively designing his’ interoperability is proposed these include knowing who knowing what knowing how and knowing which a further study implication is the use of a sensemaking perspective in exploring system design requirements
it is important to analyze and quantify the spatial relationship between the brain symmetry planes bsp on radiological images the unsupervised change detection problem is taken to detect the abnormalities based on brain symmetry assuming the left and right part of brain is roughly symmetric the input to the problem is symmetrical axial mr slices and its output is to place axisparallel boxes that circumscribe the tumor part this change detection process uses a score function based on histogram distance measure computed with gray level intensity histograms in this work histogram distance measures is analyzed for the change detection problem several image databases are utilized to test the performance of bhattacharyya chisquare correlation intersection distance measures from the experimental results chisquare distance metric seems effective in localizing brain abnormalities than the other distance metrics for the change detection problem
phasor measurement unit pmu which could provide highfrequency phasors including the amplitude and phase angle synchronized with global positioning system gpsbeidou navigation satellite system bds signal plays a vital role in dynamic monitoring of power systems however if the gpsbds signal could not be received the time of pmu which depends on the crystal oscillator co would cause measurement deviation due to frequency shifts of co and then the pmu data would engage deviations and deteriorate the relevant applications in recognizing the above problems this article proposes a method to detect the frequency deviation of the crystal oscillator fdco based on the correlation coefficient according to its relation with the characteristic of the voltage phase angle difference vpad between the two ends of the line in detail firstly the quality of the vpad of pmu between the two ends of the line is analyzed and the related problem is stated secondly the characteristics of pmu data when fdco exists are derived and analyzed especially the vpad between the line and the suitability of the data for detecting the fdco is discussed thirdly the ideas and specific methods of detecting the fdco in case of malfunction of gpsbds signal are proposed furthermore the validity of the proposed method with simulation data and modified measured data is presented finally the conjectures on the reasons for the approximately constant vpad deviation in the previous section are discussed
examines issues of access diversity and inclusivity in engineering education in particular she is interested in engineering identity problemsolving and the intersections of online learning and alternative pathways for adult nontraditional and veteran undergraduates in engineering
in a lattice 𝔏 the authors used the concept of belongingness and quasicoincidence of fuzzy point to a fuzzy set and by this notion ∈∈∨qfuzzy sublattice ∈∈∨qfuzzy ideal cartesian product of ∈∈∨qfuzzy sublattice ∈∈∨qfuzzy complemented sublattice and cartesian product of ∈∈∨qfuzzy complemented sublattice are introduced and their properties are briefly studied the relationship between fuzzy sublattice and ∈∈∨qfuzzy sublattice fuzzy ideal and ∈∈∨qfuzzy ideal of l are established the authors prove that the cartesian product of two ∈∈∨qfuzzy ideals of a lattice is not necessarily a fuzzy ideal of a lattice the theory of image and inverse image of an ∈∈∨qfuzzy sublattice and ∈∈∨qfuzzy ideal an ∈∈∨qfuzzy complemented sublattice and ∈∈∨qfuzzy complemented ideal of 𝔏 on the basis of homomorphism of lattices are also significantly established
sensory processing deficits and altered longrange connectivity putatively underlie multisensory integration msi deficits in autism spectrum disorder asd the present study set out to investigate nonsocial msi stimuli and their electrophysiological correlates in young neurotypical adolescents and adolescents with asd we report robust msi effects at behavioural and electrophysiological levels both groups demonstrated normal behavioural msi however at the neurophysiological level the asd group showed less msirelated reduction of the visual p100 latency greater msirelated slowing of the auditory p200 and an overall temporally delayed and spatially constrained onset of msi given the task design and patient sample and the age of our participants we argue that electrocortical indices of msi deficits in asd a can be detected in earlyadolescent asd b occur at early stages of perceptual processing c can possibly be compensated by later attentional processes d thus leading to normal msi at the behavioural level
in this article coded space communication links impaired by solar scintillation are investigated followed by a comprehensive endtoend approach with respect to baseband analyses this allows for a more realistic modeling of actual communication links in these scenarios though at the cost of longer simulation times and higher minimum values of the error rates assessable the effect of solar scintillation on both signal amplitude and phase is studied by considering the potential use of noncoherent demodulation to withstand phase synchronization impairments this article allows optimizing some receiver parameters such as the phaselocked loop bandwidth in a way to face critical contingency scenarios as well
for policymakers across the world the importance of budget transparency is selfevident however most scholars mainly focus on the economic performance of budget transparency while ignoring satisfaction of the public as the recipients of this policy therefore this study examines the main factors of public satisfaction with the local government budget transparency based on the theory of customer satisfaction in the context of the chinese budget transparency policy data for this study were collected through an online survey involving 235 participants structural equation modeling sem was employed to examine the proposed model the results indicate that the budget information quality budget information acquisition method and public engagement are good indicators of budget transparency perceived quality which are positively related to public satisfaction the government image also exerts a positive effect toward public satisfaction furthermore public satisfaction is also positively related to public trust toward the local government even though the hypotheses linking public expectation to public satisfaction and to budget transparency perceived quality are statistically insignificant the implications of promoting budget transparency and suggestions for future work are also included in this study
nowadays various factors such as the welfare of citizens as well as traffic and air pollution reduction demonstrate the necessity of the creation and development of urban rail transport systems d
numerous research studies on high capacity dcdc converters have been put forward in recent years targeting multiterminal mediumvoltage direct current mvdc and highvoltage direct current hvdc systems in which renewable power plants can be integrated at both mediumvoltage mv and highvoltage hv dc and ac terminals hence leading to complex hybrid acdc systems multiport converters mpcs offer the means to promote and accelerate renewable energy and smart grids applications due to their increased control flexibilities in this paper a family of mpcs is proposed in order to act as a hybrid hub at critical nodes of complex multiterminal mvdc and hvdc grids the proposed mpcs provide several controllable dc voltages from constant or variable dc or ac voltage sources the theoretical analysis and operation scenarios of the proposed mpc are discussed and validated with the aid of matlabsimulink simulations and further corroborated using experimental results from scale down prototype theoretical analysis and discussions quantitative simulations and experimental results show that the mpcs offer high degree of control flexibilities during normal operation including the capacity to reroute active or dc power flow between any arbitrary ac and dc terminals and through a particular subconverter with sufficient precision critical discussions of the experimental results conclude that the dc fault responses of the mpcs vary with the topology of the converter adopted in the subconverters it has been established that a dc fault at highvoltage dc terminal exposes subconverters 1 and 2 to extremely high currents therefore converters with dc fault current control capability are required to decouple the healthy subconverters from the faulted one and their respective fault dynamics on the other hand a dc fault at the lowvoltage dc terminal exposes the healthy upper subconverter to excessive voltage stresses therefore subconverters with bipolar cells which possess the capacity for controlled operation with variable and reduced dc voltage over wide range are required in both fault causes continued operation without interruption to power flow during dc fault is not possible due to excessive overcurrent or overvoltage during fault period however it is possible to minimize the interruption the above findings and contributions of this work have been further elaborated in the conclusions
in recent years a growing amount of research has focused on improving the performance of industrialized construction using emerging technologies it is still necessary to have an indepth understanding of the industry practitioners’ perspectives on the application of emerging technologies thus a welldesigned survey was distributed to industry practitioners who have been involved in industrialized construction projects then a set of data analysis methods were utilized on the collected data to address the proposed four specific research questions results indicate that 3d and nd models sensing techniques and business information models are the technologies with the highest current utilization level extended reality additive manufacturing and advanced data analytics are the technologies with the highest development potential project inputs eg cost time and labor as well as implementation cost and software constraints eg capital costs software upgrading and compatibility are the main factors that affect practitioners’ decisions to adopt emerging technologies in industrialized projects intergroup comparison results indicate that company background has little significant influence on practitioners’ perspectives while personal career profiles can significantly affect practitioners’ perspectives significantly by uncovering the suggestions and viewpoints of practitioners this paper aligns academic research with industry needs ultimately providing guidance on future research directions and applications
this study proposes a feature fusion classifier ffc for multiple signal classification music by integrating different feature extraction methods and various machine learning algorithms the functional principal component analysis fpca common spatial pattern csp discrete wavelet transform dwt and autoregressive ar model are considered to extract features from different domains using the features collected from the above methods we propose to fit a logistic regression model with a model selection criterion to prevent overfitting and obtain relevant features at first the relevant features are employed to learn different classifiers via different machine learning algorithms such as support vector machine random forest naive bayes extreme gradient boosting and linear discriminant analysis in addition the technique of stacked generalization is also employed to integrate the classification results of these machine learning classifiers finally the classifier with the smallest value of cross validation is selected to be the ffc in our empirical study three different music datasets are adopted to investigate the performance of the proposed method except for comparing the classification results of the ffc to the above machine learning classifiers a fusion method that existed in the literature is also considered for comparison the numerical results reveal that the ffc is capable of selecting useful features automatically and has robust and satisfactory performances for the 3 datasets
abstract most of the previous singlevehicle crash analysis studies ignored the effect of roadsegments level at higher plan that could probably be unobserved heterogeneity and vary among crashlevel factor from one roadsegment to next and possibly could lead to a potential biased estimated result this study developed a hierarchical binary logit model which have the ability to account for both unobserved heterogeneity and correlation within roadsegment to investigate and compare the impact of significant factors influencing fatal singlevehicle crash between young midage and old driver model a sevenyears from 2011 to 2017 crash data department of highway doh thailand were used in this study the intraclasscorrelation values indicate the importance of roadsegment level that 101 122 and 128 of the total variation were accounted by random effect from roadsegment heterogeneity for young midage and old driver model respectively the estimated result of this study shows that influence of alcohol and fatigue increase risk of fatal crash among young and old driver seatbeltusage reduce risk of being fatal among midage and old driver roadside safety feature guardrail significantly reduce fatality risk among young and midage driver and night time driving without light increase probability of fatal crash for midage driver this study recommends the need to enforce the law on driver under influence of alcohol and seatbelt usage educational campaign on driving and installation of guardrail on curve road
children born extremely preterm ept 28 weeks gestation are at risk for delays in development including language we use fmriconstrained magnetoencephalography meg during a verb generation task to assess the extent and functional connectivity phase locking value or plv of language networks in a large cohort of ept children and their term comparisons tc 73 participants aged 4 to 6 years were enrolled 42 tc 31 ept there were no significant group differences in age sex race ethnicity parental education or family income there were significant group differences in expressive language scores p005 language representation was not significantly different between groups on fmri with taskspecific activation involving bilateral temporal and left inferior frontal cortex there were group differences in functional connectivity seen in meg to identify a possible subnetwork contributing to focal spectral differences in connectivity we ran network based statistics analyses for both beta 2025 hz and gamma 6170 hz bands we observed a subnetwork showing hyperconnectivity in the ept group p005 network strength was computed for the beta and gamma subnetworks and assessed for correlation with language performance for the ept group exclusively strength of the subnetwork identified in the gamma frequency band was positively correlated with expressive language scores r0318 p005 thus interhemispheric hyperconnectivity is positively related to language for ept children and might represent a marker for resiliency in this population
many atmospheric correction schemes of radiancebased optical satellite data require the selection of normalized solar spectral irradiance models at the top of atmosphere toa however there is no scientific consensus in literature as to which available model is most suitable this article examines five commonly used models applied to landsat 8 operational land imager oli toa radiance and reflectance products to assess the accuracy and stability between models used to derive surface reflectance products it is assumed that the calibration of the united states geological survey usgs landsat 8 oli toa reflectance and radiance products are accurate to currently claimed levels the results show that the retrieved surface reflectance can exhibit significant variations when different solar irradiance models are used especially in the oli coastal blue band at 443 nm from the five solar irradiance models the kurucz 2005 model showed the least bias compared with oli toa reflectance product and least variance in surface reflectance furthermore improvement was obtained by adjusting the total solar irradiance tsi normalization and additional validation was provided using observed in situ water leaving reflectance data the results from this article are particularly relevant to aquatic applications and to satellite sensors that provide toa radiance such as previous landsat and other current and historical missions
abstract the article discusses the issue of training of telecommunications engineers the architecture of telecommunications solutions is changing very quickly obviously training programmes must also change cybersecurity issues are one of the main drivers of change in telecommunications solutions and therefore training programmes they have become the main issues in all processes related to digital transformation at the same time it is clear that the development of education in telecommunications clearly lags behind modern requirements such issues come to the fore in relation to the development of digital economy programmes cyber security issues for military telecommunications solutions are discussed separately
poor knowledge and adherence to pointofcare poc hiv testing standards have been reported in rural kwazulunatal kzn a high hiv prevalent setting improving compliance to hiv testing standards is critical particularly during the gradual phasing out of lay counsellor providers and the shifting of hiv testing and counselling duties to professional nurses the main objective of this study was to identify priority areas for development of poc diagnostics curriculum to improve competence and adherence to poc diagnostics quality standards for primary healthcare phc nurses in rural south africa method phc clinic stakeholders were invited to participate in a cocreation workshop participants were purposely sampled from each of the 11 kwazulunatal districts through the nominal group technique ngt participants identified training related challenges concerning delivery of quality point of care diagnostics and ranked them from highest to lowest priority an importance ranking score scale 1–5 was calculated for each of the identified challenges results study participants included three phc professional nurses one tb professional nurse one hiv lay councilor one tb assistant and three poc diagnostics researchers aged 23–50 participants identified ten poc diagnostics related challenges amongst the highest ranked challenges were the followingabsence of poc testing curriculum for nurses absence of training of staff on hiv testing and counselling as lay counsellor providers are gradually being phased out absence of continuous professional development opportunities and lack of staff involvement in poc management programs conclusion key stakeholders perceived training of phc nurses as the highest priority for the delivery of quality poc diagnostic testing at phc level we recommend continual collaboration among all poc diagnostics stakeholders in the development of an accessible curriculum to improve providers’ competence and ensure sustainable quality delivery of poc diagnostic services in rural phc clinics
in the fields of face recognition and voice recognition a growing literature now suggests that the ability to recognize an individual despite changes from one instance to the next is a considerable challenge the present paper reports on one experiment in the voice domain designed to determine whether a change in the mere style of speech may result in a measurable difficulty when trying to discriminate between speakers participants completed a speaker discrimination task to pairs of speech clips which represented either free speech or scripted speech segments the results suggested that speaker discrimination was significantly better when the style of speech did not change compared to when it did change and was significantly better from scripted than from free speech segments these results support the emergent body of evidence suggesting that withinidentity variability is a challenge and the forensic implications of such a mild change in speech style are discussed
future electricity systems are challenged by deep decarbonization and concurrently increasing demand and there are growing concerns that renewables cannot shoulder this alone starting from the proven principle of diversity we argue for keeping the nuclear option open or even for expanding its use however the perspectives are dim for the current technology as safety concerns and social aversion remain as fundamental problems while looking for future revolutionary safe and more sustainable nuclear concepts we first review the main characteristics of civil nuclear energy as well as its safety records and technical progress we then list the key requirements for innovative nuclear systems designs which are less dependent on active safety systems and human performance as well as social stability this allows us to provide a concept by concept comparison and assessment of existing and novel technologies and designs including different coolants and neutron spectra the results indicate a high potential for farreaching improvements compared to most advanced lwrs although none of the candidate concepts meets all requirements convincingly yet helium cooled small modular reactors htrpm come closest we end by stressing the need for future research and development and keeping human capital and knowhow in nuclear energy we call for an urgent increase in government and international rdd funding by the order of a few hundreds of billions of usd per year which will likely lead to breakthroughs that will restart productivity growth in severely affected stagnating modern economies
highquality frequency transfer based on existing telecommunication fiber links allows stateoftheart microwave and optical clocks to be compared on a continental and potentially even intercontinental scale we present a halfayearlong data set of unidirectional optical frequency transfer over a 2×43km urban fiber link we observe a relative frequency instability of 80×1016 at 1 s and at best 60×1018 at 106s integration time our results show that the unidirectional twoway method gives the possibility to perform comparisons of international primary standards over fiber links faster than over satellite links moreover we investigate the major limiting factors of such a unidirectional setup
w the first 5g networks are about to be launched the discussion within the scientific community about the next generation beyond 5g or 6g has already kickedoff one of the candidate technologies for a 6g radio access technology is thz communications which uses spectrum mainly beyond 275 ghz enabling the use of channel bandwidths of several 10s of ghz this contribution provides an overview on current stateofthe art of thz communications in research standardization and regulation and discusses the challenges to make thz communications a promising candidate for a 6g radio
significance plant cells have a polysaccharidebased wall that maintains their structural and functional integrity and determines their shape reorganization of wall components is required to allow growth and differentiation one matrix polysaccharide that is postulated to play an important role in this reorganization is xyloglucan xyg while the structure of xyg is well understood its biosynthesis is not through genetic studies with arabidopsis cslc genes we demonstrate that they are responsible for the synthesis of the xyg glucan backbone a quintuple cslc mutant is able to grow and develop normally but lacks detectable xyg these results raise important questions regarding cell wall structure and its reorganization during growth the series of cslc mutants will be valuable tools for investigating these questions xyloglucan xyg is an abundant component of the primary cell walls of most plants while the structure of xyg has been well studied much remains to be learned about its biosynthesis here we employed reverse genetics to investigate the role of arabidopsis cellulose synthase likec cslc proteins in xyg biosynthesis we found that single mutants containing a tdna in each of the five arabidopsis cslc genes had normal levels of xyg however higherorder cslc mutants had significantly reduced xyg levels and a mutant with disruptions in all five cslc genes had no detectable xyg the higherorder mutants grew with mild tissuespecific phenotypes despite the apparent lack of xyg the cslc quintuple mutant did not display significant alteration of gene expression at the wholegenome level excluding transcriptional compensation the quintuple mutant could be complemented by each of the five cslc genes supporting the conclusion that each of them encodes a xyg glucan synthase phylogenetic analyses indicated that the cslc genes are widespread in the plant kingdom and evolved from an ancient family these results establish the role of the cslc genes in xyg biosynthesis and the mutants described here provide valuable tools with which to study both the molecular details of xyg biosynthesis and the role of xyg in plant cell wall structure and function
virtual qwerty is the most popular method of text entry in virtual reality since virtual keyboards are not constrained by the physical limitations of actual keyboards designers are taking the liberty of designing novelty keys for these keyboards however it is unknown whether key design affects text entry performance or user experience this work presents results of a user study that investigated the effects of different key shapes and dimensions on text entry performance and user experience results revealed that key shape affects text entry speed dimension affects accuracy while both affect user experience overall squareshaped 3d keys yielded the best actual and perceived performance also was the most preferred by the users
this paper proposes a path tracking and planning approach based on backstepping neural network and resistance network for autonomous driving the improved diamond resistance network method is presented to avoid the discontinuous curvature of the generated path in this paper the path tracking controller is designed based on radial basis function neural network and backstepping method taking the failure of the actuator into account the stability of the designed controller is proven in this paper the simulation results show that the designed controller can track the planned path accurately meanwhile it is compared with the classic backstepping controller which proves the advantages of the designed controller to deal with actuator failures
energy efficiency is a major concern in the emerging mobile cellular wireless networks since massive connectivity is to be expected with high energy requirements from the network operators nonorthogonal multiple access noma being the frontier multiple access scheme for 5g there exists numerous research attempts on enhancing the energy efficiency of noma enabled wireless networks while maintaining its outstanding performance metrics such as high throughput data rates and capacity maximized optimallythe concept of green noma is introduced in a generalized manner to identify the energy efficient noma schemes these schemes will result in an optimal scenario in which the energy generated for communication is managed sustainably hence the effect on the environment economy living beings etc is minimized the recent research developments are classified for a better understanding of areas which are lacking attention and needs further improvement also the performance comparison of energy efficient noma schemes against conventional noma is presented finally challenges and emerging research trends for energy efficient noma are discussed
uncertainty and confidence have been shown to be useful metrics in a wide variety of techniques proposed for deep learning testing including test data selection and system supervision we present uncertaintywizard a tool that allows to quantify such uncertainty and confidence in artificial neural networks it is built on top of the industryleading tfkeras deep learning api and it provides a neartransparent and easy to understand interface at the same time it includes major performance optimizations that we benchmarked on two different machines and different configurations
the headrelated transfer function hrtf involves the cues for human auditory localization which turns it into an essential item of virtual auditory display technology in practice the interpolation of hrtf is necessary for the virtual auditory display systems to achieve high spatial resolution traditional geometricbased interpolation methods are generally restrained by the spatial distribution of reference on hrtf when the spatial distribution is sparse the accuracy of interpolation decreases significantly therefore an interpolation method using the commonpolezero model and the fitting neural network is proposed first we propose a commonpolezero model to represent hrtfs across multiple subjects in which the lowdimensional features of the measured hrtfs are extracted then for a new spatial direction we predict the corresponding lowdimensional hrtf with a fitting neural network finally we reconstruct the highdimensional hrtf from the predicted lowdimensional hrtf the simulation results suggest that the proposed method outperforms other interpolation methods such as linearambc bilinearambc and the combination method
since existing detectors are often sensitive to the complex background a novel detection pattern based on generative adversarial network gan is proposed to focus on the essential features of infrared small target in this article motivated by the fact that the infrared small targets have their unique distribution characteristics we construct a gan model to automatically learn the features of targets and directly predict the intensity of targets the target is recognized and reconstructed by the generator built upon unet according the data distribution a fivelayer discriminator is constructed to enhance the datafitting ability of generator besides the l2 loss is added into adversarial loss to improve the localization in general the detection problem is formulated as an imagetoimage translation problem implemented by gan namely the original image is translated to a detected image with only target remained by this way we can achieve reasonable results with no need of specific mapping function or handengineering features extensive experiments demonstrate the outstanding performance of proposed method on various backgrounds and targets in particular the proposed method significantly improve intersection over union iou values of the detection results than stateoftheart methods
objective to explore the accuracy of combined neurology expert forecasts in predicting primary endpoints for trials methods we identified one major randomized trial each in stroke multiple sclerosis ms and amyotrophic lateral sclerosis als that was closing within 6 months after recruiting a sample of neurology experts for each disease we elicited forecasts for the primary endpoint outcomes in the trial placebo and treatment arms our main outcome was the accuracy of averaged predictions measured using ordered brier scores scores were compared against an algorithm that offered noncommittal predictions results seventyone neurology experts participated combined forecasts of experts were less accurate than a noncommittal prediction algorithm for the stroke trial pooled brier score  0340 95 subjective probability interval spi 0340 to 0340 vs 0185 for the uninformed prediction and approximately as accurate for the ms study pooled brier score  0107 95 confidence interval ci 0081 to 0133 vs 0098 for the noncommittal prediction and the als study pooled brier score  0090 95 ci 0081 to 0185 vs 0090 the 95 spis of individual predictions contained actual trial outcomes among 44 of experts only 18 showed prediction skill exceeding the noncommittal prediction independent experts and coinvestigators achieved similar levels of accuracy conclusion in this firstofkind exploratory study averaged expert judgments rarely outperformed noncommittal forecasts however experts at least anticipated the possibility of effects observed in trials our findings if replicated in different trial samples caution against the reliance on simple approaches for combining expert opinion in making research and policy decisions
a microstrip tunable diplexer with two separately designable channels and multiple transmission zeroes tzs using common elementloaded uniform impedance resonator uir is proposed the common uir operating in third harmonic inlineformula texmath notationlatexf 3 texmathinlineformula and fifth harmonic inlineformula texmath notationlatexf 5 texmathinlineformula is used for designing the two diplexer channels the stub is loaded at the voltagenull point of inlineformula texmath notationlatexf 3 texmathinlineformula on the common uir and the varactor is loaded at the open end of the stub the varactorloaded stub can separately tune inlineformula texmath notationlatexf 5 texmathinlineformula but has no effect on inlineformula texmath notationlatexf 3 texmathinlineformula so that the two channels can be separately designed meanwhile the input port is tapconnected to the common uir which necessarily produces a common tz between inlineformula texmath notationlatexf 3 texmathinlineformula and inlineformula texmath notationlatexf 5 texmathinlineformula existing in both channel filtering responses in the tunable high channel design at inlineformula texmath notationlatexf 5 texmathinlineformula the major design concept relies on choosing the properly values of three varactor diodes the desired external quality factor and coupling coefficient can be easily obtained for realizing the constant absolute bandwidth abw by properly choosing coupling schemes of the two channels more tzs are realized at the desired locations for demonstration a diplexer with a fixed low channel and a tunable high channel with the constant abw is designed fabricated and measured
"the brumadinho dam disaster occurred on 25 january 2019 when a tailings dam at the corrego do feijao iron ore mine in brazil suffered a catastrophic failure of devastating consequences over hundreds of kilometres with a severe toll on the environment and the population the collapse of brumadinho tailings dam could be considered among the worst incidences related to mining operations in recent years unfortunately it will not be the last tragedy of this kind according to researchers at world mine tailings failures the risk of occurrence for these catastrophic events will increase in the near future 
mining operations are typically monitored by employing sparse networks of in situ sensors prisms 
groundbased radars piezometers etc this information does not cover the totality of the possible areas affected by deformation furthermore it is difficult to manage since it is obtained from different sources whose integration and comparison become a complex task 
in this context spaceborne synthetic aperture radar sar interferometry insar technology represents an attractive alternative to complement the monitoring of mining operations due to its capabilities on the one hand spaceborne insar could provide measurements of terrain deformation with millimetric precision over wide areas and on the other hand the monitoring can be carried out regardless of daynight cycles or the weather conditions in the areas of interest 
in recent years the insar technology has improved allowing the exploitation of key indicators revealing precursors in the trend of deformation timeseries this paper takes cadia and brumadinho as test sites and shows the potential of crosssections and inverse velocity analysis to complement in situ sensors for the rapid identification of critical deformations on tailing dams and slopes in open pits 
finally an example of an online platform is presented as a way of improving the user experience to exploit insar monitoring services jointly with in situ information over a map of reference this provides the endusers with a single platform with all the monitoring data to make easier the spatiotemporal analysis of the deformation and the assessment of critical spots of deformation"
the use of machine learning in chemistry is on the rise for the prediction of chemical properties the input feature representation or descriptor in these applications is an important factor that affects the accuracy as well as the extent of the explored chemical space here we present the periodic table tensor descriptor that combines features from behlerparrinellos symmetry functions and a periodic table representation using our descriptor and a convolutional neural network model we achieved 22 kcalmol and 94 mevatom mean absolute error mae for the prediction of the atomization energy of organic molecules in the qm9 dataset and the formation energy of materials from materials project dataset respectively we also show that structures optimized with force field can be used as input to predict the atomization energies of molecules at dft level our approach extends the application of behlerparrinellos symmetry functions without a limitation on the number of elements which is highly promising for universal property calculators in large chemical spaces
intravascular glucose sensors have the potential to improve and facilitate glycemic control in critically ill patients and might overcome measurement delay and accuracy issues this study investigated the accuracy and stability of a biosensor for arterial glucose monitoring tested in a hypo and hyperglycemic clamp experiment in pigs 12 sensors were tested over 5 consecutive days in 6 different pigs samples of sensor and reference measurement pairs were obtained every 15 minutes 1337 pairs of glucose values range 37–458 mgdl were available for analysis the systems met iso 151972013 criteria in 992 in total 100 for glucose 100 mgdl n  414 and 988 for glucose ≥100 mgdl n  923 the mean absolute relative difference mard during the entire glycemic range of all sensors was 43 the mards within the hypoglycemic 70 mgdl euglycemic ≥70–180 mgdl and hyperglycemic glucose ranges ≥180 mgdl were 61 36 and 47 respectively sensors indicated comparable performance on all days investigated day 1 3 and 5 none of the systems showed premature failures in a porcine model the performance of the biosensor revealed a promising performance the transfer of these results into a human setting is the logical next step
this paper introduces a new dataset dedicated to multirobot stereovisual and inertial simultaneous localization and mapping slam this dataset consists in five indoor multirobot scenarios acquired with ground and aerial robots in a former air museum at onera meudon france those scenarios were designed to exhibit some specific opportunities and challenges associated to collaborative slam each scenario includes synchronized sequences between multiple robots with stereo images and inertial measurements they also exhibit explicit direct interactions between robots through the detection of mounted apriltag markers 1 groundtruth trajectories for each robot were computed using structurefrommotion algorithms and constrained with the detection of fixed apriltag markers placed as beacons on the experimental area those scenarios have been benchmarked on stateoftheart monocular stereo and visualinertial slam algorithms to provide a baseline of the singlerobot performances to be enhanced in collaborative frameworks
safety is crucial for deploying robots in the real world one way of reasoning about safety of robots is by building safe sets through hamiltonjacobi hj reachability however safe sets are often computed offline assuming perfect knowledge of the dynamics due to high compute time in the presence of uncertainty the safe set computed offline becomes inaccurate online potentially leading to dangerous situations on the robot we propose a novel framework to learn a safe control policy in simulation and use it to generate online safe sets under uncertain dynamics we start with a conservative safe set and update it online as we gather more information about the robot dynamics we also show an application of our framework to a modelbased reinforcement learning problem proposing a safe modelbased rl setup our framework enables robots to simultaneously learn about their dynamics accomplish tasks and update their safe sets it also generalizes to complex highdimensional dynamical systems like 3link manipulators and quadrotors and reliably avoids obstacles while achieving a task even in the presence of unmodeled noise
scene text recognition is an important task in computer vision despite tremendous progress achieved in the past few years issues such as varying font styles arbitrary shapes and complex backgrounds etc have made the problem very challenging in this work we propose to improve text recognition from a new perspective by separating the text content from complex backgrounds thus making the recognition considerably easier and significantly improving recognition accuracy to this end we exploit the generative adversarial networks gans for removing backgrounds while retaining the text content  as vanilla gans are not sufficiently robust to generate sequencelike characters in natural images we propose an adversarial learning framework for the generation and recognition of multiple characters in an image the proposed framework consists of an attentionbased recognizer and a generative adversarial architecture furthermore to tackle the issue of lacking paired training samples we design an interactive joint training scheme which shares attention masks from the recognizer to the discriminator and enables the discriminator to extract the features of each character for further adversarial training benefiting from the characterlevel adversarial training our framework requires only unpaired simple data for style supervision each target style sample containing only one randomly chosen character can be simply synthesized online during the training this is significant as the training does not require costly paired samples or characterlevel annotations thus only the input images and corresponding text labels are needed in addition to the style normalization of the backgrounds we refine character patterns to ease the recognition task a feedback mechanism is proposed to bridge the gap between the discriminator and the recognizer therefore the discriminator can guide the generator according to the confusion of the recognizer so that the generated patterns are clearer for recognition experiments on various benchmarks including both regular and irregular text demonstrate that our method significantly reduces the difficulty of recognition our framework can be integrated into recent recognition methods to achieve new stateoftheart recognition accuracy
osseointegration oi targeted muscle reinnervation tmr and vascularized composite allotransplantation vca are just a few ways by which our reconstructive ladder is evolving it is important to recognize that amputation does not necessarily denote failure but surgeons should strive to find ways to provide these patients with means for obtaining better satisfaction and quality of life postoperatively tmr and oi have added options for mutilating lower extremity injuries that necessitate amputation more recently the senior author levin ls described the “penthouse” floor of the reconstructive ladder being vca despite the advances in vca over the last 20 years there are many challenges that face this discipline including indications for patient selection minimizing immunosuppressive regimens standardizing outcome measures establishing reliable protocols for monitoring and diagnosing and managing rejection herein the authors review tmr oi and vca as additional higher rungs of the reconstructive ladder
in naturalistic learning problems a models input contains a wide range of features some useful for the task at hand and others not of the useful features which ones does the model use of the taskirrelevant features which ones does the model represent answers to these questions are important for understanding the basis of models decisions as well as for building models that learn versatile adaptable representations useful beyond the original training task we study these questions using synthetic datasets in which the taskrelevance of input features can be controlled directly we find that when two features redundantly predict the labels the model preferentially represents one and its preference reflects what was most linearly decodable from the untrained model over training taskrelevant features are enhanced and taskirrelevant features are partially suppressed interestingly in some cases an easier weakly predictive feature can suppress a more strongly predictive but more difficult one additionally models trained to recognize both easy and hard features learn representations most similar to models that use only the easy feature further easy features lead to more consistent representations across model runs than do hard features finally models have greater representational similarity to an untrained model than to models trained on a different task our results highlight the complex processes that determine which features a model represents
this article contributes to the growing body of literature within public management on open government data by taking a political perspective we argue that open government data are a strategic resource of organizations and therefore organizations are not likely to share it we develop an analytical framework for studying the politics of open government data based on theories of strategic responses to institutional processes government transparency and open government data the framework shows that there can be different organizational strategic responses to open data—varying from conformity to active resistance—and that different institutional antecedents influence these responses the value of the framework is explored in two cases a province in the netherlands and a municipality in france the cases provide insights into why governments might release datasets in certain policy domains but not in others thereby producing “strategically opaque transparency” the article concludes that the politics of open government data framework helps us understand open data practices in relation to broader institutional pressures that influence government transparency
withinfield variation of leaf area index lai plays an essential role in field crop monitoring and yield forecasting although unmanned aerial vehicle uavbased optical remote sensing method can overcome the spatial and temporal resolution limitations associated with satellite imagery for finescale withinfield lai estimation of field crops image correction and calibration of uav data are very challenging in this study a physicalbased method was proposed to automatically calculate crop effective lai laie using uavbased 3d point cloud data regular high spatial resolution rgb images were used to generate point cloud data for the study area the proposed method simulated observation of point cloud sopc was designed to obtain the 3d spatial distribution of vegetation and bare ground points and calculate the gap fraction and laie from a uavbased 3d point cloud dataset at vertical 575° and multiview angle of a winter wheat field in london ontario canada results revealed that the derived laie using the sopc multiview angle method correlates well with the laie derived from ground digital hemispherical photography r2  076 the root mean square error and mean absolute error for the entire experiment period from may 11 to may 27 were 019 and 014 respectively the newly proposed method performs well for laie estimation during the main leaf development stages bbch 2039 of the growth cycle this method has the potential to become an alternative approach for crop laie estimation without the need for groundbased reference measurements hence save time and money
in machine learning if one class has a significantly larger number of instances majority than the other minority this condition is defined as class imbalance with regard to datasets class imbalance can bias the predictive capabilities of machine learning algorithms towards the majority negative class and in situations where false negatives incur a greater penalty than false positives this imbalance may lead to adverse consequences our paper incorporates two case studies each utilizing a unique approach of three learners gradientboosted trees logistic regression random forest and three performance metrics area under the receiver operating characteristic curve area under the precisionrecall curve geometric mean to investigate class rarity in big data class rarity a notably extreme degree of class imbalance was effected in our experiments by randomly removing minority positive instances to artificially generate eight subsets of gradually decreasing positive class instances all model evaluations were performed through crossvalidation in the first case study which uses a medicare part b dataset performance scores for the learners generally improve with the area under the receiver operating characteristic curve metric as the rarity level decreases while corresponding scores with the area under the precisionrecall curve and geometric mean metrics show no improvement in the second case study which uses a dataset built from distributed denial of service attack attack data postslowloris combined the area under the receiver operating characteristic curve metric produces very highperformance scores for the learners with all subsets of positive class instances for the second study scores for the learners generally improve with the area under the precisionrecall curve and geometric mean metrics as the rarity level decreases overall with regard to both case studies the gradientboosted trees gbt learner performs the best
with the expansion of the distribution network and the connection of distributed generations dgs the traditional centralized fault section location is difficult to meet the requirements of accuracy and reliability to solve this problem a distributed fault section location method for active distribution network adn is proposed the key points of this method are the bayesian complete analytic model and the dimension reduction firstly a distributed fault diagnosis model is constructed based on the multiagent system mas and the fundamental of fault section location is analyzed secondly considering the missing and false positives of the feeder terminal unit ftu a bayesian complete analytic model is proposed with the minimum variable dimension reduced by the fault contradiction theory finally the whole process of the fault location method is directly given simulation results show that the method has high fault tolerance and can effectively solve the single point of failure spof problem in centralized algorithms which is suitable for largescale adn
background hiv testing contributes to the prevention and control of the hiv epidemic in the general population south africa has made strides to improve hiv testing towards reaching the first of the unaids 90–90–90 targets by 2020 however to date no nationally representative analysis has examined temporal trends and factors associated with hiv testing among youth and adults in the country aim this study aimed to examine the trends and associations with ever having tested for hiv among youth and adults aged 15 years and older in south africa using the 2005 2008 2012 and 2017 nationally representative populationbased household surveys methods the analysis of the data collected used multistage stratified cluster randomised crosssectional design ptrend chisquared test was used to identify any significant changes over the four study periods bivariate and multivariate logistic regression analysis was conducted to determine factors associated with hiv testing in each of the survey periods results ever having tested for hiv increased substantially from 2005 306 n  16 112 2008 504 n  13 084 2012 655 n  26 381 to 2017 752 n  23 190 those aged 50 years and older were significantly less likely to ever have tested for hiv than those aged 25–49 years those residing in rural areas were significantly less likely to have tested for hiv as compared to people from urban areas there was a change in hiv testing among race groups with whites coloureds and indianasians testing more in 2005 and 2008 and black africans in 2017 marriage education and employment were significantly associated with increased likelihood of ever testing for hiv those who provided a blood specimen for laboratory hiv testing in the survey rounds and were found to have tested positive were more likely to have ever tested for hiv previously conclusion the results show that overall there has been an increase in ever having an hiv test in the south african population over time the findings also suggest that for south africa to close the testing gap and reach the first of the unaids 90–90–90 targets by 2020 targeted programmes aimed at increasing access and utilization of hiv testing in young people males those not married the less educated unemployed and those residing in rural areas of south africa should be prioritised
there have been disruptions in local and global food supply chains around the world due to the covid19 pandemic it has led to rethink various aspects and to consider various trends in the food sector encourage a more rapid transition and evolution to the sustainable supply chain management sscm to counteract current problems and to help towards a more sustainable and competitive vision however there is a strategic disconnection in sscm between sustainability and strategies related to competitive advantage therefore the objective of this research is to propose a decisionmaking model to assess the sustainability and competitiveness strategically of a mexican agrifood focal sme small and mediumsized enterprise that leads its supply chain it is validated with comparative analysis in the productive taro activity through the analytic hierarchy process ahp that contributes to prevent or to face problems such as the covid19 pandemic and climate change this research is based on a case study methodology divided into four stages three presented alternatives are assessed and compared by thirtyfive criteria a a current situation without sustainability b cost leadership strategy considering sustainability and c differentiation strategy considering sustainability the conclusions indicate more feasibility and compatibility to achieve sustainability with a differentiation strategy than with a cost leadership strategy the current crisis should lead us to think beyond covid19 as we have more challenges ahead such as climate change environmental impacts poverty among other aspects that could cause instability around the world therefore the current situation should be an impetus to further progress towards the 17 sustainable development goals in this case through the theory of sscm that is why this model contributes strategically to the sscm to develop greater longterm resilience in the mexican agrifood focal smes and their supply chains © 2020 astes publishers all rights reserved
in this paper we aim to develop distributed continuoustime algorithms over directed graphs to seek the nash equilibrium in a noncooperative game motivated by the recent consensusbased designs we present a distributed algorithm with a proportional gain for weightbalanced directed graphs by further embedding a distributed estimator of the left eigenvector associated with zero eigenvalue of the graph laplacian we extend it to the case with arbitrary strongly connected directed graphs having possible unbalanced weights in both cases the nash equilibrium is proven to be exactly reached with an exponential convergence rate an example is given to illustrate the validity of the theoretical results
relationships between brain atrophy patterns of typical aging and alzheimers disease ad white matter disease cognition and ad neuropathology were investigated via machine learning in a large harmonized magnetic resonance imaging database 11 studies 10216 subjects
we propose a path guidance system with a userbased charge and subsidy ubcs scheme for single od network mobility management users who are willing to join the scheme subscribers can submit travel requests along with their vots to the system before traveling those who are not willing to join outsiders only need to submit travel requests to the system our system will give all users path guidance from their origins to their destinations and collect a path payment from the ubcs subscribers subscribers will be charged or subsided in a way that renders the ubcs strategyproof revenueneutral and paretoimproving a numerical example shows that the ubcs scheme is equitable and progressive
water quality monitoring and prediction are important parts of cyber physical systems considering the complexity diversity and strong nonlinearity of water quality data a single water quality prediction model is difficult to have a significant effect on different data to solve this problem a new water quality prediction method based on the preferred classification is proposed in this study a preferred classifier is established to integrate back propagation neural network support vector machines for regression and long shortterm memory due to the fact that these three prediction models can take into account the different characteristics of water quality data when new data input the proposed method preferentially selects the prediction model that is most suitable for the data and then uses the selected model for prediction finally the proposed method is applied in two actual datasets songhua river and victoria bay experimental results demonstrate that the water quality prediction method based on preferred classification achieves better performance than any of the three single prediction models
drawing on infrequent purchase and university selection literature the purpose of this study is to make theoretical contributions to the identification and comprehension of the ‘onceinalifetime purchase’ oilp phenomenondata derived from indepth interviews with 34 taiwanese and 11 chinese students over two phases suggest that an oilp qualitatively differs from other purchasesthe principal traits of oilp are quadripartite in character zerorepurchase intention permanency of purchase high social pressure and extensive information search the results contribute to the theoretical understanding of the higher education he sector as one type of oilp provider and managerial implications are discussed specifically for the he institutionsmarketing managers of he institutions should be cognisant of the range of information collected by prospective oilp customers from a variety of different sources prospective students who are oilp customers perceive nonmarketing information as more reliable than marketing promotional materials and will better assist them during their decision makingthe paper makes explicit theoretical and instrumental contributions to our identification and comprehension of the oilp phenomenon thus shedding new light on studies of consumer purchase decision literature it also extends previous understanding of he marketing by showing that choosing an he degree is in fact an oilp and as such requires a new way of approaching prospective students as consumers
pruning the weights of neural networks is an effective and widelyused technique for reducing model size and inference complexity we develop and test a novel method based on compressed sensing which combines the pruning and training into a single step specifically we utilize an adaptively weighted ell1 penalty on the weights during training which we combine with a generalization of the regularized dual averaging rda algorithm in order to train sparse neural networks the adaptive weighting we introduce corresponds to a novel regularizer based on the logarithm of the absolute value of the weights numerical experiments on the cifar10 and cifar100 datasets demonstrate that our method 1 trains sparser more accurate networks than existing stateoftheart methods 2 can also be used effectively to obtain structured sparsity 3 can be used to train sparse networks from scratch ie from a random initialization as opposed to initializing with a welltrained base model 4 acts as an effective regularizer improving generalization accuracy
timeresolved luminescence measurement is a useful technique which can eliminate the background signals from scattering and shortlived autofluorescence however the relative instruments always require pulsed excitation sources and highspeed detectors moreover the excitation and detecting shutter should be precisely synchronized by electronic phase matching circuitry leading to expensiveness and highcomplexity to make timeresolved luminescence instruments simple and cheap the automatic synchronization method was developed by using a mechanical chopper acted as both of the pulse generator and detection shutter therefore the excitation and detection can be synchronized and locked automatically as the optical paths fixed in this paper we first introduced the timeresolved luminescence measurements and review the progress and current state of this field then we discussed lowcost timeresolved techniques especially chopperbased timeresolved luminescence detections after that we focused on autophaselocked method and some of its meaningful applications such as timegated luminescence imaging spectrometer and luminescence lifetime detection finally we concluded with a brief outlook for autophaselocked timeresolved luminescence detection systems
there is a need for stakeholders at all levels of education to be able to locate access and use the preponderance of data available about schools and students’ demographic information and academic performance the reason for findability accessibility and usability of the data is for the stakeholders to make datadriven decisions based on trends the data bring to light however in many cases data are located in portals that are cumbersome and difficult to navigate this article explores the complexity of educational data portals the wide gap in technology skills of the user of the data and interface design challenges
in this short essay i take the opportunity to highlight one further potential asymmetry that may yet emerge from the supreme court’s application of chevron’s many doctrines drawing on thenjudge kavanaugh’s disdissental from the dc circuit’s decision affirming network neutrality rules i suggest that there is at least one vote on the supreme court—and perhaps more—for an asymmetric approach to the major questions doctrine moreover i demonstrate how asymmetry in this context is deeply irrational as applied to network neutrality the asymmetry has at least one of two effects one it might simply favor one large industry over another subjecting one intersector wealth transfer to heightened scrutiny while treating an analogous wealth transfer—in the opposite direction—deferentially but the judiciary is not typically in the business of favoring one industrial sector over another two it subjects consumerprotection devices to increased regulatory scrutiny thereby shifting the costs and burdens of overcoming a regulatory default to those entities—consumers—who can likely least afford to bear them hence in more general terms justice kavanaugh’s unbalanced approach to the major questions doctrine tends to undermine many of the values— accountability and expertise among others—that agency policymaking has long served
background and objectives cardiovascular and cerebrovascular diseases caused by arterial stenosis and sclerosis are the main causes of human death although there are mature diagnostic techniques in clinical practice they are not suitable for early disease prediction and monitoring due to their high cost and complex operation the purpose of this paper is to study the coupling effect of arterial blood flow and linear gradient magnetic field and to propose a method for the reconstruction of the arterial profile which will lay a theoretical foundation for new electromagnetic artery scanning imaging technology methods and models a combination coil composed of gradient coils and drive coils is applied as a magnetic field excitation source by controlling the excitation current a linearly gradient magnetic field with a lineshaped zero magnetic field is generated and the zero magnetic field is driven to scan in a specific direction according to the magnetoelectric effect of blood flow under the action of the external magnetic field the voltage signals on the body surface can be detected by measuring electrodes the location of the artery center line can be determined by the time–space relationship between voltage signals and zero magnetic field scanning in addition based on the reciprocity theorem integral equation a numerical model between the amplitude of the voltage signal and the arterial radius is derived to reconstruct the arterial radius the above physical process was simulated in the finite element analysis software comsol and the voltage signals obtained from the simulation verified the arterial profile reconstruction results through finite element simulation verification the imaging method based on a linear gradient magnetic field has a numerical accuracy of 90 and a spatial resolution of 1 mm moreover under 100 hz lowfrequency alternating current excitation the single scanning time is 0005 s which is far shorter than the arterial blood flow change cycle meeting the requirements of realtime imaging the results demonstrate the effectiveness and high theoretical feasibility of the proposed method in realtime arterial imaging conclusions this study indicates the potential application of linear gradient magnetic fields in arterial profile imaging compared with traditional electromagnetic imaging methods the proposed method has the advantages of fast imaging speed and high resolution showing the certain application value in early realtime imaging of arterial disease however further studies are necessary to confirm its effectiveness in clinical practice by more medical data and real cases
abstract a fundamental result in additive number theory states that for every finite set a of integers the hfold sumset ha has a very simple and beautiful structure for all sufficiently large h let be the set of all integers in the sumset ha that have at least t representations as a sum of h elements of a it is proved that the set has a similar structure
the covid19 pandemic has led to calls for contributions from the social and behavioural sciences in responding to the social and behavioural dimensions of the pandemic the current delphi study explored expert opinions and consensus about the contributions that can be made by social psychology and social psychologists and research priorities and strategies to this end a tworound delphi process was employed involving a panel of 52 professors of social psychology from 25 countries responses to openended questions presented to the panel in round 1 were condensed and reformulated into 100 closedended statements that the panel rated their agreement with in round 2 consensus was reached for 55 topics to which social psychology can contribute 26 topics that should be prioritised and 19 strategies that should be implemented the findings contribute to further focusing research efforts in psychology in its response to the social and behavioural dimensions of the covid19 and future pandemics
in recent years cryptocurrencies implemented on top of blockchains became very popular with bitcoin as the most prominent example however novel blockchainbased platforms such as ethereum also support distributed applications beyond cryptocurrencies through socalled smart contracts technically smart contracts are programs whose code and execution state is stored in the blockchain inherently featuring the ability to transfer electronic money during their execution in this bachelor thesis we investigate how smart contracts can be used to implement a distributed crowdsensing application for tracking mobile objects by a crowd of privately owned mobile devices such a system could be used for instance to find lost or stolen objects such as keys vehicles cars bicycles     or pets tagged with shortrange radio transmitters implemented using readily available bluetooth or rfid technology these objects can then be detected by smartphones of private users in the vicinity of the object effectively implementing a huge sensor network covering many parts of the world without any upfront investments by a central entity although highly attractive implementing a crowdsensing application on top of a blockchain platform such as ethereum comes with several challenges first of all users need incentives to participate in searching for mobile objects a natural incentive is a monetary reward that participants automatically receive through the smart contract when reporting sightings timestamped positions of wanted objects however this directly brings up the problem of malicious participants attackers who try to get the reward without actually executing the work of searching for the object by simply reporting fake positions therefore one major goal of this bachelor thesis is to counter such attacks by proposing effective countermeasures and implementing and evaluating them for the ethereum platform in detail we propose a basic reputationbased approach for detecting fake positions which judges each sighting made by a mobile devices according to the reputation of that device implemented by a smart contract furthermore advanced attacks are identified compromising the basic reputationbased approach and effective countermeasures to these advanced attacks are proposed identified advanced attacks include reputation farming where the attacker tries to aggregate reputation first before launching the attack and the socalled copy cat attack where the attacker simply copies already submitted valid sightings form honest participants making his fake positions indistinguishable from valid positions our evaluations analyses the monetary cost of executing smart contracts with and without our security mechanisms the results show that the overhead included by our reputationbased approach is at maximum 45 of the cost of a smart contract without implemented security mechanisms
nearfield communication nfc technology introduces new and better experiences for tourists while improving operating processes in the tourism industry through the use of smartphone applications the purpose of this research is to explore and analyse the antecedents of the adoption of nfc mobile applications app regarding the search for information in a tourist destinationbased on the review of the literature three groups of antecedents are proposed the groups are associated with the usability of mobile technology itself the benefits derived from its use and the possible risks that the tourist assumes to achieve these objectives an online questionnaire was filled by 218 participants after they watched a video explaining the use of nfc tourism apps in a spanish tourist destinationpartial least squares revealed that perceived value performance risk usefulness and perceived enjoyment have a strong relationship with the intention to use of tourism appsthe conclusions and implications for management provide alternatives for companies to promote this new business by means of the new technical developments论旅游nfc移动应用使用的动因近场通讯（nfc）技术通过智能手机的使用 为游客带来新奇更佳的体验 同时 又完善了旅游业运营流程。本论文旨在探索和分析探索旅游目的地信息中 nfc移动应用app使用的动因。根据文献综述结果 本论文提出三组动因。这三组动因涉及移动技术本身、使用好处、以及旅游预想的可能危险。为了达到研究目的 本论文采用在线问卷采样方式 受访者首先看了一段解释nfc旅游app在一个西班牙旅游目的地应用的视频 共218名受访者参与了调查。plssem结果表明感知价值、性能危险、有用性、和感知愉快与旅游app使用有着紧密的联系。本论文结果和管理启示为公司提出其他可能 建议公司可以利用新科技发展来提高新商机。
in this paper we present two qualitative results concerning the solutions of nonlinear generalized differential equations with a local derivative defined by the authors in previous works the first result covers the boundedness of solutions while the second one discusses when all the solutions are in l2
in recent years deep learning techniques have been used in computer science as well as in many other disciplines successful detection of complex relationships and connections within big data enables effective use of deep learning in many areas deep learning with the detection of patterns and abnormalities in images is also a promising method for the ﬁeld of radiology detection of abnormalities in mr images enables detection of brain tumor and can automate this process however the deep learning models developed for brain tumor detection are sensitive to missing mr images in the input data and therefore the model is not robust enough one of the models used for brain tumor detection requires a combined mr image in 4 different contrast and sequence t1 t2 flair and t1c images in this study it is proposed to synthesize the missing contrast image in the input data with another deep learning technique incomplete t1c mr image was synthesized by the generative adversarial networks gan method and brain tumor detection performance was examined
in multiphase fluid flow the formation of dispersed patterns where one of the phases is completely dispersed in the other continuous medium is common for example in crude oil extraction during the transport of wateroil mixture in this work experimental and numerical studies were carried out for the flow of an oilwater mixture in a horizontal pipe the dispersed liquid being a paraffin oil with density 843 kg m−3 and viscosity 0025 pa s and the continuous medium a water solution doped with nacl 1000 μs cm−1 the tests were made for oil concentrations of 001 013 and 022 vv and velocities between 09 and 26 ms−1 of the mixture experimental work was performed in a pilot rig equipped with an electrical impedance tomography eit system information on pressure drop eit maps volumetric concentrations in the vertical diameter of the pipe and flow images were obtained simulations were performed in 2dimensional geometry using the eulerian–eulerian approach and the kε model for turbulence modelling the model was implemented in a computational fluid dynamics platform with the programme comsol multiphysics version 53 the simulations were carried out using the schiller–neumann correlation for the drag coefficient and two equations for the viscosity calculation guth and simba 1936 and pal 2000 for the validation of the simulations the pressure drop was the main control parameter the simulations predicted the fully dispersed flow patterns and the pressure drop calculated when using the pal 2000 equation for the viscosity calculation showed the best fit the results of the images of the flows obtained by the photographs and simulations were in good agreement
"permissionless blockchain environments necessitate the use of a fast and attackresilient message propagation protocol for block and transaction messages to keep nodes synchronised and avoid forks we present gossipsub a gossipbased pubsub protocol which in contrast to past pubsub protocols incorporates resilience against a wide spectrum of attacks 
firstly gossipsubs mesh construction implements an eager push model keeps the fanout of the pubsub delivery low and balances excessive bandwidth consumption and fast message propagation throughout the mesh secondly through gossip dissemination gossipsub realises a lazypull model to reach nodes faraway or outside the mesh thirdly through constant observation nodes maintain a score profile for the peers they are connected to allowing them to choose the most wellbehaved nodes to include in the mesh finally and most importantly a number of tailormade mitigation strategies designed specifically for these three components make gossipsub resilient against the most challenging sybilbased attacks we test gossipsub in a testbed environment involving more than 5000 vm nodes deployed on aws and show that it stays immune to all considered attacks gossipsub is currently being integrated as the main messaging layer protocol in the filecoin and the ethereum 20 eth20 blockchains"
as an important method for uncertainty modeling dempstershafer ds evidence theory has been widely applied in practical applications however the counterintuitive results are often generated when fusing different sources of highly conflicting evidence with dempster’s combination rule several different methods for measuring the evidence conflict have been proposed nevertheless these methods showed focus only on a single criterion to measure the conflicting evidence monocriteria factor for the measurement of the conflict between evidence is however often unreliable and inaccuracy because various factors affect the degree of conflict between the evidence such as imperfection dissimilarity disparity and uncertainty to address this issue multiple criteria factors are utilized to measure the degree of conflict between the evidence in this paper an improved analytic hierarchy process is proposed to determine the weights of each body of evidence by considering multiple criteria firstly calculating the quantitative value of the evaluation index of each evidence under every criterion the covariance matrix of the criterion layer is determined based on the covariance between the quantitative values of each criterion then the pairwise comparison matrix of the criterion layer can be obtained by transforming the covariance matrix next the variance among the quantitative values of each criterion is applied to construct the fuzzy preference relation matrix the fuzzy preference relation matrix is used to replace the pairwise comparison matrix of the scheme layer after that the weight of the criterion layer and the scheme layer are combined to acquire the final weights of each evidence finally the original evidence is modified with the final weights of the evidence before using dempster’s combination rule two numerical experiments are given to verify the efficiency of the proposed approach the result shows that the proposed method is more efficient and feasible in managing the conflicting evidence than other approaches available in the literature described
an activity recognition method developed by team dsmltdu for the sussexhuawei locomotiontransportation shl recognition challenge was descrived since the 2018 challenge our team has been developing human activity recognition models based on a convolutional neural network cnn using fast fourier transform fft spectrograms from mobile sensors in the 2020 challenge we developed our model to fit various users equipped with sensors in specific positions nine modalities of fft spectrograms generated from the three axes of the linear accelerometer gyroscope and magnetic sensor data were used as input data for our model first we created a cnn model to estimate four retention positions bag hand hips and torso from the training data and validation data the provided test data was expected to from hips next we created another pretrained cnn model to estimate eight activities from a large amount of user 1 training data hips then this model was finetuned for different users by using the small amount of validation data for users 2 and 3 hips finally an fmeasure of 967 was obtained as a result of 5foldcross validation
among metal βdiketonates nickel acetylacetonate niacac2 has been widely employed as a precursor for many chemical structures due to its catalytic properties here we investigate by means of density functional theory dft calculations the adsorption and dissociation of this complex after an evaluation of the structural and electronic properties of niacac2 a comparison between different dissociation patterns reveals that the most favorable pattern for the complex adsorbed on iron is different from the one suggested by considering the strength of the bonds in the isolated complex and an attempt to generalize this dissociation model is made in this work moreover the most favorable adsorption configurations turned out to be a long bridge positioning of the nickel atom along with an on top positioning of the oxygen atoms of niacac2 while a short bridge positioning is the most favorable for the central metallic unit alone
we extend the notion of jittered sampling to arbitrary partitions and study the discrepancy of the related point sets let ωω1…ωndocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentvarvecomega omega 1ldots omega nenddocument be a partition of 01ddocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocument01denddocument and let the ith point in pdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathcal penddocument be chosen uniformly in the ith set of the partition and stochastically independent of the other points i1…ndocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumenti1ldots nenddocument for the study of such sets we introduce the concept of a uniformly distributed triangular array and compare this notion to related notions in the literature we prove that the expected lpdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathcal lpenddocumentdiscrepancy elppωpdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathbb emathcal lpmathcal pvarvecomega penddocument of a point set pωdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathcal pvarvecomega enddocument generated from any equivolume partition ωdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentvarvecomega enddocument is always strictly smaller than the expected lpdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathcal lpenddocumentdiscrepancy of a set of n uniform random samples for p1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentp1enddocument for fixed n we consider classes of stratified samples based on equivolume partitions of the unit cube into convex sets or into sets with a uniform positive lower bound on their reach it is shown that these classes contain at least one minimizer of the expected lpdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentmathcal lpenddocumentdiscrepancy we illustrate our results with explicit constructions for small n in addition we present a family of partitions that seems to improve the expected discrepancy of monte carlo sampling by a factor of 2 for every n
bearings usually operate under harsh conditions which result in a dynamic behavior generating nonstationary vibration signals and overwhelmed by noise therefore bearing fault diagnosis and prognosis become difficult since the purpose is to extract robust features able to detect the appearance of faults monitoring the degradation of health state and to predict the remaining useful life rul of bearing the aim of this paper is to propose a method for bearing faults featureextraction using adaptive neuro fuzzy inference system anfis and autogram analysis first times domain features are applied for the raw vibration signal then the selected features are computed to will be analyzed as one of the characteristics that describes the degradation of state system after that the curve fitting smoothing is applied to normalize the amplitude of the irregular values relatively to others feature values the calculated value of acquired signal cannot be smoothed or calculated three or more times hence anfis intervenes for modeling the transfer from an indeterminate input to a more relevant value for monitoring the fault evolution then the output of anfis estimates the days of acquisition and predict the rul of bearing finally the autogram analysis is used to identify the degraded element in the bearing
in this article we study the problem of quality of service qosaware cost management of sensorcloud comprising multiple sensorcloud service providers scsps and sensorowners the rapid adaptation of the wireless sensor network wsn and internetofthings iot technology led to the conceptualization of the sensorcloud infrastructure which primarily aims to reduce the complexities associated with operating wsnbased applications by rendering sensorsasaservice seaas however the oligopolistic market scenario of sensorcloud involving multiple scsps and sensorowners significantly impacts its profitability and qos thus there is a need to explore the dynamics of this market competition elaborately in order to maintain the usability of sensorcloud the existing works fail to address the aforementioned issues in sensorcloud hence in this work we analyze the interactions among the sensorowners and the scsps using a gametheoretic approach we propose a qosaware dynamic cost management scheme named quest to determine the optimal strategies of the various actors in sensorcloud market through simulations we observe that using quest the price paid by the endusers decreases by 10312043 percent and the revenue of sensorowners improves by 66838994 percent moreover quest ensures the service satisfaction of the endusers while optimally distributing the services among the scsps and the sensorowners
the article is devoted to the development of algorithmic methods ensuring efficient complexity bounds for strongly convexconcave saddle point problems in the case when one of the groups of variables is highdimensional and the other is relatively lowdimensional up to a hundred the proposed technique is based on reducing problems of this type to a problem of minimizing a convex maximizing a concave functional in one of the variables for which it is possible to find an approximate gradient at an arbitrary point with the required accuracy using an auxiliary optimization subproblem with another variable in this case the ellipsoid method is used for lowdimensional problems if necessary with an inexact deltasubgradient and accelerated gradient methods are used for highdimensional problems for the case of a very small dimension of one of the groups of variables up to 5 an approach based on a new version of the multidimensional analog of the yu e nesterovs method on the square multidimensional dichotomy is proposed with the possibility of using inexact values of the gradient of the objective functional
realistic everyday rewards contain multiple components an apple has taste and size however we choose in single dimensions simply preferring some apples to others how can such singledimensional preference relationships refer to multicomponent choice options here we measured how stochastic choices revealed preferences for 2component milkshakes the preferences were intuitively graphed as indifference curves that represented the orderly integration of the 2 components as tradeoff parts of 1 component were given up for obtaining 1 additional unit of the other component without a change in preference the wellordered nonoverlapping curves satisfied leaveoneout tests followed predictions by machine learning decoders and correlated with singledimensional beckerdegrootmarschak bdm auctionlike bids for the 2component rewards this accuracy suggests a decision process that integrates multiple reward components into singledimensional estimates in a systematic fashion in interspecies comparisons human performance matched that of highly experienced laboratory monkeys as measured by accuracy of the critical tradeoff between bundle components these data describe the nature of choices of multicomponent choice options and attest to the validity of the rigorous economic concepts and their convenient graphic schemes for explaining choices of human and nonhuman primates the results encourage formal behavioral and neural investigations of normal irrational and pathological economic choices
we consider the topic of data imputation a foundational task in machine learning that addresses issues with missing data to that end we propose mcflow a deep framework for imputation that leverages normalizing flow generative models and monte carlo sampling we address the causality dilemma that arises when training models with incomplete data by introducing an iterative learning scheme which alternately updates the density estimate and the values of the missing entries in the training data we provide extensive empirical validation of the effectiveness of the proposed method on standard multivariate and image datasets and benchmark its performance against stateoftheart alternatives we demonstrate that mcflow is superior to competing methods in terms of the quality of the imputed data as well as with regards to its ability to preserve the semantic structure of the data
this paper describes recent activities with respect to exposure assessment method of the incident power density pd above 6 ghz when wireless devices such as the 5th generation mobile communication system 5g used in close proximity to the head and body these works have been jointly done by the international electrotechnical committee iec and the institute of electrical and electronics engineers ieee firstly definitions of the pd are briefly introduced because there was no exact definition of the pd except in the farfield region then measurement and computation procedures are introduced
jatspin this paper we discuss the feasibility of homotopy continuation method for the nonlinear matrix equations mmlmath xmlnsmmlhttpwwww3org1998mathmathml idm1mmlmrowmmlmixmmlmimmlmommlmommlmstyle displaystyletruemmlmsubsupmmlmo stretchyfalse∑mmlmommlmrowmmlmiimmlmimmlmommlmommlmn1mmlmnmmlmrowmmlmismmlmimmlmsubsupmmlmrowmmlmsubsupmmlmibmmlmimmlmiimmlmimmlmi∗mmlmimmlmsubsupmmlmsupmmlmrowmmlmixmmlmimmlmrowmmlmrowmmlmo−mmlmommlmn1mmlmnmmlmrowmmlmsupmmlmsubmmlmrowmmlmibmmlmimmlmrowmmlmrowmmlmiimmlmimmlmrowmmlmsubmmlmrowmmlmstylemmlmommlmommlmstyle displaystyletruemmlmsubsupmmlmo stretchyfalse∑mmlmommlmrowmmlmiimmlmimmlmommlmommlmismmlmimmlmommlmommlmn1mmlmnmmlmrowmmlmimmmlmimmlmsubsupmmlmrowmmlmsubsupmmlmibmmlmimmlmiimmlmimmlmi∗mmlmimmlmsubsupmmlmsupmmlmrowmmlmixmmlmimmlmrowmmlmrowmmlmsubmmlmrowmmlmitmmlmimmlmrowmmlmrowmmlmiimmlmimmlmrowmmlmsubmmlmrowmmlmsupmmlmsubmmlmrowmmlmibmmlmimmlmrowmmlmrowmmlmiimmlmimmlmrowmmlmsubmmlmrowmmlmstylemmlmommlmommlmiimmlmimmlmrowmmlmath with mmlmath xmlnsmmlhttpwwww3org1998mathmathml idm2mmlmrowmmlmn0mmlmnmmlmommlmommlmsubmmlmrowmmlmitmmlmimmlmrowmmlmrowmmlmiimmlmimmlmrowmmlmsubmmlmommlmommlmn1mmlmnmmlmrowmmlmath this iterative method does not depend on a good initial approximation to the solution of matrix equationjatsp
a compact singlepass and lowpowerconsumption methane sensing system is presented and its sensitivity is investigated for a set of reference cells with pressures of 74 74 and 740 torr and the same methane concentration of 1351 ppm a singlemode gasbbased continuouswave cw distributed feedback dfb tunable diode laser at 3270 nm and wavelength modulation spectroscopy were employed to collect 2f spectra in the ν3r3 band of 12ch4 a modulated voigt line profile model was used to fit the collected 2f spectra the best detectivity of 4 ppbm is obtained for the highest pressure cell using an allanwerle variance analysis
in this work we implement multiplicative noise to the duffing oscillator with variable coefficients the stochastic differential equations are solved using the fourthorder runge–kutta method with
navigation in endoscopic environments requires an accurate and robust localisation system a key challenge in such environments is the paucity of visual features that hinders accurate tracking this article examines the performance of three image enhancement techniques for tracking under such featurepoor conditions including contrast limited adaptive histogram specification clahs fast local laplacian filtering llap and a new combination of the two coined local laplacian of specified histograms llsh two cadaveric knee arthroscopic datasets and an underwater seabed inspection dataset are used for the analysis where results are interpreted by defining visual saliency as the number of correctly matched keypoint sift and surf features experimental results show a significant improvement in contrast quality and feature matching performance when image enhancement techniques are used results also demonstrate the llshs ability to vastly improve surf tracking performance indicating more than 87 of successfully matched frames a comparative analysis provides some important insights useful in the design of visionbased navigation for autonomous agents in featurepoor environments
adaptive sparsest narrowband decomposition is the most sparse solution to search for signals in the overcomplete dictionary library containing intrinsic mode functions which transform the signal decomposition into an optimization problem but the calculation accuracy must be improved in the case of strong noise interference therefore in combination with the algorithm of the complementary ensemble empirical mode decomposition a new method of the complementary ensemble adaptive sparsest narrowband decomposition is obtained in the complementary ensemble adaptive sparsest narrowband decomposition the white noise opposite to the paired symbol is added to the target signal to reduce the reconstruction error and realize the adaptive decomposition of the signal in the process of optimizing the filter parameters the analysis results of the simulation and experimental data show this method is superior to complementary ensemble empirical mode decomposition and adaptive sparsest narrowband decomposition in inhibiting the mode confusion endpoint effect improving the component orthogonality and accuracy and effectively identifying the gears fault types
the advancing globalization and increasing digitization across all industries led to a much more complex and dynamic business environment this environment is characterized by competition and coopetition moore 1993 presented an approach of the business ecosystem in which longtermsuccessful companies work in network structures characterized by coopetition in this context companies can take different roles in this business ecosystem in the literature there are different approaches to describe these roles however an allencompassing approach is missing and it is unclear how to classify these roles therefore this research aims to develop a framework to describe the roles of a business ecosystem such a framework allows classifying the competitive environment of an actor within the business ecosystem building on this understanding strategic decisions can be prepared and derived to ensure the validity of the developed framework it was applied to a current case study the electromobility business ecosystem
the severe acute respiratory syndrome 2 sarscov2 is an infectious virus that causes mild to severe lifethreatening upper respiratory tract infection the virus emerged in wuhan china in 2019 and later spread across the globe its genome has been completely sequenced and based on the genomic information the virus possessed 3clike main protease 3clpro an essential multifunctional enzyme that plays a vital role in the replication and transcription of the virus by cleaving polyprotein at eleven various sites to produce different nonstructural proteins this makes the protein an important target for drug design and discovery herein we analyzed the interaction between the 3clpro and potential inhibitory compounds identified from the extracts of zingiber offinale and anacardium occidentale using in silico docking and molecular dynamics md simulation the crystal structure of sarscov2 main protease in complex with 02j 5methylisoxazole3carboxylic acid and pej composite ligand pdb code 6lu7 216 å retrieved from protein data bank pdb and subject to structure optimization and energy minimization a total of twentynine compounds were obtained from the extracts of z offinale and the leaves of a occidentale these compounds were screened for physicochemical lipinski rule of five veber rule and egan filter panassay interference structure and pharmacokinetic properties to determine the pharmaceutical active ingredients of the 29 compounds only nineteen 19 possessed druglikeness properties with efficient oral bioavailability and less toxicity these compounds subjected to molecular docking analysis to determine their binding energies with the 3clpro the result of the analysis indicated that the free binding energies of the compounds ranged between − 508 and − 1024 kcalmol better than the binding energies of 02j − 410 kcalmol and pje − 507 kcalmol six compounds cid99615  − 1024 kcalmol cid3981360  975 kcalmol cid9910474  − 914 kcalmol cid11697907  − 910 kcalmol cid10503282  − 909 kcalmol and cid620012  − 853 kcalmol with good binding energies further selected and subjected to md simulation to determine the stability of the protein–ligand complex the results of the analysis indicated that all the ligands form stable complexes with the protein although cid9910474 and cid10503282 had a better stability when compared to other selected phytochemicals cid99615 cid3981360 cid620012 and cid11697907
to efficiently run dnns on the edgecloud many new dnn inference accelerators are being designed and deployed frequently to enhance the resource efficiency of dnns model quantization is a widelyused approach however different acceleratorhw has different resources leading to the need for specialized quantization strategy of each hw moreover using the same quantization for every layer may be suboptimal increasing the designspace of possible quantization choices this makes manualtuning infeasible recent work in automatically determining quantization for each layer is driven by optimization methods such as reinforcement learning however these approaches need retraining the rl for every new hw platform we propose a new way for autonomous quantization and hwaware tuning we propose a generative model aqgan which takes a target accuracy as the condition and generates a suite of quantization configurations with the conditional generative model the user can autonomously generate different configurations with different targets in inference time moreover we propose a simplified hwtuning flow which uses the generative model to generate proposals and execute simple selection based on the hw resource budget whose process is fast and interactive we evaluate our model on five of the widelyused efficient models on the imagenet dataset we compare with existing uniform quantization and stateoftheart autonomous quantization methods our generative model shows competitive achieved accuracy however with around two degrees less search cost for each design point our generative model shows the generated quantization configuration can lead to less than 35 error across all experiments
a series of robust commensurate fractional differentiators are proposed for a class of fractional order linear systems in this paper such differentiators are based on the modulating functions method first fractional integration by parts formulas and modulating functions are used to eliminate the undesired terms and keep needed items second a recursive algorithm is designed to calculate the required derivatives in this way we can get a set of estimation results in one calculation and it only requires less computational cost moreover a detailed noise error analysis of the algorithm is performed to improve the estimation results finally the efficiency and robustness of the presented method is verified by numerical simulations
silicon carbide double diffused metal oxide semiconductor sicdmos having a more comprehensive bandgap fast switching and low power losses has been rapidly developed and applied this paper elaborates a detailed analysis of switching losses by comparing si igbt and sic dmos under the same voltage parameters and identical conditions the switch’s characteristics are evaluated and contrasted in various gate resistances in addition a gate driver is designed to calculate the switching losses of igbt and sic dmos at different frequencies then they are respectively used in the buck converter the experimental platform is built to test and validate that the sic dmos buck converter achieves faster dynamic performance and higher efficiency than si igbt
despite the steep increase in facebook stories users there is scant research on this topic this study compared the associations of frequency of stories update frequency of news feed updates time spent reading stories and time spent reading news feeds with regard to social media addiction narcissism and positive affect in college students we recruited a sample of 316 college students from taiwan the analytical results show that facebook stories are more addictive and provoke more positive affect than conventional news feeds moreover only usage behaviors associated with stories predict narcissism this study also found that the prediction of news feeds with regard to addiction narcissism and positive affect also seems to be diminishing and is being replaced by those of stories future studies on the psychological consequences and predictors of social media usage should regard stories as a crucial variable
a novel notion of unpredictable strings is revealed and utilized to define deterministic unpredictable sequences on a finite number of symbols we prove the first law of large strings for random processes in discrete time which confirms that there exists the uncountable set of unpredictable realizations the hypothesis on the second law of large strings is formulated which is relative to the bernoulli theorem theoretical and numerical backgrounds for the laws are provided
research ethics has traditionally been guided by wellestablished documents such as the belmont report and the declaration of helsinki at the same time the introduction of big data methods that is having a great impact in behavioral research is raising complex ethical issues that make protection of research participants an increasingly difficult challenge by conducting 39 semistructured interviews with academic scholars in both switzerland and united states our research aims at exploring the code of ethics and research practices of academic scholars involved in big data studies in the fields of psychology and sociology to understand if the principles set by the belmont report are still considered relevant in big data research our study shows how scholars generally find traditional principles to be a suitable guide to perform ethical data research but at the same time they recognized and elaborated on the challenges embedded in their practical application in addition due to the growing introduction of new actors in scholarly research such as data holders and owners it was also questioned whether responsibility to protect research participants should fall solely on investigators in order to appropriately address ethics issues in big data research projects education in ethics exchange and dialogue between research teams and scholars from different disciplines should be enhanced in addition models of consultancy and shared responsibility between investigators data owners and review boards should be implemented in order to ensure better protection of research participants
to develop a 3d whole‐brain simultaneous t1t2t1ρ quantification method with mr multitasking that provides high quality co‐registered multiparametric maps in 9 min
a novel singlelayer opticaltransparent frequency selective surface fss with a wide stopband is proposed in this paper this structure is realized by etching patterns on the highly transparent conductive film called indium tin oxide ito the working principles of the proposed fss are explained by the analysis of the equivalent circuit models then a designed example operating at 175 ghz is fabricated using laser etching technology finally the fabricated sample is measured by free space method the measured results illustrate that this fss exhibit miniaturized unitcell size of 0193λ0 × 0193λ0 λ0 refers to the free space wavelength at the resonant frequency and stable frequency filtering response under oblique incident angles up to 30°
trigona bees food resources can be determined either directly based on flowering plants live closed to the nest or based on pollen diversity inside the nest there is no study about trogona bees food resources determination based on pollen diversity inside the bee colonys nest this study aimed to determine plant food resources based on pollen diversity found inside the trigona nest this research was conducted in serang village subdistrict of karangreja purbalingga regency pollen samples were taken from flowering plants lives around the nest and those in the nest and then were prepared using the acetolysis method the variables observed were pollen morphology with parameters such as unit shape size aperture and ornamentation the data obtained were analyzed descriptively based on pollen diversity 43 species and 22 plant families were live around trigonanest in serang village fortyone pollen types were found inside the trigonas nest with 37 of them are identic to the pollen collected from flowers around the nest it can be concluded that 37 species of flowering plants could be determined as food resources for the trigona bee based on pollen diversity inside the nest this research provides the first data about feed resources for trigona bee in serang village based on pollen diversity the results provide essential information about food resources which is vital for the development of trigona bee cultivation
arterial hypertension is an indicator of cardiovascular pathologies it causes arterial stiffness which worsens with aging this paper presents a novel photoplethysmographyelectrocardiograph ppgecg combo measurement system using a dedicated signal processing methodology which aims at the evaluation of the pulse wave velocity in order to obtain clinical parameters of the aging state of the arteries arterial stiffness preliminary experimental results are presented which demonstrates the suitability of the proposed methodology
the frequency stability has become an essential issue in power systems with high penetration of renewables the active powerfrequency pf controller has been used in wind farms to provide primary frequency regulation pfr however because of the nonlinear relationship between the frequency and provided active power improper design of the controller parameter may waste the potential ability of frequency regulation of the wind farms to fully excavate the frequency support potential of the wind farms a method for parameter design is proposed the optimal value of the pf controller parameter is presented in an analytical form with consideration of the system characteristics then the factors that dominantly influence the optimal value of the pf controller parameter are analyzed the analysis shows that the deloading coefficient the load damping rate the ability of primary frequency regulation of generators and the capacity of doublyfed induction generators dfigs will influence the optimal value of the pf controller parameter in different mechanisms case studies validate the derived analytical form and prove the effectiveness of the proposed parameter design method
"
 the past decade has seen significant progress in understanding galaxy formation and evolution using largescale cosmological simulations while these simulations produce galaxies in overall good agreement with observations they employ different subgrid models for galaxies and supermassive black holes bhs we investigate the impact of the subgrid models on the bh mass properties of the illustris tng100 tng300 horizonagn eagle and simba simulations focusing on the mbh − m⋆ relation and the bh mass function all simulations predict tight mbh − m⋆ relations and struggle to produce bhs of mrm bhleqslant 1075 rm modot  in galaxies of mstar sim 1010510115 rm modot  while the time evolution of the mean mbh − m⋆ relation is mild rm delta mrm bhleqslant 1 dex for 0 leqslant z leqslant 5 for all the simulations its linearity shape and normalization varies from simulation to simulation the strength of sn feedback has a large impact on the linearity and time evolution for mstar leqslant 10105 rm modot  we find that the lowmass end is a good discriminant of the simulation models and highlights the need for new observational constraints at the highmass end strong agn feedback can suppress the time evolution of the relation normalization compared with observations of the local universe we find an excess of bhs with mrm bhgeqslant 109 rm modot  in most of the simulations the bh mass function is dominated by efficiently accreting bhs log 10 frm eddgeqslant 2 at high redshifts and transitions progressively from the highmass to the lowmass end to be governed by inactive bhs the transition time and the contribution of active bhs are different among the simulations and can be used to evaluate models against observations"
given that heterogeneous information networks hin encompass nodes and edges belonging to different semantic types they can model complex data in realworld scenarios thus hin embedding has received increasing attention which aims to learn node representations in a lowdimensional space in order to preserve the structural and semantic information on the hin in this regard metagraphs which model common and recurring patterns on hins emerge as a powerful tool to capture semanticrich and often latent relationships on hins although metagraphs have been employed to address several specific data mining tasks they have not been thoroughly explored for the more general hin embedding in this paper we leverage metagraphs to learn relationshippreserving hin embedding in a selfsupervised setting to support various relationship mining tasks in particular we observe that most of the current approaches often underutilize metagraphs which are only applied in a preprocessing step and do not actively guide representation learning afterwards thus we propose the novel framework of mg2vec which learns the embeddings for metagraphs and nodes jointly that is metagraphs actively participates in the learning process by mapping themselves to the same embedding space as the nodes do moreover metagraphs guide the learning through both first and secondorder constraints on node embeddings to model not only latent relationships between a pair of nodes but also individual preferences of each node finally we conduct extensive experiments on three public datasets results show that mg2vec significantly outperforms a suite of stateoftheart baselines in relationship mining tasks including relationship prediction search and visualization
for low dimensional classification problems we propose the novel diopt approach which considers the construction of a discretized feature space predictions for all cells in this space are obtained by means of a reference classifier and the class labels are stored in a lookup table generated by enumerating the complete space this then leads to extremely high classification throughput as inference consists only of discretizing the relevant features and reading the class label from the lookup table index corresponding to the concatenation of the discretized feature bin indices since the size of the lookup table is limited due to memory constraints the selection of optimal features and their respective discretization levels is paramount we propose a particular supervised discretization approach striving to achieve maximal class separation of the discretized features and further employ a purposebuilt memetic algorithm to search towards the optimal selection of features and discretization levels the inference run time and classification accuracy of diopt is compared to benchmark random forest and decision tree classifiers in several publicly available data sets orders of magnitude improvements are recorded in classification runtime with insignificant or modest degradation in classification accuracy for many of the evaluated binary classification tasks
malignant melanoma has caused countless deaths in recent years many calculation methods have been created for automatic melanoma detection in this chapter based on the traditional concept of shape signature and convex hull an improved boundary description shape signature is developed the convex defectbased signature cdbs proposed in this paper scans contour irregularities and is applied to skin lesion classification in macroscopic images border irregularities of skin lesions are the predominant criteria for abcd asymmetry border color and diameter to distinguish between melanoma and nonmelanoma the performance of the cdbs is compared with popular shape descriptors shape signature indentation depth function invariant elliptic fourier descriptor iefd and rotation invariant wavelet descriptor riwd where the proposed descriptor shows better results multilayer perceptron neural network is used as a classifier in this work experimental results show that the proposed approach achieves significant performance with mean accuracy of 9049 classification of skin lesion using segmentation shape feature detection
in this article firstly based on taylor series expansion and truncation error correction technology combined with the fourthorder pade schemes of the firstorder derivatives a new fourthorder compact difference cd scheme is constructed to solve the twodimensional 2d linear elliptic equation a mixed derivative in this new scheme unknown function and its firstorder derivatives are regarded as the unknown variables in calculation then the method is extended to solve the 2d parabolic equation with a mixed derivative to match the spatial fourthorder accuracy the backward differentiation formula bdf is employed to gain the fourthorder accuracy for the temporal discretization truncation error is analyzed to display that the present scheme is fourthorder accuracy in space in order to solve the resulting largescale linear equations a multigrid method is employed to accelerate the convergence speed of the conventional relaxation methods finally numerical results indicate that the present schemes obtain fourthorder convergence and are more accurate than those in the literature
as a decentralized immutable ledger where several trustless peers can reach consensus with each other without the need of any trusted third party blockchain technology fits perfectly with the peertopeer p2p energy trading paradigm in this paper we propose design and analyze a marketplace for energy trading based on smart contracts on the blockchain the proposed system named joulin serves as a competitive and efficient marketplace where peers can both produce buy and sell energy depending on their needs as a proofofconcept we developed the prototype of the joulin system using ethereum blockchain our results in terms of usability flexibility and resiliency demonstrate the potential to achieve an easily extendable and reliable system with low transaction costs low ethereum gas costs and quick response times demonstrate usability our smart contracts have also been tested with security tools to ensure that they are not vulnerable to outside manipulations
recent techniques utilizing reflectarrays or novel metamaterialbased surfaces to control wireless propagation environments have attracted great attention while most solutions focus on sub6 ghz wireless channel to assist in throughput enhancement these intelligent surfaces have significant potentials in combating the transmission distance limitation and solving the nonlineofsight transmission problems for the millimeter wave 30–300 ghz and terahertzband 03–10 thz where signal propagation is significantly attenuated in the atmosphere the ultramassive mimo um mimo communication framework has been proposed in such frequency bands which relies on the plasmonic antenna arrays to realize different operation modes including anomalous reflection transmission and reception among others in this paper a joint beamforming scheme is developed based on fractional programming optimization to maximize the spectral efficiency under practical consideration of energy constraints of the um mimo communication platform numerical results are presented to show the performance comparison with existing solutions
the interest for smartgrids has grown worldwide and several utilities have begun to base their plans for expansion and modernization considering this concept new trends to get more efficient compact and reliable power semiconductor devices have motivated the development of new components such as the solidstate transformer sst which has become the key technology to be used in smartgrid applications these advantages can be exploited by regulating the power flow in the dualactive bridge dab of the sst in this context this paper focuses on proposing a passivitybased current controller for the dab of an sst that regulates the power flow considering the passive behavior of dab model and its porthamiltonian structure which guarantees stability in closedloop operation to validate the proposed approachs effectiveness the closedloop system is tested under different power variations including bidirectional power flow and its performance is compared with a conventional pi controller in addition input voltage variations are also considered results validated the controllers capability of maintaining constant power flow in the event of input disturbances
in this article a novel classe amplifier topology based on the functioning principle of the stacked converters is presented in this amplifier the available output electric power is determined by the addition of the electric power generated by each of the associated devices herein two mosfets connected in series are linked on the one hand to a dc voltage source and on the other hand to a resonant lc circuit and a resistance charge connected in parallel to the capacitor both mosfets are driven by a gatetosource square signal allowing a synchronized commutation promoting a quasiresonant amplifier behavior similar to that of the classical classe amplifier with the characteristic of being able to manage the double of applied dc voltage amplitude and to supply twice electric power to the same load in comparison with the classe amplifier this device is able to distribute the same electric power between both mosfets reducing at almost half the magnitude of applied draintosource voltage as a result voltage stress of each power switch is reduced in comparison with the typical classe amplifier drain efficiency de power added efficiency and gain of the proposed device were obtained in function of the applied dc voltage
motivated by the apparent societal need to design complex autonomous systems whose decisions and actions are humanly intelligible the study of explainable artificial intelligence and with it research on explainable autonomous agents has gained increased attention from the research community one important objective of research on explainable agents is the evaluation of explanation approaches in humancomputer interaction studies in this demonstration paper we present a way to facilitate such studies by implementing explainable agents and multiagent systems that i can be deployed as static files not requiring the execution of serverside code which minimizes administration and operation overhead and ii can be embedded into web front ends and other javascriptenabled user interfaces hence increasing the ability to reach a broad range of users we then demonstrate the approach with the help of an application that was designed to assess the effect of different explainability approaches on the human intelligibility of an unmanned aerial vehicle simulation
the role of professional service firms psfs has always been crucial in the development of knowledge economies the effectiveness of these firms is highly attributed to the knowledge capabilities and skills embedded in its human resources and how effectively these resources are utilized in the optimal benefit of the firm owing to the everincreasing growth of the services sector globally it’s critical for the psfs to gain indepth awareness on the application of highperformanceworkpractices hpwps so as to continually maintain quality of their services to the clients however the mechanism for systematically designing and implementing these practices in intellectual capital context is still not fully developed this research therefore theoretically investigates and suggests a linkage mechanism on how strategic hrm practices hpwps via ability motivation and opportunityenhancing bundles stimulate intellectual capital development in professional service firms by presenting a conceptual framework this study offers practically meaningful insights to the managers in the service firms on how to implement these practices for effectively meeting client needs and sustaining a competitive advantage
a dominating set in a graph g is a set of vertices s ⊆ v  g  such that any vertex of v − s is adjacent to at least one vertex of s  a dominating set s of g is said to be a perfect dominating set if each vertex in v − s is adjacent to exactly one vertex in s the minimum cardinality of a perfect dominating set is the perfect domination number γ p  g   a function f  v  g  →  0  1  2  is a perfect roman dominating function prdf on g if every vertex u ∈ v for which f  u   0 is adjacent to exactly one vertex v for which f  v   2  the weight of a prdf is the sum of its function values over all vertices and the minimum weight of a prdf of g is the perfect roman domination number γ r p  g   in this paper we prove that for any nontrivial tree t γ r p  t  ≥ γ p  t   1 and we characterize all trees attaining this bound
grounddwelling macrolichens dominate the forest floor of mature upland pine stands in the boreal forest understanding patterns of lichen abundance as well as environmental characteristics associated with lichen growth is key to managing lichens as a forage resource for threatened woodland caribou rangifer tarandus caribou the spectral signature of lightcoloured lichen distinguishes it from green vegetation potentially allowing for mapping of lichen abundance using multispectral imagery while canopy structure measured from airborne laser scanning als of forest openings can indirectly map lichen habitat here we test the use of highresolution kompsat korea multipurpose satellite3 imagery 280 cm resolution and forest structural characteristics derived from als to predict lichen biomass in an upland jack pine forest in northeastern alberta canada we quantified in the field lichen abundance cover and biomass in mature jack pine stands across low moderate and high canopy cover we then used generalized linear models to relate lichen abundance to spectral data from kompsat and structural metrics from als model selection suggested that lichen abundance was best predicted by canopy cover als points  137 m and to a lesser extent blue spectral data from kompsat lichen biomass was low at plots with high canopy cover 9896 gm2 while almost doubling for plots with low canopy cover 18630 gm2 overall the model fit predicting lichen biomass was good r2 c  035 with maps predicting lichen biomass from spectral and structural data illustrating strong spatial variations highresolution mapping of ground lichen can provide information on lichen abundance that can be of value for management of forage resources for woodland caribou we suggest that this approach could be used to map lichen biomass for other regions
knowledge management itself is a complex activity related to building organizational capabilities to integrate various forms of interactive technology with critical social cultural and organizational issues of knowledge workers this becomes the core of all aspects of km—knowledge creation sharing transformation and retention—in the organization development of an effective km strategy via adopting the appreciative inquiry approach can decide on the future of the robustness of organizations organizational transformation and eventually enhances the scope of competitive advantage this chapter will incorporate the tenets of km and ancillary practices theory of organizations though the lens of appreciative inquiry approach and the way an organizations structural transformation can be influenced by strategic management of knowledge within the organization it will concurrently demonstrate the index of strategic influence within the overall organizational network
printed electronics pe enables disruptive applications in wearables smart sensors and healthcare since it provides mechanical flexibility low cost and ondemand fabrication the progress in pe raises trust issues in the supply chain and vulnerability to reverse engineering re attacks recently re attacks on pe circuits have been successfully performed pointing out the need for countermeasures against re such as camouflaging in this article we propose a printed camouflaged logic cell that can be inserted into pe circuits to thwart re the proposed cell is based on three components achieved by changing the fabrication process that exploits the additive manufacturing feature of pe these components are optically lookalike while their electrical behaviors are different functioning as a transistor short and open the properties of the proposed cell and standard pe cells are compared in terms of voltage swing delay power consumption and area moreover the proposed camouflaged cell is fabricated and characterized to prove its functionality furthermore numerous camouflaged components are fabricated and their indistinguishability is assessed to validate their optical similarities based on the recent re attacks on pe the results show that the proposed cell is a promising candidate to be utilized in camouflaging pe circuits with negligible overhead
recently the notion of iλ–convergence in an intuitionistic fuzzy n–normed spaces was introduced by konwar et al n konwar and p debnath iλ–convergence in intuitionistic fuzzy nnormed linear space 07 2016 in this article with the help of the notion iλ–convergence  we introduce some new orlicz sequence spaces further we examine some topological properties on these spaces
at the time of writing this editorial more than 13 million people have been diagnosed with sarscov2 in the world more than half a million have died and the transmission does not reach its peak who 2020 latin america and the united states are the current epicentre of the pandemic today once again a catastrophic fact shows us the fragility of human life this pandemic coupled with climate change reminds us that we cannot continue believing and acting as if human beings were out of nature and that nature is a storehouse of unlimited resources that some can harness more than others this pandemic highlights central aspects of our contemporary life the transnationalization of risks in the context of an unequal globalization the limitations and possibilities of scientific knowledge and collaborative work the precarious living and working conditions of millions of people that make them more vulnerable the current hegemonic marketbased global economic system has somehow normalized exploitation spoliation injustice genocide of black and indigenous peoples and precarization of life thus its negative consequences are most acutely borne by populations facing a greater social economic and political disadvantage hence reproducing in this pandemic historical sociosanitary inequities it is only enough to see what is happening with the sick and dead from covid19 in brazil or with black and indigenous populations with historically precarious living conditions in colombia or with latinos and immigrants in the united states all of them are less likely to face the pandemic today and in the future oxfam 2019 estimated that billionaires had increased their wealth by us 25 billion a day while the poorest half of the world’s population had reduced it by 11 this huge concentration of wealth is in part the result of an increase in job precariousness which leads to precarious lives and a new social class the precarious processes to restructure productivity led to the reorientation of policies in countries especially in the global south which have facilitated the involvement of private actors with explicit and nonexplicit profit interests in sectors where the state traditionally played a primary role such as health and education moreover new forms of capital accumulation emerged that strengthened capital accumulation in the financial area the emergence of large transnational corporations and the entry of financial capital into the health area iriart and merhy 2017 the current sanitary crisis must be read in the context of health systems that have undergone major transformations in the last decades privatization of healthcare services reorientation towards managed care generation of intermediaries for health management—insurance companies separation of individual and collective healthcare definancing of public health providers and in particular precariousness of the contracts of healthcare workers these situations are exposed in this special issue which recognizes the ability of marketdriven actors to influence health policy decisions with the relative complacency of multilateral agencies in turn these changes have provoked social mobilization demanding universal and public health systems and this editorial is part of the special issue ‘‘marketdriven forces and public health’’ the authors of this editorial are guest editors of the special issue
in this paper we have introduced the ilocalized and the i∗localized sequences in metric spaces and investigate some basics properties of the ilocalized sequences related with icauchy sequences also we have obtained some necessary and sufficient conditions for the ilocalized sequences to be an icauchy sequences it is also defined uniformly the ilocalized sequences on metric spaces and its relation with icauchy sequences has been  obtained
this note summarizes the optimization formulations used in the study of markov decision processes we consider both the discounted and undiscounted processes under the standard and the entropyregularized settings for each setting we first summarize the primal dual and primaldual problems of the linear programming formulation we then detail the connections between these problems and other formulations for markov decision processes such as the bellman equation and the policy gradient method
isoprene is the dominant nonmethane organic compound emitted to the atmosphere1–3 it drives ozone and aerosol production modulates atmospheric oxidation and interacts with the global nitrogen cycle4–8 isoprene emissions are highly uncertain19 as is the nonlinear chemistry coupling isoprene and the hydroxyl radical oh—its primary sink10–13 here we present global isoprene measurements taken from space using the crosstrack infrared sounder together with observations of formaldehyde an isoprene oxidation product these measurements provide constraints on isoprene emissions and atmospheric oxidation we find that the isoprene–formaldehyde relationships measured from space are broadly consistent with the current understanding of isoprene–oh chemistry with no indication of missing oh recycling at low nitrogen oxide concentrations we analyse these datasets over four global isoprene hotspots in relation to model predictions and present a quantification of isoprene emissions based directly on satellite measurements of isoprene itself a major discrepancy emerges over amazonia where current underestimates of natural nitrogen oxide emissions bias modelled oh and hence isoprene over southern africa we find that a prominent isoprene hotspot is missing from bottomup predictions a multiyear analysis sheds light on interannual isoprene variability and suggests the influence of the el niñosouthern oscillation direct satellite measurements of atmospheric isoprene are compared with model predictions showing broad agreement but highlighting spatial and temporal biases in modelled isoprene and nitrogen oxide emissions
diabetic nephropathy dn is one of serious complication in diabetes cyanidin3glucoside c3g from black rice was reported has hypoglycemic effects and antiosteoporosis effect in diabetic rats whether it has preventive effects on dn has not been reported in this study we established a rat model of dn and c3g at two doses 10 and 20 mg kg1 d1 were administered to see its antidn effect eight weeks of c3g supplementation decreased blood glucose serum insulin improved the renal function relieved renal glomerular sclerosis and interstitial fibrosis of dn rats also the kidneys of dn rats had improved the oxidative defense system proinflammatory mediators were markedly reduced in serum and kidneys of the c3g treated groups transforming growth factorβ1 tgfβ1 phosphorsmad2 phosphorsmad3 protein expression levels were decreased significantly in the kidney of the c3g treated group whereas smad7 expression level was upregulated by c3g our results indicate that the c3g can ameliorate dn via antioxidative stress antiinflammation and regulating tgfβ1smad23 pathway our results suggest that c3g from black rice might be used as a renalprotective nutrient in diabetic nephropathy
the proliferation of advanced analytics and artificial intelligence has been driven by huge volumes of data that are mostly generated at the edge simultaneously there is a rising demand to perform analytics on edge platforms ie nearsensor data analytics however conventional architectures of such platforms may not execute the targeted applications in an energyefficient manner emerging near and inmemory computing paradigms can increase the energy efficiency of edge platforms by relying on emerging logic and memory devices more importantly these paradigms enable the possibility of performing computations on unconventional platforms namely flexible computing systems in this paper we explore the benefits of inmemory computing at the edge on a flexible substrate enabled by thinfilm transistors tfts and resistive ram rram as a case study we consider biosignal processing application workloads ie compressive sensing and anomaly detection we model the device circuit and architecture of our targeted platform and evaluate the corresponding systemlevel performance preliminary results indicate that inmemory computing enabled by flexible electronic devices enables a new class of edge platforms with lower power consumption compared to that of rigid tft devices
we study single crystals of the magnetic superconductor eurbfe4as4 by magnetization electron spin resonance esr angleresolved photoemission spectroscopy arpes and electrical resistance in pulsed magnetic fields up to 630 koe the superconducting state below 365 k is almost isotropic and only weakly affected by the development of eu2 magnetic order at 15 k on the other hand for the external magnetic field applied along the caxis the temperature dependence of the esr linewidth reveals a berezinskiikosterlitzthouless topological transition below 15 k this indicates that eu2planes are a good realization of a twodimensional xymagnet which reflects the decoupling of the eu2 magnetic moments from superconducting feaslayers
"
 background in the past twenty years humankind has effected from infection caused by sarscov severe acute respiratory syndrome merscov middle east respiratory syndrome and covid19 coronaviruses which have caused significant harm to human health and resulted in high mortality the possibility of using mirna mrnainhibiting rna to inhibit infections caused by the coronaviruses covid19 sarscov and merscov has been shown methods the mirtarget program determines the following characteristics of interaction between mirnas and messenger rnas mrnas the start of the mirna binding site on the mrna the locations of the mirna binding sites in the 3untranslated region 3utr 5untranslated region 5utr or coding sequence cds the interaction free energy ∆g kjmole and nucleotide interaction schemes between mirnas and mrnas results using bioinformatics approaches completely complementary mirna ccmirna complexes were predicted to be able to bind and inhibit the translation of coronavirus proteins and the replication of covid19 sarscov and merscov genomes for complexes of seven completely complementary mirna of covid19 ccmirc seven completely complementary mirna of sarscov ccmirs and eight completely complementary mirna of merscov ccmirm the interactions with the rna genomes grnas of the corresponding coronaviruses was evaluated the free energy of the interactions of ccmirnas with binding sites was significantly higher than the free energy of the interactions with other regions in grna which ensures high selectivity of the binding of ccmirnas weak binding of ccmirnas to the mrnas of 17508 human genes was shown which suggests the absence of side effects of the ccmirnas in humans a feature of this method is the simultaneous inhibition of translation and replication by several ccmirnas binding from the 5 end to the 3 end of grna conclusion the use of several ccmirnas to suppress infections allows each of them to be used at a lower concentration to avoid side effects when one ccmirna is introduced into humans at a high concentration"
we analyze an optimal control problem for a fractional semilinear pde control constraints are also considered we adopt the integral definition of the fractional laplacian and establish the wellposedness of a fractional semilinear pde we also analyze suitable finite element discretizations we thus derive the existence of optimal solutions and first and second order optimality conditions for our optimal control problem regularity properties are also studied we devise a fully discrete scheme that approximates the control variable with piecewise constant functions the state and adjoint equations are discretized via piecewise linear finite elements we analyze convergence properties of discretizations and obtain a priori error estimates
the mechanisms of infant development are far from understood learning about ones own body is likely a foundation for subsequent development here we look specifically at the problem of how spontaneous touches to the body in early infancy may give rise to first body models and bootstrap further development such as reaching competence unlike visually elicited reaching reaching to own body requires connections of the tactile and motor space only bypassing vision still the problems of high dimensionality and redundancy of the motor system persist in this work we present an embodied computational model on a simulated humanoid robot with artificial sensitive skin on large areas of its body the robot should autonomously develop the capacity to reach for every tactile sensor on its body to do this efficiently we employ the computational framework of intrinsic motivations and variants of goal babblingas opposed to motor babblingthat prove to make the exploration process faster and alleviate the illposedness of learning inverse kinematics based on our results we discuss the next steps in relation to infant studies what information will be necessary to further ground this computational model in behavioral data
support vector regression svr and its variants are widely used regression algorithms and they have demonstrated high generalization ability this research proposes a new svrbased regressor inlineformula texmath notationlatexv texmathinlineformulaminimum absolute deviation distribution regression inlineformula texmath notationlatexv texmathinlineformulamadr machine instead of merely minimizing structural risk as with inlineformula texmath notationlatexv texmathinlineformulasvr inlineformula texmath notationlatexv texmathinlineformulamadr aims to achieve better generalization performance by minimizing both the absolute regression deviation mean and the absolute regression deviation variance which takes into account the positive and negative values of the regression deviation of sample points for optimization we propose a dual coordinate descent dcd algorithm for small sample problems and we also propose an averaged stochastic gradient descent asgd algorithm for largescale problems furthermore we study the statistical property of inlineformula texmath notationlatexv texmathinlineformulamadr that leads to a bound on the expectation of error the experimental results on both artificial and real datasets indicate that our inlineformula texmath notationlatexv texmathinlineformulamadr has significant improvement in generalization performance with less training time compared to the widely used inlineformula texmath notationlatexv texmathinlineformulasvr lssvr inlineformula texmath notationlatexvarepsilon  texmathinlineformulatsvr and linear inlineformula texmath notationlatexvarepsilon  texmathinlineformulasvr finally we open source the code of inlineformula texmath notationlatexv texmathinlineformulamadr at urihttpsgithubcomasunayyvmadruri for wider dissemination
introduction while alzheimer’s disease ad is divided into severity stages mild cognitive impairment mci remains a solitary construct despite clinical and prognostic heterogeneity this study aimed to characterize differences in genetic cerebrospinal fluid csf neuroimaging and neuropsychological markers across clinicianderived mci stages methods vanderbilt memory  aging project participants with mci were categorized into 3 severity subtypes at screening based on neuropsychological assessment functional assessment and clinical dementia rating interview including mild n  18 75 ± 8 years moderate n  89 72 ± 7 years and severe subtypes n  18 78 ± 8 years at enrollment participants underwent neuropsychological testing 3t brain magnetic resonance imaging mri and optional fasting lumbar puncture to obtain csf neuropsychological testing and mri were repeated at 18months 3years and 5years with a mean followup time of 33 years ordinary least square regressions examined crosssectional associations between mci severity and apolipoprotein e apoeε4 status csf biomarkers of amyloid beta aβ phosphorylated tau total tau and synaptic dysfunction neurogranin baseline neuroimaging biomarkers and baseline neuropsychological performance longitudinal associations between baseline mci severity and neuroimaging and neuropsychological trajectory were assessed using linear mixed effects models with random intercepts and slopes and a followup time interaction analyses adjusted for baseline age sex raceethnicity education and intracranial volume for mri models results stages differed at baseline on apoeε4 status early  middle  late pvalues  003 and csf aβ early  middle  late phosphorylated and total tau early  middle  late pvalues  005 and neurogranin concentrations early  middle  late pvalues  005 mci stage related to greater longitudinal cognitive decline hippocampal atrophy and inferior lateral ventricle dilation early  late pvalues  003 discussion clinician staging of mci severity yielded longitudinal cognitive trajectory and structural neuroimaging differences in regions susceptible to ad neuropathology and neurodegeneration as expected participants with more severe mci symptoms at study entry had greater cognitive decline and gray matter atrophy over time differences are likely attributable to baseline differences in amyloidosis tau and synaptic dysfunction mci staging may provide insight into underlying pathology prognosis and therapeutic targets
cellular networks in some countries play important roles in providing various communication services planning implementing evaluating and optimizing are most common processes to maintain the networks it is interesting to discuss the gap between planning and implementation the difference between planning and implementation can lead to some problems for instance unequal traffic distribution on the other hand some research on traffic of cellular networks has been conducted and their results have also been published however it is only a few of them that dealt with technical optimization especially on smallclusterbased traffic therefore this study focuses on traffic distribution among cells in a cluster we use a correlation approach to measure the level of fluctuation similarities between a cell and its neighbours a study case based on real modified data was analyzed to show the relevancy of our method the result exhibits that it can be used for creating a rank of priority on how to share traffic of a cell in the cluster
text corpora annotated with languagerelated properties are an important resource for the development of language technology the current work contributes a new resource for chinese language technology and for chineseenglish translation in the form of a set of ted talks some originally given in english some in chinese that have been annotated with discourse relations in the style of the penn discourse treebank adapted to properties of chinese text that are not present in english the resource is currently unique in annotating discourselevel properties of planned spoken monologues rather than of written text an interannotator agreement study demonstrates that the annotation scheme is able to achieve highly reliable results
we consider the problem of cleaning a transboundary river proposed by ni and wang games econ behav 60176–186 2007 a river is modeled as a segment divided into subsegments each occupied by one region from upstream to downstream the waste is transferred from one region to the next at some rate since this transfer rate may be unknown the social planner could have uncertainty over each region’s responsibility two natural candidates to distribute the costs in this setting would be the method that assigns to each region its expected responsibility and the one that assigns to each region its median responsibility we show that the latter is equivalent to the upstream responsibility method alcaldeunzu et al in games econ behav 90134–150 2015 and the former is a new method that we call expected responsibility we compare both solutions and analyze them in terms of a new property of monotonicity
existing shadow detection methods suffer from an intrinsic limitation in relying on limited labeled datasets and they may produce poor results in some complicated situations to boost the shadow detection performance this paper presents a multitask mean teacher model for semisupervised shadow detection by leveraging unlabeled data and exploring the learning of multiple information of shadows simultaneously to be specific we first build a multitask baseline model to simultaneously detect shadow regions shadow edges and shadow count by leveraging their complementary information and assign this baseline model to the student and teacher network after that we encourage the predictions of the three tasks from the student and teacher networks to be consistent for computing a consistency loss on unlabeled data which is then added to the supervised loss on the labeled data from the predictions of the multitask baseline model experimental results on three widelyused benchmark datasets show that our method consistently outperforms all the compared stateof theart methods which verifies that the proposed network can effectively leverage additional unlabeled data to boost the shadow detection performance
bilateral cerebral peduncular infarction bcpi is a very rare disorder among stroke patients the main clinical manifestations in the previously reported bcpi case reports was associated with lockedin syndrome or persistent vegetative state here we present a 51yearold woman who had pseudobulbar palsy and quadriplegia magnetic resonance imaging showed an acute infarction in the middle areas of the cerebral peduncle with a unique “mickey mouse ears” sign diffusion tensor imaging and tractography showed relatively preserved corticospinal tracts but the corticobulbar tracts were not detected magnetic resonance angiography showed posterior cerebral artery and vertebrobasilar artery occlusion cerebral perfusion insufficiency due to stenosis or occlusion of the vertebrobasilar artery and its branches may lead to bcpi the prognosis and clinical manifestations of bcpi are related to the extent of the infarction in the involved cerebral peduncle and whether other territories are involved isolated bcpi may present a severe pseudobulbar palsy with relatively preserved limb function depending on the involvement pattern
 computed tomography ct takes xray measurements on the subjects to reconstruct tomographic images as xray is radioactive it is desirable to control the total amount of dose of xray for safety concerns therefore we can only select a limited number of measurement angles and assign each of them limited amount of dose traditional methods such as compressed sensing usually randomly select the angles and equally distribute the allowed dose on them in most ct reconstruction models the emphasize is on designing effective image representations while much less emphasize is on improving the scanning strategy the simple scanning strategy of random angle selection and equal dose distribution performs well in general but they may not be ideal for each individual subject it is more desirable to design a personalized scanning strategy for each subject to obtain better reconstruction result in this paper we propose to use reinforcement learning rl to learn a personalized scanning policy to select the angles and the dose at each chosen angle for each individual subject we first formulate the ct scanning process as an markov decision process mdp and then use modern deep rl methods to solve it the learned personalized scanning strategy not only leads to better reconstruction results but also shows strong generalization to be combined with different reconstruction algorithms
high resolution synthetic aperture sonar sas is limited by some factors such as size of the sonar carrier carrier speed underwater sound velocity and sound propagation characteristics so obviously the mapping efficiency of sas is less than that of synthetic aperture radar sar in order to break through the problem of low mapping efficiency this paper proposes a method of sampling sparse in azimuth direction to obtain higher operation efficiency the signal is transformed into sas imaging by stripe along the range direction to solve the effect of range migration caused by the motion of carrier and the scattering coefficient is reconstructed by one of the compressed sensing cs method called best orthogonal matching pursuit omp algorithm in order to obtain the final sas imaging the simulation and experimental data imaging results show that the method can still realize the imaging without ambiguity under the condition of the enormous down sampling along the azimuth direction it solves the problem of the grating lobe effect caused by sparse sampling improves the mapping efficiency greatly and has strong effectiveness and practicability
searching for desired 3d models is not easy because many of them are not well labeled annotations often contain inconsistent information eg uploaders personal way of naming and lack important details eg detailed ornaments and pattern of each model we introduce polysquare a search engine for 3d models based on tag propagationthe process of assigning existing tags to other similar but unlabeled models considering important local properties for instance a tag wheel of a wheelchair can be spread out to other objects with wheels furthermore polysquare allows people to interactively refine the search results by iteratively including desired shapes and excluding unwanted ones we evaluate the performance of tag propagation by measuring the precisionrecall of propagation results with various similarity thresholds and demonstrate the effectiveness of the use of local features we also showcase how polysquare handles the unrefined tags through a case study using real 3d model data from google poly
failure prediction is an important aspect of selfaware computing systems therefore a multitude of different approaches has been proposed in the literature over the past few years in this work we propose a taxonomy for organizing works focusing on the prediction of service level objective slo failures our taxonomy classifies related work along the dimensions of the prediction target eg anomaly detection performance prediction or failure prediction the time horizon eg detection or prediction online or offline application and the applied modeling type eg time series forecasting machine learning or queueing theory the classification is derived based on a systematic mapping of relevant papers in the area additionally we give an overview of different techniques in each subgroup and address remaining challenges in order to guide future research
"twelve years after the advent of moocs the university of the aegean greece implemented its first mooc on “violence and bullying in schools” in which about 2000 people showed interest in attending eventually 1309 people started it and 1050 8021 completed it successfully achieving high performance the present work which is part of the doctoral research of the first researcher outlines the participation of the learners in the program and the obstacles they encountered during it while identifying the reasons for its high completion rate with high performance the results showed that mainly the quality of the instructional material the instructional design of the program and its organization as well as the timely support provided to learners contributed significantly to the successful completion of the program achieving high performance these findings can be considered by future mooc program designers in order to design and implement programs that meet the requirements and facilitate the participation of those who attend 
 
δώδeκα χρόνια μeτά την eμφάνιση των moocs το πανeπιστήμιο αιγαίου υλοποίησe το πρώτο του mooc μe θέμα την eνδοσχολική βία και τον eκφοβισμό στο οποίο eκδήλωσαν eνδιαφέρον για να το παρακολουθήσουν πeρίπου 2000 άτομα τeλικά το ξeκίνησαν 1309 άτομα και το ολοκλήρωσαν eπιτυχώς 1050 8021 πeτυχαίνοντας υψηλές eπιδόσeις η παρούσα eργασία που αποτeλeί τμήμα της διδακτορικής έρeυνας του πρώτου eρeυνητή σκιαγραφeί τη συμμeτοχή των eκπαιδeυομένων στο πρόγραμμα και τα eμπόδια που αντιμeτώπισαν κατά τη διάρκeιά του eνώ eντοπίζeι τους λόγους του υψηλού ποσοστού ολοκλήρωσης του μe υψηλές eπιδόσeις τα αποτeλέσματα έδeιξαν ότι κυρίως η ποιότητα του eκπαιδeυτικού υλικού ο eκπαιδeυτικός σχeδιασμός του προγράμματος και η οργάνωσή του καθώς και η έγκαιρη υποστήριξη που παρeχόταν στους eκπαιδeυόμeνους συνέβαλαν σημαντικά στην eπίτeυξη των συγκeκριμένων αποτeλeσμάτων τα eυρήματα αυτά μπορούν να ληφθούν υπόψη από τους σχeδιαστές μeλλοντικών προγραμμάτων moocs ώστe να σχeδιάζουν και να υλοποιούν προγράμματα που θα ικανοποιούν τις απαιτήσeις και θα διeυκολύνουν τη συμμeτοχή όσων τα παρακολουθούν 
 
 article visualizations"
objective safe surgery requires the accurate discrimination of tissue intraoperatively we assess the feasibility of using multispectral imaging and deep learning to enhance surgical vision by automated identification of normal human head and neck tissues study design construction and feasibility testing of novel multispectral imaging system for surgery setting academic university hospital subjects and methods multispectral images of freshpreserved human cadaveric tissues were captured with our adapted digital operating microscope eleven tissue types were sampled each sequentially exposed to 6 lighting conditions two convolutional neural network machine learning models were developed to classify tissues based on multispectral and whitelight color images arrinetm and arrinetw respectively blinded otolaryngology residents were asked to identify tissue specimens from whitelight color images and their performance was compared with that of the arrinet models results a novel multispectral imaging system was developed with minimal adaptation to an existing digital operating microscope with 818 accuracy in tissue identification of fullsize images the multispectral arrinetm classifier outperformed the whitelightonly arrinetw model 455 and surgical residents 697 challenges with discrimination occurred with parotid vs fat and blood vessels vs nerve conclusions a deep learning model using multispectral imaging outperformed a similar model and surgical residents using traditional whitelight imaging at the task of classifying normal human head and neck tissue ex vivo these results suggest that multispectral imaging can enhance surgical vision and augment surgeons’ ability to identify tissues during a procedure
firstperson video naturally brings the use of a physical environment to the forefront since it shows the camera wearer interacting fluidly in a space based on his intentions however current methods largely separate the observed actions from the persistent space itself we introduce a model for environment affordances that is learned directly from egocentric video the main idea is to gain a humancentric model of a physical space such as a kitchen that captures 1 the primary spatial zones of interaction and 2 the likely activities they support our approach decomposes a space into a topological map derived from firstperson activity organizing an egovideo into a series of visits to the different zones further we show how to link zones across multiple related environments eg from videos of multiple kitchens to obtain a consolidated representation of environment functionality on epickitchens and egtea we demonstrate our approach for learning scene affordances and anticipating future actions in longform video
in this paper we obtain selfsimilarity solutions for a onephase onedimensional fractional space stefan problem in terms of the three parametric mittagleffler function eαmlzdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentealpha mlzenddocument we consider dirichlet and neumann conditions at the fixed face involving caputo fractional space derivatives of order 0α1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocument0alpha 1enddocument we recover the solution for the classical onephase stefan problem when the order of the caputo derivatives approaches one
abstract in this article we propose an attrivar chart to control the process mean with the attrivar chart the sampling is performed in two stages collecting attribute and variable sample data from the same sample attribute plus variable data – attrivar that is if the first m items of the sample fail to pass the go gauge test or they pass the nogo gauge test the sampling moves on to stage two where the quality characteristic x of the first m and the remaining nm items of the sample is measured otherwise the sampling is interrupted and the process is declared to be in control the number of tested items if one or two or as many as m is only known after the completion of the first stage at the second stage the value is computed and used to decide the state of the process it is worthwhile to stress that the gonogo gauge test truncates the x distribution and because of that the mathematical development to obtain the distribution is not trivial the attrivar chart signals faster than the double sampling chart and more important than that it is simpler to use because the gonogo gauge test reduces the frequency with which the quality characteristic x of the sample items is measured the attrivar chart is also faster and simpler than the mixed chart with the mixed chart the sampling is also performed in two stages the difference is that all items of the sample are always submitted to the gonogo gauge test before deciding to go to stage two where the value is computed
abstract tissue engineering te seeks to fabricate implants that mimic the mechanical strength structure and composition of native tissues cartilage te requires the development of functional personalized implants with cartilage‐like mechanical properties capable of sustaining high load‐bearing environments to integrate into the surrounding tissue of the cartilage defect in this study we evaluated the novel 14‐butanediol thermoplastic polyurethane elastomer b‐tpue derivative filament as a 3d bioprinting material with application in cartilage te the mechanical behavior of b‐tpue in terms of friction and elasticity were examined and compared with human articular cartilage pcl and pla moreover infrapatellar fat pad‐derived human mesenchymal stem cells mscs were bioprinted together with scaffolds in vitro cytotoxicity proliferative potential cell viability and chondrogenic differentiation were analyzed by alamar blue assay sem confocal microscopy and rt‐qpcr moreover in vivo biocompatibility and host integration were analyzed b‐tpue demonstrated a much closer compression and shear behavior to native cartilage than pcl and pla as well as closer tribological properties to cartilage moreover b‐tpue bioprinted scaffolds were able to maintain proper proliferative potential cell viability and supported mscs chondrogenesis finally in vivo studies revealed no toxic effects 21 days after scaffolds implantation extracellular matrix deposition and integration within the surrounding tissue this is the first study that validates the biocompatibility of b‐tpue for 3d bioprinting our findings indicate that this biomaterial can be exploited for the automated biofabrication of artificial tissues with tailorable mechanical properties including the great potential for cartilage te applications
to enhance robustness of distributed model predictive control dmpc and guarantee closedloop recursive feasibility and convergence for systems suffering massive uncertainties this work is dedicated to proposing a robust cooperative dmpc formulation for discretetime linear systems subject to unknownbutbounded disturbances a robust constraint tightening technique is developed based on setmembership approach where constraints on system states and inputs are tightened iteratively along the prediction horizon with local control gains with this formulation we discuss recursive feasibility and convergence of the closedloop control system finally we show simulation results with a massspringdamper mechanical system to illustrate the effectiveness of the proposed control strategy
"
 sittostand sts motion is a key determinant of functional independence for the senior people this paper extends a predictive dynamics formulation previously reported to predict the assisted sts motion ie the motion with a mechanical assistance unilateral grabrail bar which is placed on the right side of the virtualindividuals with a vertical orientation the formulation is able to predict kinetics and kinematics not only in the sagittal plane but also in frontal and transverse planes two different objective functions are tested the first one is the dynamic effort and the second one is the dynamic effort plus the difference between right and left side support reaction forces srfs results show that sagittal plane kinematics and kinetics are not affected by the introduction of the grabrail bar whereas some significant differences are seen in the mediallateral and anteriorposterior components of kinematics and kinetics the healthy elderly group places a priority to the stability during an assisted sts task the placement of the grabrail bar on the right side results in a significant decrease in the left knee joint torque results in this study are consistent with those reported from the literature"
the uplink of a reconfigurable intelligent surfaces risaided cellfree massive multipleinput multipleoutput mimo system is analyzed where the channel state information csi is estimated using uplink pilots first we derive analytical expressions for the achievable rate of the system with zero forcing zf receiver taking into account the effects of pilot contamination channel estimation error and the distributed riss the maxmin rate optimization problem is considered with peruser power constraints to solve this nonconvex problem we propose to decouple the original optimization problem into two subproblems namely phase shift design problem and power allocation problem the power allocation problem is solved using a standard geometric programming gp whereas a semidefinite programming sdp is utilized to design the phase shifts moreover the taylor series approximation is used to convert the nonconvex constraints into a convex form an iterative algorithm is proposed whereby at each iteration one of the subproblems is solved while the other design variable is fixed the maxmin user rate of the risaided cellfree massive mimo system is compared to that of conventional cellfree massive mimo numerical results indicate the superiority of the proposed algorithm compared with a conventional cellfree massive mimo system finally the convergence of the proposed algorithm is investigated
the purpose of the study was to identify key structures associated with leisure activity participation and develop a valid and reliable scale to measure leisure activity participation the dimensions related to leisure activity participation were determined as a result of a literature review and focus group research data from two different samples were interpreted by exploratory factor analysis n  243 and confirmatory factor analysis n  336 analysis results revealed eight dimensions related to leisure activity participation relaxing developmental socializing activity with an attractive environment productive esthetic entertaining and exciting activity the validity of the scale was evaluated by content convergent and discriminant validity tests internal consistency and stability were tested through cronbach’s alpha and pearson correlation coefficients it was concluded that a valid and reliable measuring instrument had been developed
this paper investigates secure transmission in a wireless powered cooperative communication network wpccn over nakagamim fading channels where multiple intermediate energy harvesting eh nodes with finite energy storage are deployed to assist the secure transmission between source and destination in the presence of an eavesdropper specifically the intermediate eh nodes first harvest and accumulate energy from the sources signal and then forward the confidential signal or emit jamming signal to interfere the eavesdropper according to actual requirements in order to exploit the intermediate eh nodes for secure improvement considering both energy storage status and channel gains three relayandjammer selection schemes at the intermediate nodes are proposed namely energy threshold based bestrelay and randomjammer etbr energy threshold based randomrelay and bestjammer etrb and energy threshold based bestrelay and bestjammer etbb for understanding the impact of critical parameters on the secrecy performance we first capture the evolution of the eh nodes’ storages using finitestate markov chain fsmc and then derive the analytical expressions for hybrid outage probability hop and secrecy throughput st furthermore numerical results demonstrate that our proposed selection schemes outperform the existing schemes brrj rrbj brbj due to the consideration of energy accumulation and storage status in addition there is a suitable value of the energy threshold for each proposed scheme to achieve the optimal secrecy performance
limited availability of annotated medical imaging data poses a challenge for deep learning algorithms although transfer learning minimizes this hurdle in general knowledge transfer across disparate domains is shown to be less effective on the other hand smaller architectures were found to be more compelling in learning better features consequently we propose a lightweight architecture that uses mixed asymmetric kernels maknet to reduce the number of parameters significantly additionally we train the proposed architecture using semisupervised learning to provide pseudolabels for a large medical dataset to assist with transfer learning the proposed maknet provides better classification performance with fewer parameters than popular architectures experimental results also highlight the importance of domainspecific knowledge for effective transfer learning additionally we interrogate the proposed network with integrated gradients and perturbation methods to establish the superior quality of the learned features
hourly sea surface temperature sst retrieved from himawari8 by the japan aerospace exploration agency h8sstjaxa latest version 12 is becoming an important data source for data merging as well as for resolving diurnal variation dv however the spatial and temporal variation of the errors for the full disk is still unclear in this article two years of h8sstsjaxa are validated against in situ measurements from iquam2 in general h8sstsjaxa shows small biases between −011 and −003 k with root mean square error rmse between 058 and 073 k the spatial distributions of the errors reveal the following patterns 1 a small median bias close to 01 k and rmse of 04–06 k comparing to drifters are found at satellite zenith angle sza 0°–35° 2 negative biases ∼−03 k are detected at sza s 35°–58° and 3 larger positive biases exceeding 03 k are also found along the viewing boundaries the temporal variations of the errors show that 1 there is no prominent seasonal variation 2 the amplitude of the dv of the errors is only ∼01 k for the statistical of all matchups and 3 the maximum errors appears in the morning rather than in the noon the statistics will be used in future work for dv analysis and merging purposes
we propose a method for the automatic segmentation of 3d objects into parts which can be individually 3d printed and then reassembled by preserving the visual quality of the final object our technique focuses on minimizing the surface affected by supports decomposing the object into multiple parts whose printing orientation is automatically chosen the segmentation reduces the visual impact on the fabricated model producing non‐planar cuts that adapt to the object shape this is performed by solving an optimization problem that balances the effects of supports and cuts while trying to place both in occluded regions of the object surface to assess the practical impact of the solution we show a number of segmented 3d printed and reassembled objects
as the global climate warms the fate of lacustrine fish is of huge concern especially given their sensitivity as ectotherms to changes in water temperature the arctic charr salvelinus alpinus l is a salmonid with a holarctic distribution with peripheral populations persisting at temperate latitudes where it is found only in sufficiently cold deep lakes thus warmer temperatures in these habitats particularly during early life stages could have catastrophic consequences on population dynamics here we combined lake temperature observations a 1d hydrodynamic model and a multidecadal climate reanalysis to show coherence in warming winter water temperatures in four european charr lakes near the southernmost limit of the species’ distribution current maximum and mean winter temperatures are on average  1 °c warmer compared to early the 1980s and temperatures of 85 °c adverse for high charr egg survival have frequently been exceeded in recent winters simulations of winter lake temperatures toward centuryend showed that these warming trends will continue with further increases of 3–4 °c projected an additional 324 total accumulated degreedays during winter is projected on average across lakes which could impair egg quality and viability we suggest that the perpetuating winter warming trends shown here will imperil the future status of these lakes as charr refugia and generally do not augur well for the fate of coldwateradapted lake fish in a warming climate
the creation of single photon sources on a chip is a midterm milestone on the road to chipscale quantum computing an indepth understanding of the extended multipole decomposition of nonisolated sources of electromagnetic radiation is not only relevant for a microscopic description of fundamental phenomena such as light propagation in a medium but also for emerging applications such as singlephoton sources to design single photon emitters on a chip we consider a ridge dielectric waveguide perturbed with a cylindrical inclusion for this we expanded classical multipole decomposition that allows simplifying and interpreting complex optical interactions in an intuitive manner to make it suitable for analyzing lightmatter interactions with nonisolated objects that are parts of a larger network eg individual components such as a single photon source of an optical chip it is shown that our formalism can be used to design single photon sources on a chip
in this twopart article we develop a unifying framework for the analysis of the feasibility of the power flow equations for dc power grids with constantpower loads in part i of this article we present a detailed introduction to the problem of power flow feasibility of such power grids and the associated problem of selecting a desirable operating point which satisfies the power flow equations we introduce and identify all longterm voltage semistable operating points and show that there exists a onetoone correspondence between such operating points and the constant power demands for which the power flow equations are feasible such operating points can be found by solving an initial value problem and a parametrization of these operating points is obtained in addition we give a full characterization of the set of all feasible power demands and give a novel proof for the convexity of this set
we perform the largeeddy simulation of the flow past a helicopter rotor to support the investigation of rotorcraft wake characteristics and decay mechanisms a hybrid lagrangian–eulerian vortex particle–mesh method is employed to simulate the wake development with the blades modeled using immersed lifting lines the validity of the numerical approach is first evaluated through a comparison of the rotor trim parameters with experimental results then the rotor wake at low medium and high advance ratios is simulated up to 30 rotor diameters the wake generation and rollup are described i qualitatively using rotor polar plots and threedimensional 3d vortex dynamics visualizations and ii quantitatively using classical integral diagnostics in cross sections the highly 3d unsteady near wake transitions to a system dominated by two parallel vortices over a distance that depends on the advance ratio this process is accelerated by the multiple interactions between successive tip vortices supporting the generation of selfinduced turbulence and uncovering a mechanism of vorticity alignment along the streamwise axis the vortices in the far wake are compared to typical aircraft ones and exhibit less compact cores and faster decaying energy finally we illustrate the loss of time periodicity in the far wake using the power spectral density of the kinetic energy and the backscattering of energy from high rotor harmonics to lower frequencies as complementary evidence of the intense vortex interaction activity
prospective longitudinal data collection is an important way for researchers and evaluators to assess change in schoolbased settings for lowrisk andor likelybeneficial interventions or surveys data quality and ethical standards are both arguably stronger when using a waiver of parental consent—but doing so often requires the use of anonymous data collection methods the standard solution to this problem has been the use of a selfgenerated identification code however such codes often incorporate personalized elements eg birth month middle initial that even when meeting the technical standard for anonymity may raise concerns among both youth participants and their parents potentially altering willingness to participate response quality or generating outrage there may be value therefore in developing a selfgenerated identification code and matching approach that not only is technically anonymous but also appears anonymous to a researchnaive individual this article provides a proof of concept for a novel matching approach for schoolbased longitudinal data collection that potentially accomplishes this goal
abstract positive online reviews elicit more positive consumer behaviors than negative reviews in the hospitality industry however several variables that influence information processing may affect this wellestablished link the present study focuses on the moderating roles of brand familiarity and cultural values as their influence remains unexplained in the previous literature to better understand the moderating influence of brand familiarity and cultural values on the relationship between positivenegative online reviews and booking intentions a 2 positive vs negative x 2 familiar vs unfamiliar brand experimental research design was conducted in two countries spain and mexico that differ mainly in the “indulgence versus restraint” cultural dimension the results suggest that in general positivenegative online reviews generate higherlower booking intentions for less familiar than for familiar hotels in addition culture moderates the effect of familiarity this interaction effect is more relevant in restrained cultures theoretical and managerial implications based on these results are discussed
although new deep learning approaches have recently been introduced for leaf disease identification existing deep learning models such as vgg and resnet have been used previously therefore a new deep learning architecture is proposed to consider the leaf spot attention mechanism the primary idea is that leaf disease symptoms appear in the leaf area whereas the background region does not contain any useful information regarding leaf diseases to realize this two subnetworks are designed the first is a feature segmentation subnetwork to provide more discriminative features for the separated background leaf areas and spot areas in the feature map the other is a spotaware classification subnetwork to increase the classification accuracy to train the proposed leaf spot attention network the feature segmentation subnetwork is first learned with a new image set where the background leaf area and spot area are annotated subsequently the spotaware classification subnetwork is connected to the feature segmentation subnetwork and then trained through early and later fusions to produce the semanticlevel spot feature information the experimental results confirm that the proposed network can increase the discriminative power by modeling the leaf spot attention mechanism the results prove that the proposed method outperforms conventional stateoftheart deep learning models
artificial intelligence ai has seen numerous applications in the area of education through the use of educational technologies such as intelligent tutoring systems its learning possibilities have increased significantly one of the main challenges for the widespread use of its is the ability to automatically generate questions bearing in mind that the act of questioning has been shown to improve the students learning outcomes automatic question generation aqg has proven to be one of the most important applications for optimizing this process we present a tool for generating factual questions in portuguese by proposing three distinct approaches the first one performs a syntaxbased analysis of a given text by using the information obtained from partofspeech tagging pos and named entity recognition ner the second approach carries out a semantic analysis of the sentences through semantic role labeling srl the last method extracts the inherent dependencies within sentences using dependency parsing all of these methods are possible thanks to natural language processing nlp techniques for evaluation we have elaborated a pilot test that was answered by portuguese teachers the results verify the potential of these different approaches opening up the possibility to use them in a teaching environment
hal is a multidisciplinary open access archive for the deposit and dissemination of scientific research documents whether they are published or not the documents may come from teaching and research institutions in france or abroad or from public or private research centers l’archive ouverte pluridisciplinaire hal est destinée au dépôt et à la diffusion de documents scientifiques de niveau recherche publiés ou non émanant des établissements d’enseignement et de recherche français ou étrangers des laboratoires publics ou privés application of machine learning tools for the improvement of reactive extrusion simulation fanny castéran ruben ibanez clara argerich karim delage francisco chinesta philippe cassagnau
"purpose with the embedment of artificial intelligence ai technology into the accounting software accounting function entered another phase of evolution in terms of the working method by integrating the aibased accounting software into the operational tasks the functions of account payables and account receivables become easy and efficient despite the tremendous advantages of ai the adoption of ai in malaysia is still low relatively compare to other countries furthermore there are limited academic research performed to study the impact of ai adoption on the organisations especially from the accounting perspective therefore this study aims to investigate the use and impact of aibased accounting software in organizations 
methodology faceto face indepth semi structure interviews were conducted among nine organizations that are using ai based accounting software   the representatives who participated in the interviews were from the accounting service division of the respective organisation constant comparative method was used to analyse the data collected 
finding the study found the usage of the aibased accounting software has proved to have significant impact on the organisations in terms of increased productivity improved efficiency improved customer service flexible working style process governance and manpower saving 
originality ai has emerged as a disruption technology in the era of industry 40 this research has studied on the implication of ai adoption in the accounting function from the result of this study it has evidenced that adoption of aibased accounting software in account payable function has yielded positive impact to the organisations the finding will to spur the confidence of the organisations in malaysia to invest in the ai technology"
multidimensional imaging capturing image data in more than two dimensions has been an emerging field with diverse applications due to the limitation of twodimensional detectors in obtaining the highdimensional image data computational imaging approaches have been developed to pass on some of the burden to a reconstruction algorithm in various image reconstruction problems in multidimensional imaging the measurements are in the form of superimposed convolutions in this paper we introduce a general framework for the solution of these problems called here convolutional inverse problems and develop fast image reconstruction algorithms with analysis and synthesis priors these include sparsifying transforms as well as convolutional or patchbased dictionaries that can adapt to correlations in different dimensions the resulting optimization problems are solved via alternating direction method of multipliers with closedform efficient and parallelizable update steps to illustrate their utility and versatility the developed algorithms are applied to threedimensional image reconstruction problems in computational spectral imaging for cases with or without correlation along the third dimension as the advent of multidimensional imaging modalities expands to perform sophisticated tasks these algorithms are essential for fast iterative reconstruction in various largescale problems
localization is one of the key techniques in wireless sensor network while the global positioning system gps is one of the most popular positioning technologies the weakness of high cost and energy consuming makes it difficult to install in every node in order to reduce the cost and energy consumption only a few nodes called beacon nodes are equipped with gps modules the remaining nodes obtain their locations through localization in order to find the minimum positions of beacons a resolving set with minimal cardinality has been obtained in the network which is called metric basis simultaneous local metric basis of the network is also given in which each pair of adjacent vertices of the network is distinguished by some element of simultaneous local metric basis which makes the network design more reasonable in this paper a new network the generalized möbius laddermmn has been introduced and its metric dimension and simultaneous local metric dimension of its two subfamilies have been calculated
to cope with the lack of highly skilled professionals machine learning with proper signal processing is key for establishing automated diagnosticaid technologies with which to conduct epileptic electroencephalogram eeg testing in particular frequency filtering with the appropriate passbands is essential for enhancing the biomarkers—such as epileptic spike waves—that are noted in the eeg this paper introduces a novel class of neural networks nns that have a bank of linearphase finite impulse response filters at the first layer as a preprocessor that can behave as bandpass filters that extract biomarkers without destroying waveforms because of a linearphase condition besides the parameters of the filters are also datadriven the proposed nns were trained with a large amount of clinical eeg data including 15833 epileptic spike waveforms recorded from 50 patients and their labels were annotated by specialists in the experiments we compared three scenarios for the first layer no preprocessing discrete wavelet transform and the proposed datadriven filters the experimental results show that the trained datadriven filter bank with supervised learning behaves like multiple bandpass filters in particular the trained filter passed a frequency band of approximately 10–30 hz moreover the proposed method detected epileptic spikes with the area under the receiver operating characteristic curve of 0967 in the mean of 50 intersubject validations
this paper considers a cooperative nonorthogonal multiple access ie cnoma system consisting of a base station bs and two downlink users ie a near user and a far user we derive analytical expression for the outage probabilities experienced by both the users and the system outage probability of the cnoma network in lognormal fading channels we perform extensive analytical and simulation studies to determine the impact of various parameters on the outage performance we determine numerical values for the power allocation coefficient at the bs that leads to equal outage probabilities for the users further we determine numerical values for the power allocation coefficient at the bs that ensures system outage minimization and validate results using particle swam optimization method
the increased occurrence of forest fires is a threat to the conservation of the cerrado biome thus protected areas such as conservation units uc are fundamental for its preservation however even these areas are affected by fires that compromise their integrity this study aimed to draw up a fire risk zoning scheme for the aguas emendadas ecological station conservation unit validate it with the scars of fires that occurred between 2011 and 2015 and evaluate the existence of spatial risk to wildfires no part of the uc was classified as an extreme risk zone however over 60 of the uc was classified as areas of high or very high fire risk the validation showed that 80 of fires occurred in medium risk zones such as savannah or grassland moran’s global index gave a forest fire risk value of 083 indicating a high spatial autocorrelation and the moran local index identified the wildland—urban interface areas as the most critical regions
the everrising computation demand is forcing the move from the cpu to heterogeneous specialized hardware which is readily available across modern datacenters through disaggregated infrastructure on the other hand trusted execution environments tees one of the most promising recent developments in hardware security can only protect code confined in the cpu limiting tees’ potential and applicability to a handful of applications we observe that the tees’ hardware trusted computing base tcb is fixed at design time which in practice leads to using untrusted software to employ peripherals in tees based on this observation we propose composite enclaves with a configurable hardware and software tcb allowing enclaves access to multiple computing and io resources finally we present two case studies of composite enclaves i an fpga platform based on riscv keystone connected to emulated peripherals and sensors and ii a largescale accelerator these case studies showcase a flexible but small tcb 25 kloc for io peripherals and drivers with a lowperformance overhead only around 220 additional cycles for a context switch thus demonstrating the feasibility of our approach and showing that it can work with a wide range of specialized hardware
due to the rising demand for thermal management technologies within the electronics industry there has been an increased emphasis on developing new thermal interface materials tims with enhanced performance despite bi‐based alloys having exhibited promising potential as tims in microelectronics cooling applications understanding the microstructure evolution of these alloys and the consequent effects on the resulting properties still remains an essential task to be accomplished herein a directional solidification technique is used to investigate microstructural features and vickers microhardness of bi5–20 wt sb alloys with a focus on the roles of alloy composition solidification thermal parameters and macrosegregation the results show the formation of aligned bi‐rich dendrites in a broad range of cooling rates from about 02 to 25 °c s−1 in contrast as the alloy sb content increases two morphological transitions in the bi‐rich matrix are shown to occur as follows trigonal pattern → irregular shape → trigonal pattern then primary and secondary dendritic arm spacings are related to growth and cooling rates through experimental equations finally the occurrence of macrosegregation along the length of the bi–20 wt sb alloy casting is shown to be a key factor associated with the variation of microhardness
"aims
this paper synthesizes current global evidenceinformed guidance that supports nurses and midwives to recognize and respond to intimate partner violence ipv and how these practices can be translated from facetoface encounters to care that is delivered through telehealth


background
covid19 related social and physical distancing measures increase the risk for individuals who are socially isolated with partners who perpetuate violence providing support through telehealth is one strategy that can mitigate the pandemic of ipv while helping patients and providers stay safe from covid19


design and methods
in this discursive paper we describe how practical guidance for safely recognizing and responding to ipv in telehealth encounters was developed the adaptitt assessment decisions administration production topical experts integration testing training framework was used to guide the novel identification and adaptation of evidenceinformed we focused on the first six stages of the adaptitt framework


conclusions
this paper fills a gap in available guidance specifically for ipv recognition and response via telehealth we present strategies for prioritizing safety and promoting privacy while initiating managing or terminating a telehealth encounter with patients who may be at risk for or experiencing ipv strategies for assessment planning and intervention are also summarized system level responses such as increasing equitable access to telecommunication technology are also discussed


relevance to clinical practice
integrating innovative ipvfocused practices into telehealth care is an important opportunity for nurses and midwives during the current global covid19 pandemic there are also implications for future secondary outbreaks natural disasters or other physically isolating events for improving healthcare efficiency and for addressing the needs of vulnerable populations with limited access to healthcare"
security of the data in the internet is very important everything we use in our life is digitalized every data crawling on the internet is not safe so cryptography is the encryption technique to secure the data most importantly video is a huge data type but encryption of the video data is little more difficult so video is extracted into rgb frames then rijndael algorithm is used for encrypting the rgb frames finally the encrypted rgb frames recreate into video again the reverse process is used for decryption as well time is very important to our busy world so the encryption speed is compared between the existing and the proposed technique
"
 metal foams have been widely used in many fields requiring excellent heat and mass transfer performance such as heat exchangers and catalytic reactors however the movements of gas–solid interfacial heat transfer characteristic curve with the structural parameters of foams for uncoated metal foams and the washcoat thickness for coated metal foams are not explained in depth in this work gas–solid interfacial heat transfer characteristics of metal foams without and with a washcoat were studied in detail in both laminar and turbulent flows using the bodycenteredcubic bcc unit cell model by the method of computational fluid dynamics considering that the structural parameters of uncoatedcoated foams could be accurately controlled in the numerical method the movements of gas–solid interfacial heat transfer characteristic curve with the structural parameters of foams and the washcoat thickness were successfully verified and explained using the numerical data in both laminar and turbulent flows the results show that the porosity not the porecell diameter is the reason of the moving of gas–solid interfacial heat transfer characteristic curve for uncoatedcoated foams in laminar flow the porosity influences interfacial heat transfer characteristic curve through the effective thermal conduction of fluid phase and in turbulent flow interfacial heat transfer characteristic curve is affected by porosity through the inertial effect of flow a new correlation of gas–solid interfacial heat transfer coefficient for uncoatedcoated metal foams suitable for both laminar and turbulent flows was proposed by taking into consideration this phenomenon"
abstract in this study a logistic regression model is applied to credit scoring data from a given portuguese financial institution to evaluate the default risk of consumer loans it was found that the risk of default increases with the loan spread loan term and age of the customer but decreases if the customer owns more credit cards clients receiving the salary in the same banking institution of the loan have less chances of default than clients receiving their salary in another institution we also found that clients in the lowest income tax echelon have more propensity to default the model predicted default correctly in 8979 of the cases
we suggest the methodology to predict the distribution of threshold voltage vt shift caused by random telegraph noise rtn polysilicon channels were randomized with a single trap and the neural network was modeled to predict rtn trapinduced vt fluctuation in 3d nand flash memory 3d technology computeraided design tcad simulations were performed in a unit cell to calculate the vt shift in a 3d vertical channel finally we extract the distribution of mathbfvmathbft fluctuation using machine learning
measuring of the center of pressure cop is one of the most frequently used quantitative methods for quantifying postural performance the aim of the study is to describe differentiation criteria in the coptrack for the clinical picture of chronic unspecific back pain in this study dynamic models loaded with multivariable controls are used to determine whether biomechanical questions for upright posture can be answered these models are particularly well suited for investigating the kinematics and the influence of the influencing disturbance variables these investigations are extended by power density spectrum psd analyses of cop measurements on 590 subjects with and without chronic nonspecific back pain pain patients show an average of 05 nm2 more area under the spectrum than the painfree reference group in the power density spectrum different frequency ranges can be assigned to specific body oscillation among others the frequency range between 05–08 hz corresponds to the hip movement in the range around 02 hz the movements are reflected in the upper body patients with back pain experience less activity in certain individual areas
"the interaction and competition between magmatic and tectonic processes mostly control the spatial distribution and morphology of monogenetic volcanoes the central anatolian volcanic province cavp situated in a strikeslip environment provides a remarkable opportunity to understand this relation in this study we defined six monogenetic volcanic fields within the cavp and analyzed a total number of 540 monogenetic volcanoes in terms of morphological and spatial characteristics the morphological characteristics favour the dominant role of magmatic eruptions over the phreatomagmatic ones supported by the types of monogenetic volcanoes the flank slopes are probably the best morphometric parameters that display a correlation with the ages and hence its usage in the relativedating studies might be promoted the spatial distribution of the vents in the cavp shows a selfsimilar fractal clustering that obeys the powerlaw distribution defined over a range of lower lco and upper uco cutoff distances the computed fractal dimensions df of the six monogenetic volcanic fields vary in the range of 116 to 180 possibly due to the slight variation in the crustal thickness and fracture distribution uco values interpreted as the initial depth of dike intrusions are wellcorrelated with the local tectonics and vary from north 85 to 12 km to south 16 km 
both clustered and nonclustered vent distributions are observed in the cavp according to the poisson nearest neighbor analysis the former case indicates the vents formed by a single centralized plumbing system eg erciyes volcanic complex evc while the latter refers to the formation of vents through the independent shallow or deep magma reservoirs eg acigol volcanic complex the preexisting fractures and the changes in the local and regional stress fields are the prevalent mechanisms for the emplacement and the spatial distribution of vents the evc having formed along the central anatolian fault zone cafz is here considered as a magmatic transfer zone mostly inferred from the presence of many strikeslip features rotation of extension axis and the radial pattern of the vents through the western parts of cavp the vent alignments are almost perpendicular to the regional extension axis and parallel to the orientation of the tuz golu fault zone tgfz where the preexisting fractures are probably the primary mechanisms on their formations 
our comprehensive approach together with the analysis of wellestablished literature reveals that the collision along the bitlis suture zone in the middle miocene and subsequent westward tectonic escape of the anatolia along the major fault zones have mostly controlled the volcanism not only in eastern anatolia but also in the cavp in this scenario we suggest that the cafz has been the main mechanism for the propagation of mantlederived magmas and completely shaped the spatial distribution of the volcanoes in the cavp with the help of crustaldepth tgfz and other tectonic features our recent findings presented here will hopefully offer new insights into the understanding of cavp volcanism and the intended future volcanic risk assessment"
quantum gravity is expected to contain descriptions of semiclassical spacetime geometries in quantum superpositions to date no framework for modelling such superpositions has been devised here we provide a new phenomenological description for the response of quantum probes ie unruh–dewitt detectors on a spacetime manifold in quantum superposition by introducing an additional control degree of freedom one can assign a hilbert space to the spacetime allowing it to exist in a superposition of spatial or curvature states applying this approach to static de sitter space we discover scenarios in which the effects produced by the quantum spacetime are operationally indistinguishable from those induced by superpositions of rindler trajectories in minkowski spacetime the distinguishability of such quantum spacetimes from superpositions of trajectories in flat space reduces to the equivalence or nonequivalence of the field correlations between the superposed amplitudes
human body communications hbcs have recently emerged as an innovative alternative to the current radio frequency communications for realizing wireless body area networks wbans using the human body as a transmission channel without wired or wireless connections this article addresses the provision of reliable modeling of the human body as a passage of the electric signal delivery based on the impulse response measurement through the proposal of a measurement setup and signal processing techniques applicable to wearable devices for healthcare and biosignal acquisition in the experiments customized impulse signals were applied to the body using batterypowered devices isolated to the earth ground for the operating environments of wearable devices the impulse responses passed through the body were measured by considering 52 measurement conditions determined by the device locations from the head to ankle and the body postures body channel transfer functions bctfs for the respective conditions were derived by an adaptive filter approach using an iterative algorithm to minimize the mean squared error between the measured and modeled impulse responses the channel analysis parameters such as mean path loss rootmeansquare delay spread and mean and maximum excess delays were analyzed based on the measured body impulse responses in addition the practical biterrorrate performance for hbc based on the bctfs reproducing intersymbol interference effects caused by the delay spreads of the body channels was explored to verify communication reliability in terms of the transmitter structures adopting digital transmission sorts of human body channels data rates and operating frequencies
in this article we develop proportional fractional‐integral and derivative  piλd  controllers for the regulation and tracking problems of nonlinear systems the analytic results are obtained by extending the passivity‐based approach to include fractional operators robustness under parametric uncertainty is dealt with by a combination with an adaptive scheme it is also shown their robustness under additive noise and their robustness under uncertainty in the derivation order the advantages in the controlled system performance and in the control energy consumption in comparison to classic pi and proportional integral derivative controllers are illustrated for the quadratic boost converter and a benchmark system in the literature
an improved version of alphan a selfpowered wheeldriven automated delivery robot adr is presented in this study alphanv2 is capable of navigating autonomously by detecting and avoiding objects or obstacles in its path for autonomous navigation and path planning alphan uses a vector map and calculates the shortest path by grid count method gcm of dijkstra’s algorithm the rfid reading system rrs is assembled in alphan to read landmark determination with radio frequency identification rfid tags with the help of the rfid tags alphan verifies the path for identification between source and destination and calibrates the current position along with the rrs gcm to detect and avoid obstacles an object detection module odm is constructed by faster rcnn with vggnet16 architecture that builds and supports the path planning system pps in the testing phase the following results are acquired from the alphan odm exhibits an accuracy of formula see text rrs shows formula see text accuracy and the pps maintains the accuracy of formula see text this proposed version of alphan shows significant improvement in terms of performance and usability compared with the previous version of alphan
due to the growing amount of data from insitu sensors in wastewater systems it becomes necessary to automatically identify abnormal behaviours and ensure high data quality this paper proposes an anomaly detection method based on a deep autoencoder for insitu wastewater systems monitoring data the autoencoder architecture is based on 1d convolutional neural network cnn layers where the convolutions are performed over the inputs across the temporal axis of the data anomaly detection is then performed based on the reconstruction error of the decoding stage the approach is validated on multivariate time series from insewer process monitoring data we discuss the results and the challenge of labelling anomalies in complex time series we suggest that our proposed approach can support the domain experts in the identification of anomalies
personalized production is moving the progress of industrial automation forward and demanding new tools for improving the decisionmaking of the operators this paper presents a new projectionbased augmented reality system for assisting operators during electronic component assembly processes the paper describes both the hardware and software solutions and depicts the results obtained during a usability test with the new system
the concept of acceptance understood as a selfregulation strategy based on an open and welcoming attitude toward one’s own emotions thoughts or external events williams and lynn 20101 is present in various domains of psychological research and practice previous studies have brought important knowledge on the nature of this strategy but very significant gaps in the knowledge still exist the most important issues are as follows 1 conceptual difficulties regarding acceptance as an emotion regulation strategy 2 lack of coherence in operationalizations of acceptance in various research and 3 acceptance not being recognized as a distinct emotion regulation strategy in the most influential emotion regulation models in the present paper we highlight and discuss these issues in more detail and—based on this discussion postulate directions for future experimental research on acceptance the popularity of research on acceptance has grown steadily since the 1990s when acceptancebased therapeutic approaches started to develop more rapidly for example the role of acceptance was underlined in the acceptance and commitment therapy act hayes et al 1999 as well as other approaches many different forms of acceptancebased programs were developed and tested for effectiveness they were shown to be successful in stress and pain reduction as well as decreasing anxiety and depression symptoms segal et al 2002 hayes et al 2006 veehof et al 2011 twohig and levin 2017 feliusoler et al 2018 other studies showed the role of mindfulness and acceptance for the severity of psychotic symptoms cramer et al 2016 jansen et al 2019 eating disorders prefit et al 2019 compulsive sexual behavior lewstarowicz et al 2019 addictive behaviors bowen et al 2011 suicidal ideation and selfharm tighe et al 2018 as well as other psychopathological symptom clusters aldao et al 2010 the definitional goal of acceptance as an emotion regulation strategy is not to change the experienced emotions but to receive themwithout control attempts hayes 2004 kohl et al 2012 thus acceptance is quite distinct from other frequently studied ways of regulating emotion eg suppression most forms of cognitive reappraisal rumination that are most often based on some form of active modification of emotional state in terms of quality strength length or frequency of emotion gross 2015 despite these differences acceptance is present in psychological research on emotion regulation and is often compared with other regulatory strategies eg liverant et al 2008 aldao et al 2010 naragongainey et al 2017 southward et al 2019 however despite
objective electronic health records ehrs hold promise as a public health surveillance tool but questions remain about how ehr patients compare with populations in health and demographic surveys we compared population characteristics from a regional distributed data network ddn which securely and confidentially aggregates ehr data from multiple health care organizations in the same geographic region with population characteristics from health and demographic surveys methods ten health care organizations participating in a colorado ddn contributed data for coverage estimation we aggregated demographic and geographic data from 2017 for patients aged ≥18 residing in 7 counties we used a crosssectional design to compare ddn population size by county with the following surveyestimated populations the county population estimated by the american community survey acs residents seeking any health care estimated by the colorado health access survey and residents seeking routine eg primary health care estimated by the behavioral risk factor surveillance system we also compared data on the ddn and survey populations by sex age group raceethnicity and poverty level to assess surveillance system representativeness results the ddn population included 609 840 people in 7 counties corresponding to 25 coverage of the general adult population population coverage ranged from 15 to 35 across counties demographic distributions generated by ddn and surveys were similar for many groups overall the ddn and surveys assessing careseeking populations had a higher proportion of women and older adults than the acs population the ddn included higher proportions of hispanic people and people living in highpoverty neighborhoods compared with the surveys conclusion the ddn population is not a random sample of the regional adult population it is influenced by health care use patterns and organizations participating in the ddn strengths and limitations of ddns complement those of surveybased approaches the regional ddn is a promising public health surveillance tool
with the overarching goal of developing usercentric virtual reality vr systems a new wave of studies focused on understanding how users interact in vr environments has recently emerged despite the intense efforts however current literature still does not provide the right framework to fully interpret and predict users’ trajectories while navigating in vr scenes this work advances the stateoftheart on both the study of users’ behaviour in vr and the usercentric system design in more detail we complement current datasets by presenting a publicly available dataset that provides navigation trajectories acquired for heterogeneous omnidirectional videos and different viewing platforms—namely headmounted display tablet and laptop we then present an exhaustive analysis on the collected data to better understand navigation in vr across users content and for the first time across viewing platforms the novelty lies in the useraffinity metric proposed in this work to investigate users’ similarities when navigating within the content the analysis reveals useful insights on the effect of device and content on the navigation which could be precious considerations from the system design perspective as a case study of the importance of studying users’ behaviour when designing vr systems we finally propose a usercentric server optimisation we formulate an integer linear program that seeks the best stored set of omnidirectional content that minimises encoding and storage cost while maximising the user’s experience this is posed while taking into account network dynamics type of video content and also user population interactivity experimental results prove that our solution outperforms common company recommendations in terms of experienced quality but also in terms of encoding and storage achieving a savings up to 70 more importantly we highlight a strong correlation between the storage cost and the useraffinity metric showing the impact of the latter in the system architecture design
as the alarming growth of connectivity of computers and the significant number of computerrelated applications increase in recent years the challenge of fulfilling cybersecurity is increasing consistently it also needs a proper protection system for numerous cyberattacks thus detecting inconsistency and attacks in a computer network and developing intrusion detection system ids that performs a potential role for cybersecurity artificial intelligence particularly machine learning techniques has been used to develop a useful datadriven intrusion detection system in this paper we employ various popular machine learning classification algorithms namely bayesian network naive bayes classifier decision tree random decision forest random tree decision table and artificial neural network to detect intrusions due to provide intelligent services in the domain of cybersecurity finally we test the effectiveness of various experiments on cybersecurity datasets having several categories of cyberattacks and evaluate the effectiveness of the performance metrics precision recall f1score and accuracy
the article deals with the calculation of currents and voltages of the toothed frequency of asynchronous motors in power supply systems the study is based on the analysis of electromagnetic transformations in asynchronous motors taking into account the twoway toothness and magnetic asymmetry the calculation of asymmetric currents of the toothed frequency is carried out by the method of symmetric components developed calculation algorithm and software the article establishes the factors of influence on the levels of toothed harmonics of the current in the power supply systems
pedophilia is a disorder of public concern because of its association with child sexual offense and recidivism previous neuroimaging studies of potential brain abnormalities underlying pedophilic behavior either in idiopathic or acquired ie emerging following brain damages pedophilia led to inconsistent results this study sought to explore the neural underpinnings of pedophilic behavior and to determine the extent to which brain alterations may be related to distinct psychopathological features in pedophilia to this aim we run a coordinate based metaanalysis on previously published papers reporting whole brain analysis and a lesion network analysis using brain lesions as seeds in a resting state connectivity analysis the behavioral profiling approach was applied to link identified regions with the corresponding psychological processes while no consistent neuroanatomical alterations were identified in idiopathic pedophilia the current results support that all the lesions causing acquired pedophilia are localized within a shared resting state network that included posterior midlines structures right inferior temporal gyrus and bilateral orbitofrontal cortex these regions are associated with action inhibition and social cognition abilities that are consistently and severely impaired in acquired pedophiles this study suggests that idiopathic and acquired pedophilia may be two distinct disorders in line with their distinctive clinical features including age of onset reversibility and modus operandi understanding the neurobiological underpinnings of pedophilic behavior may contribute to a more comprehensive characterization of these individuals on a clinical ground a pivotal step forward for the development of more efficient therapeutic rehabilitation strategies
this paper proposes a decentralized control method for modular robot manipulators mrms with external collisions based on torque estimation unlike some of conventional methods which depend on joint torque sensing technique we address the torque estimation and motion control problems of mrms based on only position measurements of each joint module the dynamics model of mrms is established by using the hd model and the estimated joint torque by using only local dynamic information of each joint a decentralized robust rbf neural network nn controller is designed to deal with external uncertain collision effects and joint trajectory tracking issues the closedloop system is verified to be asymptotic stable based on the lyapunov theory finally experiments are carried out to prove the effectiveness of the proposed method
a particularly simple description of separability of quantum states arises naturally in the setting of complex algebraic geometry via the segre embedding this is a map describing how to take products of projective hilbert spaces in this paper we show that for pure states of n particles the corresponding segre embedding may be described by means of a directed hypercube of dimension n1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentn1enddocument where all edges are bipartitetype segre maps moreover we describe the image of the original segre map via the intersections of images of the n1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentn1enddocument edges whose target is the last vertex of the hypercube this purely algebraic result is then transferred to physics for each of the last edges of the segre hypercube we introduce an observable which measures geometric separability and is related to the trace of the squared reduced density matrix as a consequence the hypercube approach allows to measure entanglement naturally relating bipartitions with qpartitions for any q≥1documentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentqge 1enddocument we test our observables against wellknown states showing that these provide wellbehaved and fine measures of entanglement
intrinsic and parametric regression models are of high interest for the statistical analysis of manifoldvalued data such as images and shapes the standard linear ansatz has been generalized to geodesic regression on manifolds making it possible to analyze dependencies of random variables that spread along generalized straight lines nevertheless in some scenarios the evolution of the data cannot be modeled adequately by a geodesic we present a framework for nonlinear regression on manifolds by considering riemannian splines whose segments are bezier curves as trajectories unlike variational formulations that require timediscretization we take a constructive approach that provides efficient and exact evaluation by virtue of the generalized de casteljau algorithm we validate our method in experiments on the reconstruction of periodic motion of the mitral valve as well as the analysis of femoral shape changes during the course of osteoarthritis endorsing bezier spline regression as an effective and flexible tool for manifoldvalued regression
despite the abundant research in cloud autoscaling autoscaling in kubernetes arguably the most popular cloud platform today is largely unexplored kubernetes cluster autoscaler can be configured to select nodes either from a single node pool ca or from multiple node pools canap we evaluate and compare these configurations using two representative applications and workloads on google kubernetes engine gke we report our results using monetary cost and standard autoscaling performance metrics under and overprovisioning accuracy under and overprovisioning timeshare instability of elasticity and deviation from the theoretical optimal autoscaler endorsed by the spec cloud group we show that overall canap outperforms ca and that autoscaling performance depends mainly on the composition of the workload we compare our results with those of the related work and point out further configuration tuning opportunities to improve performance and costsaving
wheat is one of the main food crops in the world today in order to increase wheat yield breeding experts are committed to discovering the connection between its genotype and phenotype existing phenotype extraction methods mostly rely on manual methods and the amount of collected data is limited and inefficient 3d computed tomography ct imaging has the advantages of fine imaging high dynamic range and nondestructive detection of internal structures which can help to extract the highthroughput phenotype of wheat nondestructively and finely however the 3d images scanned by ct have the characteristics of large amount of data and highly sparse content which brings great challenges to extract phenotype from them we propose an adaptive supervoxel algorithm for the high sparsity of 3d ct images which is used for the presegmentation then an octree convolutional network is used to optimize the sparse structure storage and further improve the efficiency of fine segmentation at last phenotype is extracted automatically from the finely segmented wheat grain
although facial expression recognition has improved in recent years it is still very challenging to recognize expressions from occluded facial images in the wild due to the lack of largescale facial expression datasets with diversity of the type and position of occlusions it is very difficult to learn robust occluded expression classifier directly from limited occluded images considering facial images without occlusions usually provide more information for facial expression recognition compared to occluded facial images we propose a stepwise learning strategy for occluded facial expression recognition that utilizes unpaired nonoccluded images as guidance in the feature and label space specifically we first measure the complexity of nonoccluded data using distribution density in a feature space and split data into three subsets in this way the occluded expression classifier can be guided by basic samples first and subsequently leverage more meaningful and discriminative samples complementary adversarial learning techniques are applied in the globallevel and locallevel feature space throughout forcing the distribution of the occluded features to be close to the distribution of the nonoccluded features we also take the variability of the different images transferability into account via adaptive classification loss loss inequality regularization is imposed in the label space to calibrate the output values of the occluded network experimental results show that our method improves performance on both synthesized occluded databases and realistic occluded databases
there is a large amount of information and maintenance data in the aviation industry that could be used to obtain meaningful results in forecasting future actions this study aims to introduce machine learning models based on feature selection and data elimination to predict failures of aircraft systems maintenance and failure data for aircraft equipment across a period of two years were collected and nine input and one output variables were meticulously identified a hybrid data preparation model is proposed to improve the success of failure count prediction in two stages in the first stage relieff a feature selection method for attribute evaluation is used to find the most effective and ineffective parameters in the second stage a kmeans algorithm is modified to eliminate noisy or inconsistent data performance of the hybrid data preparation model on the maintenance dataset of the equipment is evaluated by multilayer perceptron mlp as artificial neural network ann support vector regression svr and linear regression lr as machine learning algorithms moreover performance criteria such as the correlation coefficient cc mean absolute error mae and root mean square error rmse are used to evaluate the models the results indicate that the hybrid data preparation model is successful in predicting the failure count of the equipment
"
 for human–machine interaction the forward progression of technology particularly controls regularly brings about new possibilities indeed healthcare applications have flourished in recent years including robotic rehabilitation exercise and prosthetic devices testing these devices with human subjects is inherently risky and frequently inconsistent this work offers a novel simulation framework toward overcoming many of these difficulties specifically generating a closedloop dynamic model of a human or a human subsystem that can connect to device simulations allows simulated human–machine interaction in this work a muscleactuated open kinematic chain linkage is generated to simulate the human and a backstepping controller based on inverse dynamics is derived the control architecture directly addresses muscle redundancy and two options to resolve this redundancy are evaluated the specific case of a muscleactuated arm linkage is developed to illustrate the framework trajectory tracking is achieved in simulation the muscles recruited to meet the tracking goal are in agreement with the method used to solve the redundancy problem in the future coupling such simulations to any relevant simulation of a machine will provide safe insightful preprototype test results"
we present a numerical methodology to estimate the transient fault currents and to simulate the remote sensing of transient fault information embedded in the magnetic field emissions caused by interturn shorts in 60 hz aircore reactors thru a magneto quasistatic mqs field approximation in the method of finitedifference timedomain fdtd in 2dimensional 2d space the mqs 2d fdtd fields of reactor in normal operation are scaled by correlation against an equivalent circuit model that is derived from application of basic physics principles to parameters of the 3d aircore reactor the proposed multiscale quasistatic modeling methodology based on the reduced c modification provides finefeature access down to the singlewire level and can efficiently estimate the transient fault fields and currents due to turntoturn short in a reactor with core height in several meters core diameter in meters wire diameter in millimeters and number of turns in the thousands at 60 hz this is accomplished by using computational resources of a typical laptop computer within seconds or minutes as opposed to days that would be otherwise required without the reduced c modification
accurately predicting the time to an event of interest is an important problem in a wide range of realworld applications however prediction is often difficult because many medical datasets have a large number of unlabeled “censored” instances because labeling is costly and time consuming survival analysis focuses on labeled data to predict the time to an event of interest such as time of death or conversion to a different stage in a progressive disease grouping structure which naturally exists in medical datasets can be exploited to improve generalization performance by learning multiple related survival prediction tasks for subgroups collaboratively thus a multitask learning framework can connect multiple survival prediction tasks for different subgroups and learn them simultaneously in order to take into account both censored information as well as discover the grouping structure we propose a novel clusterboosted multitask learning framework for survival analysis that boosts survival prediction performance we develop an efficient algorithm and demonstrate the performance of the proposed clusterboosted multitask survival analysis method on the cancer genome atlas tcga dataset our results show that the proposed approach can significantly improve prediction performance in survival analysis while also identifying different subgroups of cancer patients
"
 parametric optimization solves optimization problems as a function of uncontrollable or unknown parameters such an approach allows an engineer to gather more information than traditional optimization procedures during design existing methods for parametric optimization of computationally or monetarily expensive functions can be too timeconsuming or impractical to solve therefore new methods for the parametric optimization of expensive functions need to be explored this work proposes a novel algorithm that leverages the advantages of two existing optimization algorithms this new algorithm is called the efficient parametric optimization epo algorithm epo enables adaptive sampling of a highfidelity design space using an inexpensive lowfidelity response surface model such an approach largely reduces the required number of expensive highfidelity computations the proposed method is benchmarked using analytic test problems and used to evaluate a case study requiring finite element analysis results show that epo performs as well as or better than the existing alternative predictive parameterized pareto genetic algorithm p3ga for these problems given an allowable number of function evaluations"
while modern tts technologies have made significant advancements in audio quality there is still a lack of behavior naturalness compared to conversing with people we propose a styleembedded tts system that generates styled responses based on the speech query style to achieve this the system includes a style extraction model that extracts a style embedding from the speech query which is then used by the tts to produce a matching response we faced two main challenges 1 only a small portion of the tts training dataset has style labels which is needed to train a multistyle tts that respects different style embeddings during inference 2 the tts system and the style extraction model have disjoint training datasets we need consistent style labels across these two datasets so that the tts can learn to respect the labels produced by the style extraction model during inference to solve these we adopted a semisupervised approach that uses the style extraction model to create style labels for the tts dataset and applied transfer learning to learn the style embedding jointly our experiment results show user preference for the styled tts responses and demonstrate the styleembedded tts systems capability of mimicking the speech query style
a large number of deep learning architectures use spatial transformations of cnn feature maps or filters to better deal with variability in object appearance caused by natural image transformations 
"objectives
a lack of morningness predicts greater depression symptom severity over time including in a vulnerable group of older adults family dementia caregivers dcgs evidence regarding the neurobiological basis of these correlations is needed to guide future research towards biomarkerinformed detection and prevention approaches we therefore primarily aimed to identify simple restingstate biomarkers that correlated with a lack of morningness in dcgs


methods
we examined 54 dcgs mean age70 range 6184 70 female of whom 40 were definite morning types according to composite scale of morningness csm using a 7 tesla restingstate sequence we compared the functional connectivity of nodes in networks previously implicated in depression frontoparietal default mode limbic and salience between caregivers who were and were not morning types


results
correcting for voxelwise comparisons morning type dcgs had less amygdalaposterior cingulate connectivity cohens d13 which statistically mediated 32 of the association between the degree of morningness and lower depression severity posthoc analyses of csm items found significant correlations with both amygdalaposterior cingulate fc and depression severity for 46 items pertaining to difficulty 25 items pertaining to preference and 02 items pertaining to typical patterns


discussion
prior research shows that amygdalaposterior cingulate connectivity increases when allocating attention to peripheral aspects of negative emotional stimuli as such difficulty with morning activation may relate to the ongoing direction of focus around distressing content in contrast morning activity participation may serve to limit focus on distress replication and experimental studies are required to confirm these associations and their modifiability"
the powerful software tools and techniques required for the development of data mining applications with the rapid development of technologies and business interest in using electronics and latest technologies plays important role in improvement of data mining field data mining access the meaningful and efficient information available in worldwide which is helps in decision making this paper described the a various tools and techniques used by data mining applications b compared features and limitations both in proprietary and open sources data mining tools c technical analysis of proprietary and open source data mining tools on the basis of welldesigned user interface short time analysis statistical and mathematical analysis user can select the best tool as per their requirements analysis of these tools makes easy to select appropriate tool
in order to accommodate the variety of algorithms with different performance in specific application and improve power efficiency reconfigurable architecture has become an effective methodology in academia and industry however existing architectures suffer from performance bottleneck due to slow updating of contexts and inadequate flexibility this paper presents an htree based reconfiguration mechanism hrm with huffmancodinglike and mask addressing method in a homogeneous processing element pe array which supports both programmable and datadriven modes the proposed hrm can transfer reconfiguration instructionscontexts to a particular pe or associated pes simultaneously in one clock cycle in unicast multicast and broadcast mode and shut down the unnecessary pepes according to the current configuration to verify the correctness and efficiency we implement it in rtl synthesis and fpga prototype compared to prior works the experiment results show that the hrm has improved the work frequency by an average of 234 increased the updating speed by 2× and reduced the area by 369 hrm can also power off the unnecessary pes which reduced 51 of dynamic power dissipation in certain application configuration furthermore in the datadriven mode the system frequency can reach 214 mhz which is 168× higher compared with the programmable mode
 unauthorized uncontrolled mining of amber causes serious economic losses to the state and society it is accompanied by a deterioration of the social climate leads to the degradation of large areas of agricultural and forestry land and it causes disturbance of the natural landscape structure since disturbed biotopes cannot be restored to their original state without human intervention they need to be rehabilitated legislation that regulates social relations in the field of disturbed land reclamation requires further amendments for the vast majority of regulations indicate that persons carrying out mining exploration construction and other works involving mechanical damage to the soil shall be liable for land reclamation or for the failure to undertake it failure to reclaim disturbed land is the violation that causes damage to legal owners and users of land the territorial community and the state a violator of land legislation that does not implement a set of organizational technical and biotechnological measures aimed at restoring soil cover improving the condition and productivity of disturbed land in accordance with the approved land surveying documentation illegally receives income from the funds application for other purposes than the ones specified in the cost estimation of the land management project and should have been aimed at reclamation of disturbed land of economic and legal mechanism of compensation for damages caused by illegal amber mining nonperformance of land reclamation works after completion of mineral deposits shall provide more effective protection of the interests of the state and territorial communities rights of land owners and users shall have a beneficial impact in terms of the reduction of land use violations thus reclamation becomes an integral part of the protection and rational use of natural resources
evolution has yielded biopolymers that are constructed from exactly four building blocks and are able to support darwinian evolution synthetic biology aims to extend this alphabet and we recently showed that 8letter hachimoji dna can support rulebased information encoding one source of replicative error in nonnatural dnalike systems however is the occurrence of alternative tautomeric forms which pair differently unfortunately little is known about how structural modifications impact freeenergy differences between tautomers of the nonnatural nucleo¬bases used in the hachimoji expanded genetic alphabet determining experimental tautomer ratios is technically difficult and so strategies for improving hachimoji dna replication efficiency will benefit from accurate computational predictions of equilibrium tautomeric ratios we now report that highlevel quantumchemical calculations in aqueous solution by the embedded cluster reference interaction site model ecrism benchmarked against free energy molecular simulations for solvation thermodynamics provide useful quantitative information on the tautomer ratios of both watsoncrick and hachimoji nucleobases in agreement with previous computational studies all four watsoncrick nucleobases adopt essentially only one tautomer in water this is not the case however for nonnatural nucleobases and their analogs for example although the enols of isoguanine and a series of related purines are not populated in water these heterocycles possess n1h and n3h keto tautomers that are similar in energy thereby adversely impacting accurate nucleobase pairing these robust computational strategies offer a firm basis for improving experimental measurements of tautomeric ratios which are currently limited to studying molecules that exist only as two tautomers in solution
in this paper a collisionfree navigation algorithm for flying robots is proposed the algorithm is applied to threedimensional environments with dynamic obstacles shapes of moving obstacles may be nonconvex motion of the obstacles is variable the algorithm is a sliding mode control law switching between two modes the target approaching mode and the obstacle avoidance mode mathematical analysis of the proposed algorithm is given simulation results are also provided in order to demonstrate its performance compared with other approaches the algorithm is a really practical one as the constraints of the obstacles are not too hard to be satisfied moreover the proposed algorithm is computationally efficient
despite being a very old discipline pointer analysis still attracts several research papers every year in premier programming language venues while a major goal of contemporary pointer analysis research is to improve its efficiency without sacrificing precision we also see works that introduce novel ways of solving the problem itself what does this mean research in this area is not going to die soon i too have been writing pointer analyses of various kinds specially for objectoriented languages such as java while some standard ways of writing such analyses are clear i have realized that there are an umpteen number of nooks and pitfalls that make the task difficult and error prone in particular there are several misconceptions and undocumented practices being aware of which would save significant research time on the other hand there are lessons from my own research that might go a long way in writing correct precise and efficient pointer analyses faster this paper summarizes some such learnings with a hope to help readers beat the stateoftheart in java pointer analysis as they move into their research careers beyond 2020
we report a tunable optical filter based on phase change material ge2sb2te5 embedded in a silicon microring resonator the high thermooptic coefficient of ge2sb2te5 in amorphous phase enables tuning of resonance wavelength in broad range with a very small active volume the lowloss indiumtinoxide electrodes are employed to induce joule heating in ge2sb2te5si active waveguide region the electrically induced heating in the active region alters the effective refractive index of hybrid microring resulting in a wavelength tuning of 104 nm for an applied voltage of only 3v the device exhibits high extinction ratios in the range of 2041 db and a compact active footprint 096 mu m2 only and is suitable for the large scale reconfigurable integrated photonic circuits
markov chain monte carlo mcmc is a computational approach to fundamental problems such as inference integration optimization and simulation recently the framework involutive mcmc was proposed describing a large body of mcmc algorithms via two components a stochastic acceptance test and an involutive deterministic function this paper demonstrates that this framework is a special case of a larger family of algorithms operating on orbits of continuous deterministic bijections we describe this family by deriving a novel mcmc kernel which we call orbital mcmc omcmc we provide a theoretical analysis and illustrate its utility using simple examples
a datadriven fiber channel modeling method based on deep learning dl is introduced in an optical communication system in this study bidirectional long shortterm memory bilstm is selected from a diverse range of dl algorithms to perform fiber channel modeling for on–off keying and pulse amplitude modulation 4 signals compared with the conventional modeldriven splitstep fourier ssfbased method the proposed method yields similar results based on the comprehensive comparison of multiple characteristics associated with the generated optical signals including the optical amplitude and phase waveforms in the time domain optical spectrum components in the frequency domain and eye diagrams after detection in the electrical domain additionally the effects of multiple factors on the modeled fiber channel have also be investigated including fiber length fiber nonlinearity dispersion data pattern pulse shaping and sample rate the satisfactory fitting results and acceptable mean square errors indicate that the approximate transfer function of the fiber channel is learned by the bilstm moreover compared with repetitive iteration ssf the computing time is significantly reduced by the bilstm owing to its independence on fiber length and insensitivity to data size and launch power our aim is to demonstrate the bilstm is comparable with the conventional modeldriven ssfbased method for directdetection optical fiber system we think the proposed method could be a supplementary technique that can be used for the existing simulation system and could also be a potential option for future simulation methods
optomechanical systems are known to exhibit selfsustained limit cycles once driven above the parametric instability point this breaks down the linearized approximation and induces novel nonlinear effects such as dynamical multistability staircase behavior and the generation of optical highorder sideband combs hoscs here we study the classical nonlinear dynamics of optomechanical systems we combine numerical simulations and analytical investigation to predict dynamical multistability in the resolved sideband regime a way to predict the onset of the period doubling process and to control the multistability is analytically provided by tuning the optical linewidth indeed the multistability behavior first changes to a staircase shape and gradually disappears as the system approaches the unresolved sideband limit we exploit the multistable attractors to generate optical hoscs by acting solely on the initial values instead of increasing the driving strength this is the figure of merit of our proposal to relate multistability to the hosc as a result the properties bandwidth intensity of the combs are improved as the mechanical resonator moves towards upper attractors this work opens a way for lowpower hosc generation in optomechanics and the related technological applications
ferrograph image segmentation is of significance for obtaining features of wear particles however wear particles are usually overlapped in the form of debris chains which makes challenges to segment wear debris an overlapping wear particle segmentation network owpsnet is proposed in this study to segment the overlapped debris chains the proposed deep learning model includes three parts a region segmentation network an edge detection network and a feature refine module the region segmentation network is an improved u shape network and it is applied to separate the wear debris form background of ferrograph image the edge detection network is used to detect the edges of wear particles then the feature refine module combines lowlevel features and highlevel semantic features to obtain the final results in order to solve the problem of sample imbalance we proposed a square dice loss function to optimize the model finally extensive experiments have been carried out on a ferrograph image dataset results show that the proposed model is capable of separating overlapping wear particles moreover the proposed square dice loss function can improve the segmentation results especially for the segmentation results of wear particle edge
a heterodyne grating interferometer hgi with high stability and low nonlinearity is presented an optical arrangement with littrow configuration combined with wollaston prism is proposed for the hgi the compact and symmetrical design make it insensitive to environmental disturbances the optimal separate angle of the wollaston prism is investigated with rigorous coupledwave analysis rcwa and the value of 44° corresponding to the grating pitch of 845 nm is selected the approximately equal diffraction efficiencies of 7654 and 7664 for te and tm polarizations are respectively obtained which is beneficial to the signaltonoise ratio snr of hgi additionally the nonlinear errors including polarization mixing frequency mixing and polarizationfrequency mixing are analyzed the polarization mixing error is dramatically decreased owing to the high extinction ratio of wollaston prism it reveals that the developed hgi has the potential for nanometric displacement measurement
we introduce in this paper an explainable deep learning solution for noninvasive condition monitoring of cantilever beams and we emphasize the advantages of it the explanations of the black box ai connectionist classifier are provided as featuresrelated importance ranking for the output of the probabilistic decision margin improving in this way the trust in the exact recognition of damaged beams and its characteristics ie damage depth and damage size for training the classifier we have used precomputed distributional sets with 10 natural frequencies the local sample based explanation is obtained from a model agnostic lime algorithm and the global explanation is obtained from averaging shap values both applied posthoc to the classifier we have performed intensive testing and we have observed that sometimes the decision is not to be trusted due to the features that mostly influenced that particular decision despite of its high accuracy
abstract a sliding mode control method is developed in this study for application to a class of underactuated systems with bounded unknown disturbance and sensor and actuator faults in the proposed method a robustness item compensates for the bounded unknown disturbance and a nussbaum function realises sensor and actuator faults tolerance simultaneously and all signals of the system are proven to be bounded a radial basis function rbf neural network is developed to estimate the unknown functions of the system finally hurwitz stability analysis is conducted to guarantee the stability of the closedloop system simulations are conducted wherein a coupled motor driving system is placed under the proposed control laws to validate this approach
elastic actuators can provide safe humanrobot interaction and energy efficient mobility for this purpose they are ideal for wearable robotic applications however such actuators are subject to stiffness faults we present a stiffnessfaulttolerant control strategy for complex elastic actuators capable of adapting to changes in output stiffness and demonstrate it on a smart variable stiffness actuator based on the maccepa concept we develop the dynamics of the actuator and a modelbased impedance control scheme biomechanical data extracted from the flexionextension of a real knee joint are used as trajectory reference for the evaluation of the control concept in simulation results show that the controlled actuator is capable of tracking a reference trajectory under fault conditions and interaction disturbance while maintaining physical humanrobot characteristics
we are living in a time where operations and reliability are being impacted by the limits of our own humanity decades of research in the field of human factors engineering has shown that often we as humans are in fact the weakest link in the successful operation of a plant or facility complex systems coupled with the human element give rise to a new class of problem the socio‐technical problem fortunately we now have proven technology to mitigate some of our inherent shortcomings the use of this technology can radically alter the way we operate and will give rise to what i call the operator 20 imagine automated work procedures that appear in the operators work environment that guide them through an activity instant access to remote expertise that guide them through troubleshooting from anywhere in the world and a technology that can reduce training time for operators and technicians from years down to months imagine sending a drone to perform an inspection in the process as opposed to sending an operator into harms way think of the implications of not having people in the actual process this opportunity exists today
abstract ecological systems can often be characterised by changes among a finite set of underlying states pertaining to individuals populations communities or entire ecosystems through time owing to the inherent difficulty of empirical field studies ecological state dynamics operating at any level of this hierarchy can often be unobservable or ‘hidden’ ecologists must therefore often contend with incomplete or indirect observations that are somehow related to these underlying processes by formally disentangling state and observation processes based on simple yet powerful mathematical properties that can be used to describe many ecological phenomena hidden markov models hmms can facilitate inferences about complex system state dynamics that might otherwise be intractable however hmms have only recently begun to gain traction within the broader ecological community we provide a gentle introduction to hmms establish some common terminology review the immense scope of hmms for applied ecological research and provide a tutorial on implementation and interpretation by illustrating how practitioners can use a simple conceptual template to customise hmms for their specific systems of interest revealing methodological links between existing applications and highlighting some practical considerations and limitations of these approaches our goal is to help establish hmms as a fundamental inferential tool for ecologists
background chinese medicine education is part of professional medical training in hong kong an important element of this is herbal medicine which requires both theoretical and practical knowledge a field trip programme was adopted to provide students with direct experience of medicinal plants studied in lectures however problems with the current programme were identified in learning outcome assessment and longterm knowledge management to improve the teaching quality a moodle elearning module was designed for augmentation this study aimed to quantitatively evaluate the effectiveness of the moodle module in supplementing the current field trip programme methods prospective quasiexperiment participants were 49 year2 students in the bachelor of chinese medicine programme a moodle module including five online activities regarding two groups of herbal plants was integrated before and after the field trip fillintheblank questions were used to assess the learning outcome also a questionnaire was developed to collect student feedback as the secondary outcome results for herbal plants in group a the assessment score was higher in moodle group 2965 ± 50 than for the control group 2165 ± 65  p    001 for herbal plants in group b the assessment score was higher for the moodle group 2868 ± 47 than for the control group 2426 ± 77  p    001 the questionnaire results showed that students were satisfied with the moodle platform conclusions a specially designed moodle module may be effective in augmenting the field trip for chinese herbal medicine education
given a text t of length n and a pattern p of length m the string matching problem is a task to find all occurrences of p in t in this study we propose an algorithm that solves this problem in on  mq time considering the distance between two adjacent occurrences of the same qgram contained in p we also propose a theoretical improvement of it which runs in on  m time though it is not necessarily faster in practice we compare the execution times of our and existing algorithms on various kinds of real and artificial datasets such as an english text a genome sequence and a fibonacci string the experimental results show that our algorithm is as fast as the stateoftheart algorithms in many cases particularly when a pattern frequently appears in a text
this study investigates the role of stasis an ancient rhetorical tool with both heuristic and analytic capabilities in entrepreneurial rhetoric specifically in pitching and questionandanswer sessions drawing from a multiyear sample of shark tank pitches the author found that funders expect entrepreneurs to account for stases of being quality quantity and place the findings suggest a series of associated questions within each stasis when these questions are answered unsuccessfully standstills occur within the funding argument when they are successfully addressed the stasis passes and ventures are more likely to receive funding the author discusses the implications of this study for entrepreneurship and professional communication
contentbased image retrieval is one of the interesting subjects in image processing and machine vision in image retrieval systems the query image is compared with images in the database to retrieve images containing similar content image comparison is done using features extracted from the query and database images in this paper the features are extracted based on the human visual system since the human visual system considers the texture and the edge orientation in images for comparison the colour difference histogram associated with the image’s texture and edge orientation is extracted as a feature in this paper the features are selected using the shannon entropy criterion the proposed method is tested using the corel5k and corel10k databases the precision and recall criteria were used to evaluate the proposed system the experimental results show the ability of the proposed system for more accurate retrieval rather than recently contentbased image retrieval systems
a lowcost wifi robot as a teaching equipment is developed the robot can be used to teach students in the subjects related to robotics system and internet of things iot a wifi robot is a robot equipped with a wifi communication system for connecting to the internet integrating the robot with an iot platform makes the robot able to communicate with other devices the developed wifi robot in this study is a threewheeled robot type a nodemcu esp12 which is a microcontroller equipped with wifi module is applied in the robot the robot is connected to the blynk iot platform and paired to a smartphone it results in communication between the robot and the smartphone through the internet the communication is demonstrated by remotely operating the robot using the smartphone mechanical structure and electronic wiring of the robot are simple such that the robot is easily built moreover the cost of required components for building the robot is quite cheap as less than usd 20
this paper presents a novel fluxweakening control strategy for interior permanent magnet synchronous motors the proposed method achieves the extended dclink voltage utilization and improves the tracking performance in the proposed flux regulator the stator flux linkage reference is adjusted based on the modulation index torque reference and motor speed the voltage trajectory is extended to the overmodulation region to increase the dclink voltage utilization rate and the output torque moreover a current predictive controller is implemented to improve the dynamic tracking performance the proposed fluxweakening method is insensitive to parameters variation and shows fast dynamic performance simulations are carried out to validate the proposed method
the nonmarkovianity of the stochastic process called the quantum semimarkov qsm process is studied using a recently proposed quantification of memory based on the deviation from semigroup evolution that provides a unified description of divisible and indivisible channels this is shown to bring out the property of qsm processes to exhibit memory effects even in the cpdivisible regime in agreement with an earlier result an operational meaning to the nonmarkovian nature of semimarkov processes is also provided
"we construct a privacypreserving distributed and decentralized marketplace where parties can exchange data for tokens in this market buyers and sellers make transactions in a blockchain and interact with a third party called notary who has the ability to vouch for the authenticity and integrity of the data 
we introduce a protocol for the datatoken exchange where neither party gains more information than what it is paying for and the exchange is fair either both parties gets the others item or neither does no third party involvement is required after setup and no dispute resolution is needed"
the quantitative study of cell morphology is of great importance as the structure and condition of cells and their structures can be related to conditions of health or disease the first step towards that is the accurate segmentation of cell structures in this work we compare five approaches one traditional and four deeplearning for the semantic segmentation of the nuclear envelope of cervical cancer cells commonly known as hela cells images of a hela cancer cell were semantically segmented with one traditional imageprocessing algorithm and four three deep learning architectures vgg16 resnet18 inceptionresnetv2 and unet three hundred slices each 2000 × 2000 pixels of a hela cell were acquired with serial block face scanning electron microscopy the first three deep learning architectures were pretrained with imagenet and then finetuned with transfer learning the unet architecture was trained from scratch with 36 000 training images and labels of size 128 × 128 the imageprocessing algorithm followed a pipeline of several traditional steps like edge detection dilation and morphological operators the algorithms were compared by measuring pixelbased segmentation accuracy and jaccard index against a labelled ground truth the results indicated a superior performance of the traditional algorithm accuracy  99 jaccard  93 over the deep learning architectures vgg16 93 90 resnet18 94 88 inceptionresnetv2 94 89 and unet 92 56
multitask learning can leverage information learned by one task to benefit the training of other tasks despite this capacity naı̈ve formulations often degrade performance and in particular identifying the tasks that would benefit from cotraining remains a challenging design question in this paper we analyze the dynamics of information transfer or transference across tasks throughout training specifically we develop a similarity measure that can quantify transference among tasks and use this quantity to both better understand the optimization dynamics of multitask learning as well as improve overall learning performance in the latter case we propose two methods to leverage our transference metric the first operates at a macrolevel by selecting which tasks should train together while the second functions at a microlevel by determining how to combine task gradients at each training step we find these methods can lead to significant improvement over prior work on three supervised multitask learning benchmarks and one multitask reinforcement learning paradigm
sugar based xanthates viz dsorbitol xanthate and dmannitol xanthate dsx1 dsx2 dmx1 and dmx2 have been synthesized by the reaction of dsorbitol ds dmannitol dm with carbon disulfide cs2 in basic medium at room temperature by varying carbon disulphide and base ratio with respect to sugars the synthesized xanthates were extracted with ether and dried in air the xanthates were characterized by micro analytical data elemental analysis c h o s fourier transform infrared ftir spectroscopy and h nuclear magnetic resonance 1hnmr spectroscopy the synthesized xanthates were used for the removal of cuand ni form aqueous solutions the metal complexes formed during the process of removal of metal ions were confirmed with ftir spectroscopy the removal of copper and nickel metal from water was quantitatively determined by uvvisible absorption spectroscopy
multiphase machines have been attracting more and more attentions in highreliabilityrequired applications due to their inherent faulttolerant capability postfault control strategies withstanding opencircuit faults ocfs for several multiphase machine types like fivephase symmetrical sixphase s6 and asymmetrical sixphase a6 machines have been widely investigated in recent literatures however faulttolerant control for symmetrical dual threephase d3 machines have rarely been studied so far to fill the gap this paper addresses key issues in postfault decoupling modeling and field orient control foc for the dual threephase surfacemounted permanent magnet synchronous machine spmsm with isolated neutrals under singlephase ocf to do so a postfault decoupling model with reducedorder transformation is established for d3 machines postfault current references are reconfigured with two mainstream criteria minimumloss ml and maximumtorque mt furthermore third harmonic flux linkagebackemf are taken into account in modeling and control since it causes third harmonic currents and hence torque ripples under singlephase ocf proportionalresonant controllers are employed to decrease the third harmonic currents and torque pulsations the validity of the postfault control strategies is proved by experiments
this paper describes an improvement of a matlab based supervisory and data acquisition scada system the scada system was developed and implemented for controlling and monitoring a physical model of a micro grid at power system stability laboratory pssl of technical university of sofia the scada system is consisted of different functional blocks such as block for reading writing and uploading on a server the microgrid parameters
as the number of computer science cs jobs become increasingly available in this country and computing skills become essential tools for managing all aspects of our personal lives cs is quickly becoming an essential element of k12 education and recently there has been increased attention to bringing computer science to the elementary grades however with a schedule that emphasizes literacy and mathematics and other subjects competing for instructional time creating opportunities for cs in the elementary school day is challenging this study aimed to address this problem by investigating the use of problembased transdisciplinary modules ie “time4cs” modules that combined english language arts ela science and social studies lessons with the codeorg “fundamentals” cs education program results indicated that teachers who taught time4cs modules completed more cs lessons than teachers who did not teach the modules further across all classrooms completing a higher percentage of nongrade level assigned codeorg fundamentals lessons ie codeorg lessons above or below grade level that were available to teachers but not required for their particular grade level was positively associated with students’ achievement outcomes on state ela and mathematics tests additionally higher amounts of interdisciplinary teaching practices were associated with higher student achievement specifically students’ state assessment ela scores this study demonstrated that transdisciplinary problembased modules that integrate the teaching of cs with other subject areas are a feasible way to bring more cs opportunities to younger learners moreover it showed that implementing such modules is linked to more positive student academic achievement outcomes with attentive revision the modules featured in this study may be useful tools for elementary schools these findings have implications for researchers school district administrators and those individuals who are incharge of public policy initiatives seeking ways to bring cs to all elementary school students specifically they highlight that it is possible to make time in the elementary school day for cs and that there are no negative consequences for core subjects eg ela and mathematics
this paper illustrates the deployment of the background subtraction bgs approach in detecting and tracking the targeted moving objects mos through the bgs method there is a potential of costsaving because the process of storing data occurs once the motion is detected the aim was to detect the mos effectively the applied technique is applicable at all scenarios and places that need the realtime video surveillance systems vss including airports forest frequent entrances for criminals traffic monitoring country borders cash machines schools banks among other challenging outdoor and indoor areas the concept of installing vss is substantially much needed the bgs method was employed with fewer complications so that the approach can be utilised in a pragmatic manner conditions where the vss is highly needed the vss must be more convenient effective and efficient to enhance advanced security systems © 2020 published by faculty of engineeringg
fire is a powerful ecological and evolutionary force that regulates organismal traits population sizes species interactions community composition carbon and nutrient cycling and ecosystem function it also presents a rapidly growing societal challenge due to both increasingly destructive wildfires and fire exclusion in fire‐dependent ecosystems as an ecological process fire integrates complex feedbacks among biological social and geophysical processes requiring coordination across several fields and scales of study here we describe the diversity of ways in which fire operates as a fundamental ecological and evolutionary process on earth we explore research priorities in six categories of fire ecology a characteristics of fire regimes b changing fire regimes c fire effects on above‐ground ecology d fire effects on below‐ground ecology e fire behaviour and f fire ecology modelling we identify three emergent themes the need to study fire across temporal scales to assess the mechanisms underlying a variety of ecological feedbacks involving fire and to improve representation of fire in a range of modelling contexts synthesis as fire regimes and our relationships with fire continue to change prioritizing these research areas will facilitate understanding of the ecological causes and consequences of future fires and rethinking fire management alternatives
the detection and correction of errors during memory read operation performed per clock cycle play a significant role in atspeed testing of embedded memories the majority of key role applications like in satellites medical database etc require a faster and perfect recovery of data stored this paper aims at processing the multiple adjacent errors per clock cycle the codes use xoring of data bits to obtain parity bits and extract the syndrome for error evaluation and correction the devised code is capable of correcting 32 adjacent data bits among 64 data bits than the existing codes the encoder and decoder codes are modeled in verilog hdl and verified in xilinx vivado tool for the zynq 7000 series fpga xc7z0201clg484 interestingly the code rate can be increased with lower bit overhead
the classical belief rulebased brb systems are usually constructed by arranging and combining referential values of antecedent attributes or by setting special fixed values which can lead to overly large size of brb systems in complex problems this paper combines the decision tree classification method to analyze the information of data and extract the rules based on this a new rule representation method with referential interval is proposed and the rule base is constructed according to the support degree and belief degree of the data in the newly proposed method the introduction of decision tree ensures that the size of the rule base is reasonable moreover the rule parameters trained by the differential evolution de algorithm are optimized and adjusted to further improve the system performance the experiments are conducted on several commonly used public classification datasets and the proposed algorithm can achieve better accuracy results compared with classical classification methods and the existing classification methods of brb systems on average the experimental results validate the reasonableness and effectiveness of the brb construction method proposed in this paper
a compilation of zz polynomials aka zhang–zhang polynomials or clar covering polynomials for all isomers of small 56fullerenes cn with n  20–50 is presented the zz polynomials concisely summarize the most important topological invariants of the fullerene isomers the number of kekulé structures k the clar number cl the first herndon number h1 the total number of clar covers c and the number of clar structures the presented results should be useful as benchmark data for designing algorithms and computer programs aiming at topological analysis of fullerenes and at generation of resonance structures for valencebond quantumchemical calculations
content moderation has exploded as a policy advocacy and public concern but these debates still tend to be driven by highprofile incidents and to focus on the largest us based platforms in order to contribute to informed policymaking scholarship in this area needs to recognise that moderation is an expansive sociotechnical phenomenon which functions in many contexts and takes many forms expanding the discussion also changes how we assess the array of proposed policy solutions meant to improve content moderation here nine content moderation scholars working in critical internet studies propose how to expand research on content moderation with implications for policy
graph neural networks gnns which extend deep neural networks to graphstructured data have attracted increasing attention they have been proven to be powerful for numerous graph related tasks such as graph classification link prediction and node classification to adapt gnns to graph classification recent works aim to learn graphlevel representation through a hierarchical pooling procedure one major direction is to select important nodes to hierarchically coarsen the input graph and gradually reduce the information into the graph representation however most of the existing methods only select important nodes which can be redundant and cannot represent the original graph well meanwhile the information of nonselected nodes is often overlooked when generating a new coarser graph which may lead to the tremendous loss of important structural and node feature information in this paper we propose a novel pooling operator reppool to learn hierarchical graph representations specifically we introduce the concept of representativeness that is combined with the importance for node selection and we provide a learnable way to integrate nonselected nodes by combining the reppool operator with conventional gcn convolutional layers a hierarchical graph classification architecture is developed extensive experiments on various public benchmarks have demonstrated the effectiveness of the proposed method the implementation of the proposed framework is available11httpsgithubcomjuanhui28reppooltreemasterreppool
active lambwavebased structural health monitoring techniques have been widely studied to inspect large structures using permanently installed arrays of sensors and actuators most of these methods depend on comparing baseline signals recorded from the structure before going into service and test signals acquired during inspection temperature changes affect the propagation of the wave in a nonlinear and modedependent manner as a result baseline comparison methods fail when the test and baseline signals are acquired at vastly different temperatures approximate methods that compensate for the effects of temperature on the waves using signal stretch models have been introduced in the literature these methods are effective when the temperature changes are small and the propagation distances are short however they perform poorly when these conditions are not satisfied consequently there is a need for better temperature compensation algorithms than presently available this article presents a datadriven approach that separately compensates for the effects of temperature on different mode components of the sensor signals the performance of the temperature compensation algorithm of this article is compared with that of a commonly used baseline signal stretch bss algorithm using experimental signals measured from an aluminum panel and a unidirectional composite panel analysis results indicate that the method of this article outperforms the bss algorithm for large temperature differences the usefulness of the temperature compensation algorithm is further validated by demonstrating the ability of compensated signals to accurately reconstruct anomaly maps associated with damaged composite structures
diseases that should be considered in the differential diagnosis of hashimotos thyroiditis ht include subacute thyroiditis and reidles thyroiditis graves disease euthyroid sick syndrome goiter hypopituitarism lithiuminduced goiter simple nontoxic goiter thyroid lymphoma toxic nodular goiter and types i and ii polyglandular autoimmune syndrome the characteristic diagnostic clinical biochemical imaging sonographic and histologicalcytological features of ht will help make a differential diagnosis this chapter explores the differential diagnosis of hashimotos disease
this article introduces a novel scheme termed coded compressed sensing for unsourced multipleaccess communication the proposed divideandconquer approach leverages recent advances in compressed sensing and forward error correction to produce a novel uncoordinated access paradigm along with a computationally efficient decoding algorithm within this framework every active device partitions its data into several subblocks and subsequently adds redundancy using a systematic linear block code compressed sensing techniques are then employed to recover subblocks up to a permutation of their order and the original messages are obtained by stitching fragments together using a treebased algorithm the error probability and computational complexity of this access paradigm are characterized an optimization framework which exploits the tradeoff between performance and computational complexity is developed to assign paritycheck bits to each subblock in addition two emblematic parity bit allocation strategies are examined and their performances are analyzed in the limit as the number of active users and their corresponding payloads tend to infinity the number of channel uses needed and the computational complexity associated with these allocation strategies are established for various scaling regimes numerical results demonstrate that coded compressed sensing outperforms other existing practical access strategies over a range of operational scenarios
this paper gives an empirical study of various pseudorandom number generators prng along with their brief history from the traditional ones such as middle square method to the most advanced ones like xoroshiro128 along with various modifications that were added to them comparing them on the basis of their computational times code complexities and the algorithms used by them it can be deduced that which of the prng is the fastest or which gives the most equidistant random numbers and which generator is most useful for a particular purpose this paper doesn’t cover any cryptographically secure pseudorandom number generator csprng and all the generators mentioned henceforth are cryptographically insecure
the spread of deep learning on embedded devices has prompted the development of numerous methods to optimize the deployment of deep neural networks dnns works have mainly focused on 1 efficient dnn architectures 2 network optimization techniques such as pruning and quantization 3 optimized algorithms to speed up the execution of the most computational intensive layers and 4 dedicated hardware to accelerate the data flow and computation however there is a lack of research on crosslevel optimization as the space of approaches becomes too large to test and obtain a globally optimized solution thus leading to suboptimal deployment in terms of latency accuracy and memory in this work we first detail and analyze the methods to improve the deployment of dnns across the different levels of software optimization building on this knowledge we present an automated exploration framework to ease the deployment of dnns the framework relies on a reinforcement learning search that combined with a deep learning inference framework automatically explores the design space and learns an optimized solution that speeds up the performance and reduces the memory on embedded cpu platforms thus we present a set of results for stateoftheart dnns on a range of arm cortexa cpu platforms achieving up to 4times  improvement in performance and over 2times  reduction in memory with negligible loss in accuracy with respect to the blas floatingpoint implementation
we present a courseware design and implementation for preengineering university summer school outreach program intended for 6th to 12th grade pupils k12 the course focuses on building hardware and software for three iot devices of increasing complexity and in doing so introduces various important computer science and engineering concepts device design constraints allow for very low cost of the built prototypes 5  10 € per device we used nodemcu with esp8266 as a hardware platform lua as a part of nodemcu firmware was used as a programming language due to simple syntax possible interactive use and compact source code that consisted of about 150 lines of code on average for each device pupils were provided with source code templates and encouraged to supplement the code with missing parts the approach was scaled to primary and secondary schools’ curricula by organizing and evaluating a continuing education course for teachers in preuniversity education
the advancement of machine learning ml models has received remarkable attention by several science and engineering applications within the material engineering ml models are usually utilized for building an expert system for supporting material design and attaining an optimal formulation material sustainability and maintenance the current study is conducted on the based of the utilization of ml models for modeling compressive strength cs of ground granulated blast furnace slag concrete ggbfsc random forest rf model is developed for this purpose the predictive model is constructed based on multiple correlated properties for the concrete material including coarse aggregate ca curing temperature t ggbfsc to total binder ratio ggbfscb water to binder ratio wb water content w fine aggregate fa superplasticizer sp a total of 268 experimental dataset are gather form the opensource previous published researches are used to build the predictive model for the verification purpose a predominant ml model called support vector machine svm is developed the efficiency of the proposed predictive and the benchmark models is evaluated using statistical formulations and graphical presentation based on the attained prediction accuracy rf model demonstrated an excellent performance for predicting the cs using limited input parameters overall the proposed methodology showed an exceptional predictive model that can be utilized for modeling compressive strength of ggbfsc
abstract the internet of things which was known in the era of the industrial revolution 40 can make it easier for teenagers to get information and access the internet everywhere excessive use of the internet and always sacrificing important activities by teenagers can be called information technology addiction examples of information technology addictions that have a negative impact are aggressive and nonaggressive online game addiction the negative impact of online game addiction for adolescents is the disruption of social development social interaction selfesteem sympathy affection for others and the inability to control themselves the study aimed to examine the description of social intelligence love and selfregulation in adolescents with aggressive and nonaggressive online game addiction the research population of adolescents from junior high and high school education levels in malang city this research uses a quantitative approach with a survey design the instruments used were an online game addiction questionnaire a social intelligence questionnaire a love questionnaire and a selfregulation questionnaire which was tested for validity and reliability using the rasch model the results showed 1 the majority of adolescents experienced the more aggressive type of online game addiction than nonaggressive online games 2 the majority of adolescents experiencing aggressive and nonaggressive types of online game addiction have low social intelligence love and selfregulation abstrak internet of things yang dikenal pada era revolusi industri 40 dapat mempermudah generasi remaja untuk mendapatkan informasi dan mengakses internet dimanamanapenggunaan internet yang berlebihan dan selalu mengorbankan kegiatan penting oleh remaja dapat disebut dengan adiksi teknologi informasi contoh adiksi teknologi informasi yang berdampak negatif yaitu adiksi game online jenis agresif dan non agresif dampak negatif dari adiksi game online bagi remaja adalah terganggunya pengembangan sosial interaksi sosial harga diri simpati kasih sayang kepada orang lain dan tidak mampu mengontrol dirinya tujuam penelitian adalah untuk menelaah gambaran social intelligence love  dan selfregulation pada remaja yang adiksi game online jenis agresif dan non agresif populasi penelitian remaja yang dari jenjang pendidikan smp dan sma di kota malang penelitian menggunakan pendekatan kuanititif dengan desain survei instrumen yang digunakan adalah kuesioner adiksi game online kuesioner social intelligence  kuesioner love  dan kuesioner selfregulation yang di uji valid dan realibitas menggunakan rasch model hasil penelitian menunjukkan 1 sebagian besar remaja lebih banyak mengalami adiksi game online jenis agresif dibandingkan game online jenis non agresif 2 sebagian besar remaja mengalami adiksi game online jenis agresif dan non agresif memiliki social intellogence  love  dan selfregulation rendah
the objective of this research is to describe the effect of social media use by medical professionals in facilitating the treatment of chronic illnesses in chronically ill individuals this study focuses on the prominent social media used by medical professionals and the way it is used to manage health conditions of chronically ill individuals
abstract despite significant improvement in moderate resolution imaging spectroradiometer modis aerosol optical depth aod retrieval highresolution–highaccuracy aod retrieval remains a challenging task this study utilizes machine learning for aod retrieval of modis data the global longtimeseries data of aeronet sites and corresponding modis data in time and space were used as sample training data for aerosol retrieval via use of the random forest rf approach the accuracy and stability of retrieval were ensured by processing aeronet site data performing time–space matching between different data types and determining related parameters in the rf model modis data use bands 1–7 of the topofatmosphere reflectance toa – extending from the visible to nearinfrared radiation spectra – along with the corresponding observation geometry data global land surface satellite glass albedo dataset and normalized difference vegetation index ndvi data the proposed method facilitates realizationthe of highaccuracy aerosol retrieval furthermore significant enhancement in the efficiency of aerosol inversion is an added advantage
modern lane detection methods have achieved remarkable performances in complex realworld scenarios but many have issues maintaining realtime efficiency which is important for autonomous vehicles in this work we propose laneatt an anchorbased deep lane detection model which akin to other generic deep object detectors uses the anchors for the feature pooling step since lanes follow a regular pattern and are highly correlated we hypothesize that in some cases global information may be crucial to infer their positions especially in conditions such as occlusion missing lane markers and others thus this work proposes a novel anchorbased attention mechanism that aggregates global information the model was evaluated extensively on three of the most widely used datasets in the literature the results show that our method outperforms the current stateoftheart methods showing both higher efficacy and efficiency moreover an ablation study is performed along with a discussion on efficiency tradeoff options that are useful in practice code and models are available at httpsgithubcomlucastabelinilaneatt
by applying the weight functions and the idea of introduced parameters we give a new hilberttype integral inequality involving the upper limit functions and the beta and gamma functions we consider equivalent statements of the best possible constant factor related to a few parameters as applications we obtain a corollary in the case of a nonhomogeneous kernel and some particular inequalities
abstract background restriction of food intake is a central feature of anorexia nervosa an and other eating disorders yet also occurs in the absence of psychopathology the neural mechanisms of restrictive eating in health and disease are unclear methods this study examined behavioral and neural mechanisms associated with restrictive eating among individuals with and without eating disorders dietary restriction was examined in four groups of women n  110 healthy controls dieting healthy controls patients with subthreshold nonlow weight an and patients with an a food choice task was administered during fmri scanning to examine neural activation associated with food choices and a laboratory meal was conducted results behavioral findings distinguished between healthy and ill participants healthy individuals both dieting and nondieting chose significantly more highfat foods than patients with an or subthreshold an among healthy individuals choice was primarily influenced by tastiness whereas among both patient groups healthiness played a larger role dorsal striatal activation associated with choice was most pronounced among individuals with an and was significantly associated with selecting fewer highfat choices in the task and lower caloric intake in the meal the following day conclusions a continuous spectrum of behavior was suggested by the increasing amount of weight loss across groups yet data from this food choice task with fmri suggest there is a behavioral distinction between illness and health and that the neural mechanisms underlying food choice in an are distinct these behavioral and neural mechanisms of restrictive eating may be useful targets for treatment development
computermediated communication cmc portal services provision through information technologies it in higher educational institutions heis should not be an impossible task considering the growth in information systems is and an upsurge of internet users there have been numerous efforts aimed at implementing cmc portals by heis but just a few could be said to be successful the aim of this research is to develop a framework that can help to provide a better understanding of how to manage the entire implementation process so as to bring the expected advantages to institutions implementing it results of past research had been studied to develop the conceptual framework utilizing significant theories in the field of information system implementation and institutional change the framework addresses the adoption implementation as well as institutionalization stages of cmc portal implementation and a number of institutional contextual factors influencing implementation efforts within the stages
this study presents the results of a congressionally mandated independent assessment of federally funded health services research hsr and primary care research pcr spanning the us department of health and human services hhs and us department of veterans affairs va from fys 2012 to 2018 through technical expert panels stakeholder interviews and a systematic environmental scan of research grants and contracts funded by hhs and the va the authors characterize the distinct contributions of agencies in these departments to the federal hsr and pcr enterprise the authors also identify opportunities to improve detection and coordination of overlap in agency research portfolios the impacts of hsr and pcr and how they cumulate across research portfolios and gaps in research funding methods and dissemination the authors offer recommendations to maximize the outcomes and value of future investments in federal hsr and pcr to better guide and serve the needs of a complex and rapidly changing us health care system
ipv6 routing protocol for lowpower and lossy networks rpl is the standard network layer protocol for achieving efficient routing in ipv6 over lowpower wireless personal area networks 6lowpan resourceconstrained and nontamper resistant nature of smart sensor nodes makes rpl protocol susceptible to different threats an attacker may use insider or outsider attack strategy to perform denialofservice dos attacks against rpl based networks security and privacy risks associated with rpl protocol may limit its global adoption and worldwide acceptance a proper investigation of rpl specific attacks and their impacts on an underlying network needs to be done in this paper we present and investigate one of the catastrophic attacks named as a copycat attack a type of replay based dos attack against the rpl protocol an indepth experimental study for analyzing the impacts of the copycat attack on rpl has been done the experimental results show that the copycat attack can significantly degrade network performance in terms of packet delivery ratio average endtoend delay and average power consumption to the best of our knowledge this is the first paper that extensively studies the impact of rpl specific replay mechanism based dos attack on 6lowpan networks
ehailing platforms have become an important component of public transportation in recent years the supply online drivers and demand passenger requests are intrinsically imbalanced because of the pattern of human behavior especially in time and locations such as peak hours and train stations hence how to balance supply and demand is one of the key problems to satisfy passengers and drivers and increase social welfare as an intuitive and effective approach to address this problem driver repositioning has been employed by some realworld ehailing platforms in this paper we describe a novel framework of driver repositioning system which meets various requirements in practical situations including robust driver experience satisfaction and multidriver collaboration we introduce an effective and userfriendly driver interaction design called “driver repositioning task” a novel modularized algorithm is developed to generate the repositioning tasks in real time to our knowledge this is the first industrylevel application of driver repositioning we evaluate the proposed method in realworld experiments achieving a 2 improvement of driver income our framework has been fully deployed in the online system of didi chuxing and serves millions of drivers on a daily basis
we address the estimation of the ricean inlineformula texmath notationlatexk texmathinlineformula factor when the available complex channel samples are noisy and subject to nakagamiinlineformula texmath notationlatexm texmathinlineformula shadowing ie the lineofsight component is modeled as a nakagamiinlineformula texmath notationlatexm texmathinlineformula random variable we propose two estimators one based on the expectationmaximization em procedure and a second one based on the method of moment mom the mom estimator can be used to initialize the em procedure we show by simulations that the two proposed estimators outperform the existing ones
motivation the dynamic development of innovation in the financial market and the process of globalisation were at the heart of creating a new financial technology sector called fintech in order to allow for a safe and intensive development of innovations and create opportunities for all entities including startups state supervisors and regulators create dedicated market environments — regulatory sandboxes aim 1 to define the concept of a regulatory sandbox 2 to identify the forms of support for innovative solutions in the financial market 3 to identify the outcomes of a regulatory sandbox in the case of the united kingdom results regulatory sandboxes are a new supervisory tool which despite only a few years of history has gained recognition of financial market participants research has shown that a regulatory sandbox has many benefits not only for the companies joining it but also for the supervisory institution sandboxes allow participants to receive continuous substantive support obtain licences faster and resolve legal doubts participation in a regulatory sandbox is also a form of promotion and facilitates raising investment capital on the other hand regulatory sandboxes are still in the early stages of development and the support they provide is limited some companies also experience problems in dealing with the long and complicated process of applying for admission and the limited number of participants the latter obstacle may result in unequal competition on the market and failure to fully exploit the development potential of fintech the most important benefit for a supervisor is that through its engagement in a regulatory sandbox it gains additional knowledge of new technologies and new business models and its employees develop important competences the dialogue conducted with professional financial market participants allows for a better assessment of the risks associated with new technologies the first outcomes of participation in a regulatory sandbox are encouraging for new innovative players and supervisory authorities it is recommended that this initiative be extended in order to enable more market players to conduct testing
abstract we consider the hpolarized plane wave scattering from an infinite flat grating of perfectly electrically conducting strips placed on the interface of a dielectric slab we reduce this problem to a dual series equation for the complex amplitudes of the floquet spatial harmonics then we perform analytical regularization of this equation based on the inversion of the static part of the problem with the aid of the riemannhilbert problem this yields a fredholm secondkind infinite matrix equation numerical solution of which has a guaranteed convergence numerical results obtained demonstrate how the rate of convergence depends on the geometrical parameters and then concentrate on the resonance effects in the reflection and transmission we reveal and discuss ultrahighq resonances on the lattice modes of such a composite grating overlooked in earlier studies
abstractafter 5 years in orbit the global precipitation measurement gpm mission has produced enough qualitycontrolled data to allow the first validation of their precipitation estimates over sp
in this paper a dual halfbridge llc resonant converter with magnetic control is proposed for the battery charger application the primary switches are shared by two llc resonant networks and their outputs are connected in series one of the llc resonant converters is designed to operate at the series resonant frequency which is also the highest efficiency operating point and the constant output voltage characteristic is achieved at this operating point the second llc resonant converter adopts magnetic control to regulate the total output current and voltage during both constant current charge mode and constant voltage charge mode meanwhile the function decoupling idea is adopted to further improve the system efficiency the significant amount of the power is handled by the llc resonant converter operating at the series resonant frequency whereas the second llc resonant converter fulfills the responsibility to achieve closedloop control by carefully designing the resonant networks the zerovoltage switching for primary switches and zerocurrent switching for secondary diodes can be achieved for whole operation range a 320w experimental prototype is built to verify the theoretical analysis and the maximum efficiency is measured about 955
language is crucial for human intelligence but what exactly is its role we take language to be a part of a system for understanding and communicating about situations in humans these abilities emerge gradually from experience and depend on domaingeneral principles of biological neural networks connectionbased learning distributed representation and contextsensitive mutual constraint satisfactionbased processing current artificial language processing systems rely on the same domain general principles embodied in artificial neural networks indeed recent progress in this field depends on querybased attention which extends the ability of these systems to exploit context and has contributed to remarkable breakthroughs nevertheless most current models focus exclusively on languageinternal tasks limiting their ability to perform tasks that depend on understanding situations these systems also lack memory for the contents of prior situations outside of a fixed contextual span we describe the organization of the brain’s distributed understanding system which includes a fast learning system that addresses the memory problem we sketch a framework for future models of understanding drawing equally on cognitive neuroscience and artificial intelligence and exploiting querybased attention we highlight relevant current directions and consider further developments needed to fully capture humanlevel language understanding in a computational system
mobility implies a great variability of capturing conditions which is not easy to control and directly affects to face detection and the extraction of facial features deep learning solutions seem to be the most interesting choice for automatic face recognition but they are highly dependent on the model generated during the training stage in addition the size of the models makes it difficult for their integration into applications oriented to mobile devices particularly when the model must be embedded in this work a smallsize deeplearning model was trained for face recognition on low capacity devices and evaluated in terms of accuracy size and timings to provide quantitative data this evaluation is aimed to cover as many scenarios as possible so different databases were employed including public and private datasets specifically oriented to recreate the complexity of mobile scenarios also publicly available models and traditional approaches were included in the evaluation to carry out a fair comparison moreover given the relevance of template matching and face detection stages the assessment is complemented with different classifiers and detectors finally a javaandroid implementation of the system was developed and evaluated to obtain performance data of the whole system integrated on a mobile phone
the lena kolyma and indigirka rivers are among the largest rivers that inflow to the arctic ocean their discharges form a freshened surface water mass over a wide area in the laptev and eastsiberian seas and govern many local physical geochemical and biological processes in this study we report coastal upwelling events that are regularly manifested on satellite imagery by increased sea surface turbidity and decreased sea surface temperature at certain areas adjacent to the lena delta in the laptev sea and the kolyma and indigirka deltas in the eastsiberian sea these events are formed under strong easterly and southeasterly wind forcing and are estimated to occur during up to 10–30 of icefree periods at the study region coastal upwelling events induce intense mixing of the lena kolyma and indigirka plumes with subjacent saline sea these plumes are significantly transformed and diluted while spreading over the upwelling areas therefore their salinity and depths abruptly increase while stratification abruptly decreases in the vicinity of their sources this feature strongly affects the structure of the freshened surface layer during icefree periods and therefore influences circulation ice formation and many other processes at the laptev and eastsiberian seas
in this brief we consider a sparse linearphase fir filter design problem recent methods assume that all the coefficients can be nullified and thus various 0 or 1normbased optimization techniques are applied on each of them in contrast the proposed algorithm is based on two important observations 1 given design specifications some coefficients cannot be nullified otherwise the specifications cannot be satisfied 2 impulse responses on neighboring positions of an fir filter cannot vary dramatically so as to guarantee the smoothness of the corresponding magnitude responses over most of frequencies in view of these facts several rules are adopted in the proposed algorithm to select indices of potential zero coefficients to be used in 1norm optimization simulation results have demonstrated the effectiveness of the proposed design algorithm
this work is motivated by the recent advances in deep neural networks dnns and their widespread applications in humanmachine interfaces dnns have been recently used for detecting the intended hand gesture through the processing of surface electromyogram semg signals objective although dnns have shown superior accuracy compared to conventional methods when large amounts of data are available for training their performance substantially decreases when data are limited collecting large datasets for training may be feasible in research laboratories but it is not a practical approach for reallife applications the main objective of this work is to design a modern dnnbased gesture detection model that relies on minimal training data while providing high accuracy methods we propose the novel fewshot learning hand gesture recognition fshgr architecture fewshot learning is a variant of domain adaptation with the goal of inferring the required output based on just one or a few training observations the proposed fshgr generalizes after seeing very few observations from each class by combining temporal convolutions with attention mechanisms this allows the metalearner to aggregate contextual information from experience and to pinpoint specific pieces of information within its available set of inputs data source  summary of results the performance of fshgr was tested on the second and fifth ninapro databases referred to as the db2 and db5 respectively the db2 consists of 50 gestures rest included from 40 healthy subjects the ninapro db5 contains data from 10 healthy participants performing a total of 53 different gestures rest included the proposed approach for the ninapro db2 led to 8594 classification accuracy on new repetitions with fewshot observation 5way 5shot 8129 accuracy on new subjects with fewshot observation 5way 5shot and 7336 accuracy on new gestures with fewshot observation 5way 5shot moreover the proposed approach for the ninapro db5 led to 6465 classification accuracy on new subjects with fewshot observation 5way 5shot
this article presents a learning robust controller for highquality position tracking control of robot manipulators a basic timedelay estimator is adopted to effectively approximate the system dynamics a lowlevel control layer is structured from the control error as an indirect control objective using new nonlinear slidingmode synthetization to realize the control objective with desired transient time a robust sliding mode control signal is then designed based on the obtained estimation results in a highlevel control layer to promptly suppress unpredictable disturbances adaptation ability is integrated to the controller using twolevel gainlearning laws reaching gains and sliding gain are automatically tuned for asymptotic control performance effectiveness of the designed controller is concretely confirmed by the lyapunovbased stability criterion comparative simulations and realtime experiments
the paper is devoted to solutions of the third order pseudoelliptic type equations an energy estimates for solutions of the equations considering transformation’s character of the body form were established by using of an analog of the saintvenant principle in consequence of this estimate the uniqueness theorems were obtained for solutions of the first boundary value problem for third order equations in unlimited domains the energy estimates are illustrated on two examples
feedbackcontrolled electromigration fce has been employed to control atomic junctions with quantized conductance an fce scheme is controlled by many parameters such as the threshold differential conductance gth feedback voltage vfb and voltage step vstep it is considered possible to achieve a precise and stable control of the quantized conductance by automatically optimizing the fce parameters this motivated us to develop an approach based on machine learning ml to tune the feedback parameters of fce the ml system is composed of three kinds of engines namely learning evaluation and inference the learning engine performs the fce procedure with random parameters collects various experimental data and updates the database subsequently four variables and a cost function are defined to evaluate the controllability of the quantized conductance the evaluation engine scores the experimental data by using the defined cost function then the control quality is evaluated in real time during the fce procedure the inference engine selects the new fce parameter according to the evaluated data these engines determine the optimal parameters without human intervention and according to the situation finally we actually applied this system to the fce procedure the parameter is selected from sample data in the database according to the variation in controllability as a result the controllability gradually improves during the fce procedure that uses the ml system the results indicate that the proposed ml system can evaluate the controllability of the fce procedure and change the vfb parameter in real time according to the situation
person reidentification in the wild needs to simultaneously framewise detect and reidentify persons and has wide utility in practical scenarios however such tasks come with an additional openset reid challenge as all probe persons may not necessarily be present in the framewise dynamic gallery traditional or closeset reid systems are not equipped to handle such cases and raise several false alarms as a result to cope with such challenges openset metric learning osml based on the concept of large margin nearest neighbor lmnn approach is proposed we term our method openset lmnn oslmnn the goal of separating impostor samples from the genuine samples is achieved through a joint optimization of the weibull distribution and the mahalanobis metric learned through this oslmnn approach the rejection is performed based on low probability over distance of imposter pairs exhaustive experiments with other metric learning techniques over the publicly available prw dataset clearly demonstrate the robustness of our approach
dynamic slicing and its underlying dynamic dependence analysis have been extensively studied and used as the foundation for numerous automateddebugging techniques one limitation of dynamic slicing when used for debugging is that it only considers program dependences that are actually observed during the executions of interest some faults however involve potential rather than actual dependences—dependences that would be observed if the correct program was executed but are missing when the faulty program is executed in particular traditional dynamic slicing may fail to locate faults that involve assignments that should have occurred in a correct execution and did not occur in the failing execution being debugged relevant slicing techniques partially address this problem by identifying missing assignments due to incorrect controlflow however they do not consider the case of assignments that do occur but modify the wrong memory location eg the wrong element of an array debugging techniques based on existing dynamic slicing approaches may therefore miss faults in the presence of this kind of incorrect assignments to address this problem we introduce the concept of potential memoryaddress dependence pmd intuitively pmds represent the dependence relationship between an instruction s that affects the computation of a memoryaddress ma eg by defining an array index or a pointer offset and memory read instructions that are not observed to be dependent on s but could be affected by s ie access the memory at ma in a counterfactual execution of s we also present a technique that computes pmds and represents them on standard dynamic dependence graphs to assess the effectiveness of our technique for debugging we implemented pmdslicer a dynamic slicer that accounts for pmds and performed an empirical evaluation on a benchmark of 364 real faults and 880 faultrevealing test cases our results are promising in that almost 10 of the failing tests contained cases in which pmdslicer generated slices that included the corresponding fault while a traditional dynamic slicer did not furthermore considering pmds only moderately increased slice sizes
the continuous development of online social services and the everincreasing number of devices demanding highspeed and ubiquitous broadband wireless access have resulted in severe bandwidth congestion such that the radio frequency rf spectrum will no longer be able to support the exponential growth in demand along with being faster cheaper greener cleaner and safer than current technology visible light communication vlc can overcome the bottleneck issues with lastmile connectivity by offering 10 000 times broader bandwidth in this paper we propose a system with portable lowcost hybrid rfvlc and freespace optics fso transceivers with a simple graphical user interface and the capability of indoor wireless communication and multimedia broadcasting thus presenting an economical and cablefree solution to various multimedia applications such as file transmission and realtime audio and video streaming the proposed system deploys vlc as a hotspot for data broadcasting within an enclosed room fso as the backbone for data transmission between multiple rooms and wifi for lightsoff mode preliminary results show a transmission rate of 1kbps at a maximum distance of 4 cm
orthogonal time frequency space otfs modulation is an emerging multicarrier technique designed in the delaydoppler domain it converts a timevarying channel into a 2d invariant channel in the delaydoppler domain through 2d transformations resulting in better performance gain over orthogonal frequency division multiplexing ofdm in highmobility scenarios in this paper we first derive the feasibility of threshold aided channel estimation for otfs systems considering the poor performance of classical threshold aided scheme with a single impulse under a high noisy condition we further propose a novel channel estimation scheme based on priori channel information ie maximum doppler shift and maximum multipath delay with multiple impulses to extract the diversity gain so as to improve the performance in addition the specific method for threshold selection is given on the basis of statistical signal processing the simulation results indicate that the proposed scheme can achieve better channel state information over the classical scheme
we suggest a provable and practical approximation algorithm for fitting a set p of n points in r d to a sphere here a sphere is represented by its center x ∈ r d and radius r  0  the goal is to minimize the sum ∑ p ∈ p ∣ p − x − r ∣ of distances to the points up to a multiplicative factor of 1 ± ε  for a given constant ε  0  over every such r and x our main technical result is a data summarization of the input set called coreset that approximates the above sum of distances on the original big set p for every sphere then an accurate sphere can be extracted quickly via an inefficient exhaustive search from the small coreset most articles focus mainly on sphere identification eg circles in 2 d image rather than finding the exact match in the sense of extent measures and do not provide approximation guarantees we implement our algorithm and provide extensive experimental results on both synthetic and realworld data we then combine our algorithm in a mechanical pressure control system whose main bottleneck is tracking a falling ball full open source is also provided
summary in this paper a happiness cups hcups system is proposed to bidirectionally convey the holdingcup motions of paired cups between two remote users to achieve this goal the hcups system uses three important components firstly paired cups are embedded with accelerometers and gyro sensors to transmit the threedimensional acceleration and angular signals to a motion recognizer application this application is designed on an android phone the sensors then receive the remotely recognized motions and ﬂash a speciﬁc color on the local cup’s rgbled via bluetooth secondly the application considers holdingcup motion recognition from the cup based on long shortterm memory lstm and sends the local recognized motion through an intermediate server to transmit to the remote paired cup via the internet finally an intermediate server is established and used to exchange and forward the recognized holdingcup motions between two paired cups in which ﬁve holdingcup motions including drinking horizontal shaking vertical shaking swaying and raising toasts are proposed and recognized by lstm the experimental results indicate that the recognition accuracy of the holdingcup motion can reach 973 when using our method
a transfer learningbased architecture for melanoma detection from dermatoscopic images is presented ten preexisting wellknown deep convolutional neural networks were retrained to detect melanoma using automatically preprocessed and segmented skin images preprocessing consisted of automatic hair removal from the dermatoscopic images and segmentation consisted of detection of the skin lesion area the evaluation dataset used consisted of lesions of several skin pathologies including melanoma and the experimental results showed that the best performing retrained deep convolutional neural network model was resnet101 with melanoma detection accuracy equal to 9772 and sensitivity equal to 8518
aims with rising rates of mental health disorders being reported globally it is imperative that we investigate economical and accessible ways to increase relaxation and reduce stress while there is a plethora of anecdotal evidence as to the positive effects of domestic crafts on mental wellbeing there is little empirical research in this area as such we aimed to explore perceived links between crochet and wellbeing methods an online survey was developed and piloted based on an existing tool that explored knitting and wellbeing the final survey was promoted through social media over a 6week period resulting in valid responses from 8391 individuals results most respondents were female 991 aged between 41 and 60 years 495 and living in 87 different countries many respondents reported crocheting for between 1 and 5 years 426 the three most frequent reasons reported for crocheting were to be creative 821 to relax 785 and for a sense of accomplishment 752 respondents reported that crochet made them feel calmer 895 happier 82 and more useful 747 there was a significant improvement in reported scores for mood before crocheting m  419 sd  107 and mood after crocheting m  578 sd  082 z  −6986 p  001 r  −056 content analysis of freetext responses identified five major themes 1 health benefits 2 process of crochet 3 personal connection 4 crochet as contribution and 5 online crochet communities conclusion the data suggests that crochet offers positive benefits for personal wellbeing with many respondents actively using crochet to manage mental health conditions and life events such as grief chronic illness and pain crochet is a relatively lowcost portable activity that can be easily learnt and seems to convey all of the positive benefits provided by knitting this research suggests that crochet can play a role in promoting positive wellbeing in the general population adding to the social prescribing evidence base
the ideas algorithms and models developed for application in one particular domain can be applied for solving similar issues in a different domain using the modern concept termed as transfer learning the connection between spatiotemporal forecasting of traffic and video prediction is identified in this paper with the developments in technology traffic signals are replaced with smart systems and video streaming for analysis and maintenance of the traffic all over the city processing of these video streams requires lot of effort due to the amount of data that is generated this paper proposed a simplified technique for processing such voluminous data the large data set of realworld traffic is used for prediction and forecasting the urban traffic a combination of predefined kernels are used for spatial filtering and several such transferred techniques in combination will convolutional artificial neural networks that use spectral graphs and time series models spatially regularized vector autoregression models and non‐spatial time series models are the baseline traffic forecasting models that are compared for forecasting the performance in terms of training efforts development as well as forecasting accuracy the efficiency of urban traffic forecasting is high on implementation of video prediction algorithms and models further the potential research directions are presented along the obstacles and problems in transferring schemes
colorectal cancer affects more than 1 million people worldwide and half of this population develops liver metastases imageguided thermal ablation is an acceptable local therapy for the management of oligometastatic colorectal cancer liver disease in patients who are noneligible for surgery or present with recurrence after hepatectomy continuous technological evolutions understanding of tumor variability through disease biology and genetics and optimization of ablation parameters with ablation margin assessment have allowed patients with resectable smallvolume disease to be treated by thermal ablation with curative intent the growing role of imaging and image guidance in thermal ablation for patient selection procedure planning tumor targeting and assessment of technical success is discussed in this article
recent literature emphasizes the importance of comfort in the design of exosuits and other assistive devices that physically augment humans however there is little quantitative data to aid designers in determining what level of force makes users uncomfortable to help close this knowledge gap we characterized human comfort limits when applying forces to the shoulders thigh and shank our objectives were i characterize the comfort limits for multiple healthy participants ii characterize comfort limits across days and iii determine if comfort limits change when forces are applied at higher vs lower rates we performed an experiment n  10 to quantify maximum tolerable force pulling down on the shoulders and axially along the thigh and shank we termed this force the comfort limit we applied a series of forces of increasing magnitude using a robotic actuator to soft sleeves around their thigh and shank and to a harness on their shoulders participants were instructed to press an offswitch immediately removing the force when they felt uncomfortable such that they did not want to feel a higher level of force on average participants exhibited comfort limits of 09–13 times body weight on each segment 621±245 n shoulders 867±296 n thigh 702±220 n shank which were above force levels applied by exosuits in prior literature however individual participant comfort limits varied greatly 250–1200 n average comfort limits increased over multiple days p3e5 as users habituated from 550–700 n on the first day to 650–950 n on the fourth specifically comfort limits increased 20 35 and 22 for the shoulders thigh and shank respectively finally participants generally tolerated higher force when it was applied more rapidly these results provide initial benchmarks for exosuit designers and endusers and pave the way for exploring comfort limits over larger time scales within larger samples and in different populations
imaging applications based on microlens arrays mlas have a great potential for the depth sensor wide fieldofview camera and the reconstructed hologram however the narrow depthoffield remains the challenge for accurate reliable depth estimation multifocal microlens array mfmlas is perceived as a major breakthrough but existing fabrication methods are still hindered by the expensive lowthroughput and dissimilar numerical aperture na of individual lenses due to the multiple steps in the photolithography process this paper reports the fabrication method of high na mfmlas for the extended depthoffield using singlestep photolithography assisted by chemical wet etching the various lens parameters of mfmlas are manipulated by the multisized hole photomask and the wet etch time theoretical and experimental results show that the mfmlas have three types of lens with different focal lengths while maintaining the uniform and high na irrespective of the lens type additionally we demonstrate the multifocal plane image acquisition via mfmlas integrated into a microscope
there is a mushroom growth of malware which has caused a serious threat towards computer software and the internet the number of malware is increasing with each passing day there are two methods to deal with malware detection namely signaturebased malware detection and behaviorbased malware detection both methods have their advantages and disadvantages in this paper the pipeline process of both signaturebased malware detection and behaviorbased malware detection is explained this will help researchers to understand these techniques in a detailed manner in addition to this an experiment is performed in which a dataset of 1494 malware and 1347 benign samples is collected then two kinds of features are extracted from these samples one is string feature for static analysis and one is nonrepetitive consecutive api calls for dynamic analysis then accuracy is calculated by using various machine learning classifiers like knearest neighbors gaussian naive bayes multi naive bayes decision tree support vector machine and random forest
realtime embedded systems that combine processes of various criticalities ie mixedcriticality realtime systems represent an emerging research that faces many issues this paper describes a new asic design of a coprocessor that realizes process scheduling for mixedcriticality realtime systems the solution proposed in this paper uses robust earliest deadline red algorithm due to the onchip implementation of the scheduler all scheduler operations always take two clock cycles to execute the proposed solution was verified by simulations that applied millions of random inputs chip area costs are evaluated by synthesis into asic using 28 nm tsmc technology the proposed redbased scheduler is compared with an existing edfbased scheduler that supports hard realtime processes only even though the redbased scheduler costs more chip area it can handle any combinations of process criticalities variations of process execution times and deadlines achieves higher cpu utilization and can be used for scheduling of nonrealtime soft realtime and hard realtime processes combined within one system
stagedtrees is an r package which includes several algorithms for learning the structure of staged trees and chain event graphs from data scorebased and distancebased algorithms are implemented as well as various functionalities to plot the models and perform inference the capabilities of stagedtrees are illustrated using mainly two datasets both included in the package or bundled in r
the purpose of this paper is to investigate and discuss the influence of printing parameters on the mechanical properties of acrylonitrile butadiene styrene abs print by fused deposition modelling fdm the mechanical properties of abs are highly influenced by printing parameters and they determine the final product quality of printed piecesfor the paper’s purpose five main parameters extrusion temperature infill pattern air gap printing speed and layer thickness were selected and varied during abs printing on an opensource and selfreplicable fdm printer three different colors of commercially available abs were also used to investigate color and printing parameter’s influence on the tensile strengththe research results suggest that two parameters infill pattern and layer thickness were most influential on the mechanical properties of print abs being able to enhance its tensile strength another key influential factor was material color selected prior to printing which influenced the tensile strength of the print specimenthis study provides information on print parameters’ influence on the tensile strength of abs print on replicable opensource threedimensional 3d printers it also suggests the influence of materials’ color on print pieces’ tensile strength indicating a new parameter for materials selection for 3d printing
the potential of in situ melt pool monitoring mpm for parameter development and furthering the process understanding in laser powder bed fusion lpbf of cucr1zr was investigated commercial mpm systems are currently being developed as a quality monitoring tool with the aim of detecting faulty parts already in the build process and thus reducing costs in lpbf a detailed analysis of coupon specimens allowed two processing windows to be established for a suitably dense material at layer thicknesses of 30 µm and 50 µm which were subsequently evaluated with two complex thermomechanicalfatigue tmf panels variations due to the location on the build platform were taken into account for the parameter development importantly integrally averaged mpm intensities showed no direct correlation with total porosities while the robustness of the melting process impacted strongly by balling affected the scattering of the mpm response and can thus be assessed however the mpm results similar to material properties such as porosity cannot be directly transferred from coupon specimens to components due to the influence of the local part geometry and heat transport on the build platform different mpm intensity ranges are obtained on cuboids and tmf panels despite similar lpbf parameters nonetheless besides identifying lpbf parameter windows with a stable process mpm allowed the successful detection of individual defects on the surface and in the bulk of the large demonstrators and appears to be a suitable tool for quality monitoring during fabrication and nondestructive evaluation of the lpbf process
constructing a machine that understands human language is one of the most elusive and longstanding challenges in artificial intelligence this thesis addresses this challenge through studies of reading comprehension with a focus on understanding entities and their relationships more specifically we focus on question answering tasks designed to measure reading comprehension we focus on entities and relations because they are typically used to represent the semantics of natural language
electrostatics plays a critical function in most biomolecules therefore monitoring subtle biomolecular bindings and dynamics via the electrostatic changes of biomolecules at biointerfaces has been an attractive topic recently and has provided the basis in diagnosis and biomedical science here we present a bioelectrostatic responsive microlaser based on liquid crystal lc droplet and explored its application for ultrasensitive detection of negatively charged biomolecules whispering gallery mode wgm lasing from positively charged lc microdroplets was applied as the optical resonator where the lasing wavelength shift was employed as a sensing parameter with the dual impacts from whisperinggallery mode and liquid crystal molecular binding signals will be amplified in such lc droplet sensors it is found that molecular electrostatic changes at the biointerface of droplet triggered wavelength shift in lasing spectra the total wavelength shift increased proportionally with the adhering target concentrations compared to a conventional polarized optical microscope significant improvements in sensitivity and dynamic range by four orders of magnitude were achieved our work indicated that the surfacetovolume ratio plays a critical role in the detection sensitivity in wgm laserbased microsensors finally bovine serum albumin and specific biosensing using streptavidin and biotin were exploited to demonstrate the potential applications of microlasers with a detection limit on the order of 1 pm we anticipate this approach will open new possibilities for the ultrasensitive labelfree detection of charged biomolecules and molecular interactions by providing a lower detection limit than conventional methods
sliding mode control smc systems are developed for a hydraulic manipulator the control model is obtained via statedependent parameter sdp system identification in contrast to previous research using discretetime sdp models in which the model coefficients are functions of the sampling interval the present work develops a new continuoustime approach it is well known that for conventional smc there is a tradeoff between chattering and robust performance hence a recently developed approach to address this problem is investigated in which the controller is designed via a fractional exponent of the sliding surface the approach is developed for both conventional and nonsingular terminal smc ntsmc the new continuous version of the ntsmc algorithm successfully reduces chattering and provides the best overall performance of various smc designs however for the preliminary experiments reported in this article a pid leadlag controller yields the lowest absolute errors albeit at the cost of a higher control effort hence given that deadzone and other uncertainties provide the main motivation for use of smc in this application further research into the robustness of the new algorithm is required
microservices are a loosely coupled distributed systems architecture with the uncertainty in prediction of the size of the application microservices play an important role in development and scaling as they are independently functioning applications difficulties in caching data becomes manifold caching in microservices is achieved either by maintaining a local cache with peer to peer communication or a global cache with single store communication however multiple local caches come with communication overheads and data consistency issues while a global cache has data management issues this project attempts to find a combination of both to reduce the communication overheads and data size while solving the problem of data consistency focus is to create a mechanism which uses both a global cache and a local cache the global cache would act as a verification cache and the local cache would act as a data cache this will minimize the size of the global cache and the communication call size in comparison to the existing cache management techniques this system will act as a middle ground it inherits the low communication overheads from the global caching systems and also manages to keep the global cache size minimum by storing only verification data the impact of this research topic is multifaceted not only in scaling web applications to a global scale but also in maintaining modular dataconsistent caches in a cluster of microservices another advantage of the proposed solution is that it can ameliorate the problem of bandwidth always falling short in high load applications
in recent years unmanned aerial vehicles uav of various shapes sizes and functions have been increasingly used for civilian applications in a wide range of fields uavs are flexible inexpensive with the ability to carry high resolution sensing systems uavs fill the gap between manned aircraft and satellite remote sensing systems when searching for new methodologies especially in forestry research uavs are a unique platform remote forest sensing forest mapping forest precise measurement forest fire monitoring and support for intensive forest management are areas in which remote sensing is fully utilized the advantages of remote sensing are mainly low material and operating costs flexible space and time deployment and high intensity data collection in order to move remote sensing applications from the experimental phase into real deployment it is necessary to come up with new methodologies that can be determined using uavs
at present the education sector continues to face challenges in preparing the future generation in this 21 century the output of education sector is not only regarding the intelligence level of students sufficient skills and attitudes should also become a consideration various measures are done by all actors associated with the education sector one of the efforts that can be done as the form of contribution toward education sector is by performing innovations in learning patterns the learning model of piil pesenggiri team work learning pptwl is developed based on the local wisdom of lampung people pptwl is aimed to develop collaborative solving skill and scientific attitudes of students this model is designed to be able to provide the opportunity for students in achieving learning purposes by integrating local culture values one of the supporting elements in the model’s realization is through the availability of module supplement as the learning platform for students this study is a research and development rd of education on pptwlbased learning module that has five components orientation relating exploration transferring and evaluation this study used thiagarajan’s 4d development model namely define design develop and disseminate the sample used in the small test amounted to 10 students and the big test used 27 students as the sample the test results showed that the ngain of students’ scientific attitudes was 049 which was categorized as moderate this outcome showed that the pptwl module provided impacts on students’ scientific attitudes although it has yet to be optimal the result of a module development which was based on piil pesenggiri team work learning was responded positively by science teachers and the responses were categorized as very good
in the domain of the dutch cultural heritage various data sets describe different aspects of life during the dutch golden age these data sets in the form of rdf graphs use different standards and contain noise in the values of literal nodes such as misspelled names and uncertainty in dates the golden agents project aims at answering queries about the dutch golden ages using these distributed and independently maintained data sets a problem in this project among many other problems is the identification of persons who occur in multiple data sets but under different uris this paper aims to solve this specific problem and generate a linkset ie a set of pairs of uris which are judged to represent the same person we use domain knowledge in the application of an existing node context generation algorithm to serve as input for glove an algorithm originally designed for embedding words this embedding is then used to train a classifier on pairs of uris which are known duplicates and nonduplicates using just the cosine similarity between uripairs in embedding space for prediction we obtain a simple classifier with an f½score of around 085 even when very few training examples are provided on larger training sets more complex classifiers are shown to reach an f½score of up to 088
this letter focuses on a dayahead scheduling problem on thermal power plants and storage batteries based on the information of the interval prediction of photovoltaic pv power and demand to realize an optimal operation there are three kinds of decision variables to plan which correspond to a generating power schedule a chargedischarge cd power schedule and a state of charge soc schedule our purpose is to obtain the exact range of each optimal schedule for any possible pvdemand profiles of the interval prediction in the previous work we proposed a method to find the exact ranges of the optimal generation power schedule and the optimal cd power schedule by showing the sign patterns of the jacobians with respect to an uncertain renewable parameter however the range of the optimal soc schedule is not derived so far if the exact range is found we can offer a guideline for a necessary and sufficient capacity of storage batteries at each time step to get the range we show that the sign pattern of the jacobian for soc is invariant more specifically its sign pattern has negative elements in the lower triangle and positive elements in others the key to analyzing the sign pattern is to utilize particular properties of an m matrix and a diagonally dominant matrix as a result we clarify the sign pattern mathematically which enables to effectively calculate the exact range of the optimal soc schedule this letter completes the monotonicity analysis of all the decision variables in the optimal scheduling problem
environmental concerns push governments to invest in renewable energy re they are natural sources with a low carbon footprint and do not pollute locally however it is technically difficult to deploy high penetration of re into the utility grid due to the generation uncertainties and high installation costs which are some of the most critical issues in res use in this area to address this issue dc microgrids arise as a solution to integrate local distributed generation dg and storage and to mitigate the issues related to acdc and dcac converters thanks to their main advantages for the power grid and energy consumers microgrids have gained significant interest in recent years  by another side the electric vehicles evs market is expected to grow in the coming years which represent a new load that must be properly managed to avoid grid issues thus this paper discusses the operation of dc microgrid considering the introduction of evs a nonlinear control is presented including the modeling of charging of evs the simulated dc microgrid includes solar pv a battery and a supercapacitor significant variations from pv generation were included to highlight the performance of the methodology the results show that the voltage fluctuations are small which provides the dc microgrid with the required voltage stability moreover it has been demonstrated that dc microgrids can be integrated in isolated locations that are not connected to the main grid in view of the ress and evs
polymer bounded explosives pbxs consist of energetic crystals coated with a polymer binder these materials exhibit a highly heterogeneous microstructure the initiation of the detonation phenomenon in pbxs is believed to be generated at the microstructure scale through hotspots hence many of the explosives properties initiation desensitization etc are understood as a direct consequence of their microstructure mesoscale modeling directly addresses the physics of hotspot formation unfortunately high computational cost prevents their use on laboratorysized and large scale experiments in practice continuumscale models remain mandatory we describe a new reactive burn model named wgt aiming to represent at the continuum scale some of the complexity of the pbx’s microstructure the initiation regime is driven by the shock temperature and results from surrogate modeling of the kinetics of a heterogeneous nucleation and growth model the other regimes follow the formulation of the whs2d2 reactive burn model and are driven by the local temperature this model was calibrated on experimental results for pbx 9502 available in the literature such as detonation velocity–curvature laws popplot data or embedded electromagnetic particle velocity gauges the model was also tested against desensitization and propagation datapolymer bounded explosives pbxs consist of energetic crystals coated with a polymer binder these materials exhibit a highly heterogeneous microstructure the initiation of the detonation phenomenon in pbxs is believed to be generated at the microstructure scale through hotspots hence many of the explosives properties initiation desensitization etc are understood as a direct consequence of their microstructure mesoscale modeling directly addresses the physics of hotspot formation unfortunately high computational cost prevents their use on laboratorysized and large scale experiments in practice continuumscale models remain mandatory we describe a new reactive burn model named wgt aiming to represent at the continuum scale some of the complexity of the pbx’s microstructure the initiation regime is driven by the shock temperature and results from surrogate modeling of the kinetics of a heterogeneous nucleation and growth model the other regimes follow the formulation of the whs2d2 reac
training a deep neural network dnn via federated learning allows participants to share model updates gradients instead of the data itself however recent studies show that unintended latent information eg gender or race carried by the gradients can be discovered by attackers compromising the promised privacy guarantee of federated learning existing privacypreserving techniques eg differential privacy either have limited defensive capacity against the potential attacks or suffer from considerable model utility loss moreover characterizing the latent information carried by the gradients and the consequent privacy leakage has been a major theoretical and practical challenge in this paper we propose two new metrics to address these challenges the empirical mathcalvinformation a theoretically grounded notion of information which measures the amount of gradient information that is usable for an attacker and the sensitivity analysis that utilizes the jacobian matrix to measure the amount of changes in the gradients with respect to latent information which further quantifies private risk we show that these metrics can localize the private information in each layer of a dnn and quantify the leakage depending on how sensitive the gradients are with respect to the latent information as a practical application we design latentz a federated learning framework that lets the most sensitive layers to run in the clients trusted execution environments tee the implementation evaluation of latentz shows that teebased approaches are promising for defending against powerful property inference attacks without a significant overhead in the clients computing resources nor trading off the models utility
digital transformation is reshaping many areas of work and life within contemporary society these include healthcare education government politics law human rights and ethical controversies this chapter addresses the following questions what is the current conception of digital transformation as an emerging interdisciplinary field of research and study the objectives of the chapter are twofold 1 to conceptualize digital transformation as an emerging interdisciplinary field and 2 to identify key research areas that currently constitute digital transformation the chapter contributes by positing a comprehensive systems definition of digital transformation as an interdisciplinary research field to help guide researchers and other leaders in the field
abstract this the first in a series of articles on how psychodynamic therapy with infants and parents ptip can inspire work with adult therapy ptip helps infants and parents improve their relationship and facilitate child development during sessions developmental hazards are dramatized by parent and baby giving the therapist firsthand impressions of how conflictual relationships impact on the wellbeing of mother and child this article argues that ptip experiences may also inspire analytic work with adult patients 1 it gives the analyst a foothold when reconstructing a patient’s infantile trauma and linking it with hisher present distress 2 it deepens hisher attention on primitive anxieties paraverbal communication and psychosomatic functioning 3 ptip experiences with highspeed interchanges between container and contained personified by baby and parent seem to induce more internal images and metaphors in adult work as well 4 working with two patients simultaneously means the analyst’s position resembles that of a couple therapist or a participant observer of the traffic between container and contained this can make himher more agile in dealing with corresponding movements between himherself and the patient the present article focuses on 1 reconstructive work in adult work inspired by ptip experiences
in this paper a hybrid faulttolerant control method with offline design and online scheduling is proposed for ncs with actuator faults random delay and external finite energy disturbance the problem of less conservatism of robust generalized hybrid faulttolerant control is studied firstly a closedloop fault model of the system with random delay parameters was established according to the bernoulli 01 distribution all possible prior faults are divided into a few intervals according to certain rules and then an interval faulttolerant controller is designed offline according to the prior faults of each interval secondly when the fault is estimated online the corresponding interval faulttolerant controller is called through the scheduling mechanism to achieve rapid fault tolerance of prior faults within the interval and mitigate the impact of other faults within the interval which provides a guarantee for subsequent safe reconstruction control finally the effectiveness of the proposed method is verified by matlab simulation
in conducting research one typically considers the potential harms associated with the research and how to minimize or mitigate the negative impacts thus leaving less time to consider designing for optimized benefits the definition of positive impact relies heavily on the situational power structures culture and social norms of the given community this workshop will give researchers across the group community the opportunity to explore how we might design or use technologies to maximize wider societal benefits
the univariate distorted distributions were introduced in risk theory to represent changes distortions in the expected distributions of some risks later they were also applied to represent distributions of order statistics coherent systems proportional hazard rate and proportional reversed hazard rate models etc in this paper we extend this concept to the multivariate setup we show that in some cases they are a valid alternative to the copula representation especially when the marginal distributions may not be easily handled several examples illustrate the applications of such representations in statistical modeling they include the study of paired dependent ordered data joint residual lifetimes order statistics and coherent systems
currency counterfeiting is a serious crime that affects a countrys finances the proposed system will be useful to detect counterfeit banknotes in banking systems india is facing more serious problems due to the increase in fake notes in the market to get rid of this problem various fake note detection methods are available worldwide but most of them are hardware based and costly the proposed system focuses on having access to the public to identify counterfeit banknotes the proposed system can identify the legitimacy of a banknote by checking for specific security features such as watermarks latent images security threads etc identification of counterfeit banknotes is done using machine learning techniques the methodology involves extracting and encoding these security features security features are extracted from the input image feature detection and classification are performed using a support vector machine svm experimental results shown in the paper shows promising results br
asgrown agf2 has a remarkably similar electronic structure as insulating cuprates but it is extremely electronegative which makes it hard to handle and dope furthermore buckling of layers reduces magnetic interactions and enhances unwanted selftrapping lattice effects we argue that epitaxial engineering can solve all these problems by using a high throughput approach and first principle computations we find a set of candidate substrates which can sustain the chemical aggressiveness of agf2 and at the same time have good lattice parameter matching for heteroepitaxy enhancing agf2 magnetic and transport properties and opening the possibility of fieldeffect carrier injection to achieve a new generation of hightc superconductors assuming a magnetic mechanism and extrapolating from cuprates we predict that the superconducting critical temperature of a single layer can reach 195 k
micro and nanoparticles are not only understood as components of materials but as small functional units too particles can be designed for the primary transduction of physical and chemical signals and therefore become a valuable component in sensing systems due to their small size they are particularly interesting for sensing in microfluidic systems in microarray arrangements and in miniaturized biotechnological systems and microreactors in general here an overview of the recent development in the preparation of micro and nanoparticles for sensing purposes in microfluidics and application of particles in various microfluidic devices is presented the concept of sensor particles is particularly useful for combining a direct contact between cells biomolecules and media with a contactless optical readout in addition to the construction and synthesis of micro and nanoparticles with transducer functions examples of chemical and biological applications are reported
working in the setting of banach spaces we give a simpler proof of a result concerning the ulam stability of the composition of operators several applications are provided then we give an example of a discrete semigroup with ulam unstable members and an example of ulam stable operators on a banach space such that their sum is not ulam stable another example is concerned with a c 0 semigroup  t t  t ≥ 0 of operators for which each t t is ulam stable we present an open problem concerning the ulam stability of the members of the bernstein c 0 semigroup two other possible problems are mentioned at the end of the paper
spiking neural network snn is considered as one of the most promising candidates for designing neuromorphic hardware due to its low power computing capability since snns are made from imitating features of the human brain bioplausible spiketimingdependent plasticity stdp learning rule can be adjusted to perform unsupervised learning of snn in this paper we present a spike count based early termination technique for stdp learning in snn to reduce redundant timesteps and calculations spike counts of output neurons can be used to terminate the training process beforehand thus latency and energy can be decreased the proposed scheme reduces 507 of timesteps and 511 of total weight update during training with 035 accuracy drop in mnist application
we begin by charting the evolution of the dominant perspective on job performance from one that viewed performance as static to one that viewed it as dynamic over long timeframes eg months yea
biology is a science with a broad scope and provides learning experiences that encourage students to become independent learners interest and development of thinking skills are needed in learning metacognitive is an important aspect of achieving learning competence and building a student’s character as an independent learner metacognitive motivates the students to have awareness of something they know and about something they don’t this research aims to analyze the metacognitive of the science and the social students in biology lessons and to compare the metacognitive of those different students this research was conducted at high schools in jember this research was a qualitative descriptive research by using mix methods combine the qualitative and quantitative approach qualitative data were descriptive and they were based on observations to students whereas the quantitative data were based on the results of the metacognitive awareness inventory mai test there was two mai indicators used in this research knowledge about cognition and regulation of cognition the result of this research showed that the metacognitive average of science students was 646785 meanwhile the social students reached 563533 which means the metacognitive of science students was 17 higher than social students
this paper highlights a review of scientific papers published in the year 2019 regarding pesticides and herbicides the scientific review presented in this paper includes the presence and occurrence of pesticides and herbicides in the environment the entire review divided into different sections which are grouped into four main sections each of these sections provides studies conducted on toxicology ecological risk assessment strategies of treatment policies modeling and guidelines regarding pesticides and herbicides management
this work examines trajectory learning clustering and user association policies for dynamically connectable unmanned aerial vehicle base stations uavbss here uavbss are allowed to dynamically form physicallyconnected collocated antenna arrays that enable joint transmission and cooperative energy sharing among the connected uavbss the uavs’ trajectories their clustering decisions and the user association are jointly determined for the case with static users by maximizing a proportionalfair objective given by the sum lograte of all users then for the case with dynamic users the uavs’ trajectory learning reclustering and user handover policies are developed to adapt to changes in the users’ locations in particular the uavs’ locations are adjusted gradually in each time slot based on a stochastic gradient ascent algorithm and user handover decisions are made in each time slot based on a reward function that is inspired by the solution in the static case the clustering decisions are updated every t time slots by combining two uav clusters or by separating a cluster into two to determine the transmission schemes in each time slot a semiorthogonal user scheduling policy is first employed to determine the users that are served in each time slot then joint design of the transmit beamformers and powers is proposed to maximize the sum of log signaltoleakageplusnoise ratio slnr of scheduled users simulations are provided to demonstrate the effectiveness of the proposed schemes
urban centers and dense populations are expanding hence there is a growing demand for novel applications to aid in planning and optimization in this article a smart parking system that operates both indoor and outdoor is introduced the system is based on bluetooth low energy ble beacons and uses particle filtering to improve its accuracy through simple ble connectivity with smartphones an intuitive parking system is designed and deployed the proposed system pairs each spot with a unique ble beacon providing users with guidance to free parking spaces and a secure and automated payment scheme based on realtime usage of the parking space three sets of experiments were conducted to examine different aspects of the system a particle filter is implemented in order to increase the system performance and improve the credence of the results through extensive experimentation in both indoor and outdoor parking spaces the system was able to correctly predict which spot the user has parked in as well as estimate the distance of the user from the beacon
we introduce electric an energybased cloze model for representation learning over text like bert it is a conditional generative model of tokens given their contexts however electric does not use masking or output a full distribution over tokens that could occur in a context instead it assigns a scalar energy score to each input token indicating how likely it is given its context we train electric using an algorithm based on noisecontrastive estimation and elucidate how this learning objective is closely related to the recently proposed electra pretraining method electric performs well when transferred to downstream tasks and is particularly effective at producing likelihood scores for text it reranks speech recognition nbest lists better than language models and much faster than masked language models furthermore it offers a clearer and more principled view of what electra learns during pretraining
aerialaquatic robots possess the unique ability of operating in both air and water however this capability comes with tremendous challenges such as communication incompatibility increased airborne mass potentially inefficient operation in each of the environments and manufacturing difficulties such robots therefore typically have small payloads and a limited operational envelope often making their field usage impractical we propose a novel robotic water sampling approach that combines the robust technologies of multirotors and underwater microvehicles into a single integrated tool usable for field operations the proposed solution encompasses a multirotor capable of landing and floating on the water and a tethered mobile underwater pod that can be deployed to depths of several meters the pod is controlled remotely in three dimensions and transmits video feed and sensor data via the floating multirotor back to the user the ‘dualrobot’ approach considerably simplifies robotic underwater monitoring while also taking advantage of the fact that multirotors can travel long distances fly over obstacles carry payloads and manoeuvre through difficult terrain while submersible robots are ideal for underwater sampling or manipulation the presented system can perform challenging tasks which would otherwise require boats or submarines the ability to collect aquatic images samples and metrics will be invaluable for ecology and aquatic research supporting our understanding of local climate in difficulttoaccess environments video attachment httpsyoutubev4xwmehusm4
"

the major area of work of pathologists is concerned with detecting the diseases and helping the patients in
their healthcare and wellbeing the present method used by pathologists for this purpose is manually viewing the slides
using a microscope and other instruments but this method suffers from a lot of problems like there is no standard way of
diagnosing human errors and it puts a heavy load on the laboratory men to diagnose such a large number of slides daily



the slide viewing method is widely used and converted into digital form to produce high resolution images
this enables the area of deep learning and machine learning to deep dive into this field of medical sciences in the present
study a neural based network has been proposed for classification of blood cells images into various categories when
input image is passed through the proposed architecture and all the hyper parameters and dropout ratio values are used in
accordance with proposed algorithm then model classifies the blood images with an accuracy of 9524



 after training the models on 20 epochs the plots of training accuracy testing accuracy and corresponding
training loss testing loss for proposed model is plotted using matplotlib and trends



 the performance of proposed model is better than existing standard architectures and other work done by
various researchers thus the proposed model enables the development of pathological system which will reduce human
errors and daily load on laboratory men this can also in turn help pathologists in carrying out their work more efficiently
and effectively



in the present study a neural based network has been proposed for classification of blood cells images into
various categories these categories have significance in the medical sciences when input image is passed through the
proposed architecture and all the hyper parameters and dropout ratio values are used in accordance with proposed
algorithm then model classifies the images with an accuracy of 9524 this accuracy is better than standard
architectures further it can be seen that the proposed neural network performs better than present related works carried
by various researchers

"
this paper will explore the development of a powerline detection system using deep learning to autonomously recognize powerlines in realtime using an unmanned aerial system additionally the detection system can identify the individual components of the powerline and electric utility pole such as the crossarms insulators transformers and primary wires this proposed model has fast recognition speed and high accuracy which allows the unmanned aerial system to inspect quickly decreasing both time and cost and increasing safety to achieve this the realtime powerline detection system leverages the capability of the yolact algorithm a recently proposed realtime instance segmentation model that achieves 298 minimum average precision on the mscoco dataset at 335 fps the results using this approach is promising and perform much better when compared to the mask rcnn algorithm this paper demonstrates the application and results of the realtime powerline detection system in combination with the yolact algorithm for detecting individual components of the powerline using instance segmentation
abstract
in a project a mission completion time for each mission should be presented as stochastic due to uncertainly available machineshuman resources besides a charge for each mission is affected according to the completion time of the mission in order to describe the uncertain completion time and charges a stochastic completion time project sctp is formulated in this paper then sctp reliability is defined as the probability that total completion time and budget constraints can be satisfied when completing a project according to the previous reference maximal and minimal capacity vectors bind all the feasible capacity vectors specifically a previous reference calculated an estimated sctp reliability in terms of an interval on the other hand we develop a breakdown approach such that all the feasible capacity vectors can be decomposed then a recursive function is constructed based on the sum of disjoint products sdp principle an algorithm is further presented to calculate the exact sctp reliability project supervisors and teams can analyze and manage their projects in terms of sctp reliability
light manipulation has been widely employed in lighting display and energy storage becoming an inseparable part of human lives however the conventional optical devices suffer from the diffraction limit of electromagnetic waves to overcome the limitation plasmonic and dielectric nanoantennas are introduced for the control of light direction at nanoscale the directionality of the nanoantennas stems from their electromagnetic resonance properties or the interference between different resonance modes the near‐field interaction between the nanoantennas and fluorescent emitters or raman molecules will generate directional light emission the directional nanoantennas are widely applied in various fields although light manipulation at nanoscale is briefly discussed in several review articles they are not specifically devoted to directional nanoantennas a more selective focused and up‐to‐date review article about light manipulation by directional nanoantennas is deemed necessary because of the rapid development in this field in this article the properties and mechanisms of various types of nanoantennas made of plasmonic or dielectric nanostructures are introduced the current developments of the nanoantennas are summarized and an outlook for future investigation on directional nanoantennas is offered it is believed that the directional light control with the nanoantennas will be a flourishing and promising research field in the future
sustainable development is the theme that has remained in the interest for more than three decades as the result of which various solutions have been developed that seek to implement and control sustainability development in companies the hydropower sector seeks to address its environmental and social problems through various models the objective of this work is to select a sustainable development model to evaluate hydroelectric sustainability through a method of multicriteria decision making through the method of analytical hierarchical process ahp and a focus group it is possible to determine the weightings of the criteria subcriteria and alternatives of sustainable development models once the ahp method was applied the sustainable development models were hierarchized it was found that the hydropower sustainability assessment protocol obtained the highest priority among the models followed by the bs 8900 standard and the dow jones sustainability indexes djsi in the sensitivity analysis the selection of the hydropower sustainability assessment protocol model was validated as it remained in the first place in three of the four proposed scenarios the results showed that the hydropower sustainability assessment protocol is the most appropriate sustainable development model to evaluate hydroelectric sustainability this provides support to those who seek to justify their decision to select a sustainable development model using multicriteria methods
the existing test equipment for safety valve with lowflow start and overflow closure characteristics of hydraulic support cannot provide the test conditions stipulated in relevant standards which affects the accuracy of test results aiming at the shortage of the existing test equipment a test system of high pressure and low flow rate of safety valve with hydraulic support is designed and a technical solution of using straight line driver to push the plunger cylinder to realize high pressure and low flow rate of liquid supply is put forward the simulation software amesim was used to construct the virtual prototype of the test system and the simulation analysis was carried out to determine the main parameters and the influencing rules of the rigidity of the liquid supply velocity based on the simulation study prototype trial production flow calibration and field experiment were completed the experimental results showed that the instantaneous output flow fluctuation of the test system was within the range of ±10 of the rated value of 40mlmin and the output flow fluctuation range was reduced by 25 compared with the existing way
online debates specifically the ones about public health issues eg vaccines medications and nutrition occur frequently and intensely and are having an impact on our world many public health topics are debated online one of which is the efficacy and morality of vaccines when people examine such online debates they encounter numerous and conflicting sources of information this information forms the basis upon which people take a position on such debates this has profound implications for public health it necessitates a need for public health stakeholders to be able to examine online debates quickly and effectively they should be able to easily perform sensemaking tasks on the vast amount of online information such as sentiments online presence focus or geographic locations in this paper we report the results of a user study of a visual analytic system vas and whether and how this vas can help with such sensemaking tasks specifically we report a usability evaluation of vincent visual analytics system for investigating the online vaccine debate a vas previously described to help the reader we briefly discuss vincent’s design in this paper as well vincent integrates webometrics natural language processing data visualization and humandata interaction in the reported study we gave users tasks requiring them to make sense of the online vaccine debate thirtyfour participants were asked to perform these tasks by investigating data from 37 vaccinefocused websites half the participants were given access to the system while the other half were not selected study participants from both groups were subsequently asked to be interviewed by the study administrator examples of questions and issues discussed with interviewees were how they went about completing specific tasks what they meant by some of the feedback they provided and how they would have performed on the tasks if they had been placed in the other group overall we found that vincent was a highly valuable resource for users helping them make sense of the online vaccine debate much more effectively and faster than those without the system eg users were able to compare websites similarities identify emotional tone of websites and locate websites with a specific focus in this paper we also identify a few issues that should be taken into consideration when developing vases for online public health debates
im privaten und beruflichen umfeld sind wir von technologien umgeben und interagieren täglich mit ihnen das smartphone ist unser täglicher begleiter hilfreiche digitale haushaltshilfen wie saugroboter sind in jedem haushalt vertreten und die meisten von uns haben bereits versucht mit chatbots zu kommunizieren auch in der industrie wird vermehrt den einsatz von technologien genutzt um etwa grosse objekte wie fahrzeuge mithilfe von robotern zu bauen industrie und servicerobotern nehmen also an bedeutung zu robotik ist ein umfangreiches thema daher führt dieser beitrag den begriff robotik aus sicht der wirtschaftsinformatik ein geht auf die verschiedenen robotertypen ein und beschreibt das robotic process automation das insbesondere in der wirtschaftsinformatik anwendung findet der beitrag schliesst mit einer diskussion über die potenziale der robotik
abstract biological processes like microbial growth  physiological response are usually dynamic and require the monitoring of metabolic variation at different timepoints moreover there is clear shift from casecontrol n2 study to multiclass n2 problem in current metabolomics which is crucial for revealing the mechanisms underlying certain physiological process disease metastasis etc these timecourse and multiclass metabolomics have attracted great attention and data normalization is essential for removing unwanted biologicalexperimental variations in these studies however no tool including noreva 10 focusing only on casecontrol studies is available for effectively assessing the performance of normalization method on timecoursemulticlass metabolomic data thus noreva was updated to version 20 by i realizing normalization and evaluation of both timecourse and multiclass metabolomic data ii integrating 144 normalization methods of a recently proposed combination strategy and iii identifying the wellperforming methods by comprehensively assessing the largest set of normalizations 168 in total significantly larger than those 24 in noreva 10 the significance of this update was extensively validated by case studies on benchmark datasets all in all noreva 20 is distinguished for its capability in identifying wellperforming normalization methods for timecourse and multiclass metabolomics which makes it an indispensable complement to other available tools noreva can be accessed at httpsidrblaborgnoreva
aeronautical structures are increasingly aging and the occurrence of unexpected loads could reduce their operability a health and usage monitoring system would enable the continuous monitoring of the state of health of a structure and track its aging by a load monitoring system which aims at the real‐time reconstruction of the loads acting on a structure however sometimes the loads and the induced strain and stress fields are difficult to be reconstructed exactly as for complex loading due to flight maneuvers in this work the full strain and load fields of a structure are reconstructed by an inverse‐direct approach leveraging on the calibration matrix approach the latter exploits a least‐squares minimization of an error functional defined as the comparison between measured strains in discrete positions and a numerical formulation of the same to reconstruct an equivalent however representative load set by assuming a linear relationship between strain and load through a calibration matrix this minimization can be performed analytically leading to a computationally very efficient algorithm that can be operated online once the equivalent load set is computed the full strain field can be estimated relying on a second calibration matrix linking the external loads to the strain field of the complete structure the method has been numerically tested with an unmanned aerial vehicle uav subjected to aerodynamic pressure loads simulating flight maneuvers finally the results are experimentally validated during a ground test program on a real uav proving the robustness to different experimental uncertainties
the structural design of a refrigerant plate and its flow property affect the thermal management performance of a battery this study aimed at improving the cooling and thermal uniformity based on the r134a cooling method analyzed the flow resistance of the plate temperature and heat‐transfer coefficient of the top board of eight types type a‐g by the computational fluid dynamics method by trade‐off between flow resistance and heat‐transfer efficiency we determined the optimal type g plate with a modest pressure difference of 16706 pa and the highest heat‐transfer coefficient of 8108 wm2 k in addition the relationship between the operating parameters and temperature variation in battery modules and its uniformity has also been analyzed the results indicate that with this novel cooling method temperature uniformity of battery modules can be maintained within 5 k at 2c discharging rate both increasing inlet r134a flow rate and decreasing evaporative pressure obviously increase the temperature decrease rate of battery modules with the inlet r134a mass flux ranging from 245 to 490 kgm2 s the evaporative pressure ranging from 412 530 to 291 220 pa and the subcooling degree ranging from 0 to 4 k the battery temperature can be maintained under 300 k in conclusion this study develops the guidelines for battery thermal management based on refrigerant cooling which has a promising application foreground due to its good thermal control performance flow resistance saving and lightweight
in this paper we propose a consensusbased algorithm for nonconvex optimization on the stiefel manifold for a given objective function on the stiefel manifold we construct a stochastic interacting particle system for sample points so that all the sample points are expected to asymptotically converge to a single point which is close enough to a global minimizer we show the global existence and uniqueness of solutions to our stochastic differential equation sde model for consensus a predictorcorrector type numerical scheme is then proposed for implementing the sde model with the guarantee that each sample point stays on the stiefel manifold a salient feature of our algorithm is that it is gradientfree thereby applicable to a wide range of problems the results of our numerical experiments demonstrate that the proposed method can successfully find a global minimizer even when the objective function is nonconvex
to overcome the problem of storing massive stream data in power distribution systems this article presents a method of stream data compression for power distribution systems which is based on tensor decomposition first to preserve the spatial structure of highdimensional data in the presentation stage we establish a tensor representation model of multi phasor measurement unit stream data and video stream data of power distribution systems then a stream data compression method that is based on incremental tensor decomposition is proposed compared with the traditional method the proposed method uses the characteristics of a large amount of data accumulated over time in the distribution system we use a small amount of realtime newly added data to update the compression results of existing historical data thereby avoiding the direct compression of accumulated data and reducing the time and space complexities of the traditional method therefore the proposed approach can compress the stream data in less time and with lower memory consumption finally the effectiveness of the method is verified using real data
metal ions are essential components that help maintain the processes of normal life and they can be used to fabricate selfassembled building blocks for peptide derivatives proteins and nucleic acids here we have developed a novel strategy to construct supramolecular hydrogels modulated using metal cations upon introducing a variety of metal ions into aqueous solutions of a gelator naproxenff including a nonsteroidal antiinflammatory drug nsaid and dipeptide we obtain stable hydrogels under neutral or alkaline conditions it is found that these hydrogels with threedimensional nanofiber networks exhibit excellent mechanical properties and thixotropy as well as superb responsivity to multiple metal ions due to the significance of potassium ions in biological processes the ktriggered hydrogel has been chosen as a model and its selfassembly mechanism has been explored via various spectral analysis processes in addition the selfassembly performances of peptides are significantly affected by the chemical structures of the gelator molecules this work provides deep insight into the aggregation mechanism of dipeptideconjugating drug molecules through introducing a variety of metal ions laying the foundation for further biological applications
after demonetization the emphasis was given to a cashless economy by the government of india keeping in view the concept of cashless economy the importance of crypto currency cannot be denied crypto currency cc is a virtual currency and it works as a medium of exchange by using cryptography for security it comprises diverse currencies such as bitcoin btc ethereum eth ripple xrp litecoin ltc cardano ada neo neo stellar lumens xlm and so on many countries like canada australia bulgaria chile denmark estonia finland germany and luxembourg have adopted bitcoin in order to moving towards a digital ecosystem the research was conducted to find out the awareness perception and understanding about the functioning of bitcoin among individuals this article is all about awareness of bitcoin amongst individuals and prospective if allowed by the government of india
"
 we demonstrate a remarkable change of chemical trend of iron under high pressure that is of great importance for understanding the distribution of elements in the earth’s mantle and core using a massivescale first principles study we show that while reacting with pblock elements under increasing pressure from ambient to that of erath’s core iron tends to reverse its chemical nature changing from an electron donor reductant to an electron acceptor and oxidizes pblock elements such reverse chemistry significantly impacts the stoichiometries bond types and strengths structures and properties of iron compounds under deep planetary conditions making many pblock elements that are conventionally labeled lithophile or chalcophile to highly siderophile the chemical binding strengths with iron show an inverse correlation with the depletion of pblock elements in silicate earth furthermore silicon shows a distinct anomaly in its bonding to iron which suggests silicon may readily be incorporated into earth’s core"
youtube is one of the most popular video sharing platforms that hosts many video tutorials which aim to teach concepts of various programming languages most of these tutorials include code snippets in the videos however it is important that the learners have handson experience while learning various programming concepts providing a code editor along with the video tutorial could help learners get a better learning experience as they have a scope to learn by practice existing solutions of accompanying video tutorials with code editors are either preprogrammed or require a separate web portal we are not aware of any solutions in the current literature that aim to support youtube video tutorials hence we present ytcoder in this paper that aims to improve the learning experience by integrating videos related to various programming languages with the development environments of the respective programming languages demonstration of ytcoder can be found here  httpsyoutubeionto7cuqwo
reducing the precision of deep neural network dnn inference accelerators can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floatingpoint by enabling more multiplication operations per unit area a wide range of precisions fall on the paretooptimal curve of hardware efficiency vs accuracy with no single precision dominating making the variable precision capabilities of fpgas very valuable we propose three types of logic block architectural enhancements and fully evaluate a total of six architectures that improve the area efficiency of multiplications and additions implemented in the soft fabric increasing the lut fracturability and adding two adders to the alm 4bit adder double chain architecture leads to a 15× area reduction for arithmetic heavy machine learning ml kernels while increasing their speed in addition this architecture also reduces the logic area of general applications by 6 while increasing the critical path delay by only 1 however our highest impact option which adds a 9bit shadow multiplier to the logic clusters reduces the area and critical path delay of ml kernels by 24× and 12× respectively these large gains come at a cost of 15 logic area increase for general applications
deep energybased models ebms are very flexible in distribution parametrization but computationally challenging because of the intractable partition function they are typically trained via maximum likelihood using contrastive divergence to approximate the gradient of the kl divergence between data and model distribution while kl divergence has many desirable properties other fdivergences have shown advantages in training implicit density generative models such as generative adversarial networks in this paper we propose a general variational framework termed febm to train ebms using any desired fdivergence we introduce a corresponding optimization algorithm and prove its local convergence property with nonlinear dynamical systems theory experimental results demonstrate the superiority of febm over contrastive divergence as well as the benefits of training ebms using fdivergences other than kl
multiclass extensions of the support vector machine svm have been formulated in a variety of ways a recent empirical comparison of nine such formulations doǧan et al 2016 recommends the variant proposed by weston and watkins ww despite the fact that the wwhinge loss is not calibrated with respect to the 01 loss in this work we introduce a novel discrete loss function for multiclass classification the ordered partition loss and prove that the wwhinge loss is calibrated with respect to this loss we also argue that the ordered partition loss is maximally informative among discrete losses satisfying this property finally we apply our theory to justify the empirical observation made by doǧan et al that the wwsvm can work well even under massive label noise a challenging setting for multiclass svms
in this study a new continuous distribution which is a twocomponent finite mixture of exponential and gamma distribution is suggested various properties of the proposed distribution such as survival function hazard rate function moments moment generating function and characteristic function have been discussed the maximum likelihood estimation is used for estimating the unknown parameters involved in this work finally the proposed distribution is fitted to reallife data to illustrate its application and a comparison against some existing distributions are drawn
the relevance of creating new measuring instruments and systems for monitoring electric current and magnetic field strength using modern software and hardware is shown this is demonstrated this is demonstrated by the wide area of scientific activity around the world in the field of circuit development the use of physical effects for the construction of devices and control systems indicating the importance of developing elements systems and devices with improved quality indicators and contributing to accelerated scientific and technological progress which is of great economic importancethe goal of the work is formulated the state of developments in the field of measuring systems for monitoring electric current and magnetic field strength is assessed to select effective ways to achieve the goal of this work a new measuring system for electric current and magnetic field strength is created the possibilities of using the magnetooptical faraday effect to create a new measuring system of electric current and magnetic field strength are investigated the sequence of procedures for achieving the task is formulated
machine learning ml aims at designing models that learn from previous experience without being explicitly formulated applications of machine learning are inexhaustible including recognizing patterns predicting future trends and making decisions and they are capable of handling sizable quantities of multidimensional data in the form of large vectors and tensors to perform these operations on classical computers however requires vast time and computational resources unlike the classical computers that rely on computations using binary bits quantum computers qc benefit from qubits which can hold combinations of 0 and 1 at the same time via superposition and entanglement this makes qcs powerful at handling and post processing large tensors making them a prime target for implementing ml algorithms while several models used for ml on qcs are based on concepts from their classical computing counterparts utilization of the qc’s potential has made them the superior of the two this paper presents an overview of the current state of knowledge in application of ml on qc and evaluates the speed up and complexity advantages of using quantum machines
this paper proposes a sensory language retrieval slr algorithm and implicitly initiates a new approach the algorithm utilizes the relationship between objects and sensory functions by assigning human sensory states to elements of spoken language the algorithm is based on the approach accepted in cognitive linguistics stating that there is a causal relationship between body and language the novelty of the algorithm is that the described algorithmic functions of the language can be interpreted as the object constructing capability as key element of human communication the proposed algorithm assigns a new model with which the language communication objects and meaning can be defined in a single interpretive space
unsupervised landmark learning is the task of learning semantic keypointlike representations without the use of expensive input keypoint annotations a popular approach is to factorize an image into a pose and appearance data stream then to reconstruct the image from the factorized components the pose representation should capture a set of consistent and tightly localized landmarks in order to facilitate reconstruction of the input image ultimately we wish for our learned landmarks to focus on the foreground object of interest however the reconstruction task of the entire image forces the model to allocate landmarks to model the background using a motionbased foreground assumption this work explores the effects of factorizing the reconstruction task into separate foreground and background reconstructions in an unsupervised way allowing the model to condition only the foreground reconstruction on the unsupervised landmarks our experiments demonstrate that the proposed factorization results in landmarks that are focused on the foreground object of interest when measured against groundtruth foreground masks furthermore the rendered background quality is also improved as illsuited landmarks are no longer forced to model this content we demonstrate this improvement via improved image fidelity in a videoprediction task code is available at httpsgithubcomnvidiaunsupervisedlandmarklearning
a ubiquitous tool in science physics and engineering at large the transfer matrix method tmm is particularly suited to deal with complex nonuniform systems nus in the field of electrical engineering the method is employed in a variety of disciplines that span the electromagnetic spectrum from power frequencies through rf millimeterwaves and terahertz in this work three nested goals are pursued the first is to present a general comprehensive review of the transfer matrix method utilizing two distinct languages the state space via the matricant and the modal analysis via matrix diagonalization the second goal focused on electrical engineering issues is the application of tmm to the interdisciplinary theme of nonuniform multiconductor transmission lines mtl –a good example of a complex reciprocal multiport system the third goal is to offer the reader novel research results in the context of mtl analysis making use of a simple microwave striplinecoupler structure new theoretical results are presented showing that in some cases load impedance matching of nonuniform mtls using only passive components may not be physically possible negative resistors being required even if the global nus is longitudinally and transversally symmetric with or without losses
groundbased lidar also known as terrestrial laser scanning tls technology is an active remote sensing imaging method said to be one of the latest advances and innovations for plant phenotyping basal stem rot bsr is the most destructive disease of oil palm in malaysia that is caused by whiterot fungus ganoderma boninense  the symptoms of which include flattening and hangingdown of the canopy shorter leaves wilting green fronds and smaller crown size therefore until now there is no critical investigation on the characterisation of canopy architecture related to this disease using tls method was carried out this study proposed a novel technique of bsr classification at the oil palm canopy analysis using the point clouds data taken from the tls a total of 40 samples of oil palm trees at the age of nineyearsold were selected and 10 trees for each health level were randomly taken from the same plot the trees were categorised into four health levels  t0 t1 t2 and t3 which represents the healthy mildly infected moderately infected and severely infected respectively the tls scanner was mounted at a height of 1 m and each palm was scanned at four scan positions around the tree to get a full 3d image five parameters were analysed s200 canopy strata at 200 cm from the top s850 canopy strata at 850 cm from the top crown pixel number of pixels inside the crown frond angle degree of angle between fronds and frond number the results taken from statistical analysis revealed that frond number was the best single parameter to detect bsr disease as early as t1 in classification models a linear model with a combination of parameters abd – a frond number b frond angle and d s200 delivered the highest average accuracy for classification of healthyunhealthy trees with an accuracy of 8667 per cent it also can classify the four severity levels of infection with an accuracy of 80 per cent this model performed better when compared to the severity classification using frond number the novelty of this research is therefore on the development of new approach to detect and classify bsr using point clouds data of tls
the methodology to quantify the numerical dissipation in underresolved dns udns based on the balance of the kinetic energy equation by schranner et al comput fluids 11484–97 2015 has been examined in this study furthermore this methodology has been extended for considering active scalars based on both balance of the kinetic energy and thermal variance equations for assessing the quality of udns as a first step towards the analysis of reactive flows where temperature is actively coupled to the momentum equation the turbulent rayleigh–bénard convection in a cubical enclosure is considered here the simulations have been carried out for different prandtl numbers  pr01 10 10 p r  01  10  10  to analyse different levels of momentumtemperature coupling for a representative value of nominal rayleigh number  ra107 r a  10 7  in order to investigate the effects of grid resolution and discretisation schemes on the numerical dissipation it has been found that the numerical dissipation for both kinetic energy  rk r k  and thermal variance  rt r t  can range between positive or negative values depending on the combined effects of temporal and spatial discretisation schemes and grid spacing the positive values of rk r k and rt r t indicate a loss of the kinetic energy and thermal variance due to numerical errors whereas the negative values of rk r k and rt r t result in a moderate attenuation of these quantities accordingly the numerical dissipation results obtained here ie rk r k and rt r t  have been utilised for evaluating the effective values of the rayleigh and prandtl number ie raeff r a eff  preff p r eff  for udns using the scaling of the mean nusselt number  nu sim raa prb n u ∼ r a a p r b  it has been shown that the error relative to exact mean nusselt number can be estimated based on rk r k  rt r t and the effective mean nusselt number of the udns  nuudns n u udns  overall this methodology has been found a promising tool for the quality assessment of udns for the applications of nonisothermal flows eg rayleigh–bénard convection
purpose of the study the objectives of the study were to critically examine the challenges faced by entrepreneurs in tourism business in establishing their businesses in oman to critically analyze whether the entrepreneurs in the tourism business are motivated to establish tourism business in oman and to critically analyze the prospects for entrepreneurs to venture into the tourism sector in oman designmethodology the data was collected through a welldefined questionnaire through which 241 tourism entrepreneurs from oman including unemployed youth who were interested in tourism were selected on a random sampling basis structural equation modelling through smart pls was used to analyze the data findings the results of the research show that the initial capital working capital and good location are the essential factors required to establish a tourism business in oman it was confirmed that the prevailing rules and regulations are strict and rigid to start a tourism business in oman and it takes lots of time to start the operation of tourism businesses seeking labor clearance procedure is not simple for tourism cultural values and physical working conditions do not encourage tourismrelated businesses and the nonpreference of spouses from the tourism sector is another major constraint for omanis in taking up tourism businesses implications it was suggested that the government should provide strategic support to the tourismrelated entrepreneurs the government should provide financial guidance to encourage tourism entrepreneurs irrespective of their ages governmental licensing authorities and all licenses to set up tourism businesses need to be obtained in one place the government should invest in superstructure projects to enhance the scope of tourism training must be provided by the government to enhance the tourism business in oman originality this is the study of its kind and no research was carried out ever before to study solely the challenges of tourism entrepreneurship in oman keywords tourism challenges tourism entrepreneurship financial constraints technological support government support procedures and formalities
while the semiconductor manufacturing process is shrinking the bytes per flop bf ratio on recent machines is becoming lower particlebased computational fluid dynamics cfd methods such as moving particle simulation mps require a higher bf ratio than that of stencilbased cfd methods techniques to reduce the bf ratio by exploiting temporal parallelism is becoming popular in stencilbased cfd methods on cpu and gpu it is also reported that a technique to combine temporal blocking with stencil buffer is suitable for fpga and can outperform cpu and gpu on the other hand it has been considered that temporal parallelism cannot be exploited in the particlebased cfd methods this is because the number of particles in each bucket a threedimensional grid covering a computational domain changes every timestep in this paper we propose a technique to exploit temporal parallelism in mps method a particlebased cfd method for incompressive fluid the key idea is that the buckets in mps can be considered as stencils in stencilbased cfd this is because the maximum number of particles in a bucket can be assumed empirically in the case of an incompressible fluid to the best of our knowledge this is the first research which exploits temporal parallelism in the particlebased incompressible fluid method we implemented the proposed technique with a degree of temporal parallelism of three we also optimized it on intel arria10 fpga in intel hls and measured the performance and resource consumption the result shows that the optimized implementation with a degree of temporal parallelism of three achieved 21 times speedup compared with implementation without exploiting temporal parallelism on cpu
the paper considers the stability issue of linear systems with commensurate delays this issue can be well characterised by the distribution of roots of systems characteristic equation at first distribution boundary of the roots with positive real parts is explicit given in a practical way subsequently a reliable graphical stability criterion for calculating the number of unstable roots is deduced associating with auxiliary polynomial which plays an important role in the analysis of high order and complex systems moreover a procedure for drawing the winding curve of characteristic function in finite path is proposed at last typical examples are given to illustrate that the result carried out is reliable and efficient
neural networks have become increasingly prevalent in many realworld applications including security critical ones due to the high hardware requirement and time consumption to train highperformance neural network models users often outsource training to a machinelearningasaservice mlaas provider this puts the integrity of the trained model at risk in 2017 liu et al found that by mixing the training data with a few malicious samples of a certain trigger pattern hidden functionality can be embedded in the trained network which can be evoked by the trigger pattern 33 we refer to this kind of hidden malicious functionality as neural trojans in this paper we survey a myriad of neural trojan attack and defense techniques that have been proposed over the last few years in a neural trojan insertion attack the attacker can be the mlaas provider itself or a third party capable of adding or tampering with training data in most research on attacks the attacker selects the trojans functionality and a set of input patterns that will trigger the trojan training data poisoning is the most common way to make the neural network acquire the trojan functionality trojan embedding methods that modify the training algorithm or directly interfere with the neural networks execution at the binary level have also been studied defense techniques include detecting neural trojans in the model andor trojan trigger patterns erasing the trojans functionality from the neural network model and bypassing the trojan it was also shown that carefully crafted neural trojans can be used to mitigate other types of attacks we systematize the above attack and defense approaches in this paper
boundarylayeringesting fans and compressors in the nextgeneration turbofan engines require highperformance operations under distorted inflow the aim of this work is to study the effects of inlet distortions including inlet stagnation pressure and temperature distortion on the aerodynamic performance of a transonic axial fan firstly the validated fullannulus unsteady threedimensional computational fluid dynamic code in conjunction with detached eddy simulation approach is used here to simulate the fan flows assembly with individual inlet stagnation pressuretemperature distortion then the propagation process of the inlet distortion waves is analyzed to understand how the aerodynamic performance degradation is triggered the simulation results show that the fan performance is remarkably degraded when the inlet distortion is introduced the leadingedge spillage the trailing edge back flow and the “tornado vortex” occur when parts of fan blades encounter the incoming distorted flows finally the responses of fan to the combined inlet stagnation distortion effects are discussed in this paper it is found that the combined distortion effects can be predicted based on the sum of the performance responses to the individual constituent distortions furthermore the relative location of the constituent distortions shows a nonignorable influence on the overall fan performance especially for the intensified inlet distortion
in this paper a novel ac to ac wireless power transfer wpt system is introduced for electric vehicle ev charging applications to reduce cost and design complexity the presented wireless power transfer concept achieves unity power factor upf on the gridside by using a hybrid frequency ac  ac converter without an additional converter stage and closed loop control compared to the conventional systems due to inherent merit of the proposed ac  ac converter ac input with 60 hz grid frequency can be directly transferred to the load by superimposing with high frequency switching signal through the wireless coils to validate the theoretical analysis of the proposed wpt system the experimental results of the proposed converter are provided for 650 w output power by using 6 inches air gap between the couplers with the input of 110 vrms ac source the system overall efficiency is measured 89  achieving 099 power factor pf and 15  current total harmonic distortion thd
in order to use membrane computing models for real life applications there is a real need for software that can read a model from some form of input media and afterwards execute it according to the execution rules that are specified in the definition of the model another requirement of this software application is for it to be capable of interfacing the computing model with the real world this chapter discusses how this problem was solved along the years by various researchers around the world after presenting notable examples from the literature the discussion continues with a detailed presentation of three membrane computing simulators that have been developed by the authors at the laboratory of natural computing and robotics at the politehnica university of bucharest romania
reservoir engineering is of great significance for the reduction of regional flood disasters and ensuring the sustainable development of agriculture this paper proposed a decisionmaking model based on generalized intuitionistic fuzzy soft sets and topsis first an evaluation index system was comprehensively identified and constructed then generalized intuitionistic fuzzy soft sets were used to describe the index attribute values of emergency plans to fully reflect the certainty uncertainty and hesitancy of indexes and their weights were calculated by fuzzy ordered weighted averaging fowa to adequately consider the ambiguity of experts’ judgment finally the topsis method was extended via the generalized intuitionistic fuzzy soft sets to the sequencing of emergency plans in addition the wangjiazhou reservoir project in china was selected as a case study the case study demonstrated that full use of emergency materials and personnel was the most important factor and the plan of the overflow rockfill dam was the optimal flood prevention emergency plan compared with the classical topsis the new model proposed in this paper was found to have improved feasibility and effectiveness and its evaluation results were more objective and reasonable therefore the proposed method could provide both theoretical and practical reference
food waste is a global challenge from collection to disposal the problem associated with food waste is on the increase ranging from its discharged lost degradation and contamination food wastage can be effectively managed through proper storage purchasing what is needed and giving excess to those in need the most effective means of managing food waste is through effective sorting at source and recycling for industrial processes for the production of valueadded products thereby reducing the options of incineration and landfilling research has been carried out on food waste for the production of energy and other valueadded products this review aims to provide a brief overview of food waste from the farm gate retailer household and the impact of the pandemic in the increase of food waste the potential strategies of effectively management of food waste both in developed and developing countries are discussed
the complementary fusion of global and local features can effectively improve the performance of image retrieval this article proposes a new local texture descriptor combined with statistical modeling in transform domain for texture image retrieval the proposed local descriptor calculates the eight directions of the central pixel by using the relationship between the central pixel and the neighboring pixels in six directions which is called the local eight direction pattern ledp in the texture image retrieval system of this article the feature extraction part combines global statistical features and local pattern features among them both the relative magnitude rm subband coefficients and relative phase rp subband coefficients are modeled as wrapped cauchy wc distribution in the dualtree complex wavelet transform dtcwt domain and the global statistical features employ the parameters of this model while the local pattern features respectively choose the local binary pattern lbp histogram features in the spatial domain and the ledp histogram features of each direction subband in the dtcwt domain on the other hand the similarity measurement selects matching distances for different features and combines them in the form of convex linear optimization texture image retrieval experiments are conducted in the corel1k database db1 brodatz texture database db2 and mit vistex texture database db3 respectively experimental results show that compared with the best existing methods the approach proposed in this article has achieved better retrieval performance
highperformance motor drives that operate in harsh conditions require an accurate and robust angular position measurement to correctly estimate the speed and reduce the torque ripple produced by angular estimation error for that reason a resolver is used in motor drives as a position sensor due to its robustness a resolvertodigital converter rdc is an observer used to get the angular position from the resolver signals most rdcs are based on angle tracking observers atos on the other hand generalized predictive control gpc has become a powerful tool in developing controllers and observers for industrial applications however no gpcbased rdc with zero steadystate error during constant speed operation has been proposed this paper proposes an rdc based on the secondorder difference gpc sodgpc in sodgpc the secondorder difference operator is applied to design a gpc model with two embedded integrators thus the sodgpc is used to design a typeii ato whose steadystate angle estimation error tends to zero during constant speed operation simulation and experimental results prove that the proposed rdc system has better performance than other literature approaches
we present the results of short observational program for asteroid 6478 gault with using of v and r johnson filters realized at the kyiv comet station during januaryapril 2019 color indices and distribution of brightness in the cometlike tail were calculated we made a comparative analysis of circumstances of the asteroid 6478 gault activity with others morphologically similar active main belt asteroids
wireless channel utilization prediction is useful in a number of applications such as the recently proposed modalities of lte networks allowing them to use unlicensed bands lteu otherwise used by wifi devices wireless utilization as we are also able to also confirm exhibits nonstationary behavior the presented research provides an overall prediction strategy that can be implemented at the network edge while the legacy view of ”busy” hours vs ”nonbusy” hours is still relevant we approach the modeling with a finer definition for this busynonbusy distinction we split the utilization time series into intervals each of them approximated as a stationary process modeled as a markov chain each of those micromodels captures the shortterm behavior and is characterized by its steady state distribution the steady state distributions are used to define similarity among intervals in terms of their shortterm behavior ie the micromodels become a ”library” of prior behaviors we use a shallow neural network that combines features that express the similarity to a set of prior intervals together with features arising from the time series using an autoregressive model following the boxjenkins method alongside features capturing straightforward steptostep lag one transitions the shallow network allow us to interpret the relative importance of the various features it allows us to glean from the weights assigned why naive models predicting next what has just been observed
cyberphysical production systems cpps are mechatronic systems monitored and controlled by software brains and digital information despite its fast development along with the advancement of industry 40 paradigms an adaptive monitoring system remains challenging when considering integration with traditional manufacturing factories in this paper a failure predictive tool is developed and implemented the predictive mechanism underpinned by a hybrid model of the dynamic principal component analysis and the gradient boosting decision trees is capable of anticipating the production stop before one occurs the proposed methodology is implemented and experimented on a repetitive milling process hosted in a realworld cpps hub the online testing results have shown the accuracy of the predicted production failures using the proposed predictive tool is as high as 73 measured by the auc score
—this work presents a multiday ahead streamﬂow forecasting scheme for the madeira river the forecasting model is based on unidimensional convolutional neural networks to design the model we present an analysis of the streamﬂow and turbidity time series at the studied river this analysis indicated that they have a period of 362 days and are 38 days out of phase lagged streamﬂow furthermore we conclude that the proposed forecasting model is ∼ 5 times smaller when using the river’s turbidity alongside its streamﬂow
peter a koss 2 ∗ reza tavakoli dinani † luc bienstman georg bison and nathal severijns fraunhofer institute for physical measurement techniques ipm 79110 freiburg germany instituut voor kernen stralingsfysica university of leuven b3001 leuven belgium faculty of engineering technology university of leuven b3001 leuven belgium paul scherrer institute 5232 villigen switzerland dated march 22 2021
 singlephase multilevel neutralpointclamped npc voltage source inverter has been widely applied in ac traction drive system the carrierbased pulse width modulation cbpwm method and the space vector pulse width modulation svpwm method are mainly two modulation methods for singlephase npc multilevel converters and there is a certain relationship between these two modulation strategies in this study the unified relationship between svpwm and cbpwm in singlephase threelevel and fivelevel npc has been analysed then by injecting the zerosequence component into the initial modulation wave the same switching state and output current as svpwm can be achieved through modulation waves decomposed under the cbpwm strategy finally the unified theory of svpwm and cbpwm in an n level inverter with an arbitrary number of segments sequence was deduced the simulation and experimental results illustrate that the unified theory between cbpwm and svpwm is correct with the zerosequence injected cbpwm can realise the same effect as svpwm but the former is simpler than the latter which is easy to promote for higherlevel applications
past work investigating spatial cognition suggests better mental rotation abilities for those who are fluent in a signed language however no prior work has assessed whether fluency is needed to achieve this performance benefit or what it may look like on the neurobiological level we conducted an electroencephalography experiment and assessed accuracy on a classic mental rotation task given to deaf fluent signers hearing fluent signers hearing nonfluent signers and hearing nonsigners two of the main findings of the study are as follows 1 sign language comprehension and mental rotation abilities are positively correlated and 2 behavioral performance differences between signers and nonsigners are not clearly reflected in brain activity typically associated with mental rotation in addition we propose that the robust impact sign language appears to have on mental rotation abilities strongly suggests that sign language use should be added to future measures of spatial experiences
in order to improve the stability control ability of flexible lower limb exoskeleton robot a dynamic trajectory tracking control algorithm of flexible lower limb exoskeleton robot based on steadystate closedloop learning is proposed gyroscope and rangefinder are used as information sensors of flexible lower limb exoskeleton robot to collect position information of flexible lower limb exoskeleton robot fuse collected positioning information of flexible lower limb exoskeleton robot fuse physical information and measure parameters of flexible lower limb exoskeleton robot by using dynamic information measurement method and obtain mapping feature set of cartesian space according to trajectory components of flexible lower limb exoskeleton robot the pose information of the flexible lower limb exoskeleton robot is obtained through the forward kinematics and the information is enhanced according to the spatial position information the steadystate closedloop learning method is adopted to realize the adaptive learning of the robot dynamic trajectory tracking control the simulation results show that this method is adaptive to the dynamic trajectory tracking control of the robot and the positioning control ability of the robot is strong
indoor positioning techniques have become very important in recent years due to the wide deployment of surveillance cameras it has become feasible to use the videos for indoor positioning the success of using this approach can also reduce the load of security persons of watching the monitors all the time in this study the authors propose a visionbased indoor positioning system the proposed method uses a frame processing technique and applies the gaussian mixture learning for video background model the foreground object can be extracted by using the background subtraction based on the foreground object the objects can be tracked and used in the direct linear transform and generate a birdseye map with camera information a realtime demonstration has been also provided it shows the tracing of the moving objects and the birdseye view
recently the concept of evaluating an unusually large learning effort of an adaptive system to detect novelties in the observed data was introduced the present paper introduces a new measure of the learning effort of an adaptive system the proposed method also uses adaptable parameters instead of a multiscale enhanced approach the generalized pareto distribution is employed to estimate the probability of unusual updates as well as for detecting novelties this measure was successfully tested in various scenarios with i synthetic data ii real time series datasets and multiple adaptive filters and learning algorithms the results of these experiments are presented
optimizing vehicle routing is a key step to reduce the carbon emissions in the transportation industry which is an important goal for achieving national sustainable development so multiobjective vehicle routing problem is studied under low carbon emission in this paper we propose a multiobjective mixedinteger programming model to minimize the number of vehicles travel distance and carbon emission costs by considering each vehicles loading capacity and the number of customers served the nsgaii is designed for solving the model and the effectiveness of the algorithm is verified by the actual example of the sh logistics company it is shown that the carbon emission costs are reduced by 245 and the total travel distance is reduced by 1421 after comparing the optimized result and the former one the research results of this paper provide a decision basis for logistics companies to plan routes under a lowcarbon background
we propose and demonstrate a novel method for controlling stimulated brillouin scattering with light in a phasesensitive manner our results indicate that brillouin gain can be enhanced or suppressed in a polarizationmaintaining fiber via acoustic wave interference by controlling the relative phase of orthogonally polarized light which induces acoustic waves this method paves the way for the alloptical control of brillouin interaction
the separation of desired monomers from a liquidphase mixture of lignin depolymerization products is necessary to facilitate their upscaling and upgrading for industrial applications one effectiv
with the global pandemic of the coronavirus disease virtual reality simulation vsim has emerged as a simulation educational method the purpose of this study is to examine the learning effects of vsim by comparing three different educational modalities of nursing care for children with asthma a quasiexperimental design with three different teaching methods vsim highfidelity simulation hfs and vsim with hfs were used in the study the group using vsim with hfs showed the highest scores in knowledge confidence in practice and performance compared to groups using vsim or hfs alone simulation practice using vsim combined with hfs could be an effective educational method for nursing students
nonorthogonal multiple access noma is a key technology for future wireless systems keeping the number of antennas on the devices unchanged noma increases the spectral efficiency of the wireless system and raises the aggregated system throughput for this reason noma is a very attractive technology for implementation in future wifi standards that will come after wifi 7 the paper proposes and evaluates uplink noma in wifi the obtained results show approximately 100 gain in throughput of the uplink noma comparing with the legacy wifi systems
résumé en se basant sur les travaux liés aux acteurs de la supply chain zdans une logique de qualification d’une part et sur ceux liés aux outils numériques de la supply chain dans une logique d’intégration d’autre part cet article se propose d’identifier les déterminants del’acceptation des technologies d’information ti dans les supplychains africaines pour aborder ce questionnement nous avons adopté une démarche méthodologique de type quantitative via une régression logistique binaire et des tests additionnels les résultats indiquent tout d’abord une faible diffusion du digital dans les tâches effectuées par les acteurs de la supply chain mais ils montrent aussi que ces projets de transformation digitale sontdes préalables importants à la fois pour la coopération et pour le partage d’information logistique cet article souligne enfin le rôle clé et incitateur du taux d’équipements dans l’acceptation des technologies mises à disposition
the relationship between dark triad traits and risky behaviours has been shown in recent years however few studies have attempted to disentangle this relationship using a personcentred approach the goal of the current study was to identify subgroups of individuals on the basis of their scores on machiavellianism psychopathy and narcissism and analyse the differences between them in a set of risky behaviours ie frequency of substance use reactive and proactive aggression risk perception and risk engagement and problematic internet use the sample consisted of 317 undergraduates aged 18–34 46 males the results of the latent profile analysis showed five subgroups of individuals that were identified based on their scores on the dark triad traits lowdark triad narcissistic machiavelliannarcissistic psychopathic and machiavellianpsychopathic overall the machiavelliannarcissistic and machiavellianpsychopathic subgroups showed higher scores for most risky behaviours the lowdark triad scored higher for risk perception no significant differences between subgroups were found as regards frequency of alcohol tobacco and cannabis use these findings suggest that the combination of the dark triad traits lead to more negative outcomes as regards risky behaviour than individual components moreover they highlight the relevance of using a personcentred approach in the study of dark personalities
"
content enhancement of realworld environments is demonstrated through the combination of machine learning methods with augmented reality displays advances in machine learning methods and neural network architectures have facilitated fast and accurate object and image detection recognition and classification as well as providing machine translation natural language processing and neural network approaches for environmental forecasting and prediction these methods equip computers with a means of interpreting the natural environment augmented reality is the embedding of computergenerated assets within the realworld environment here i demonstrate through the development of four sample mobile applications how machine learning and augmented reality may be combined to create localised context aware and usercentric environmental information delivery channels the sample mobile applications demonstrate augmented reality content enhancement of static realworld objects to deliver additional environmental and contextual information language translation to facilitate accessibility of forecast information and a location aware rain event augmented reality notification application that leverages a nowcasting neural network
"
abstract object tracking plays a crucial role in remote sensing for the unmanned aerial vehicle uav in recent years deep learning contributes hugely to the visual object tracking and one typical application is that deep features extracted from convolutional neural networks are widely employed for robust representations of the tracked object as early layers retain higher spatial accuracy and the latter ones contain more semantic information however the potential of deep features as well as their fusion has not been thoroughly achieved in order to fully utilize multilevel deep features multiple recommenders based on discriminative correlation filters are constructed in this work and provided with a combination of deep features from different layers each recommender tracks the object independently and its reliability is evaluated based on the voting from other recommenders as well as from itself the result of the recommender evaluated as the best will be learned by others adaptively extensive experiments on 100 challenging uav image sequences have demonstrated that the proposed method outperforms recently developed 25 stateoftheart trackers in terms of robustness and accuracy
secret information sharing through image carriers has aroused much research attention in recent years with images’ growing domination on the internet and mobile applications the technique of embedding secret information in images without being detected is called image steganography with the booming trend of convolutional neural networks cnn neuralnetworkautomated tasks have been embedded more deeply in our daily lives however a series of wrong labeling or bad captioning on the embedded images has left a trace of skepticism and finally leads to a selfconfession like exposure to improve the security of image steganography and minimize task result distortion models must maintain the feature maps generated by taskspecific networks being irrelative to any hidden information embedded in the carrier this paper introduces a binary attention mechanism into image steganography to help alleviate the security issue and in the meantime increase embedding payload capacity the experimental results show that our method has the advantage of high payload capacity with little feature map distortion and still resist detection by stateoftheart image steganalysis algorithms
market power behaviour often occurs in modern wholesale electricity markets mixed complementarity problems mcps have been typically used for computational modelling of market power when it is characterised by an oligopoly with competitive fringe however such models can lead to myopic and contradictory behaviour previous works in the literature have suggested using conjectural variations to overcome this modelling issue we first show however that an oligopoly with competitive fringe where all firms have investment decisions will also lead to myopic and contradictory behaviour when modelled using conjectural variations consequently we develop an equilibrium problem with equilibrium constraints epec to model such an electricity market structure the epec models two types of players pricemaking firms who have market power and pricetaking firms who do not in addition to generation decisions all firms have endogenous investment decisions for multiple new generating technologies the results indicate that when modelling an oligopoly with a competitive fringe and generation investment decisions an epec model can represent a more realistic market structure and overcome the myopic behaviour observed in mcps the epec considered found multiple equilibria for investment decisions and firms profits however market prices and consumer costs were found to remain relatively constant across the equilibria in addition the model shows how it may be optimal for pricemaking firms to occasionally sell some of their electricity below marginal cost in order to deincentivize pricetaking firms from investing further into the market such strategic behaviour would not be captured by mcp or costminimisation models
 there is a large need for eﬀective and eﬃcient testing processes and tools for mobile applications due to their continuous evolution and to the sensitivity of their users to failures industries and researchers focus their eﬀort to the realization of eﬀective fully automatic testing techniques for mobile applications many of the proposed testing techniques lack in eﬃciency because their algorithms cannot be executed in parallel in particular active learning testing techniques usually relay on sequential algorithms in this paper we propose a active learning technique for the fully automatic exploration and testing of android applications that parallelizes and improves a general algorithm proposed in the literature the novel parallel algorithm has been implemented in the context of a prototype tool exploiting a componentbased architecture and has been experimentally evaluated on 3 open source android applications by varying diﬀerent deployment conﬁgurations the measured results have shown the feasibility of the proposed technique and an average saving in testing time between 33 deploying two testing resources and about 80 deploying 12 testing resources
photoacoustic imaging has shown great potential for noninvasive highresolution deeptissue imaging minimizing the optical and acoustic paths for excitation and detection could significantly increase the signaltonoise ratio this could be accomplished by transparent transducers permitting throughtransducer illumination however most ultrasound transducers are not optically transparent capacitive micromachined ultrasound transducer cmut technology has compelling properties compared to piezoelectric transducers such as wide bandwidth and high receive sensitivity here we introduce transparent cmut linear arrays with high transparency in the visible and nearinfrared range to fabricate the devices we used an adhesive wafer bonding technique using photosensitive benzocyclobutene bcb as both a structural and adhesive layer with a glassindiumtinoxide ito substrate silicon nitride is used as the membrane material ensuring hermiticity and optical transparency our fabricated transducer arrays consist of 64 and 128 elements with immersion operation frequency of 8 mhz enabling highresolution imaging ito along with thin metal strips are used as a conductive layer for the top electrodes with minimal impact on device transparency fabricated devices have shown average transparency of 70 in the visible wavelength range that goes up to 90 in the nearinfrared range arrays are wirebonded to interfacing electronics and connected to a research ultrasound platform for phantom imaging arrays exhibited signaltonoise snr of 40 db with 30v bias voltage and laser fluence of 135 mjcm2 arrays with 128 channels provided lateral and axial resolutions of 234 µm and 220 µm respectively
synthetic aperture radar sar tomography is a technique to provide direct threedimensional 3d imaging of the illuminated targets by processing sar data acquired from different trajectories in a large part of the literature 3d imaging is achieved by assuming monodimensional 1d approaches derived from sar interferometry where a vector of pixels from multiple sar images is transformed into a new vector of pixels representing the vertical profile of scene reflectivity at a given range azimuth location however monodimensional approaches are only suited for data acquired from very closelyspaced trajectories resulting in coarse vertical resolution in the case of continuous media such as forests snow ice sheets and glaciers achieving fine vertical resolution is only possible in the presence of largelyspaced trajectories which involves significant complications concerning the formation of 3d images the situation gets even more complicated in the presence of irregular trajectories with variable headings for which the one theoretically exact approach consists of going back to raw sar data to resolve the targets by 3d backprojection resulting in a computational burden beyond the capabilities of standard computers the first aim of this paper is to provide an exhaustive discussion of the conditions under which highquality tomographic processing can be carried out by assuming a 1d 2d or 3d approach to image formation the case of 3d processing is then further analyzed and a new processing method is proposed to produce highquality imaging while largely reducing the computational burden and without having to process the original raw data furthermore the new method is shown to be easily parallelized and implemented using gpu processing the analysis is supported by results from numerical simulations as well as from real airborne data from the esa campaign alptomosar
this paper presents a review of smart led lighting systems applied to smart buildings the study is focused on drivers protocols technologies communication networks and applications an extended overview of the methodologies used for led lighting control in smart buildings is addressed it also presents an integrated architecture in order to achieve the necessary services and control methodologies for intelligent building energy management system ibems for led lightings systems in smart buildings
a review of the global feature color comprising of seven different metrics are discussed here the color is the most powerful feature for describing the images this study contains four sections the first section explains the related techniques used in various papers the second section explains the two different kinds of metrics 1 similarity metrics such as cosine and correlation and 2 dissimilarity metrics such as euclidean manhattan bhattacharyya chisquared and intersection the third section explains experiment results using caltechucsd birds200 image library the fourth section gives the conclusion and future work in this experiment the query image can be divided into trained indexed or untrained nonindexed in the similarity metric analysis the experimental results show that the cosine similarity gives better similarity score than correlation similarly in the dissimilarity metric analysis the bhattacharyya gives a better result than other distance metrics
when a converter is composed of cascaded system with another unknown converter the optimal design of the converter is especially important owing to stability of the cascaded systems battery ‘s model is introduced to analysis battery converter’s different work conditions are considered battery converter is the front stage when it discharges and it is the rear one when it charges in a cascade dc system from the view of impedance it’s good for the cascaded system stability to reduce output impedance of battery converter when the converter is the first stage another increasing the converter’s input impedance could improve the cascade system stability when the converter becomes the rear one the battery equivalent circuit model is introduced as a key component of system battery bidirectional converter should act as the front or rear stage of cascaded system selection rules of lc filter are put forward according to analyzing the inductor and capacitor’s effects on two conditions the pi controller parameters’ effects on impedance are analyzed to furnish the basis for the pi controller parameters selection finally experiment is carried out to verify the correctness of the analyses
abstract for several decades second homes have gained popularity across europe for various socioeconomic reasons it is important to understand the factors prompting owners to migrate to the destination area or preventing them from doing so discussions about “home” and “migration” here consider the emerging explanatory opportunities brought about by the “new mobility paradigm” the purpose of this work is to examine whether secondhome owners are prone to switch their housing pattern hence permanently move to their second home or to maintain the status quo following a more flexible lifestyle by using both homes an empirical investigation aimed at identifying the key factors fostering secondhome owners’ intentions of future relocation to a holiday destination is proposed individual observations were collected through a survey posted to secondhome owners in the lake maggiore region southern switzerland results show that most of the secondhome owners are happy to continue their current flexible housing patterns and enjoy the best of both homes rather than opting for permanent relocation this study also demonstrates the importance of the owner’s sociodemographic and psychological traits as well as objective and subjective hostcommunity characteristics in explaining individuals’ future housing intentions
transportation systems play a major role in modern urban contexts where citizens are expected to travel in order to engage in social and economic activities modern transportation systems incorporate technologies that generate huge volumes of data which can be processed to extract valuable mobility information this article describes a proposal for studying public transportation systems following an urban data analysis approach a thorough analysis of the transportation system in montevideo uruguay and its usage is outlined combining several sources of urban data furthermore origindestination matrices which describe mobility patterns in the city are generated using ticket sales data the computed results are validated with a recent mobility survey finally a visualization web application is presented which allows conveying mobility information in an intuitive way
an important issue in the mobile sink wireless sensor networks mswsns is sensor energy optimization in order to alleviate the problem of unbalanced network load and high energy consumption in mswsns we proposed a new data collection protocol in this paper seamless clustering multihop routing protocol based on improved artificial bee colony algorithm iabcp because of limited by the communication sensing range and intelligence of ordinary nodes routing paths can only be constructed by crude methods and the movement of the sink node will generate a large amount of energy consumption for locating the sink node in order to solve this problem we assign the task of routing table generation to the sink node which will generate the routing table through the improved artificial bee colony algorithm in addition we adopt a new method to select cluster head ch nodes node uses the average energy of the surrounding nodes and its own residual energy to calculate the claimed cluster head time moreover we added a sub cluster head chβ node when the ch node reaches the number of replacement rounds the chβ node becomes ch directly the simulation results show that our routing protocol is more robust compared with three other protocols
as one of the most promising emerging nonvolatile memory nvm technologies spintransfer torque magnetic random access memory sttmram has attracted significant research attention due to several features such as high density zero standby leakage and nearly unlimited endurance however a highquality test solution is required prior to the commercialization of sttmram in this paper we present all sttmram failure mechanisms manufacturing defects extreme process variations magnetic coupling sttswitching stochasticity and thermal fluctuation the resultant fault models including permanent faults and transient faults are classified and discussed moreover the limited test algorithms and designfortestability dft designs proposed in the literature are also covered it is clear that test solutions for sttmrams are far from well established yet especially when considering a defective part per billion dppb level requirement we present the main challenges on the sttmram testing topic at three levels failure mechanisms fault modeling and testdft designs
depth imagebased rendering dibr plays an important role in 3d video and free viewpoint video synthesis however artifacts might occur in the synthesized view due to viewpoint changes and stereo depth estimation errors holes are usually outoffield regions and disocclusions and filling them appropriately becomes a challenge in this paper a virtual view synthesis approach based on asymmetric bidirectional dibr is proposed a depth image preprocessing method is applied to detect and correct unreliable depth values around the foreground edges for the primary view all pixels are warped to the virtual view by the modified dibr method for the auxiliary view only the selected regions are warped which contain the contents that are not visible in the primary view this approach reduces the computational cost and prevents irrelevant foreground pixels from being warped to the holes during the merging process a color correction approach is introduced to make the result appear more natural in addition a depthguided inpainting method is proposed to handle the remaining holes in the merged image experimental results show that compared with bidirectional dibr the proposed rendering method can reduce about 37 rendering time and achieve 97 hole reduction in terms of visual quality and objective evaluation our approach performs better than the previous methods
neural networks achieve outstanding accuracy in classification and regression tasks however understanding their behavior still remains an open challenge that requires questions to be addressed on the robustness explainability and reliability of predictions we answer these questions by computing reachable sets of neural networks ie sets of outputs resulting from continuous sets of inputs we provide two efficient approaches that lead to over and underapproximations of the reachable set this principle is highly versatile as we show first we use it to analyze and enhance the robustness properties of both classifiers and regression models this is in contrast to existing works which are mainly focused on classification specifically we verify nonrobustness propose a robust training procedure and show that our approach outperforms adversarial attacks as well as stateoftheart methods of verifying classifiers for nonnorm bound perturbations second we provide techniques to distinguish between reliable and nonreliable predictions for unlabeled inputs to quantify the influence of each feature on a prediction and compute a feature ranking
this study produces technological pedagogical and content knowledge tpack instruments which conventionally use print modules the aims of this study were 1 to develop appropriate websitebased tpack instruments and 2 to determine the effect of the produced websitebased tpack instruments toward the pedagogy competence of science teachers in their learning preparation in the classroom the methodology used was based on the addie stages which stand for analyze design develop implement and evaluate the results of this study include 1 instructions for producing tpack instruments based on the results of synthesis 2 instructions for making a website format based on the synthesis results 3 tpack framework design in website content 4 tpack instruments validation results and 5 website validation results moreover limited and field tests were conducted upon the websitebased tpack instruments to determine their effect toward the pedagogy competence of science teachers  quantitatively the tpack instruments scored 70 out of 76 while the website scored 60 out of 90 finally the limited and field tests results showed that websitebased tpack instruments positively affect the pedagogy competence of science teachers in their learning preparation in the classroom with an overall good category and a very good category for the usage evaluation from the users
mental imagery is the mental ability for representing static or dynamic mainly optical information in working memory mental imagery is involved in supporting mental operations as perception strategic planning concept formation pattern recognition problem solving etc and furthermore in dyslexia dyscalculia and other learning disabilities the usage of mobile and other applications to assess intervene and finally improve mental imagery abilities is under investigation in this article mental imagery via mobile tech apps can offer great opportunities to people who encounter many problems as far as concerned mental health and more specifically meliorate their mental state brain activity high spatial ability development of their perception in this paper we identified that advantage of technology through avatars virtual reality and mobile devices mental imagery and mobile tech apps are extremely beneficial in anxiety stress disorders depression and people with brain injury or other disabilities
this paper presents a butler matrix design and construction in the k band using 3 db and 0 db hybrid couplers designed in substrate integrated waveguide siw technology that covers the 18  22 ghz bandwidth with excellent performance in terms of phase error and amplitude imbalance the measured results supported by simulations show an input coincidence better than 20 db transmission and coupling losses of around 8 db and isolation better than 15 db
based on matrix completion algorithm we proposed a simple method to recover the missing regions in the xray crystal structures using the corresponding nuclear magnetic resonance nmr measurement data for the proteins with both xray and nmr experimental data deposited in protein data bank pdb by selecting 10 test proteins deposited in pdb and comparing with the standard modeller results from the rootmeansquare deviation and molprobity aspects we validated that our method can provide a better protein structure model which combines both xray crystallographic structure data and nmr data together than modeller algorithm this method is particularly useful for building the initial structures in molecular dynamics when studying the protein folding process
in this work we solve the problem of the coexistence of periodic orbits in homogeneous boolean graph dynamical systems that are induced by a maxterm or a minterm boolean function with a direct underlying dependency graph specifically we show that periodic orbits of any period can coexist in both kinds of update schedules parallel and sequential this result contrasts with the properties of their counterparts over undirected graphs with the same evolution operators where fixed points cannot coexist with periodic orbits of other different periods these results complete the study of the periodic structure of homogeneous boolean graph dynamical systems on maxterm and minterm functions
this paper proposes a new topology for onboard chargers obcs of light electric vehicles levs a 2 kw obc is designed with its bidirectional operation to adjust the output power in a controlled manner while keeping input current under iec standards a twostage obc is designed the first stage is a voltage source converter which regulates the power quality and the voltage at the dc link capacitor the second stage of it is the interleaved cuk icuk converter it controls the discharging operation different controllers are designed for various stages during the change of power flow direction between the grid and the lev the transients are regulated by the controllers
abstract a novel heuristic model based upon chaotic complex systems theory and quantum mechanics is proposed to overcome the dichotomy between mind and body the mind–body interface represents a chaotic system ruled by the probability principle as shown in quantum mechanics neuronal activity shows many patterns of chaotic behavior and applications of chaotic patterns seem to be relevant for research regarding the mind–body relationship and the process of trance a quantum consciousness theory has been proposed largely controversial since quantum physics applies to subatomic world and not to macrostructures such as the brain quantum cognition is an emerging field that applies the formalism of quantum theory to model cognitive phenomena such as information processing by the human brain it overcomes limits and shortcomings of cartesian dualism as well as quantum general theory as hypnosis is a state of consciousness it applies to hypnotic cognitive functioning rather than hypnotic structure
the structural behavior of the nuclear rod bundles that consisted of cylindrical beams was predicted using the spectral element method sem while considering the interaction with the surrounding fluid viscous fluid behavior was utilized in order to calculate the forces acting on the nuclear rod bundles from the incident pressure waves the added mass and fluid coupling on the nuclear rod bundles were determined for the position patterns and gaps of each of the cylindrical beams the pressure field from propagating waves in the surrounding fluid was calculated with respect to the boundary conditions of the surface of the vibrating structures with the increasing number of nuclear rods and decreasing pressure wavelengths the structural vibration of the nuclear rod bundles that were induced by the propagating forces affected the scattering events of the pressure field the frequency response of the nuclear rod bundles from the pressure waves in the water exhibited smaller damping because the incident pressure wave travels without fluid coupling due to the longer wavelength when compared with distance between rods the proposed numerical method can be utilized for the detailed design for effective parameters of a supporting system to reduce the vibration of nuclear fuel rod bundles for safety control
despite high levels of digital technology access among college students technology disruption remains an issue this study was conducted to understand how technology disruption might contribute to socioeconomic disparities in academic performance data were analyzed from a nonrepresentative sample of 748 undergraduate students we examined socioeconomic differences in types of technology problems students experience the consequences of those problems and beliefs about how to handle future problems socioeconomic status was not associated with types of technology problems but it was associated with greater negative consequences and lessefficacious beliefs about handling future situations these findings are consistent with sociological work on socioeconomic differences in student helpseeking they also elaborate mechanistic understanding of the technology maintenance construct finally for those interested in designing to reduce socioeconomic inequalities they suggest the need for interfaces that go beyond information accessibility to facilitate student empowerment and studentteacher communication
this work proposes evaluating statistically the metrological performance of threedimensional reconstructions built with fused longwavelength infrared lwir and visiblelight vl images the image fusion procedure was essentially based on twodimensional wavelet transform and two pixellevel fusion rules the maximum intensity level presented in a previous work of the authors and a new fusion rule which replaces the vl information with the lwir information in the region of the measured object on the images the reconstructions of a translucent cube were performed with a point triangulationbased procedure and its dimension measurements were employed as evaluation criteria the results show that the fused images have more contrast but also more artifacts the fusion procedures generated denser reconstructions with at least 3483 more points considering the metrological result reconstructions with only visiblelight images resulted in maximal 8931 less measurement bias but at least 4725 more uncertainty than the fusion ones the new fusion rule provided the best results with more points in the dense cloud and lower uncertainty the work is important to provide a metrologically viable alternative for threedimensional reconstruction of objects in situations of low contrast or poor texture information in the visible spectrum and in which no target can be applied to the inspected part
robots have become a popular educational tool in secondary education introducing scientific technological engineering and mathematical concepts to students all around the globe in this paper europa an extensible open software and open hardware robotic platform is presented focusing on teaching physics sensors data acquisition and robotics europa’s software infrastructure is based οn robot operating system ros it includes easy to use interfaces for robot control and interaction with users and thus can easily be incorporated in science technology engineering and mathematics stem and robotics classes europa was designed taking into account current trends in educational robotics an overview of widespread robotic platforms is presented documenting several critical parameters of interest such as their architecture sensors actuators and controllers their approximate cost etc finally an introductory stem curriculum developed for europa and applied in a class of high school students is presented
in this paper a promising phasemodulated underwater wireless optical communication uwoc system with silicon photomultiplier sipm based receiver is proposed for the first time and its feasibility has been experimentally demonstrated in a laboratory environment the phase modulation enables the additional degree of freedom available for encoding of information in uwoc given that previous stateoftheart systems mainly employ intensity modulation im schemes with direct detection dd in the proposed uwoc system the information is encoded in the phase of light wave carrier based on differential phaseshift keying dpsk a highly sensitive receiver is built from sipm which is able to compensate the transmission loss and dramatically enhances the performance of the dpsk uwoc system we experimentally show the feasibility of the proposed system at a data rate of 200 mbps for comparison the commonly used avalanche photodiode apd based receiver is also tested comparing with the apd the use of sipm reduces the bers by about two orders of magnitudes the minimum required optical power for achieving a ber below the fec threshold is about −402 dbm which is about 116 db lower than the case of apd
the purpose of this research is to design and realize a pipe leak detection system that can be monitored with an android application water flow sensor is used to determine the flow of water with water flow rate analysis it is possible to know the leakage area pipe leakage rate and the number of leaks in pipe area if there is a water flow rate decrease the data is processed with an nodemcu microcontroller based on internet of things iot the accuracy of all water flow sensors after calibration is 9753 when testing with the determination of each leaked area the results are appropriate the reading of each water flow rate decrease that occurs has an accuracy of determiningthe leakage rate of 902 while the suitability of reading the number of leakage areas depends on the detection of the leakage area and the classification of the leakage rate the greater the water flow reads on the sensors the precission level to determine the level of water pipe leakage is getting better and vice versa this is due to sensors having variable variations in flow rate readings
"during a surface acquisition process using 3d scanners noise is inevitable and an important step in geometry processing is to remove these noise components from these surfaces given as pointsset or triangulated mesh the noiseremoval process denoising can be performed by filtering the surface normals first and by adjusting the vertex positions according to filtered normals afterwards therefore in many available denoising algorithms the computation of noisefree normals is a key factor a variety of filters have been introduced for noiseremoval from normals with different focus points like robustness against outliers or large amplitude of noise although these filters are performing well in different aspects a unified framework is missing to establish the relation between them and to provide a theoretical analysis beyond the performance of each method 
in this paper we introduce such a framework to establish relations between a number of widelyused nonlinear filters for face normals in mesh denoising and vertex normals in point set denoising we cover robust statistical estimation with msmoothers and their application to linear and nonlinear normal filtering although these methods originate in different mathematical theories  which include diffusion bilateral and directional curvaturebased algorithms  we demonstrate that all of them can be cast into a unified framework of robust statistics using robust error norms and their corresponding influence functions this unification contributes to a better understanding of the individual methods and their relations with each other furthermore the presented framework provides a platform for new techniques to combine the advantages of known filters and to compare them with available methods"
to evaluate the relationship between physiological and psychological stress on pregnancy outcome in women undergoing in vitro fertilization‐embryo transfer ivf‐et treatment
multiobject tracking mot is a challenging practical problem for vision based applications most recent approaches for mot use precomputed detections from models such as faster rcnn performing finetuning of bounding boxes and association in subsequent phases however this is not suitable for actual industrial applications due to unavailability of detections upfront in their recent work wang et al proposed a tracking pipeline that uses a joint detection and embedding model and performs target localization and association in realtime upon investigating the tracking by detection paradigm we find that the tracking pipeline can be made faster by performing localization and association tasks parallely with model prediction this and other computational optimizations such as using mixed precision model and performing batchwise detection result in a speedup of the tracking pipeline by 578 19 fps to 30 fps on fullhd resolution moreover the speed is independent of the object density in image sequence the main contribution of this paper is showcasing a generic pipeline which can be used to speed up detection based object tracking methods we also reviewed different batch sizes for optimal performance taking into consideration gpu memory usage and speed
financial markets are often modeled using a random walk for example in the binomial option pricing model which is a discrete version of the blackscholes formula this paper presents an alternative approach to option pricing based on a quantum walk model the quantum walk which incorporates superposition states and allows for effects such as interference was originally developed in physics but has also seen application in areas such as cognitive psychology where it is used to model dynamic decisionmaking processes it is shown here that the quantum walk model captures key aspects of investor behavior while the collapsed state captures the observed behavior of markets the resulting option price model agrees quite closely with the classical random walk model but helps to explain some observed anomalies the method also has the advantage that it can be run directly on a quantum computer
it has been noticed that a lot of fatalities of lives occur every day due to manually operated rail gates all over bangladesh these happen mainly at places where the rail road passes through a city locality or unmanned gates of the crossing zones currently gatemen mainly operate on the assumption of a train departure schedule from the station to reach a crossing zone but at times there are departure delays or maybe the train reaches crossing zones earlier leaving the gateman unprepared to close the gate accidents are more likely to happen in such cases causing severe damage to human lives and properties near the rail crossings this paper presents the development and implementation of automatic rail gate control system as well as real time monitoring of train and obstacle detection for developing countries like bangladesh this research project was carried out using arduino nano along with ir sensor ir led flame sensor servo motor ultrasonic sensor dc gear motor and usb uart board this project is a combination of old technology with recent wireless technology and analytics to provide the best possible service to the nation this paper also suggests the effectiveness of real time information of train position the main objective of the proposed humanitarian project is to ensure the efficiency quality time management and most importantly public safety using wireless based communication network for the development of the railway industry in bangladesh
background coronavirus disease 2019 covid19 has caused a serious epidemic around the world but it has been effectively controlled in the mainland of china the chinese government limited the migration of people almost from all walks of life medical workers have rushed into hubei province to fight against the epidemic any activity that can increase infection is prohibited the aim of this study was to confirm that timely lockdown largescale casescreening and other control measures proposed by the chinese government were effective to contain the spread of the virus in the mainland of china methods based on disease transmissionrelated parameters this study was designed to predict the trend of covid19 epidemic in the mainland of china and provide theoretical basis for current prevention and control an seiqr epidemiological model incorporating asymptomatic transmission short term immunity and imperfect isolation was constructed to evaluate the transmission dynamics of covid19 inside and outside of hubei province with covid19 cases confirmed by the national health commission nhc the optimal parameters of the model were set by calculating the minimum chisquare value results before the migration to and from wuhan was cut off the basic reproduction number in china was 56015 from 23 january to 26 january 2020 the basic reproduction number in china was 66037 from 27 january to 11 february 2020 the basic reproduction number outside hubei province dropped below 1 but that in hubei province remained 37732 because of stricter controlling measures especially after the initiation of the largescale casescreening the epidemic rampancy in hubei has also been contained the average basic reproduction number in hubei province was 34094 as of 25 february 2020 we estimated the cumulative number of confirmed cases nationwide was 82 186 and 69 230 in hubei province on 9 april 2020 conclusions the lockdown of hubei province significantly reduced the basic reproduction number the largescale casescreening also showed the effectiveness in the epidemic control this study provided experiences that could be replicated in other countries suffering from the epidemic although the epidemic is subsiding in china the controlling efforts should not be terminated before may
die digitalisierung fordert auch von unternehmen eine anpassung an moderne kommunikationsmodelle und arbeitsgestaltung vor allem grose und internationale unternehmen stehen dabei gleichzeitig vor grosen innovationspotenzialen und grosen herausforderungen einerseits bildet der einsatz von sozialen technologien innerhalb des betrieblichen kontextes social business die moglichkeit der nutzung des gesamten innovationspotenzials und damit einer effizienzsteigerung andererseits stellen die notwendigen umstrukturierungen bisher unbekannte herausforderungen an das unternehmen dar im hier beschriebenen fall wird der einsatz eines social media tools zur kanalisierung von interner und externer kommunikation gezeigt dabei werden am beispiel des datenschutzes sowohl die technischen als auch die organisatorischkulturellen hindernisse beschrieben schlieslich werden losungsansatze erlautert und weitere im beispiel nicht beachtete aspekte aufgezeigt
a new method for identifying pixels as corrupted by noise is proposed in this paper for effectively denoising corrupted images unlike conventional filtration approaches that apply the filtration operation to each pixel unconditionally or by implementing a noise detection mechanism before filtration we propose to identify noise pixels by their position as a model for noise pixels placement we consider random or regular point patterns to distinguish such pixels placements we form point patterns for image intensity from predefined range and apply a clark evans test to identify the right ones next the conventional filtration approach is applied to each pixel of intensities that form random or regular patterns this approach allowed to increase a pnsr value and to reduce dssim one
this paper presents an inventory model with a constant rate of deterioration of the product and shortages are allowed at the end of the cycle the stockdependent and pricesensitive demand have b
"die entwicklung moderner softwaresysteme basiert oft auf mehreren artefakten diese artefakte teilen sich oft redundante oder abhangige informationen welche wahrend der entwicklung des softwaresystems konsistent gehalten werden mussen die manuelle durchfuhrung dieses prozesses ist arbeitsaufwendig und fehleranfallig konsistenzerhaltungsmechanismen ermoglichen diese artefakte automatisch konsistent zu halten konsistenzerhaltung basiert oftmals auf bidirektionalen transformationen welche ein zielmodell aktualisieren wenn ein quellmodell modifiziert wird wahrend das gebiet der bidirektionale transformationen stark erforscht ist hat konsistenzerhaltung von mehr als zwei modellen bisher weniger aufmerksamkeit erhalten allerdings umfasst die entwicklung von softwaresystemen jedoch oft mehr als zwei modelle folglich benotigt man konsistenzerhaltung zwischen mehr als zwei modellen welche durch netzwerke bidirektionaler transformationen erreicht werden kann 
 
solche transformationsnetzwerke kombinieren mehrere transformationen wobei jede einzelne fur die konsistenzerhaltung zweier modelle verantwortlich ist da die entwicklung jeder transformation individuelles domanenwissen erfordert werden sie in der regel von mehreren domanenexperten unabhangig voneinander entwickelt zusatzlich konnen einzelne transformationen in anderen netzwerken wiederverwendet werden dies wird jedoch in bisherigen arbeiten nicht berucksichtigt macht aber die konsistenzerhaltung durch netzwerke bidirektionaler transformationen anfallig fur probleme in einem netzwerk von transformationen kann es beispielsweise zwei oder mehr verkettungen von transformationen geben die dieselben metamodelle mit verschiedenen anderen metamodellen in beziehung setzen jedoch konnen sie die elemente unterschiedlich miteinander in beziehung setzen dies kann zum beispiel zu einer doppelten erstellung derselben elemente uber die verschiedenen transformationsketten fuhren es gibt jedoch kein systematisches wissen uber die problemarten die in solchen netzwerken auftreten konnen oder ob und wie derartige probleme systematisch verhindert werden konnen 
 
diese thesis fuhrt eine fallstudie durch die ermitteln soll welche arten von problemen bei der konsistenzerhaltung durch netzwerke bidirektionaler transformationen auftreten konnen fur diese probleme leiten wir eine klassifizierung hinsichtlich des erforderlichen wissens fur ihre vermeidung ab fur probleme die transformationsentwickler verhindern konnen schlagen wir strategien zur systematischen vermeidung wahrend ihrer konstruktion vor in unserer fallstudie sind 90 der gefundenen probleme verhinderbar die ubrigen probleme lassen sich wahrend der entwicklung einer einzelnen transformation nicht ohne das wissen uber weitere transformationen im netzwerk vermeiden folglich hilft diese thesis transformationsentwicklern fehler bei der erstellung von transformationen systematisch zu vermeiden und ermoglicht es netzwerkentwicklern fehler zu erkennen die bei der konstruktion der transformation nicht verhindert werden konnen"
aminoalkylated chalcone compounds 4a4c have been designed using quantitative structureactivity relationship qsar analysis synthesized and evaluated for their in vitro antimalarial activity the best qsar model obtained was log ic50  705132 qc7 65573 qc3 24845 qc4 4634 qc13 220479 and statistical analysis showed r 2 of 0937 suggesting that the qsar model was able to predict the actual antimalarial activity by 937 accuracy the addition of secondary amines to the chalcone compounds was successfully carried out using the mannich reaction which was confirmed by spectroscopic analysis the in vitro antimalarial activity of the synthesized compounds were screened against the 3d7 strain of plasmodium falciparum cq sensitive all of the compounds exhibited strong activity with ic50 values ranging from 054 ± 0649 to 112 ± 0369 μm the molecular docking studies investigated interactions of the prepared compounds to the binding site of wildtype plasmodium falciparum dihydrofolate reductasethymidylate synthase pfdhfrts pdb id ij3i and quadruple mutant pfdhfrts pdb id ij3k some hydrogen bond and π – π interactions were observed with the side chain of ala16 asp54 cys15 leu164 tyr170 and met55 in both the wild and mutant pfdhfr types it has also been found that all the tested compounds were obeyed the lipinski’s rule this study proposed that compound 4b can be developed as the new lead of the antimalarial agent
numerous animals adapt their stiffness during natural motions to increase efficiency or environmental adaptability for example octopuses stiffen their tentacles to increase efficiency during reaching and several species adjust their leg stiffness to maintain stability when running across varied terrain inspired by nature variable‐stiffness machines can switch between rigid and soft states however existing variable‐stiffness systems are usually purpose‐built for a particular application and lack universal adaptability here reconfigurable stiffness‐changing skins that can stretch and fold to create 3d structures or attach to the surface of objects to influence their rigidity are presented these “jamming skins” employ vacuum‐powered jamming of interleaved discrete planar elements enabling 2d stretchability of the skin in its soft state stretching allows jamming skins to be reversibly shaped into load‐bearing functional tools on‐demand additionally they can be attached to host structures with complex curvatures such as robot arms and portions of the human body to provide support or create a mold we also show how multiple skins can work together to modify the workspace of a continuum robot by creating instantaneous joints jamming skins thus serve as a reconfigurable approach to creating tools and adapting structural rigidity on‐demand
multimodal hashing methods could support efficient multimedia retrieval by combining multimodal features for binary hash learning at the both offline training and online query stages however existing multimodal methods cannot binarize the queries when only one or part of modalities are provided in this article we propose a novel flexible multimodal hashing fmh method to address this problem fmh learns multiple modalityspecific hash codes and multimodal collaborative hash codes simultaneously within a single model the hash codes are flexibly generated according to the newly coming queries which provide any one or combination of modality features besides the hashing learning procedure is efficiently supervised by the pairwise semantic matrix to enhance the discriminative capability it could successfully avoid the challenging symmetric semantic matrix factorization and on2 storage cost of semantic matrix finally we design a fast discrete optimization to learn hash codes directly with simple operations experiments validate the superiority of the proposed approach
a common scenario in distributed computing involves a client who asks a server to perform a computation on a remote computer an important problem is to determine the minimum amount of communication needed to specify the desired computation here we extend this problem to the quantum domain analyzing the total amount of classical and quantum communication needed by a server in order to accurately execute a quantum process chosen by a client from a parametric family of quantum processes we derive a general lower bound on the communication cost establishing a relation with the precision limits of quantum metrology if a inlineformula texmath notationlatexv texmathinlineformuladimensional family of processes can be estimated with mean squared error inlineformula texmath notationlatexnbeta  texmathinlineformula by using inlineformula texmath notationlatexn texmathinlineformula parallel queries then the communication cost for inlineformula texmath notationlatexn texmathinlineformula parallel executions of a process in the family is at least inlineformula texmath notationlatexbeta  v 2epsilon  log n texmathinlineformula qubits at the leading order in inlineformula texmath notationlatexn texmathinlineformula for every inlineformula texmath notationlatexepsilon 0 texmathinlineformula for a class of quantum processes satisfying the standard quantum limit inlineformula texmath notationlatexbeta  1 texmathinlineformula we show that the bound can be attained by transmitting an approximate classical description of the desired process for quantum processes satisfying the heisenberg limit inlineformula texmath notationlatexbeta 2 texmathinlineformula our bound shows that the communication cost is at least twice as the cost of communicating standard quantum limited processes with the same number of parameters
as a major health problem traumatic brain injury has received increasing attention in recent years mild traumatic brain injury mtbi is the representative of the vast majority of traumatic brain injury however most studies have focused on moderate to severe traumatic brain injury according to reports mtbi patients during the acute period have the most prominent cognitive disorders hence we collected the functional magnetic resonance imaging data in restingstate rsfmri of the acutemtbi patients and the healthy controls to explore the differences of brain functional connectivity between them we chose dynamic functional connectivity in restingstate as features and performed feature selectionextraction finally the classifier based on machine learning methods achieved a good classification accuracy of 8548 most of the extracted brain regions of interest rois with high identification power belong to the sensorimotor and functional connectivities extracted within the sensorimotor has the highest proportion which suggested the sensorimotor may be the most severely damaged part of mtbi patients in the acute stage
sulawesis electricity system needs to maintain the sustainability of the electricity supply which is stable reliable safe ecofriendly and able to meet the needs of the community for that reason the transmission expansionplanning program 20182050 is proposed considering the load growth primary energy and the power flow it is necessary to build the transmission backbone which for indonesian case maybe 275 kv or 500 kv the determination of the backbones voltage requires some criteria one of them is to performing contingency analysis which can be obtained by performance index analysis performance index analysis consist of voltage performance index and active power performance index is simulated using the digsilent powerfactory 1517 software for two generation planning scenarios regional balanced and resourced based if the transmission system has exceeded the level of set severity of the contingency case it is necessary to choose the 500 kv extra high voltage overhead transmission line
in multiple real life situations involving several agents cooperation can be beneficial for all for example some telecommunication or electricity providers may cooperate in order to address occasional resources needs by giving to coopetitors some quantities of their own surplus while expecting in return a similar service however since agents are a priori egoist the risk of being exploited is highin this work we propose to model this kind of situations as a social dilemma a situation where nash equilibrium is non optimal in which each agent knows only its own state we design an algorithm modelling the agents whose goal is to make transactions in order to augment their own utility the algorithm needs to be robust to defection and encourage cooperationour framework modelling each agent consists in iterations divided in four major steps the communication of demandsneeds the detection of opponent cooperation the cooperation response policy and finally the allocation of resourcesin this paper we focus on the cooperation response policy we propose a new version of titfortat and we evaluate it with metrics such as safety and incentivecompatibility several experiments are performed and confirm the relevance of our improvement
a solidstate transformer sst is being proposed for a distribution grid of 138 kv380 v the sst is based on the inputseries outputparallel isop arrangement of twelve modules using the 12 kv sic switches the cost and losses analysis are discussed through a case study regarding the number of modules using the 12 kv and 17 kv sic switches the modules are composed of two stages the first one is an acac lowfrequency rectifier and the second one is an acac medium frequency hybrid switchedcapacitor llc seriesresonant converter hscsrc a single backend acac lowfrequency inverter is employed to generate the low voltage ac output port of the sst the switched capacitor ladder cell in the hscsrc converter enables the reduction of the number of modules employed on the isop arrangement because the voltage stresses on the mv side’s switches are halved while the resonant stage increases the efficiency of the structure due to softswitching on all the switches moreover the frontend acac and backend acac lowfrequency converters enable the use of twoquadrant switches on the acac hscsrc converter reducing the switch count on the medium frequency stage the operating behavior and design methodology of the sst’s modules are presented a single module reduced scale prototype with the rated power of 167 kva and an input voltage of 115 kv and an output voltage of 220 v is experimentally verified the maximum efficiency is 97
the twouser gaussian interference channel gic is revisited with a particular focus on practically amenable discrete input signalling and treating interference as noise tin receivers the corresponding deterministic interference channel dic is first investigated and coding schemes that can achieve the entire capacity region of the dic under tin are proposed these schemes are then systematically translated into multilayer superposition coding schemes based on purely discrete inputs for the realvalued gic our analysis shows that the proposed scheme is able to achieve the entire capacity region to within a constant gap for all channel parameters to the best of our knowledge this is the first constantgap result under purely discrete signalling and tin for the entire capacity region and all the interference regimes furthermore the approach is extended to obtain coding schemes based on discrete inputs for the complexvalued gic for such a scenario the minimum distance and the achievable rate of the proposed scheme under tin are analyzed which takes into account the effects of random phase rotations introduced by the channels simulation results show that our scheme is capable of approaching the capacity region of the complexvalued gic and significantly outperforms gaussian signalling with tin in various interference regimes
the review presents the latest data on the development of a new direction of interdisciplinary integration of radiation and molecular biological technologies ‘omi с s including high technologies in the field of genomics transcriptomics proteomics and metabolomics which are the basis of systems biology and the future of medicine the integration of medical imaging and advances in genetics have created a new direction of research  radiogenomics which is a key step in the development of omhstechnologies radiogenomics  phenotype imaging computer vision  is an interdisciplinary integration of visual radiology and biological systems that study biomedical imaging involving phenotypic and genotypic parameters that reflect the molecular and genotypic basis of tissue from which to predict patient risk and outcomes coupled with stateoftheart analytical software quantitative and qualitative imaging biomarkers bring unprecedented insight into complex tumor biology and contribute to a deeper understanding of cancer development and progression using the latest advances in digital information and molecular biological technology is an active convergence of specialties radiologist and genetics giving the opportunity at the stage of studying medical images of the breast to obtain information about the biological characteristics of the tumor molecular subtype of cancer determining prognosis evaluating risk of recurrence which is important for the choice of adequate tactics of individual monitoring and selection of medical benefits development of visual symptom medical images of the breast characteristic for different molecular subtypes of cancer will contribute to more accurate diagnosis of different manifestations of cancer the choice of adequate treatment tactics that increase the duration and preservation of the high quality of a woman s life
the world is moving progressively towards the power generation from renewable energy resources because these are inexhaustible pollution free and available richly in the nature due to the advantages of renewable power generation the grid interconnection of distributed generator dg is growing in the power market the major drawback of this interconnection is an unintentional islanding the islanding takes place in the power system when the distributed generator continues supply power even when the grid is not connected to the system however an unintentional islanding is an unplanned islanding which is harmful to the power system and the people working in it therefore it is essential to detect an unintentional islanding so to prevent from its harmful consequences the islanding must be detected within minimum time delay ie within a 2 sec as per the ieee standard in this paper a solar photovoltaic dg is integrated with the main grid a total harmonic distortion thd based passive method is employed here to detect the islanding in a grid tied solar pv system in this method changes in level of harmonics are observed in local parameters based on which detects the islanding this method has reduced nondetection zone over other passive methods
in internet of things iot data processing cloud computing alone does not suffice due to latency constraints bandwidth limitations and privacy concerns by introducing intermediary nodes closer to the edge of the network that offer compute services in proximity to iot devices fog computing can reduce network strain and high access latency to application services while this is the only viable approach to enable efficient iot applications the issue of component placement among cloud and intermediary nodes in the fog adds a new dimension to system design state‐of‐the‐art solutions to this issue rely on simulation or solving a formalized assignment problem through heuristics only which both have their drawbacks in this article we present a five‐step process for designing practical fog‐based iot applications that combines best practices simulation and testbed analysis to converge towards an efficient system architecture we then apply this process in a smart factory case study by deploying filtered options to a physical testbed we show that each step of our process converges towards more efficient application designs
background longstanding ulcerative colitis uc is associated with an increased risk of colonic neoplasia various endoscopic modalities such as chromoendoscopy ce narrow band imaging nbi and random biopsy have been introduced for surveillance however there exists a paucity of direct comparisons between them we aimed to conduct a network metaanalysis of randomized controlled trials rcts performed for surveillance of neoplasia in uc aim to provide a comparative evaluation of the efficacy of the abovementioned various modalities methods we searched medlinepubmed web of science embase google scholar and cochrane central registry through may 2016 for rcts evaluating the efficacy of endoscopic modalities for surveillance of neoplasia in uc the primary outcomes of interest were dysplasia low or highgrade detection rates per biopsy and per patient and dysplasia numbers per patient studies were simultaneously analyzed using a randomeffects network metaanalysis under the bayesian framework to identify the modality with the highest dysplasia detection rate the best ranking probability for the dysplasia detection rate was analyzed by surface under the cumulative ranking sucra technique results six prospective rcts of a total 1038 patients were identified we identified 4 different modalities white light wl high definition hd or standard definition sd ce hd and nbi hd for dysplasia per biopsy direct metaanalysis showed superiority of nbi hd over wl hd and ce hd over wl sd network metaanalysis demonstrated the rank order of best modality as nbi hd ce hd wl hd and wl sd with close sucra scores of the first two for dysplasia per patient direct metaanalyses showed equivocal results between each modality network metaanalysis demonstrated the rank order of best modality as wl hd nbi hd ce hd and wl sd with small differences of the sucra score among the first two for dysplasia numbers per patient direct metaanalysis showed superiority of ce hd over wl sd network metaanalysis demonstrated the rank order of best modality as wl hd nbi hd ce hd and wl sd with small differences of the sucra score among the first three conclusion we demonstrated that there were small differences among wl hd nbi hd and ce hd while wl sd was inferior in detecting dysplasia in uc
ships navigable in the polar regions are troubled by sea ice in a changing state of ice all year round and collisions with sea ice can easily cause damage to the ship’s hull in order to solve the above problems this paper proposes an improved artificial potential field method to avoid ship ice collision the nonnavigable area near the planned voyage is defined by the ice number and ice multiplier which is regarded as a dynamic dangerous target then according to the time and space relationship between the nonnavigable area and the ship the collision risk factor is used to adjust the resultant force in the artificial potential field finally a pidbased mmg ship maneuverability model is used to simulate the navigation process of the ship to solve the problem of local minimum in the artificial potential field in this paper simulation experiments on static and dynamic sea ice rinks show that this method will effectively realize the local collision avoidance of ship ice and the ship can safely pass through the above scenes
search and rescue sar missions often use unmanned aerial vehicles uavs with cameras for finding targets in unknown cluttered environments due to the unpredictable nature of these environments reinforcement learning techniques can be used to perform these tasks in this paper we employ reinforcement learning algorithm integrated with a position controller to locate a missing victim in a sar scenario using uav to deal with large state space sizes we use function approximation to achieve faster convergence the results show how the uav can successfully carry out the task in a realistic urban sar scenario however common approaches limit their work to target search in practice searching for mobile targets is not enough as their position may change before the rescue team arrives for evacuation to address this we uniquely combine yolo with optical flow to allow the uav to track and follow the target victim extensive simulations demonstrate its application in sar missions
background diet and exercise may be associated with quality of life and survival in men with prostate cancer objective this study aimed to determine the feasibility and acceptability of a remotely delivered webbased behavioral intervention among men with prostate cancer methods we conducted a multisite 4arm pilot randomized controlled trial of a 3month intervention truenth community of wellness eligibility included selfreported prostate cancer diagnosis having a personal device that connected to the internet age ≥18 years and ability to read english and receive text messages and emails men receiving chemotherapy or radiation or those who reported contraindications to exercise could participate with physician clearance participants were randomized 1111 to additive intervention levels website website and personalized diet and exercise prescription website personalized prescription fitbit and text messages and website personalized prescription fitbit text messages and 2 30minute phone calls—one with an exercise trainer and one with a registered dietician primary outcomes were feasibility accrual and attrition and acceptability survey data and website use we described selfreported diet and exercise behavior at the time of enrollment 3 months and 6 months as secondary outcomes results in total 202 men consented and were randomized between august 2017 and september 2018 level 1 49 level 2 51 level 3 50 level 4 52 a total of 160 men completed the onboarding process and were exposed to their randomly assigned intervention 38 38 42 and 42 in levels 1 2 3 and 4 respectively the followup rate was 827 167202 at 3 months and 772 156202 at 6 months participants had a median age of 70 years and were primarily white and college educated website visit frequency over the 3month intervention period increased across levels median 2 9 11 and 16 visits for levels 1 2 3 and 4 respectively most were satisfied or very satisfied with the intervention 2039 51 2742 64 2344 52 and 2742 64 for levels 1 2 3 and 4 respectively the percentage of men who reported being very satisfied was highest among level 4 participants 1042 24 vs 439 10 542 12 and 544 11 for levels 1 2 and 3 respectively dissatisfaction was highest in level 1 539 13 vs 142 2 344 7 and 242 5 for levels 2 3 and 4 respectively we observed small improvements in diet and physical activity at 3 months among men in level 4 versus those in level 1 conclusions a webbased remotely delivered tailored behavioral intervention for men with prostate cancer is feasible future studies are warranted to increase the effect of the intervention on patient behavior while maintaining sustainability and scalability as well as to design and implement interventions for more diverse populations trial registration clinicaltrialsgov nct03406013 httpclinicaltrialsgovct2shownct03406013
objective medical electrical impedance tomography is a nonionizing imaging modality in which lowamplitude lowfrequency currents are applied on electrodes on the body the resulting voltages are measured and an inverse problem is solved to determine the conductivity distribution in the region of interest due the illposedness of the inverse problem the boundaries of internal organs are typically blurred in the reconstructed image methods a deep learning approach is introduced in the dbar method for reconstructing a 2d slice of the thorax to recover the boundaries of organs this is accomplished by training a deep neural network on labeled pairs of scattering transforms and the boundaries of the organs in the data from which the transforms were computed this allows the network to “learn” the nonlinear mapping between them by minimizing the error between the output of the network and known actual boundaries further a “sparse” reconstruction is computed by fusing the results of the standard dbar reconstruction with reconstructed organ boundaries from the neural network results results are shown on simulated and experimental data collected on a salinefilled tank with agar targets simulating the conductivity of the heart and lungs conclusions and significance the results demonstrate that deep neural networks can successfully learn the mapping between scattering transforms and the internal boundaries of structures
utilizando los métodos de transferencia de beneficios análisis de rentabilidad a tasas de descuento diferenciadas y de sensibilidad económica se evaluaron ocho escenarios que toman como referencia una serie de supuestos asociados a las tasas de crecimiento de los bosques en un plazo de 40 años precios ajustados de los mercados de la madera carbono mercado internacional y nacional miel y la posibilidad de acceder a un subsidio proporcionado por el estado para recuperar bosques nativos ley 20283
this article analyses the competitive priorities underlying manufacturing location initiatives in developed economies specifically building on secondary data we compare and contrast manufacturing backshoring from china by companies headquartered in developed economies 308 cases and chinese foreign direct investment to developed economies 155 cases results suggest that both types of initiatives share some common priorities such as exploiting the ‘country of origin’ effect and innovation opportunities in developed countries at the same time results highlight differences that may be attributed to the home country of the firm in particular cost priorities appear to be more important for chinese companies than for backshoring ones findings offer insight into why manufacturing in developed economies may expand as a result of both repatriations and of foreign direct investments from emerging economies such as china and point to potential areas of policy intervention
this manuscript presents an iterative learning control strategy for a class of singleinput singleoutput siso nonlinearly parameterized systems with unknown timevarying state delays and actuator faults based on some basic assumptions and the property of the state delays and actuator faults of the siso nonlinear system we design the ptype iterative learning reliable controller to deal with the nonlinearity caused by the timedelays term and actuator faults and then a composite energy functioncef is used to show the convergence property of the state tracking error finally a numerical simulation is used to verify the correctness and effectiveness of control scheme
an investigation of the structural magnetic thermodynamic and charge transport properties of noncentrosymmetric hexagonal scfege reveals it to be an anisotropic metal with a transition to a weak itinerant incommensurate helimagnetic state below tn  36 k neutron diffraction measurements discovered a temperature and field independent helical wavevector textbftextitk  0 0 0193 with magnetic moments of 053 mub per formula unit confined to the it abplane density functional theory calculations are consistent with these measurements and find several bands that cross the fermi level along the it caxis with a nearly degenerate set of flat bands just above the fermi energy the anisotropy found in the electrical transport is reflected in the calculated fermi surface which consists of several warped flat sheets along the caxis with two regions of significant nesting one of which has a wavevector that closely matches that found in the neutron diffraction the electronic structure calculations along with a strong anomaly in the it caxis conductivity at tn signal a fermi surface driven magnetic transition similar to that found in spin density wave materials magnetic fields applied in the it abplane result in a metamagnetic transition with a threshold field of approx 67 t along with a sharp strongly temperature dependent discontinuity and a change in sign of the magnetoresistance for inplane currents thus scfege is an ideal system to investigate the effect of inplane magnetic fields on an easyplane magnetic system where the relative strength of the magnetic interactions and anisotropies determine the topology and magnetic structure
the progress of life science and social science research is contingent on effective modes of data storage data sharing and data reproducibility in the present digital era data storage and data sharing play a vital role for productive datacentric tasks findable accessible interoperable and reusable fair principles have been developed as a standard convention however fair principles have specific challenges from computational implementation perspectives the purpose of this paper is to identify the challenges related to computational implementations of fair principles after identification of challenges this paper aims to solve the identified challengesthis paper deploys petri netbased formal model and petri net algebra to implement and analyze fair principles the proposed petri netbased model theorems and corollaries may assist computer system architects in implementing and analyzing fair principlesto demonstrate the use of derived petri netbased theorems and corollaries existing data stewardship platforms – fairdom and dataverse – have been analyzed in this paper moreover a data stewardship model – “datalection” has been developed and conversed about in the present paper datalection has been designed based on the petri netbased theorems and corollariesthis paper aims to bridge information science and life science using the formalism of data stewardship principles this paper not only provides new dimensions to data stewardship but also systematically analyzes two existing data stewardship platforms fairdom and dataverse
since the number of eneolithic cultures has grown considerably in contrast with the previous epochs it was necessary to determine the criteria and select the cultural content the choice of the cucutenitripolye and the harappamohenjodaro as well as of the eurasian steppe cultures has been substantiated and the artifacts for analysis have been selected comparisons of the space depicting features with the characteristics of neolithic painting have been performed a detailed reconstruction of the world tree myth has been carried out and features of the eneolithic version have been compared with the neolithic ones the correlations of markers and the state of levels and channels of subjective space are determined a generalized psychological portrait of a man of the eneolithic era is compiled and his behavioral patterns are described
because remote sensing rs data are spatially and temporally explicit and available across the globe they have the potential to be used for predicting runoff in ungauged catchments and poorly gauged regions a challenging area of research in hydrology there is potential to use remotely sensed data for calibrating hydrological models in regions with limited streamflow gauges this study conducts a comprehensive investigation on how to incorporate gridded remotely sensed evapotranspiration aet and water storage data for constraining hydrological model calibration in order to predict daily and monthly runoff in 30 catchments in the yalong river basin in china to this end seven rs data calibration schemes are explored and compared to direct calibration against observed runoff and traditional regionalization using spatial proximity to predict runoff in ungauged catchments the results show that using bias‐corrected remotely sensed aet bias‐corrected pml‐aet data for constraining model calibration performs much better than using the raw remotely sensed aet data nonbias‐corrected aet obtained from pml model estimate using the bias‐corrected pml‐aet data in a gridded way is much better than using lumped data and outperforms the traditional regionalization approach especially in headwater and large catchments combining the bias‐corrected pml‐aet and grace water storage data performs similarly to using the bias‐corrected pml‐aet data only this study demonstrates that there is great potential in using bias‐corrected rs‐aet data to calibrating hydrological models without the need for gauged streamflow data to estimate daily and monthly runoff time series in ungauged catchments and sparsely gauged regions
writing formats have expanded beyond the pen to include new technologies particularly in this era of immediate digital communication rehabilitation professionals are not routinely incorporating writing formats using technology such as texting keyboard or tablet interfaces when evaluating and treating adult writing disorders the purpose of this study was to obtain normative baseline information about the writing behaviors of typical adults across generations a total of 199 respondents ages 18–106 provided data on rating importance of format frequency of use and ranking of writing activities findings indicated that the importance of technology for communication learning and work activities exists across the generations except the centenarian written forms of enjoyment such as crossword puzzles song lyrics and story writing were meaningful to younger as well as older generations obtaining client perspectives on writing activities and formats that are personally relevant and meaningful can help the clinician create a more clientbased program
room temperature photolysis of the bisazidecobaltateii complex nathfxketguancon32 ketguan  tbu2cncndipp2 dipp  26diisopropylphenyl 3a in thf cleanly forms the binuclear cobalt nitride nathf4ketguancon32μn 1 compound 1 represents the first example of an isolable bimetallic cobalt nitride complex and it has been fully characterized by spectroscopic magnetic and computational analyses density functional theory supports a coiiincoiii canonical form with significant πbonding between the cobalt centers and the nitride atom unlike other group 9 bridging nitride complexes no radical character is detected at the bridging natom of 1 indeed 1 is unreactive towards weak ch donors and even cocrystallizes with a molecule of cyclohexadiene chd in its crystallographic unit cell to give 1·chd as a room temperature stable product notably addition of pyridine to 1 or photolyzed solutions of ketguancon3py2 4a leads to destabilization via activation of the nitride unit resulting in the mixedvalent coiiiii bridged imido species ketguancopyketguancoμnhμn¬3 5 formed from intermolecular hydrogen atom abstraction haa of strong ch bonds bde  100 kcalmol kinetic rate analysis of the formation of 5 in the presence of c6h12 or c6d12 gives a kie  25±01 supportive of a haa formation pathway the reactivity of our system was further probed by photolyzing benzenepyridine solutions of 4a under h2 and d2 atmospheres 150 psi which leads to the exclusive formation of the bisimido complexes ketguancoμnh2 6 and ketguancoμnd2 6d respectively as a result of dihydrogen activation these results provide unique insights into the chemistry and electronic structure of late 3dmetal nitrides while providing entryway into ch activation pathways
we propose the use of aluminum nitride aln membranes acting as sensitive elements for the surface acoustic wave sawbased acceleration measurement the proposed solution is compared against existing prototypes based on the use of quartz sio2lithium niobate linbo3 membranes that are characterized by extensive anisotropic properties using comsol multiphysics 54 computer simulations we show explicitly that sensitive elements based on less anisotropic aln membranes overcome both the low sensitivity limitations of sio2 and low temperature stability of linbo3 moreover aln membranes exhibit nearly double the robustness against irreversible mechanical deformations when compared against sio2 which in turn allows for further 15fold sensitivity enhancement over linbo3 based sensors taking into account their acceptable frequency characteristics we thus believe that the aln membranes are a good candidate forsensitive elements especially for high acceleration measurements
graph convolutional networks gcns have gained great popularity in tackling various analytics tasks on graph and network data however some recent studies raise concerns about whether gcns can optimally integrate node features and topological structures in a complex graph with rich information in this paper we first present an experimental investigation surprisingly our experimental results clearly show that the capability of the stateoftheart gcns in fusing node features and topological structures is distant from optimal or even satisfactory the weakness may severely hinder the capability of gcns in some classification tasks since gcns may not be able to adaptively learn some deep correlation information between topological structures and node features can we remedy the weakness and design a new type of gcns that can retain the advantages of the stateoftheart gcns and at the same time enhance the capability of fusing topological structures and node features substantially we tackle the challenge and propose an adaptive multichannel graph convolutional networks for semisupervised classification amgcn the central idea is that we extract the specific and common embeddings from node features topological structures and their combinations simultaneously and use the attention mechanism to learn adaptive importance weights of the embeddings our extensive experiments on benchmark data sets clearly show that amgcn extracts the most correlated information from both node features and topological structures substantially and improves the classification accuracy with a clear margin
understanding thermohydrodynamic characteristics of microchannels is of importance to fabricate efficient heat sinks due to wide applications of cooling modules in microscales in this work wate
in this article the terrain classifications of polarimetric synthetic aperture radar polsar images are studied a novel semisupervised method based on improved tritraining combined with a neighborhood minimum spanning tree nmst is proposed several strategies are included in the method 1 a highdimensional vector of polarimetric features that are obtained from the coherency matrix and diverse target decompositions is constructed 2 this vector is divided into three subvectors and each subvector consists of onethird of the polarimetric features randomly selected the three subvectors are used to separately train the three different base classifiers in the tritraining algorithm to increase the diversity of classification and 3 a helptraining sample selection with the improved nmst that uses both the coherency matrix and the spatial information is adopted to select highly reliable unlabeled samples to increase the training sets thus the proposed method can effectively take advantage of unlabeled samples to improve the classification experimental results show that with a small number of labeled samples the proposed method achieves a much better performance than existing classification methods
anisotropic twodimensional materials possess intrinsic angledependent physical properties that originate from their low crystal symmetry yet how these properties are aﬀected by external impurities or structural defects in the material is still wholly unclear here we address this question by investigating the electrical transport in the anisotropic layered model system germanium arsenide first we show that the ratio of conductivities along the armchair and zigzag crystallographic directions exhibits an intriguing dependence with respect to both temperature and carrier density then by using a conceptually simple model we demonstrate that this unexpected behavior is directly related to the presence of impurityinduced localized states in the band gap that introduce isotropic hopping conduction the presence of this conduction mechanism in addition to the intrinsic band conduction signiﬁcantly inﬂuences the anisotropic electrical properties of the material especially at room temperature ie at applicationrelevant conditions
the usage of the wireless power transfer wpt technique for charging lithiumion liion batteries of electric vehicles evs is increasing rapidly the liion battery requires constant current cc followed by constantvoltage cv supply for efficient battery charging and to enhance its lifespan however it is hard to obtain cc and cv outputs due to the variation of battery resistance during charging moreover a zerophase angle zpa of the input source is essential to improve the power transfer capability therefore this paper proposes a new hybrid compensation topology to achieve loadindependent cc and cv behavior of the wpt system along with zpa two symmetrical coils have been used in the proposed wpt system as transmitter tx and receiver rx coils a comprehensive mathematical analysis for achieving cc and cv characteristics as well as for power losses is carried out a new hybrid compensation is designed by combining series–series ss and inductor–capacitor–capacitorseries lccs compensation topologies along with two additional switches the proposed compensation works on single resonance frequency and it is simple reliable and easy to implement with only one compensation capacitor from the rx side simulations are performed and a prototype is fabricated to verify the mathematical analysis and simulation results the overall result shows that the mathematical analysis and simulations comply with the experimental results full load efficiencies of 9033 and 8891 are achieved in the cc and cv modes respectively
the sørensendice index sdi is a widely used measure for evaluating medical image segmentation algorithms it offers a standardized measure of segmentation accuracy which has proven useful however it offers diminishing insight when the number of objects is unknown such as in white matter lesion segmentation of multiple sclerosis ms patients we present a refinement for finer grained parsing of sdi results in situations where the number of objects is unknown we explore these ideas with two case studies showing what can be learned from our two presented studies our first study explores an interrater comparison showing that smaller lesions cannot be reliably identified in our second case study we demonstrate fusing multiple ms lesion segmentation algorithms based on the insights into the algorithms provided by our analysis to generate a segmentation that exhibits improved performance this work demonstrates the wealth of information that can be learned from refined analysis of medical image segmentations
"frozen orbit is an attractive option for orbital design owing to its
 characteristics its argument of pericenter and eccentricity are kept constant on an
 average solar sails are attractive solutions for massive and expensive missions
 however the solar radiation pressure effect represents an additional force on the solar
 sail that may greatly affect its orbital behavior in the long run thus this force must
 be included as a perturbation force in the dynamical model for more accuracy this study
 shows the calculations of initial conditions for a lunar solar sail frozen orbit the
 disturbing function of the problem was developed to include the lunar gravitational
 field that is characterized by uneven mass distribution third body perturbation and
 the effect of solar radiation an averaging technique was used to reduce the dynamical
 problem to a long period system lagrange planetary equations were utilized to formulate
 the rate of change of the argument of pericenter and eccentricity using the reduced
 system frozen orbits for the moon sail orbiter were constructed the resulting frozen
 orbits are shown by two 3dsurface semimajor eccentricity inclination figures to
 simplify the analysis we showed inclination–eccentricity contours for different values
 of semimajor axis argument of pericenter and values of sail lightness number
"
untethered control of softbodied robots is attractive for interactions in a variety of unstructured and dynamic environments however soft robotics systems are currently limited in terms of wireless selective and scalable control of multiple actuators therefore we propose a method to wirelessly drive multiple soft actuators by laser projection a small amount of lowboilingpoint liquid inside a planar thin pouch can be heated by a laser and evaporated to inflate the whole body laser projection enables both wireless energy supply and the selection of target actuators further the lowboilingpoint liquid serves as an actuation source and as a receiver of laser irradiation thus we do not need additional components such as electric circuits and batteries to achieve simple and scalable implementation of multiple soft actuators we evaluated the mechanical properties and demonstrated that the system can wirelessly control the gestures of fingers of a robot hand we also verified that our method can activate a group of mobile soft robots simultaneously and individually while tracking the actuator positions our approach contributes to the scalable deployment of soft robotic systems by removing tethers for power and communication
to discuss the role of ontology matching algorithm in linguistic features the application fields of ontology matching algorithm are introduced firstly then the major ontology systems and typical ontology matching frameworks are discussed so that the concept of ontology matching algorithm is clear thirdly to further explore the importance of ontology matching algorithm in linguistic the ontology matching algorithm based on semantic omabos is analysed at last the ontology matching algorithm based on linguistic features omablf is designed and verified the results show that the presented algorithm has good effects however any kind of matching algorithm always has its drawbacks and is difficult to do well in each aspect of matching therefore in practical applications matching systems are usually synthesized by a variety of different types of matching algorithms element level or structure level to achieve a good matching effect
in this paper we develop a reliable system for smart irrigation of greenhouses using artificial neural networks and an iot architecture our solution uses four sensors in different layers of soil to predict future moisture using a dataset we collected by running experiments on different soils we show high performance of neural networks compared to existing alternative method of support vector regression to reduce the processing power of neural network for the iot edge devices we propose using transfer learning transfer learning also speeds up training performance with small amount of training data and allows integrating climate sensors to a pretrained model which are the other two challenges of smart irrigation of greenhouses our proposed iot architecture shows a complete solution for smart irrigation
this paper considers the effects of manufacturing factors on glove quality four factors are considered namely the curing temperature profile temperature of latex percentage of calcium nitrate and the oven temperature after coagulant dip the glove quality is measured based on the weight of the gloves tensile strength after ageing number of gloves with pinholes and finger thickness the analysis is carried out using wilcoxon signed rank tests where the significance of each factor on the glove quality can be determined the glove quality is subsequently predicted using neural networks it is shown that the weight of the gloves can be predicted quite well using a simple neural network structure however the other quality measures cannot be predicted equally well more complicated inputoutput relationships are likely to exist for these the results from this work are important for aiding decisions on process changes in glove manufacturing
in this paper we focus on large intelligent reflecting surfaces irss and propose a new codebook construction method to obtain a set of predesigned phaseshift configurations for the irs unit cells since the overhead for channel estimation and the complexity of online optimization for irsassisted communications scale with the size of the phaseshift codebook the design of small codebooks is of high importance we show that there exists a fundamental tradeoff between power efficiency and the size of the codebook we first analyze this tradeoff for baseline designs that employ a linear phaseshift across the irs subsequently we show that an efficient design for small codebooks mandates higherorder phaseshift variations across the irs consequently we propose a quadratic phaseshift design derive its coefficients as a function of the codebook size and analyze its performance our simulation results show that the proposed design yields a higher power efficiency for small codebooks than the linear baseline designs
the employees of any organization institute or industry spend a significant amount of time on computer network where they develop their own routine of activities in the form of network transactions over a time period insider threat detection involves identifying deviations in the routines or anomalies which may cause harm to the organization in the form of data leaks and secrets sharing if not automated this process involves feature engineering for modeling human behavior which is a tedious and timeconsuming task anomalies in the human behavior are forwarded to a human analyst for final threat classification we developed an unsupervised deep neural network model using lstm autoencoder which learns to mimic the behavior of individual employees from their daywise time stamped sequence of activities it predicts the threat scenario via significant loss from anomalous routine employees in a community tend to align their routine with each other rather than the employees outside their communities this motivates us to explore a variation of the autoencoder lstm autoencoder trained on the interleaved sequences of activities in the community lac we evaluate the model on the cert v62 dataset and perform analysis on the loss for normal and anomalous routine across 4000 employees the aim of our paper is to detect the anomalous employees as well as to explore how the surrounding employees are affecting that employees’ routine over time
affective interaction between players of video games can elicit rich and varying patterns of emotions in multiplayer activities that take place in a common space such as sports and board games players are generally aware of the emotions of their teammates or opponents as they can directly observe their behavioral patterns facial expressions head pose body stance and so on players of online video games however are not generally aware of the other players’ emotions given the limited channels of direct interaction among them eg via emojis or chat boxes it also turns out that the impact of realtime emotionawareness on play is still unexplored in the space of online digital games motivated by this lack of empirical knowledge on the role of the affect of others to one’s gameplay performance in this article we investigate the degrees to which the expression of manifested emotions of an opponent can affect the emotions of the player and consequently his gameplay behavior in this initial study we test our hypothesis on a twoplayer adversarial car racing game we perform a comprehensive user study to evaluate the emotions behaviors and attitudes of players in emotion aware versus emotion agnostic game versions our findings suggest that expressing the emotional state of the opponent through an emoji in realtime affects the emotional state and behavior of players that can consequently affect their playing experience
in this letter we present an experimental assessment of vegetation height retrieval in tropical forests based on pband synthetic aperture radar sar acquisitions two approaches are implemented and compared 1 parametric height estimation by minimizing the leastsquare problem between random volume over ground rvog model predictions and multibaseline sar data and 2 thresholding the vertical backscattering profiles that are focused by sar beamforming tomography the data set under analysis is from the esa afrisar campaign that was flown over gabon in 2016 results show that at a resolution of 25textmtimes 25textm  which corresponds to about 80 independent looks both of the two approaches are able to retrieve forest height to within an accuracy of about 3 m or better over the interval of forest height between 30 and 50 m when compared to light detection and ranging lidar measurements
we report results of a comparison of the accuracy of crowdworkers and seven natural language processing nlp toolkits in solving two important nlp tasks namedentity recognition ner and entitylevel sentiment els analysis we here focus on a challenging dataset 1000 political tweets that were collected during the us presidential primary election in february 2016 each tweet refers to at least one of four presidential candidates ie four named entities the groundtruth established by experts in political communication has entitylevel sentiment information for each candidate mentioned in the tweet we tested several commercial and opensource tools our experiments show that for our dataset of political tweets the most accurate ner system google cloud nl performed almost on par with crowdworkers but the most accurate els analysis system tensistrength did not match the accuracy of crowdworkers by a large margin of more than 30 percent points
in this paper the variable coefficients davey–stewartson system represents many physical phenomena in shallow water waves quantum and optics etc is transformed directly into nonlinear ordinary differential system by using the new modification to the direct similarity reduction method after solving the reduced system new jacobi hyperbolic and periodic wave solutions are achieved for complex variable coefficients davey–stewartson system the application of the new modification of the direct similarity reduction method reflects how this method is powerful easy and simple if it is compared with other symmetry techniques
lithium argyrodite superionic conductors of the form li6ps5x x  cl br and i have shown great promise as electrolytes for all‐solid‐state batteries because of their high ionic conductivity and processability the ionic conductivity of these materials is highly influenced by the structural disorder of s2−x− anions however it is unclear if and how this affects the li distribution and how it relates to transport which is critical for improving conductivities here it is shown that the site‐disorder once thought to be inherent to given compositions can be carefully controlled in li6ps5br by tuning synthesis conditions the site‐disorder increases with temperature and can be “frozen” in neutron diffraction shows this phenomenon to affect the li substructure by decreasing the jump distance between so‐called “cages” of clustered li ions expansion of these cages makes a more interconnected pathway for li diffusion thereby increasing ionic conductivity additionally ab initio molecular dynamics simulations provide li diffusion coefficients and time‐averaged radial distribution functions as a function of the site‐disorder corroborating the experimental results on li distribution and transport these approaches of modulating the li substructure can be considered essential for the design and optimization of argyrodites and may be extended to other lithium superionic conductors
local master equations are a widespread tool to model open quantum systems especially in the context of manybody systems these equations however are believed to lead to thermodynamic anomalies and violation of the laws of thermodynamics in contrast here we rigorously prove that local master equations are consistent with thermodynamics and its laws without resorting to a microscopic model as done in previous works in particular we consider a quantum system in contact with multiple baths and identify the relevant contributions to the total energy heat currents and entropy production rate we show that the second law of thermodynamics holds when one considers the proper expression we derive for the heat currents we confirm the results for the quantum heat currents by using a heuristic argument that connects the quantum probability currents with the energy currents using an analogous approach as in classical stochastic thermodynamics we finally use our results to investigate the thermodynamic properties of a set of quantum rotors operating as thermal devices and show that a suitable design of three rotors can work as an absorption refrigerator or a thermal rectifier for the machines considered here we also perform an optimisation of the system parameters using an algorithm of reinforcement learning
codes correcting pairerrors substitutions for symbolpair read channels were introduced recently by cassuto and blaum2011 in this work we study codes correcting synchronization errors including deletions and stickyinsertions for symbolpair read channels owing to their application in racetrack memory we first investigate deletioncorrectingcode in symbolpair read channels and then construct several codes that are larger than known codes in classical deletionchannels finally we examine codes correcting a combination of deletions and substitutions
the multilevel inverters have been used especially in renewable energy aspects in order to assess total harmonic distortion thd  thd is considered to produce a good quality of current signal of bldc motor drive in this paper a solar pv inveter fed brushless dc bldc motor drive is discussed considering two topologies of three phase multilevel inverters these topologies are flying capacitor and clamped diode inverters the inverter efficiency and output power have an effectiveness on thd values a boost dc to dc converter using incremental conductance maximum power point tracking incmppt is implemented with a solar pv the mathematical model of the proposed systems ares tudied via simulation using matlabsimulink the simulation results of a bldc motor based on two multilevel inverters are compared with each other considering the  thd and the utilization of the dcbus voltage the comparition verified  that the clamped diode inverter has a better harmonic spectrastill the overshoot is a little bit high in the two types of inverter that are proposed
in this paper the stability of a closedloop cascade control system in the trajectory tracking task is addressed the considered plant consists of underlying secondorder fully actuated perturbed dynamics and the first order system which describes dynamics of the input the main theoretical result presented in the paper concerns stability conditions formulated based on the lyapunov analysis for the cascade control structure taking advantage of the active rejection disturbance approach in particular limitations imposed on a feasible set of an observer bandwidth are discussed in order to illustrate characteristics of the closedloop control system simulation results are presented furthermore the controller is verified experimentally using a twoaxis telescope mount the obtained results confirm that the considered control strategy can be efficiently applied for mechanical systems when a high tracking precision is required
clinical datasets usually carry numerous features biomarkers characteristics etc concerning the examined populations this fact although beneficial challenges the statistical analysis via standard univariate approaches in the twosample setting the majority of the clinical studies evaluate their assumptions relying on a variety of available univariate tests such as the student’s ttest or mannwhitney wilcoxon we developed an easytouseandinterpret nonparametric twosample hypothesis testing framework tsauc particularly using machine learning and the auc maximization criterion we test and verify the effectiveness of tsauc in real data containing posturographic features of parkinsonian patients ps with and without history of falling
research related to automatically detecting alzheimers disease ad is important given the high prevalence of ad and the high cost of traditional methods since ad significantly affects the content and acoustics of spontaneous speech natural language processing and machine learning provide promising techniques for reliably detecting ad we compare and contrast the performance of two such approaches for ad detection on the recent adress challenge dataset 1 using domain knowledgebased handcrafted features that capture linguistic and acoustic phenomena and 2 finetuning bidirectional encoder representations from transformer bertbased sequence classification models we also compare multiple featurebased regression models for a neuropsychological score task in the challenge we observe that finetuned bert models given the relative importance of linguistics in cognitive impairment detection outperform featurebased approaches on the ad detection task
the main purpose of this research is to determine the effect of the volume of concept on the level of understanding the relationship between concepts the mixed method was used in the research volume of concept form concept map form and puzzle scales were used to collect quantitative data qualitative data was collected through a fully structured interview form a homogeneous sample was used in the study a total of 54 students participated in the study there are 27 students in both experimental and control groups the data was analyzed with the help of spss and microsoft excel program ttest wilcoxon frequency and percentage techniques were used for descriptive analysis according to the findings it was determined that volume of concept had a positive effect on relationship between concepts no significant relationship was found between the experimental and control groups in regards to concept knowledge in addition according to the quantitative findings concept map was found to be more successful in determining the relationships between concepts on the other hand according to the qualitative findings the volume of concept form and concept map was equally effective in determining the relationship amongst different concepts in line with the results volume of concept can be used to determine the relationship between concepts
this paper addresses the problem of multiface tracking the proposed framework first assembles discrete detections into short reliable sequences codenamed ‘tracklets’ and second links them via hierarchical clustering a key challenge in face tracking in real world scenarios ‘on the wild’ or under adverse conditions is the drastic variations across scenes eg variations in illumination pose occlusion motion blur etc to this end the paper presents a simple hierarchical clustering approach which exploits the intrascene compactness while minimizing the impact of interscene variations the algorithm adapts to different domains by dynamically learning domainspecific parameters through density dispersion analysis the proposed framework is evaluated on three databases which demonstrate competitive performance with stateof theart clustering and tracklet linking approaches in addition the paper presents a challenging database which is publicly available on which the endtoend tracking performance is reported
two new orthogonal functions named the left and the rightshifted fractionalorder legendre polynomials sflps are proposed several useful formulas for the sflps are directly generalized from the classic legendre polynomials the left and right fractional differential expressions in caputo sense of the sflps are derived as an application it is effective for solving the fractionalorder differential equations with the initial value problem by using the sflp tau method
compressive strength is one of the test factors used to determine whether cement production is in a controlled state or not portland type composite cement or pcc is the cement that is widely used in infrastructure development the cement of 3days compressive strength 7days compressive strength and 28days compressive strength are the variables that will be controlled in this study the normal distribution test and correlation test show that the data on each variable is normally distributed and each variable has a strong correlation univariate cement control using the cumulative sum control chart cusum and multivariate control using a multivariate cumulative sum mcusum control chart is performed to obtain the best control results correlated variables show that control using a multivariate control chart results in fewer outs of control observations compared to a univariate control chart this explains that the mcusum control chart is more sensitive than the cusum control chart in controlling observations of data out of control
this study aims to examine how green branding strategies affect paypremium behaviour of consumers for high vs lowinvolvement green products in an emerging countrydata were collected from 500 consumers by survey method and structural equation modelling was run to analyse the hypothesesconsequently it was found that for involvement level consumers’ paypremium behaviour was affected indirectly by perceived quality and directly by green brand equity and brand credibility moreover in addition to those factors for lowinvolvement green products performance risk and financial risk have an impact on willingness to pay more however for highinvolvement green products only performance risk influences the pay more behaviour of consumersin the research two involvement levels and two brands were used brand names in particular may have caused a bias in the measurement and the findings are limited by the sample which includes respondents from an emerging countrymanagers should focus on green brand equity brand quality and credibility to support willingness to pay more for green products moreover they should monitor performance risk and financial risk perceptions which may differ according to the involvement levelsthere is no other study at least to the best of the author’s knowledge testing the effects of brandrelated factors on consumers’ willingness to paypremium for green brands
we propose a global optimization approach to solve ℓ0norm penalized leastsquares problems using a dedicated branchandbound methodology a specific tree search strategy is built with branching rules inspired from greedy exploration techniques we show that the subproblem involved at each node can be evaluated via ℓ1normbased optimization problems with box constraints for which an activeset algorithm is built our method is able to solve exactly moderatesize yet difficult sparse approximation problems without resorting to mixedinteger programming mip optimization in particular it outperforms the generic mip solver cplex
abstract caves and other subterranean habitats fulfill the requirements of experimental model systems to address general questions in ecology and evolution yet the harsh working conditions of these environments and the uniqueness of the subterranean organisms have challenged most attempts to pursuit standardized research two main obstacles have synergistically hampered previous attempts first there is a habitat impediment related to the objective difficulties of exploring subterranean habitats and our inability to access the network of fissures that represents the elective habitat for the so‐called “cave species” second there is a biological impediment illustrated by the rarity of most subterranean species and their low physiological tolerance often limiting sample size and complicating laboratory experiments we explore the advantages and disadvantages of four general experimental setups in situ quasi in situ ex situ and in silico in the light of habitat and biological impediments we also discuss the potential of indirect approaches to research furthermore using bibliometric data we provide a quantitative overview of the model organisms that scientists have exploited in the study of subterranean life our over‐arching goal is to promote caves as model systems where one can perform standardized scientific research this is important not only to achieve an in‐depth understanding of the functioning of subterranean ecosystems but also to fully exploit their long‐discussed potential in addressing general scientific questions with implications beyond the boundaries of this discipline
the radiation intensity of observed auroras in the farultraviolet fuv band varies dramatically with location for aerospace applications requiring a photon counting imaging apparatus with a wide dynamic range however combining high spatial resolution imaging with high event rates is technically challenging we developed an fuv photon counting imaging system for aurora observation our system mainly consists of a microchannel plate mcp stack readout using a wedge strip anode wsa with charge induction and highspeed electronics such as a charge sensitive amplifier csa and pulse shaper moreover we constructed an anode readout model and a time response model for readout circuits to investigate the counting error in high counting rate applications this system supports global rates of 500 kilo counts 0610 dark counts s−1 cm−2 at an ambient temperature of 300 k and 111 µm spatial resolution at 400 kilo counts s−1 kcps we demonstrate an obvious photon count loss at incident intensities close to the counting capacity of the system to preserve image quality the response time should be improved and some noise performance may be sacrificed finally we also describe the correlation between counting rate and imaging resolution which further guides the design of space observation instruments
the rise in popularity of internet of things iot devices has opened doors for privacy and security breaches in cyberphysical systems like smart homes smart vehicles and smart grids that affect our daily existence iot systems are also a source of big data that gets shared via cloud iot systems in a smart home environment have sensitive access control issues since they are deployed in a personal space the collected data can also be of highly personal nature therefore it is critical to build access control models that govern who under what circumstances can access which sensed data or actuate a physical system traditional access control mechanisms are not expressive enough to handle such complex access control needs warranting the incorporation of new methodologies for privacy and security in this paper we propose the creation of the pals system that builds upon existing work in attribute based access control model captures physical context collected from sensed data attributes and performs dynamic reasoning over these attributes and context driven policies using semantic web technologies to execute access control decisions reasoning over user context details of information collected by cloud service provider and device type our mechanism generates as a consequent access control decisions our system’s access control decisions are supplemented by another subsystem that detects intrusions into smart home systems based on both network and behavioral data the combined approach serves to determine indicators that a smart home system is under attack as well as limit what data breach such attacks can achieve
understanding and removing bias from the decisions made by machine learning models is essential to avoid discrimination against unprivileged groups despite recent progress in algorithmic fairness there is still no clear answer as to which biasmitigation approaches are most effective evaluation strategies are typically usecase specific rely on data with unclear bias and employ a fixed policy to convert model outputs to decision outcomes to address these problems we performed a systematic comparison of a number of popular fairness algorithms applicable to supervised classification our study is the most comprehensive of its kind it utilizes three real and four synthetic datasets and two different ways of converting model outputs to decisions it considers fairness predictiveperformance calibration quality and speed of 28 different modelling pipelines corresponding to both fairnessunaware and fairnessaware algorithms we found that fairnessunaware algorithms typically fail to produce adequately fair models and that the simplest algorithms are not necessarily the fairest ones we also found that fairnessaware algorithms can induce fairness without material drops in predictive power finally we found that dataset idiosyncracies eg degree of intrinsic unfairness nature of correlations do affect the performance of fairnessaware approaches our results allow the practitioner to narrow down the approaches they would like to adopt without having to know in advance their fairness requirements
parallel corpora are key to developing good machine translation systems however abundant parallel data are hard to come by especially for languages with a low number of speakers when rich morphology exacerbates the data sparsity problem it is imperative to have accurate alignment and filtering methods that can help make the most of what is available by maximising the number of correctly translated segments in a corpus and minimising noise by removing incorrect translations and segments containing extraneous data this paper sets out a research plan for improving alignment and filtering methods for parallel texts in lowresource settings we propose an effective unsupervised alignment method to tackle the alignment problem moreover we propose a strategy to supplement stateoftheart models with automatically extracted information using basic nlp tools to effectively handle rich morphology
this paper investigates finitehorizon optimal control problem of continuoustime uncertain nonlinear systems the uncertainty here refers to partially unknown system dynamics unlike the infinitehorizon the difficulty of finitehorizon optimal control problem is that the hamilton–jacobi–bellman hjb equation is timevarying and must meet certain terminal boundary constraints which brings greater challenges at the same time the partially unknown system dynamics have also caused additional difficulties the main innovation of this paper is the proposed cyclic fixedfinitehorizonbased reinforcement learning algorithm to approximately solve the timevarying hjb equation the proposed algorithm mainly consists of two phases the data collection phase over a fixedfinitehorizon and the parameters update phase a leastsquares method is used to correlate the two phases to obtain the optimal parameters by cyclic finally simulation results are given to verify the effectiveness of the proposed cyclic fixedfinitehorizonbased reinforcement learning algorithm
mechanical elastic energy storage mees system completes the energy storage process through permanent magnet synchronous motor pmsm rotates and tightens the energy storage boxes which contains large spiral torsion springs stss to convert electrical energy into elastic potential energy the torque of the storage box is gradually increased while the moment of inertia is gradually reduced in the process of energy storage thus this paper proposed an backstepping control scheme with torque and inertia adaptive law and the stability was proved in the sense of lyapunov theory the effectiveness of the proposed scheme was verified through hardware experiments the results indicated that the proposed scheme ensures the speed to track the given value rapidly and the energy storage process can be carried out smoothly
skin is easily accessible for transdermal drug delivery and also attractive for biomarker sampling these applications are strongly influenced by hydration where elevated hydration generally leads to increased skin permeability thus favorable transdermal delivery and extraction conditions can be easily obtained by exploiting elevated skin hydration here we provide a detailed in vivo and in vitro investigation of the skin hydration dynamics using three techniques based on electrical impedance spectroscopy good correlation between in vivo and in vitro results is demonstrated which implies that simple but realistic in vitro models can be used for further studies related to skin hydration eg cosmetic testing importantly the results show that hydration proceeds in two stages firstly hydration between 5 and 10 min results in a drastic skin impedance change which is interpreted as filling of superficial voids in skin with conducting electrolyte solution secondly a subtle impedance change is observed over time which is interpreted as leveling of the water gradient across skin leading to structural relaxationchanges of the macromolecular skin barrier components with respect to transdermal drug delivery and extraction of biomarkers 1 h of hydration is suggested to result in beneficial and stable conditions in terms of high skin permeability and extraction efficiency
mpi collective operations can often be performance killers in hpc applications we seek to solve this bottleneck by offloading them to reconfigurable hardware within the switch itself rather than eg the nic we have designed a hardware accelerator mpifpga to implement six mpi collectives in the network preliminary results show that mpifpga achieves 10 times  speedup in the most likely scenarios over conventional clusters we introduce a novel mechanism that enables the hardware to support a large number of communicators of arbitrary shape and that is scalable to very large systems mpifpga is fully integrated into mpich and so transparent to mpi applications
the small and mediumsize business activities smes coming from different commercial sectors are generally found in italian small towns and municipalities recently smes are characterized by negative economic cycles factors negatively affecting commercial and tourism activities are historical centers depopulation phenomena productive delocalization business transfer and changes in service delivery processes and logistics to overcome these negativities the promotion of cultural assets the use of new technologies for economic development cooperation through networks and clusters and the involvement and integration of different local stakeholders are crucial the aim of this research was to identify key performance indicators and hotspots of business networks created for smart tourism development the analysis was conducted through the compilation of a mapping of potentially usable technologies and through the analysis of the results of four case studies on the application of a business network in the italian lazio region
use of artificial networks to signify the onset of dynamic catastrophes in engineering systems is a simple and efficient strategy here an ordinal partition network and its construction from time s
big data encompasses huge amounts of raw material which influence multitude of research fields as well as different industries performance such as business marketing social network analysis educational systems healthcare iot meteorology fraud detection it aimed to uncover hidden trends and has prompted a development from a modeldriven perspective to a datadriven approach among numerous properties of big data datasets of big data are identified primary as 3vs attributes which have high variety velocity and volume these provide an invaluable insight and assist in making precise decisions analyzing this information and outlining the outcome into helpful data is the method for extricating an incentive from these enormous volumes of datasets nevertheless big data containing unique features that cannot be handled and processed using the conventional methods this has presented a significant challenge to the industry this research paper presents a general outline of the characteristics of big data as well as expounds on the present challenges and limitations in this area it further discusses the future scope in particular the future direction for big data research
the fivelevel inverters are preferred solutions for mediumvoltage pv applications because of lower total harmonic distortion thd lower switching stress and lower electromagnetic interference in contrast to twolevel inverter topologies in order to reduce the leakage current in the fivelevel inverter applications a singlephase fivelevel transformerless inverter topology is proposed in this paper the proposed inverter is based on an flying capacitor fc configuration and the voltage of fc can be balanced through the coordination of the two halfbridge within a switching period two bidirectional circuits are added to provide the new freewheeling paths when outputting zero voltage level the mechanism of suppressing the leakage current is analyzed in detail with operational states and modulation strategy a 1 kw prototype is also built and tested in the laboratory the experimental results are finally presented to show the excellent performance in leakage current reduction
abstract weiner 1988 “on editing a usage guide” in words for robert burchfield’s sixtyfifth birthday edited by e g stanley and t f hoad 171–183 cambridge ds brewer 173 describes usage guides as being ‘as broad as the english language covering spelling punctuation phonology morphology syntax and lexis and involving sociolinguistic considerations’ this paper focusses on these ‘sociolinguistic considerations’ to try and answer the question of why people like greengrocers sports commentators estate agents and television presenters are stigmatised for certain perceived linguistic errors the greengrocer’s apostrophe is well known beal joan c 2010 “the grocer’s apostrophe popular prescriptivism in the twentyfirst century” english today 26 2 57–64 but the other three categories the sports commentators’ adverb the flat adverb the estate agent’s pronoun yourselves for you and the television presenter’s demonstrative pronoun thesethose ones i first encountered in caroline taggart’s her ladyship’s guide to the queen’s english 2010 one of the three most recent and most prescriptive publications in the huge database of usage guides and usage problems discussing taggart’s usage guide as a case study i will go into the question of why certain groups of speakers are made into the object of prescriptivism and will argue that the british class system plays an important role in this as a case study this article highlights the need for more linguists to view usage guides as a genre that needs to be treated critically rather than be ignored as is generally the case at present
we discuss conceptual limitations of generic learning algorithms acting in a competitive environment and demonstrate that they are subject to constraints that are analogous to the constraints on knowledge imposed by the famous theorems of godel church and turing
in response to the growing need to perform assessments online we have developed a secure answer book as well as a tool for automatically grading it for our course on spreadsheet modeling we applied these techniques to a cohort of about 160 students who took the course last term in this paper we describe the design implementation and the techniques employed to enhance both the security of the answer book and the ease accuracy and consistency of grading in addition we summarize the experience and takeaways both from the instructor and the student perspectives although the answer book and grading tool described may be specific to the course for which they were developed many of their design principles may be extended to other courses and electronically submitted assignments
to ameliorate tradeoffs between a fixed spatial resolution and signal‐to‐noise ratio snr for hyperpolarized 13c mri
in this paper we addressed the problem of estimating the 3d structure of an object from shapefromfocus sff cue by utilizing sparsity techniques a set of spacevariantly defocused images acquired with respect to relative motion between image capturing device and the specimen is considered to obtain the shape and depth of the 3d target image by calculating the focus points the estimation of clear image and 3d shape is posed as an inverseproblem as inverse problems are typically illposed and has several solutions hence we use a prior information to regularize the solution in this work we have shown the results for various types of sparsity based and markov random field priors and their effects on the solution of the illposed shape estimation problem initially latent clear image of the 3d target is well reconstructed from the huge stack of captured images we have shown the experimental results for various synthetic and real case with different priors and optimization techniques since we need to employ the integral stack of spacevariantly defocused pictures we analyze the possibility of estimation of both the focused image and the 3d structure of the target image since this work is a severely illposed inverseproblem we use prior information of both the quantities to be estimated in an alternating minimization approach
in recent years with the rapid development of air transport communication technology in air transport is facing new challenges the emergence of mobile adhoc network technology brings us new solutions and the concept of aviation adhoc network however due to the inherent defects of aviation adhoc network business continuity cannot be guaranteed based on the magic routing algorithm this paper proposes a cache optimization algorithm with life cycle added based on the traditional method of obtaining the benefit of cache replacement it introduces the concept of cache life cycle it top service the cache replacement of nodes more flexible and can flexibly adapt to the non  uniform request situation this paper uses omnet simulation tool to simulate the algorithm the simulation results show that the proposed algorithm performs well in response time and cache distribution than some existing cache optimization algorithms
this paper presents a brief review on the current applications and perspectives on the stability of complex dynamical systems with an emphasis on three main classes of systems such as delayfree systems timedelay systems and systems with uncertainties in its parameters which lead to some criteria with necessary andor sufficient conditions to determine stability andor stabilization in the domains of frequency and time besides criteria on robust stability and stability of nonlinear timedelay systems are presented including some numerical approaches
formalin fixation has been shown to substantially reduce t2 estimates when performing postmortem imaging primarily driven by the presence of bulk fixative in tissue prior to scanning postmortem tissue samples are often placed into a fluid that has more favourable imaging properties such as matched magnetic susceptibility this study investigates whether there is any evidence for a change in t2 in regions close to the tissue surface in postmortem t2 maps due to fixative outflux into this surrounding fluid furthermore we investigate whether a simulated spatial map of fixative concentration can be used as a confound regressor to reduce t2 inhomogeneity to achieve this t2 maps and diffusion tensor estimates were obtained in 14 whole formalin fixed postmortem brains placed in fluorinert approximately 48 hours prior to scanning this consisted of 7 brains fixed with 10 formalin and 7 brains fixed with 10 neutral buffered formalin nbf fixative outflux was modelled using a kinetic tensor kt model which incorporates voxelwise diffusion tensor estimates to account for diffusion anisotropy and tissuespecific diffusion coefficients brains fixed with 10 nbf revealed a spatial t2 pattern consistent with the modelled fixative outflux confound regression of fixative concentration reduced t2 inhomogeneity across both white and grey matter with the greatest reduction attributed to the kt model vs simpler models of fixative outflux no such effect was observed in brains fixed with 10 formalin correlations with ferritin and myelin proteolipid protein plp histology lead to an increased similarity for the relationship between t2 and plp for the two fixative types after kt correction only small correlations were identified between t2 and ferritin before and after kt correction
learning technologies are generating a vast quantity of data every day this data is often presented to students through learning analytics dashboards lads with a goal of improving learners selfregulated learning however are students actually using these dashboards and do they perceive that using dashboards lead to any changes in their behavior in this paper we report on the development and implementation of several dashboard views which we call my learning analytics myla this study found that students thought using the dashboard would have more of an effect on the way they planned their course activity at preuse after a demo than post use low selfregulated learners believed so significantly less postuse and used the grade distribution view the least students made several suggestions for ways to improve the grade distribution view and rated mylas usability more positively at pre than postuse given the low use and low perceived impact of the current dashboard we suggest that researchers use participatory design to illicit students needs and better incorporate student suggestions
tway testing is a sampling strategy that generates a subset of test cases from a pool of possible tests many tway testing strategies appear in the literature todate ranging from general computational ones to metaheuristic based owing to its performance man the metaheuristic based tway strategies have gained significant attention recently eg particle swarm optimization genetic algorithm ant colony algorithm harmony search jaya algorithm and cuckoo search jaya algorithm ja is a new metaheuristic algorithm has been used for solving different problems however losing the searchs diversity is a common issue in the metaheuristic algorithm in order to enhance jas diversity enhanced jaya algorithm strategy called latin hypercube sampling jaya algorithm lhsja for test suite generation is proposed latin hypercube sampling lhs is a sampling approach that can be used efficiently to improve search diversity to evaluate the efficiency of lhsja lhsja is compared against existing metaheuristicbased tway strategies experimental results have shown promising results as lhsja can compete with existing tway strategies
with the development of internet and big data technology the scale of data is growing exponentially and these data contain a lot of valuable information as the most intuitive way of knowledge expression knowledge map can effectively organize and express data as an important means of knowledge map completion knowledge inference aims to deduce new knowledge or identify wrong knowledge based on existing knowledge in the knowledge map different from traditional knowledge inference methods knowledge inference methods based on knowledge graphs are also diversified according to their simple intuitive flexible and rich knowledge expression forms according to the types of reasoning methods knowledge reasoning methods based on knowledge graph can be divided into singlestep reasoning and multistep reasoning according to the different methods adopted for each type each type also includes reasoning based on distributed representation reasoning based on neural network and mixed reasoning these methods are summarized in detail and the future research direction and prospect of knowledge inference based on knowledge map are discussed and prospected
abstract the multiple correlation andor regression information that two competing forecast systems have on the same observations is decomposed into four components adapting the method of multivariate information decomposition of williams and beer  2010   wibral et al  2015   and lizier et al  2018   their concept is to divide source information about a target into total target redundant or shared and unique information from each source it is applied here to the comparison of forecast systems using classic regression additionally nontarget redundant or shared information is newly defined that resumes the redundant information of the forecasts which is not observed this provides views that go beyond classic correlation differences these five terms share the same units and can be directly compared to put prediction results into perspective the redundance terms in particular provide a new view all components are given as maps of explained variance on the observations and for the nontarget redundance on the models respectively exerting this concept to lagged damped persistence is shown to be related to directed information entropy to emphasize the benefit of the toolkit on all timescales two analysis examples are provided firstly two forecast systems of the german decadal prediction system of “mittelfristige klimaprognose” namely the preoperational version and a special version using ensemble kalman filter for the ocean initialization are compared the analyses reveal the clear added value of the latter and provide an as yet unseen map of their nontarget redundance secondly 4 d lead forecasts from the european centre for mediumrange weather forecasts ecmwf are compared to a simple autoregressive andor damped persistence model the analysis of the information partition on this timescale shows that interannual changes in damped persistence seen as target redundance changes between forecasts and damped persistence models are balanced by associated changes in the added value of the dynamic forecasts in the extratropics but not in the tropics
fruit image classification has several applications and can be used as alternative to traditionally fruit classification performed by human expert this paper aims to propose fruits classification method from image using extreme learning machine elm mpeg7 visual descriptors and principle component analysis pca the optimum parameters of elm and pca were determined using grid search optimization the best classification performance of 9733 has been achieved in classifying indonesian fruit images consisted of 15 classes by applying the ensemble of elms the classification accuracy was increased to 9803 this result shows that the proposed method produces high classification performance
the design of polycrystalline alloys hinges on a predictive understanding of the interaction between the diffusing solutes and the motion of the constituent crystalline interfaces existing frameworks ignore the dynamic multiplicity of and transitions between the interfacial structures and phases here we develop a computationallyaccessible theoretical framework based on shorttime equilibrium fluctuations to extract the drag force exerted by the segregating solute cloud using three distinct classes of computational techniques we show that the random walk of a soluteloaded interface is necessarily nonclassical at short timescales as it occurs within a confining solute cloud the much slower stochastic evolution of the cloud allows us to approximate the shorttime behavior as an exponentially subdiffusive brownian motion in an external trapping potential with a stiffness set by the average drag force at longer timescales the interfacial and bulk forces lead to a gradual recovery of classical random walk of the interface with a diffusivity set by the extrinsic mobility the shorttime response is accessible via it abinitio computations offering a firm foundation for high throughput rational design of alloys for controlling microstructural evolution in polycrystals and in particular for nanocrystalline alloysbydesign
abstract in this paper we apply to the case of approximate minimizers for vector optimization problems with setvalued maps some techniques developed previously in literature for the situation of standard vector efficiency the approach we propose here allows us on one hand to extend some known results and on the other hand to emphasize some numerical aspects that could be of certain importance for scientific computation of approximate minima
in this brief design methodology and measurement results of wband dicke radiometer blocks are presented the dicke radiometer blocks are implemented in ihps 013inlineformula texmath notationlatexmu textm texmathinlineformula sige bicmos technology all the implemented blocks namely the spdt switch lna and the power detector demonstrate the state of the art performance at wband the spdt has a measured il of 18 db and 20 db isolation the lna achieves a peak gain of 223 db and 42 db nf and the pd has a nep better than 05 pwinlineformula texmath notationlatexsqrt hz texmathinlineformula to achieve the minimum netd all the blocks are designed to be as wideband as possible using the measurement and simulation results the achievable netd of the radiometer is calculated to be better than 05 k
background and aim platycodon grandiflorum pg has been widely used for treating chronic bronchitis cb however the material basis and underlying mechanism of action of pg against cb have not yet been elucidated methods to analyze the ingredients in pg ultraperformance liquid chromatographyquadrupoletimeofflight tandem mass uplcqtofmsms technology was performed subsequently using data mining and network pharmacology methodology combined with discovery studio 2016 ds cytoscape v371 and other software active ingredients drugdisease targets and key pathways of pg in the treatment of cb were evaluated finally the reliability of the core targets was evaluated using molecular docking technology and in vitro studies results a total of 36 compounds were identified in pg according to the basic properties of the compounds 10 major active ingredients including platycodin d were obtained based on the data mining approach the traditional chinese medicine systems pharmacology database and the analysis platform tcmsp genecards and other databases were used to obtain targets related to the active ingredients of pg and cb network analysis was performed on 144 overlapping gene symbols and twenty core targets including interleukin6 il6 and tumor necrosis factor tnf which indicated that the potential signaling pathway that was most relevant to the treatment of cb was the il17 signaling pathway conclusion in this study ingredient analysis network pharmacology analysis and experiment verification were combined and revealed that pg can be used to treat cb by reducing inflammation our findings provide novel insight into the mechanism of action of chinese medicine furthermore our data are of value for the research and development of novel drugs and the application thereof
devops can be defined as a cultural movement and a technical solution to improve and accelerate the delivery of business value by making the collaboration between development and operations effective which is rapidly spreading in software industry however this movement is relatively recent being necessary more empirical evidence about the real reasons why companies move to devops and what results they expect to obtain when adopting devops culture this paper describes empirical research on practicing devops through an exploratory multiple case study of 30 multinational softwareintensive companies that consists of interviews to relevant stakeholders this study aims to help practitioners and researchers to better understand the context and the problems that many companies face day to day in their organizations when they do not reach the levels of innovation and software delivery they expect as well as the main drivers that move these companies to adopting devops this would contribute to strengthening the evidence and support practitioners in making better informed decisions furthermore we have made available the methods to increase the reliability of findings and the instruments used in this study to motivate others to provide similar evidence to help mature devops research and practice
swapping text in scene images while preserving original fonts colors sizes and background textures is a challenging task due to the complex interplay between different factors in this work we present swaptext a threestage framework to transfer texts across scene images first a novel text swapping network is proposed to replace text labels only in the foreground image second a background completion network is learned to reconstruct background images finally the generated foreground image and background image are used to generate the word image by the fusion network using the proposing framework we can manipulate the texts of the input images even with severe geometric distortion qualitative and quantitative results are presented on several scene text datasets including regular and irregular text datasets we conducted extensive experiments to prove the usefulness of our method such as image based text translation text image synthesis
the last few decades have witnessed a growing interest in locationbased services using localization systems based on radio frequency rf signals has proven its efficacy for both indoor and outdoor applications however challenges remain with respect to both complexity and accuracy of such systems machine learning ml is one of the most promising methods for mitigating these problems as ml especially deep learning offers powerful practical datadriven tools that can be integrated into localization systems in this paper we provide a comprehensive survey of mlbased localization solutions that use rf signals the survey spans different aspects ranging from the system architectures to the input features the ml methods and the datasets a main point of the paper is the interaction between the domain knowledge arising from the physics of localization systems and the various ml approaches besides the ml methods the utilized input features play a major role in shaping the localization solution we present a detailed discussion of the different features and what could influence them be it the underlying wireless technology or standards or the preprocessing techniques a detailed discussion is dedicated to the different ml methods that have been applied to localization problems discussing the underlying problem and the solution structure furthermore we summarize the different ways the datasets were acquired and then list the publicly available ones overall the survey categorizes and partly summarizes insights from almost 400 papers in this field this survey is selfcontained as we provide a concise review of the main ml and wireless propagation concepts which shall help the researchers in either field navigate through the surveyed solutions and suggested open problems
abstract the current study is a collective selfstudy on how we as 15 teacher educators at a university in norway tried to improve our teaching through working with cases with the aim of better supporting student teachers in making links between theory and practice we wanted to address the common criticism in teacher education concerning a perceived gap between practice and theory our presupposition was that one way to prepare student teachers for work and bring together theoretical and practical knowledge would be through casebased teaching we agreed that we wanted to try different ways of working with cases and to follow our own actions with research and conduct a selfstudy throughout the project each teacher educator experienced to learn about casebased teaching but our joint learning was limited due to practical issues and lack of time with teacher education as a shared responsibility our conclusion is that teacher educators need time to develop as a team not only as individual teachers
recommendation systems or recommender system rss is one of the hottest topics nowadays which is widely utilized to predict an item to the enduser based on hisher preferences primary recommendation systems applied in many areas mainly in commercial applications this work aims to collect evidence of utilizing social network information between users to enhance the quality of traditional recommendation system it provides an overview of traditional and modern approaches used by rss such as collaborative filter cf approach contentbased cb approach and hybrid filter approach cf is one of the most famous traditional approaches in rss which is facing many limitations due to the lack of information available during a performance such as cold start sparsity and shilling attack additionally this content focused on the role of incorporating a trust relationship from the social network to enhance the weaknesses of cf and achieve better quality in the recommendation process trustaware recommendation systems tarss is a modern approach proposed to overcome the limitations of cf recommendation system in a social network the trust relationship between users can boost and enhance cf limitations many researchers are focusing on trust in the recommendation system but fewer works are highlighting the role of trust in the recommendation system in the end limitations and open issues of the current picture of the recommendation system come across
a spectral risk measure srm is a weighted average of value at risk where the weighting function also known as risk spectrum or distortion function characterizes a decision makers risk attitude
with the continuous development of aerospace technology space exploration missions have been increasing year by year and higher requirements have been placed on the upper level rocket the purpose of this paper is to improve the ability to identify and detect potential targets for upper level rocketaiming at the upperlevel recognition of space satellites and core components this paper proposes a deep learningbased spatial multitarget recognition method which can simultaneously recognize space satellites and core components first the implementation framework of spatial multitarget recognition is given second by comparing and analyzing convolutional neural networks a convolutional neural network model based on yolov3 is designed finally seven satellite scale models are constructed based on systems tool kit stk and solidworks multi targets such as nozzle star sensor solaretc are selected as the recognition objectsby labeling training and testing the image data set the accuracy of the proposed method for spatial multitarget recognition is 9017 which is improved compared with the recognition accuracy and rate based on the yolov1 model thereby effectively verifying the correctness of the proposed methodthis paper only recognizes space multitargets under ideal simulation conditions but has not fully considered the space multitarget recognition under the more complex space lighting environment nutation precession roll and other motion laws in the later period training and detection can be performed by simulating more realistic space lighting environment images or multitarget images taken by upperlevel rocket to further verify the feasibility of multitarget recognition algorithms in complex space environmentsthe research in this paper validates that the deep learningbased algorithm to recognize multiple targets in the space environment is feasible in terms of accuracy and ratethe paper helps to set up an image data set containing six satellite models in stk and one digital satellite model that simulates spatial illumination changes and spins in solidworks and use the characteristics of spatial targets such as rectangles circles and lines to provide prior values to the network convolutional layer
cellular networks are becoming ever more sophisticated and overcrowded imposing the most delay jitter and throughput damage to endtoend network flows in todays internet we therefore argue for finegrained mobile endpointbased wireless measurements to inform a precise congestion control algorithm through a welldefined api to the mobiles cellular physical layer our proposed congestion control algorithm is based on physicallayer bandwidth measurements taken at the endpoint pbecc and captures the latest 5g new radio innovations that increase wireless capacity yet create abrupt rises and falls in available wireless capacity that the pbecc sender can react to precisely and rapidly we implement a proofofconcept prototype of the pbe measurement module on softwaredefined radios and the pbe sender and receiver in c an extensive performance evaluation compares pbecc head to head against the cellularaware and wirelessoblivious congestion control protocols proposed in the research community and in deployment in mobile and static mobile scenarios and over busy and idle networks results show 63 higher average throughput than bbr while simultaneously reducing 95th percentile delay by 18x
in this paper the main circuit topology of the threephase fourwire active power filter apf and its harmonic current detection and control algorithm are studied an active power filter system combining lclcl filter and a threephase hbridge structure is proposed this paper has applied harmonic detection algorithm based on an improved synchronous phaselocked loop srfpll and recursive discrete fourier transform rdft while using triangular carrier modulation control based on pi adjustment to realize the harmonic compensation of the power filter to the grid current the system has a notch effect on the harmonic at the switching frequency it can detect the fundamental component and each harmonic component of the load current separately with a small amount of calculation good realtime performance finally a 5kva singlephase apf prototype was built to test the superiority of the main circuit topology and the algorithm of current detection and control
abstract objectives the national institute for occupational safety and health niosh agriculture forestry and fishing centers aff centers collaborated to initiate a joint youtube channel in order to raise awareness of agricultural forestry and fishing occupational hazards provide information to prevent aff injuries and illnesses increase the visibility and influence of the aff centers and establish a collaborative model that can be replicated by other organizations methods the collaborators sought to produce a structured channel with high scientific standards policies procedures and a standard review process were established representatives from the aff centers coordinated the review process and the procedures by which videos were uploaded to the site a marketing plan was created including a press release and ideas to promote new videos promotions are targeted to agricultural cooperative extension agents educators producers owners operators first responders families and community organizations viewership was tracked using youtube metrics results the site was launched in november 2013 over a 6year period the channel grew from 48 videos to 125 videos with over 10500 cumulative watch time hours the channel is promoted by the aff centers through email social media conference presentations and outreach exhibits the channel is also publicized during coordinated national outreach events conclusion each aff center benefitted from increased exposure of their content and the collaboration provided an opportunity to achieve labor efficiencies youtube metrics demonstrated that coordinated marketing increases views watch time and subscriptions in addition the success of the channel communicates the benefits to collaboration among organizations with common missions
this work investigates handover in hybrid light fidelity lifi and wireless fidelity wifi networks hlwnets in such a network the handover process becomes challenging due to two main factors i the relatively short coverage range of a single access point ap and ii the largely overlapping coverage areas of different networks as a result hlwnets are susceptible to frequent handovers to reduce the handover rate the concept of handover skipping hs was introduced which enables handovers between nonadjacent aps however conventional hs methods rely on knowledge about the user’s trajectory which is not readily available at the ap in this paper a novel hs scheme is proposed on the basis of reference signal received power rsrp and its rate of change with an adaptive network preference adopted since rsrp is commonly used in the existing handover schemes the proposed method requires no additional signalling between the user and the ap simulation results show that the new method can effectively reduce unnecessary handovers especially those between lifi and wifi compared to the standard and trajectorybased handover schemes the proposed method improves network throughput by up to about 120 and 30 respectively
despite high expectations for the growth of fintech it has not reached the expected growth in the real world as fintech is innovative but inherently unpredictable customers are still hesitant to adopt and use fintech which ultimately affects its growth to achieve the sustainable development and growth of fintech an indepth investigation of fintech continuance intentions is required to investigate continuoususe behavior in a fintech context this study focuses on two relevant issues uncertainty and information technology it quality uncertainty is more critical in fintech than in traditional ebanking transactions because fintech transactions are complicated and less predictable it quality is also crucial to fintech success because it plays a key role in fintech transactions this study mainly explores the relationship between uncertainty and it quality both of which significantly affect fintech continuance intentions for the purpose we integrated an it quality–based perspective with a trustbased model to investigate fintech continuance intentions our results demonstrate that system quality is negatively related to perceived risk whereas information quality is positively related to trust service quality is the most important quality factor for controlling uncertainty and encouraging continued use of fintech we found a more extended role of it in fintech than in other digital services this study provides fintech providers with the practical guidance in the design and implementation of fintech innovation thereby achieving the sustainable development of fintech
silverbased bimetallic catalysts for the oxygen reduction reaction orr are promising for a wide variety of renewable energy technologies including alkaline fuel cells and metalair batteries t
a set of embedded atom model eam interatomic potentials was developed to represent highly idealized facecentered cubic fcc mixtures of fe–ni–cr–co–al at nearequiatomic compositions potential functions for the transition metals and their crossed interactions are taken from our previous work for fe–ni–cr–co–cu d farkas and a caro j mater res 33 19 3218–3225 2018 while crosspair interactions involving al were developed using a mix of the component pair functions fitted to known intermetallic properties the resulting heats of mixing of all binary equiatomic random fcc mixtures not containing al is low but significant shortrange ordering appears in those containing al driven by a large atomic size difference the potentials are utilized to predict the relative stability of fcc quinary mixtures as well as ordered l12 and b2 phases as a function of al content these predictions are in qualitative agreement with experiments this interatomic potential set is developed to resemble but not model precisely the properties of this complex system aiming at providing a tool to explore the consequences of the addition of a large sizemisfit component into a high entropy mixture that develops multiphase microstructures
background most simulation models used at university dental clinics are typodonts usually models show idealized eugnathic situations which are rarely encountered in everyday practice the aim of this study was to use 3d printing technology to manufacture individualized surgical training models for root tip resection apicoectomy on the basis of real patient data and to compare their suitability for dental education against a commercial typodont model methods the training model was designed using cadcam computeraided designcomputeraided manufacturing technology the printer used to manufacture the models employed the polyjet technique dental students about one year before their final examinations acted as test persons and evaluated the simulation models on a visual analogue scale vas with four questions q1–q4 results a training model for root tip resection was constructed and printed employing two different materials hard and soft to differentiate anatomical structures within the model the exercise was rated by 35 participants for the typodont model and 33 students for the 3dprinted model wilcoxon rank sum tests were carried out to identify differences in the assessments of the two model types the alternative hypothesis for each test was “the rating for the typodont model is higher than that for the 3dprinted model” as the pvalues reveal the alternative hypothesis has to be rejected in all cases for both models the gingiva mask was criticized conclusions individual 3dprinted surgical training models based on real patient data offer a realistic alternative to industrially manufactured typodont models however there is still room for improvement with respect to the gingiva mask for learning surgical incision and flap formation
this study aims to explore bangladeshi college students’ perception and challenges with regard to the application of computer assisted language learning call in learning english a mixed method research was designed and administered to obtain the purposes of the present study one hundred undergraduate students 52 male and 48 female of english department studying at a government college in bangladesh participated in the questionnaire survey while twenty students 11 male and 09 female were purposively chosen for the semistructured interview section the collected data of quantitative part were analyzed by spss software and the responses of the qualitative part were thematically analyzed the findings revealed that students showed positive perception towards the integration of call in english language learning and teaching process the results also revealed that students’ face some difficulties in using computer and other relevant technological equipment for english language learning the results provide concepts and suggestions to the future researchers for further investigations in relation to call for the benefits of learners and learning
the integration of information technology into medical environments introduces a variety of opportunities and challenges for the healthcare community medical devices such as ventilators patient monitors and infusion pumps which once operated as standalone devices now integrate network communication technology as a result these modern medical devices produce store and transmit large amounts of patient and therapy information from a digital forensics perspective this information could provide a forensic investigator with a treasure trove of potential digital evidence hence the purpose of this paper is to introduce and discuss the potential place and value of digital forensics processes within the context of healthcare providers through five scenarios the aim of the paper is twofold first it raises incentives for integrating digital forensics into various scenarios involving healthcare providers second it encourages future research to address the adoption of digital forensics tools and techniques to assist stakeholders in the medical domain
burnout in kindergarten teachers is influenced by individual factors social factors and organizational factors kindergarten organizational climate as an external work resource may cause teacher burnout when it cannot meet their work demands to explore the mechanisms that underlie the effects of kindergarten organizational climate on teacher burnout we investigated the mediating effect of workfamily conflict ie work interfering with family and family interfering with work on the relationship between kindergarten organizational climate and teacher burnout the study sample included 436 kindergarten teachers in henan china the chinese versions of the kindergarten organizational climate scale kindergarten teachers workfamily conflict scale and kindergarten teachers burnout scale were applied the results showed that kindergarten organizational climate was positively correlated with work–family conflict and teacher burnout work–family conflict was positively correlated with teacher burnout work–family conflict partially mediated the effects of kindergarten organizational climate on teacher burnout the mediating effect of family interfering with work was significantly stronger than the mediating effect of work interfering with family the results are discussed with respect to the general literature on the correlation between organizational climate wfc and burnout
within the framework of digital sustainability the increase in internet consumption and especially online social networks offers social benefits but is not without its drawbacks for example it can lead to psychological andor psychiatric disorders in some people numerous researches are highlighting the similarities of these addictions with the consumption of toxic substances university students are heavy users of the internet and in certain situations addiction to online social networks can be the result of depression harassment and anxiety among others affecting their daily life including their academic responsibilities in recent months an anomaly has occurred that may have contributed to intensifying this problem namely the confinement produced by the covid19 pandemic which has affected the whole world to a greater or lesser extent in this crosssectional study with a descriptive and quantitative methodology students from 14 spanish universities were investigated in the first wave of the covid19 pandemic in order to understand the effects of this situation on the problem described the results show a high consumption of social networks during that time with significant incidences of addiction in parallel the presence of comorbidity has been determined in this scenario it would be necessary to implement university educational programs to redirect these addictive behaviors as well as preventative recommendations and actions to minimize negative impacts this is a major problem that is growing exacerbated by the global pandemic produced by the sarscov2 coronavirus situations of this gravity call for the development of preventive and educational measures for the responsible and sustainable use of ict
this paper is the confluence of two streams of ideas in the literature on generating numerical invariants namely 1 templatebased methods and 2 recurrencebased methods a templatebased method begins with a template that contains unknown quantities and finds invariants that match the template by extracting and solving constraints on the unknowns a disadvantage of templatebased methods is that they require fixing the set of terms that may appear in an invariant in advance this disadvantage is particularly prominent for nonlinear invariant generation because the user must supply maximum degrees on polynomials bases for exponents etc on the other hand recurrencebased methods are able to find sophisticated nonlinear mathematical relations including polynomials exponentials and logarithms because such relations arise as the solutions to recurrences however a disadvantage of past recurrencebased invariantgeneration methods is that they are primarily loopbased analyses they use recurrences to relate the prestate and poststate of a loop so it is not obvious how to apply them to a recursive procedure especially if the procedure is nonlinearly recursive eg a treetraversal algorithm in this paper we combine these two approaches and obtain a technique that uses templates in which the unknowns are functions rather than numbers and the constraints on the unknowns are recurrences the technique synthesizes invariants involving polynomials exponentials and logarithms even in the presence of arbitrary controlflow including any combination of loops branches and possibly nonlinear recursion for instance it is able to show that i the time taken by mergesort is on logn and ii the time taken by strassen’s algorithm is onlog27
this paper addresses the problem of learning to complete a scene’s depth from sparse depth points and images of indoor scenes specifically we study the case in which the sparse depth is computed from a visualinertial simultaneous localization and mapping vislam system the resulting point cloud has low density it is noisy and has nonuniform spatial distribution as compared to the input from active depth sensors eg lidar or kinect since the vislam produces point clouds only over textured areas we compensate for the missing depth of the lowtexture surfaces by leveraging their planar structures and their surface normals which is an important intermediate representation the pretrained surface normal network however suffers from large performance degradation when there is a significant difference in the viewing direction especially the roll angle of the test image as compared to the trained ones to address this limitation we use the available gravity estimate from the vislam to warp the input image to the orientation prevailing in the training dataset this results in a significant performance gain for the surface normal estimate and thus the dense depth estimates finally we show that our method outperforms other stateoftheart approaches both on training scannet 1 and nyuv2 2 and testing collected with azure kinect 3 datasets
several alternatives have been proposed to model the radiation profile emitted by flames among them there is the weightedmultipointsourcemodel wmp which is able to provide good predictions for medium and far distances despite being a simple model the determination of its parameters can be difficult which creates the opportunity to apply optimisation methods in order to find the best answer moreover correlations of these parameters as function of flame variables can be applied aiming to relate the model to the physical phenomenon making the problem even harder to solve previous studies applied the generalised extreme optimisation geo to find the best possible combinations of these parameters but there is a chance that other methods could improve the results considerably this study has the objective to further explore the wmp model as formulated previously in the literature and apply the simulated annealing optimisation method to assess if the answers could be improved the same correlation and experimental data were used as well a total of 77 different cases were explored over 50 runs each configuring the answers statistically significant the sa method had better performance showing similar average results for the objective function but overall best answers abbreviations geo generalised extreme optimisation pso particle swarm optimisation sa simulated annealing sp single point source model wmp weighted multipoint source model
the object of this article is to design an observerbased adaptive neural network sliding mode controller for active suspension systems a general nonlinear suspension model is established and the electrohydraulic actuator dynamics are considered the proposed controller is decomposed into two loops since the dynamics of the actuator is assumed highly nonlinear with uncertainties the adaptive neural network is presented in the inner loop to ensure the control system robustness against uncertainties and the selftuning weighting vector is adjusted online according to the updated law obtained by lyapunov stability theory in the outer loop a model reference sliding mode controller is developed to track the desired states of the hybrid reference model that combines skyhook and groundhook control methods besides to obtain the unmeasured states of the system an unscented kalman filter is utilized to provide necessary information for the controller simulation results show that the exerted force can be tracked precisely even in the existence of uncertainties moreover the proposed controller can improve the suspension’s performance effectively
czasopismo open access wszystkie artykuły udostępniane są na mocy licencji creative commons uznanie autorstwaużycie niekomercyjnena tych samych warunkach 40 międzynarodowe cc byncsa 40 httpcreativecommonsorglicensesbyncsa40 wkład autorów authors’ contribution a zaplanowanie badań study design b zebranie danych data collection c dane – analiza i statystyki data analysis d interpretacja danych data interpretation e przygotowanie artykułu preparation of manuscript f wyszukiwanie i analiza literatury literature analysis g zebranie funduszy funds collection streszczenie celem artykułu jest ocena intensywności użytkowania facebooka przez młodych dorosłych w odniesieniu do ich kompetencji społecznych i rozumienia emocji materiał i metody w badaniach wykorzystano metodę sondażu diagnostycznego w skład kwestionariusza ankiety wchodziły kwestionariusz intensywności użytkowania facebooka kiuf autorstwa j kusia i m szulżyckiego profil kompetencji społecznych prokos autorstwa a matczak i k kartowskiej test rozumienia emocji tre autorstwa a matczak i j piekarskiej oraz metryczka wyniki uzyskane wyniki wskazują na to że większość osób nie podporządkowuje swojego życia facebookowi kompetencje społeczne oraz rozumienie emocji przez badanych są zazwyczaj na poziomach poniżej przeciętnego co może prowadzić do nadmiernego angażowania się w niektóre aktywności online przeprowadzona analiza statystyczna wykazała że największy wpływ na facebookowych celebrytów mają kompetencje towarzyskie a na graczy kompetencje społecznikowskie wnioski ze względu na wykazane niskie kompetencje społeczne i rozumienie emocji wśród młodych dorosłych należy w toku kształcenia poświęcić więcej miejsca tym zagadnieniom
recent technological and architectural advancements in 5g networks have proven their worth as the deployment has started over the world key performance elevating factor from access to core network are softwareization cloudification and virtualization of key enabling network functions along with the rapid evolution comes the risks threats and vulnerabilities in the system for those who plan to exploit it therefore ensuring fool proof endtoend e2e security becomes a vital concern artificial intelligence ai and machine learning ml can play vital role in design modelling and automation of efficient security protocols against diverse and wide range of threats ai and ml has already proven their effectiveness in different fields for classification identification and automation with higher accuracy as 5g networks primary selling point has been higher data rates and speed it will be difficult to tackle wide range of threats from different points using typicaltraditional protective measures therefore ai and ml can play central role in protecting highly datadriven softwareized and virtualized network components this article presents ai and ml driven applications for 5g network security their implications and possible research directions also an overview of key data collection points in 5g architecture for threat classification and anomaly detection are discussed
objectives recently it has been demonstrated that patients with subtle preexisting cognitive impairment were susceptible to delayed neurocognitive recovery dnr this present study investigated whether preoperative alterations in gray matter volume spontaneous activity or functional connectivity fc were associated with dnr methods this was a nested casecontrol study of older adults ≥60 years undergoing noncardiac surgery all patients received mri scan at least 1 day prior to surgery cognitive function was assessed prior to surgery and at 714 days postsurgery preoperative gray matter volume amplitude of lowfrequency fluctuation alff and fc were compared between the dnr patients and nondnr patients the independent risk factors associated with dnr were identified using a multivariate logistic regression model results of the 74 patients who completed assessments 1674 216 had dnr following surgery there were no differences in gray matter volume between the two groups however the dnr patients exhibited higher preoperative alff in the bilateral middle cingulate cortex mcc and left fusiform gyrus and lower preoperative fc between the bilateral mcc and left calcarine than the nondnr patients the multivariate logistic regression analysis showed that higher preoperative spontaneous activity in the bilateral mcc was independently associated with a higher risk of dnr or  311 95 ci 130745 p  0011 a longer education duration or  057 95 ci 041081 p  0001 and higher preoperative fc between the bilateral mcc and left calcarine or  040 95 ci 018092 p  0031 were independently correlated with a lower risk of dnr conclusions preoperative higher alff in the bilateral mcc and lower fc between the bilateral mcc and left calcarine were independently associated with the occurrence of dnr the present fmri study identified possible preoperative neuroimaging risk factors for dnr this trial is registered with chinese clinical trial registry chictrdcd15006096
with the direct detection of gravitational waves gws by advanced ligo detector a new ‘window’ to quantum gravity phenomenology has been opened at present these detectors achieve the sensitivity to detect the length variation δl o≈10−17−10−21 metre recently a more stringent upperbound on the dimensionless parameter β 0 bearing the effect of generalized uncertainty principle gup has been given which corresponds to the intermediate length scale limβ0lpl∼10−23m  hence the flavour of the gup can be realised by observing the response of the vibrations of phonon modes in such resonant detectors in the near future in this paper therefore we calculate the resonant frequencies and transition rates induced by the incoming gws on these detectors in the gup framework it is observed that the effects of the gup bears its signature in both the time independent and dependent part of the gravitational waveharmonic oscillator hamiltonian we also make an upper bound estimate of the gup parameter
children affected with autism spectrum disorder asd produce speech that consists of distinctive acoustic patterns as compared to normal children hence acoustic analyses can help classifying speech of asd affected children from that of normal children in this study the aim is to identify those discriminating characteristics of speech production that help classification between speech of children with asd and normal children two separate datasets were recorded for this study the english speech of children affected with asd and the english speech of normal children comparative analyses of acoustic features derived for both datasets are carried out changes in the speech production characteristics are examined in three parts firstly changes in the excitation source features f0 and strength of excitation soe are analyzed secondly changes in the vocal tract filter features the formants f1 to f5 and dominant frequencies fd1 fd2 are analyzed thirdly changes in the combined sourcefilter features signal energy and zerocrossing rate are analyzed different combinations of the feature sets are then classified using three different classifiers for validation of results svm knn and ensemble classifiers performance evaluation is carried using different combinations of features sets and classifiers results up to 971 are obtained for classification accuracy between speech of asd affected children and normal children using a combination of feature set with svm classifier the results are better than other similar few studies this study should be helpful in developing an automated system for identffying asd speech in future
during most part of western classical music history tempo the speed of music was not specified for it was considered obvious from musical context only in 1815 maelzel patented the metronome beethoven immediately embraced it so much as to add tempo marks to his already published eight symphonies however these marks are still under dispute as many musicians consider them too quick to be played and even unmusical whereas others claim them as bethoven’s supposedly written will in this work we develop a methodology to extract and analyze the performed tempi from 36 complete symphonic recordings by different conductors our results show that conductor tempo choices reveal a systematic deviation from beethoven’s marks which highlights the salience of “correct tempo” as a perceptive phenomenon shaped by cultural context the hasty nature of these marks could be explained by the metronome’s ambiguous scale reading point which beethoven probably misinterpreted
spatial energy distribution characteristics edcs as well as power and efficiency should be considered to ensure the electromagnetic safety of people nearby when analyzing some magnetically coupled resonant wireless power transfer systems which is difficult for circuit theory in this article we first propose an energyconcentrating effect ece of unit transmitting active power tap based on the edcs of the system with series–seriesparallel–series compensation and we find that the area integral of the active power density on the infinite plane perpendicular to the transmission direction between the coils equals the tap of the system which is consistent with the result obtained by circuit theory several couplers are designed and optimized based on the ece simulation and experimental results reveal that the optimized inner–outer ring ior and disccoil couplers increase the ece by 10 and 32 respectively compared with the original spiralcoil coupler furthermore the electromagnetic exposure of the human body in the same position around the three couplers indicates that the maximum induced electric field and specific absorption rate in the human body around the optimized ior and disccoil couplers are substantially decreased and thus the two couplers increase the maximum allowable tap of the system from 297 to 379 w and to 700 w
the goal of this article is to provide a finegrained analysis of international human resource management research that addresses the different perspectives applied in that research we coded 203 peerreviewed international human resource management articles published between 2011 and 2018 with content analytical methods guided by the compass of management research developed by sieben which is rooted in critical management research we were particularly attentive to the various discursive orientations international human resource management scholars have adopted including ideologically critical poststructuralist functionalist and interpretive perspectives we further examined which methods theoretical perspectives and topics were common within and across different perspectives this analysis indicated that critical research intending to politicize and question existing structures and ways of organizing is still marginal along with the dominance of functionalist and interpretive studies papers in our dataset commonly use a strategic human resource perspective are predominantly interested in the human resource management–performance link and focus rather narrowly on multinational corporations and expatriates furthermore while international human resource management scholars increasingly account for the contextual embeddedness of organizations through macrolevel theories they mainly apply institutional perspectives that view organizations as adapting to institutional constraints we propose a more diverse and reflexive approach – inspired by ideologically critical and poststructuralist perspectives – that may help to overcome these blind spots such an approach might for instance look at types of organizations other than multinational corporations and individuals other than highly skilled expatriates and might explicitly bring multiple external stakeholders into the picture we conclude by suggesting that international human resource management research and practice would benefit from more research diversity which enables more holistic analyses of phenomena more innovative research and resultant insights and more space for metatheoretical reflections
in this article initially we propose a new cryptosystem based on conjugacy using automorphism over noncommutative groups we applied the proposed cryptosystem to design expert smart meters based on homomorphic encryption with double conjugacy smart meters will communicate mostly errorless client electricity consumption readings to suppliers although this provides benefits for both entities it results in a severe loss of privacy for customers we integrate a monitoring purpose system that preserves customer’s privacy by homomorphically accumulating the consumptions of all n members of a domain this expert smart system has an proficient linear on communication cost and is proven to protect customer’s privacy even in the presence of a corrupted substation and some malicious smart meters it need not have any secure communication channels or a trusted third partyexcept for allotting public key certificates the security of cryptosystem and smart metering depends on conjugacy and homomorphism we also demonstrated that the comparison of smart meters with electronic meters by real time data
esophageal cancer is categorized as a type of disease with a high mortality rate early detection of esophageal abnormalities ie precancerous and early cancerous can improve the survival rate of the patients recent deep learningbased methods for selected types of esophageal abnormality detection from endoscopic images have been proposed however no methods have been introduced in the literature to cover the detection from endoscopic videos detection from challenging frames and detection of more than one esophageal abnormality type in this paper we present an efficient method to automatically detect different types of esophageal abnormalities from endoscopic videos we propose a novel 3d sequential denseconvlstm network that extracts spatiotemporal features from the input video our network incorporates 3d convolutional neural network 3dcnn and convolutional lstm convlstm to efficiently learn short and long term spatiotemporal features the generated feature map is utilized by a region proposal network and roi pooling layer to produce a bounding box that detects abnormality regions in each frame throughout the video finally we investigate a postprocessing method named frame search conditional random field fscrf that improves the overall performance of the model by recovering the missing regions in neighborhood frames within the same clip we extensively validate our model on an endoscopic video dataset that includes a variety of esophageal abnormalities our model achieved high performance using different evaluation metrics showing 937 recall 927 precision and 932 fmeasure moreover as no results have been reported in the literature for the esophageal abnormality detection from endoscopic videos to validate the robustness of our model we have tested the model on a publicly available colonoscopy video dataset achieving the polyp detection performance in a recall of 8118 precision of 9645 and fmeasure 8816 compared to the stateoftheart results of 7884 recall 9051 precision and 8427 fmeasure using the same dataset this demonstrates that the proposed method can be adapted to different gastrointestinal endoscopic video applications with a promising performance
in this paper we clarify human perception of force by carrying out quality of experience qoe assessment in cooperation between two remote robot systems with force feedback in the cooperation robot arms of the two remote robot systems grasp both ends of each wooden stick and then one side of the stick is moved only in one direction of front and back left and right and up and down a user can perceive force sensed by a force sensor attached to the tip of the robot arm via a haptic interface device then we investigate to what extent humans can accurately perceive the force direction via the haptic interface device we also examine the influence of the length of the wooden stick on the human perception of force direction  
datacenters are adopting heterogeneous hardware in the form of different cpu isas and accelerators advances in lowlatency and highbandwidth interconnects enable hardware vendors to tighten the coupling of multiple cpu servers and accelerators the closer connection of components facilitates bigger machines which pose a new challenge to operating systems we advocate to build a heterogeneous os for large heterogeneous systems by combining multiple os design principles to leverage the benefits of each design because a securityoriented design enabled by simplicity and clear encapsulation is vital in datacenters we choose to survey various design principles found in microkernelbased systems we explain that heterogeneous hardware employs different mechanisms to enforce access rights for example for memory accesses or communication channels we outline a way to combine enforcement mechanisms of cpus and accelerators in one system a consequence of this is a heterogeneous access rights management which is implemented as a heterogeneous capability system in a microkernelbased os
six sigma has been one of the most popular initiatives to improve management processes over the past few decades amidst the success stories on the six sigma there exist some literature studies on the criticisms of six sigma the purpose of this paper is to analyze the existing literature on the criticisms of six sigma through a systematic literature review the criticisms need to be analyzed so that there is growth in the knowledge and understanding of six sigma this paper analyzes literature through various electronic databases such as academic source premier ebsco google scholar business source premier ebsco emerald insight ieee xplore digital library jstor proquest science direct taylor  francis pubmed wiley inderscience scopus and world public library sixtyone relevant articles are found and analyzed in depth this paper finds 12 major themes of criticisms on six sigma based on the findings 16 research directions are offered amidst the plethora of literature review on the success of six sigma this is the first comprehensive systematic literature review on the criticisms of six sigma the authors firmly believe that such criticisms will help the academicians as well as the practitioners to understand some of the rudimentary gaps in implementing six sigma as a business improvement strategy
in this paper a comparison of the impact of major sudden stratospheric warmings ssws in the arctic in february 2018 ssw1 and january 2019 ssw2 on the midlatitude mesosphere is given the mesospheric carbon monoxide co and zonal wind in these two major ssw events were observed at altitudes of 70–85 km using a microwave radiometer mwr at kharkiv ukraine 500°n 363°e data from erainterim and merra2 reanalyses and aura microwave limb sounder measurements were also used it is shown that i the differences between ssw1 and ssw2 in terms of local variability in zonal wind temperature and co in the stratosphere and mesosphere were clearly defined by the polar vortex westerly in cyclonic circulation and midlatitude anticyclone easterly migrating over the mwr station therefore ii mesospheric intrusions of corich air into the stratosphere over the kharkiv region occurred only occasionally iii the larger zonal wave 1–3 amplitudes before ssw1 were followed by weaker polar vortex recovery than that after ssw2 iv the strong vortex recovery after ssw2 was supported by earlier event timing midwinter favoring vortex cooling due to low solar irradiance and enhanced zonal circulation and v vortex strengthening after ssw2 was accompanied by wave 1–3 amplification in march 2019 which was absent after ssw1 finally the influence of the largescale circulation structures formed in individual major ssw events on the locally recorded characteristics of the atmosphere is discussed
under covert attention our visual perception deteriorates dramatically as eccentricity increases this reduction of peripheral visual acuity pva is partially due to the coarse sampling of the retinal ganglion cells towards the periphery but this property cannot be solely responsible other factors such as character crowding have been studied yet the origin of the poor pva is not entirely understood this gap motivated us to investigate the pva by varying the crowding conditions systematically under completely crowded conditions ie resembling a full page of text pva was observed to be eight times worse than the pva under uncrowded conditions by partially crowding the periphery we obtained pva values between the fully crowded and uncrowded conditions on the other hand crowding the fovea center while leaving the periphery uncrowded improved pva relative to the uncrowded case these results support a model for a topdown “covert attention vector” that assists the resulting pva in a manner analogous to saccadic eye movement for overt attention we speculate that the attention vector instructs the dorsal pathway to transform the peripheral character to the foveal center then the scaleinvariant logpolar retinotopy of the ventral pathway can scale the centered visual input to match the prior memory of the specific character shape
the mw 75 palu earthquake struck the palukoro fault on september 28 2018 in the sulawesi island indonesia previous studies suggested that the predominantly unilateral southward rupture of this earthquake unzipped the central part of the palukoro fault two seismic gaps were left to the north and south ends of the palukoro fault to date the cause for earthquake generation has yet to be investigated as a result the reasons behind the earthquake ruptures southward unilateral propagation and termination remain unclear the hazards of the two seismic gaps at the north and south ends of the palukoro fault have yet to be explored as such the present study calculated the coulomb stress change that was induced by historical large earthquakes within the target region the stress partition and evolution on the palukoro fault were examined before and after the 2018 palu earthquake we found that the 1996 earthquake mw  79 on the minahassa thrust triggered the 2018 palu earthquake by promoting stress at the epicenter of the 2018 earthquake the stress shadow close to the north of the epicenter of the earthquake acted as a stress barrier by preventing the earthquake northward rupture however the southward earthquake propagation was terminated by the stress shadow to the south of the rupture these stress shadows were caused by the historical earthquakes on the palukoro fault stress promotions in the two seismic gaps of the palukoro fault produced by 2018 palu earthquake have resulted in increased seismic and tsunami hazards calling close attention to hazard prevention in central sulawesi our results are important for understanding the stress migration and the induced seismic activity in the thrust and strikeslip faults system globally
in recent years a range of problems under the broad umbrella of computer vision based analysis of ancient coins have been attracting an increasing amount of attention notwithstanding this research effort the results achieved by the state of the art in published literature remain poor and far from sufficiently well performing for any practical purpose in the present paper we present a series of contributions which we believe will benefit the interested community we explain that the approach of visual matching of coins universally adopted in existing published papers on the topic is not of practical interest because the number of ancient coin types exceeds by far the number of those types which have been imaged be it in digital form eg online or otherwise traditional film in print etc rather we argue that the focus should be on understanding the semantic content of coins hence we describe a novel approach—to first extract semantic concepts from realworld multimodal input and associate them with their corresponding coin images and then to train a convolutional neural network to learn the appearance of these concepts on a realworld data set we demonstrate highly promising results correctly identifying a range of visual elements on unseen coins with up to 84 accuracy
we propose an extension of the γdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentgamma enddocumentou barndorffnielsen and shephard model taking into account jump clustering phenomena we assume that the intensity process of the hawkes driver coincides up to a constant with the variance process by applying the theory of continuousstate branching processes with immigration we prove existence and uniqueness of strong solutions of the sde governing the asset price dynamics we propose a measure change of selfexciting esscher type in order to describe the relation between the riskneutral and the historical dynamics showing that the γdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentgamma enddocumentou hawkes framework is stable under this probability change by exploiting the affine features of the model we provide an explicit form for the laplace transform of the asset logreturn for its quadratic variation and for the ergodic distribution of the variance process we show that the proposed model exhibits a larger flexibility in comparison with the γdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentgamma enddocumentou model in spite of the same number of parameters required we calibrate the model on market vanilla option prices via characteristic function inversion techniques we study the price sensitivities and propose an exact simulation scheme the main financial achievement is that implied volatility of options written on vix is upward shaped due to the selfexciting property of hawkes processes in contrast with the usual downward slope exhibited by the γdocumentclass12ptminimal usepackageamsmath usepackagewasysym usepackageamsfonts usepackageamssymb usepackageamsbsy usepackagemathrsfs usepackageupgreek setlengthoddsidemargin69pt begindocumentgamma enddocumentou barndorffnielsen and shephard model
numerous developments in optical biomedical imaging research utilizing gold nanostructures as contrast agents have advanced beyond basic research towards demonstrating potential as diagnostic tools some of which are translating into clinical applications recent advances in optics lasers and detection instrumentation along with the extensive yet developing knowledgebase in tailoring the optical properties of gold nanostructures has significantly improved the prospect of nearinfrared nir optical detection technologies of particular interest are optical coherence tomography oct photoacoustic imaging pai multispectral optoacoustic tomography msot raman spectroscopy rs and surface enhanced spatially offset raman spectroscopy sesors due to their respective advancements here we discuss recent technological developments as well as provide a prediction of their potential to impact on clinical diagnostics a brief summary of each techniques capability to distinguish abnormal disease sites from normal tissues using endogenous signals alone is presented we then elaborate on the use of exogenous gold nanostructures as contrast agents providing enhanced performance in the abovementioned techniques finally we consider the potential of these approaches to further catalyse advances in preclinical and clinical optical diagnostic technologies
a hydrological characterization is presented through computational modeling of the vaquerías watershed basin this basin lacking any monitoring system represents the main source of water of the vaquerías natural reserve national university of córdoba chirps licencia de creative commons reconocimientonocomercial 40 internacional
this paper analyzes a variety of generalizations of a coinflipping magic trick invented independently by martin gardner and karl fulves in the original trick a blindfolded magician asks the spectator to flip three coins forcing them into an allequal state by surprisingly few moves we generalize to any number of coins coinsdice with more than two sides and multiple flips at once next we consider a generalization described by martin gardner in which the spectator can rearrange the coins in certain ways in between each flip finally we consider the variation in which the magician equalizes the number of heads and tails which can be achieved exponentially faster
abstract underwater images are degraded due to the complexity of the underwater environment and medium scattering and absorption the image degeneration includes low contrast color distortion and blur which makes the task of underwater vision difficult a perceptually optimized cycle consistency generative adversarial network cycleganvgg is proposed to restore distorted underwater images we adopt the framework of cyclegan so that our method does not require pairs of distorted underwater images and corresponding ground truth images for training inspired by perceptual loss we use a combination of perceptual loss cycleconsistent loss and adversarial loss the multiterm loss functions guarantee that the output has the same content and structure as the input while color looks like ground truth images in an objective assessment our method is significantly higher than other methods in terms of colorfulness metrics and contrast metrics it confirms that our method effectively restores the color of underwater scenes and enhances some image details and shows a superior performance when compared to other stateoftheart methods
opening gatedcommunities gcs has been widely discussed for urban inclusion and revitalization with the policies of opening gcs being promoted in china quantitative and comprehensive evaluation of the potential benefits is heavily needed taking shanghai as an example this study quantifies and analyzes the accessibility benefits and risks of opening gcs with factors including gc types opening levels travel modes and travel destinations considered we found that 1 opening gcs can bring 50m accessibility gains to 17 and 52 of the residents in moderately opening mo and completely opening co scenarios respectively 2 cycling benefits more than walking in all cases and scenarios 3 for different gcs conventional gcs have fewer benefits in mo but more in co than the newlyestablished one for different facilities trips to bus stations demonstrate the largest accessibility gains 4 the accessibility benefit of a residential building is highly determined by its closeness to the gates and relative location in the block 5 only 1 and 57 of external trips may penetrate the opened communities in mo and co scenarios respectively which are far less than both expectation and the benefits these findings precipitate at least two policy implications in china
driving behavior tracking and recognition are essential for traffic safety this paper proposes a driving behavior monitoring and analysis method based on motion capture and artificial intelligence techniques different driving actions can be identified as normal or abnormal driving behaviors the realtime motion is monitored by multiple miniature inertial measurement units imus gradient descent method is used to fuse the raw data and update the driver’s attitude information constantly the body’s joint angle series can be obtained by the iteration operation of the consecutive segment under the assumption of rigid structure in order to accurately identify two arms’ maneuvers under different traveling routes we compare the traditional machine learningbased method with the proposed deep neural networkbased approach using joint angle series the recognition rates of both methods are above 99 in the experiment and the algorithm performance in real scenario is also satisfactory these results show that joint anglebased driving behavior recognition is an effective method and it can be applied for driving training or guiding the novice
nonorthogonal multiple access noma is recognised as an improved multiple access technique as compared with oma technique to fulfil the requirements of fifthgeneration wireless communication systems in this study the outage probability and sumrate analysis of downlink multiuser noma system over the generalised fading channels ie η–µ and κ–µ are presented specifically the outage probability of noma is analysed with fixed target rate and oma rate the mathematical expressions for the outage probability and sum rate with a fixed set of power allocation coefficients are derived and verified through the simulation results the impact of system parameters such as number of users power allocation coefficients fading parameters etc on the performance of noma system is investigated noma with given fixed target rate offers better outage performance as compared with oma however noma with oma rate offers better outage performance for the near user only the sum rate and near user rate of noma improves with signaltonoise ratio however the far user rate does not follow the same trend
modular flavor symmetry can be used to explain the quark and lepton flavor structures the susy partners of quarks and leptons which share the same superpotential with the quarks and leptons will also be constrained by the modular flavor structure and show a different flavormixing pattern at the gut scale so in realistic modular flavor models with susy completion constraints from the collider and dm constraints can also be used to constrain the possible values of the modulus parameter in the first part of this work we discuss the possibility that the s3 modular symmetry can be preserved by the fixed points of t2zn orbifold especially from t2z2 to illustrate the additional constraints from collider etc on modular flavor symmetry models we take the simplest uv susycompletion s3 modular invariance su5 gut model as an example with generalized gravity mediation susy breaking mechanism we find that such constraints can indeed be useful to rule out a large portion of the modulus parameters our numerical results show that the uvcompleted model can account for both the sm plus neutrino flavor structure and the collider dm constraints such discussions can also be applied straightforwardly to other modular flavor symmetry models such as a4 or s4 models
background the beta distribution is useful for fitting variables that measure a probability or a relative frequency methods we propose a sarmanov distribution with beta marginals specified as generalised linear models we analyse its theoretical properties and its dependence limits results we use a real motor insurance sample of drivers and analyse the percentage of kilometres driven above the posted speed limit and the percentage of kilometres driven at night together with some additional covariates we fit a beta model for the marginals of the bivariate sarmanov distribution conclusions we find negative dependence in the high quantiles indicating that excess speed and nighttime driving are not uniformly correlated
traditional recommendation systems utilise past users’ preferences to predict unknown ratings and recommend unseen items however as the number of choices from content providers increases additional information such as context has to be included in the recommendation process to improve users’ satisfaction contextaware recommendation systems exploit the users’ contextual information eg location mood company etc using three main paradigms contextual prefiltering contextual postfiltering and contextual modelling in this work we explore these three ways of incorporating context in the recommendation pipeline and compare them on contextaware datasets with different characteristics the experimental evaluation showed that contextual prefiltering and contextual modelling yield similar performance while the postfiltering approach achieved poorer accuracy emphasising the importance of context in producing good recommendations
the current research demonstrates how conversational robo advisors as opposed to static nonconversational robo advisors alter perceptions of trust the evaluation of a financial services firm and consumer financial decision making we develop and empirically test a novel conceptualization of conversational robo advisors building on prior work in humantohuman communication and interpersonal psychology showing that conversational robo advisors cause greater levels of affective trust compared to nonconversational robo advisors and evoke a more benevolent evaluation of a financial services firm we demonstrate that this increase in affective trust not only affects firm perception in terms of benevolence attributions or a more positivelyvalenced onboarding experience but has important implications for investor behavior such as greater recommendation acceptance and an increase in asset allocation toward conversational robo advisors these findings have important implications for research on trust formation between humans and machines the effective design of conversational robo advisors and public policy in the digital economy
the spatial information of the signal is neglected by the conventional frequencytime decompositions such as the fast fourier transformation principal component analysis and independent component analysis framing of the data being as a threeway array indexed by channel frequency and time allows the application of parallel factor analysis which is known as a unique multiway decomposition the parallel factor analysis was used to decompose the wavelet transformed ongoing diagnostic channel–frequency–time signal and each atom is trilinearly decomposed into spatial spectral and temporal signature the time–frequency–space characteristics of the singlesource fault signal was extracted from the multisource dynamic feature recognition of mechanical nonlinear multifailure mode and the corresponding relationship between the nonlinear variable multifault mode and multisource fault features in time frequency and space was obtained in this article a new method for the multifault condition monitoring of slurry pump based on parallel factor analysis and continuous wavelet transform was developed to meet the requirements of automatic monitoring and fault diagnosis of industrial process production lines the multiscale parallel factorization theory was studied and a threedimensional time–frequency–space model reconstruction algorithm for multisource feature factors that improves the accuracy of mechanical fault detection and intelligent levels was proposed
wireless healthcare sensor network whsn is a benchmarking technology deployed to levitate the quality of lives for the patients and doctors whsn systems must fit ieee 802156 standard for specific application criteria unlike some standard criteria that are difficult to meet therefore many security models were suggested to enhance the security of the whsn and promote system performance yu and park proposed a threefactor authentication scheme based on the smart card biometric and password and their scheme can be easily employed in threetier whsn architecture furthermore they claimed that their scheme can withstand guessing attack and provide anonymity although after cryptanalysis we found that their scheme lacks both accordingly we suggested a threefactor authentication scheme with better system confusion due to multiplex parametric features hash function and higher key size to increase the security and achieve anonymity for the connected nodes moreover the scheme included initialization authentication reauthentication secure node addition user revocation and secure data transmission via blockchain technology the formal analysis of the scheme was conducted by ban logic burrows abadi nadeem and the simulation was carried out by tamarin prover to validate that the proposed scheme is resistant to replay session hijacking and guessing attacks plus it provides anonymity perfect forward secrecy and authentication along with the key agreement
agent programming languages have proved useful for formally modelling implemented systems such as prs and jack and for reasoning about their behaviour over the past decades many agent programming languages and extensions have been developed a key feature in some of them is their support for the specification of ‘concurrent’ actions and programs however their notion of concurrency is still limited as it amounts to a nondeterministic choice between sequential action interleavings thus the notion does not represent ‘true concurrency’ which can more naturally exploit multicore computers and multirobot manufacturing cells this paper provides a true concurrency operational semantics for a bdi agent programming language allowing actions to overlap in execution we prove key properties of the semantics relating to true concurrency and to its link with interleaving
market competitiveness is the main driver in the massive adoption of wind energy and the optimization of the power converter is crucial for this purpose because it has higher efficiency than the two level voltage source converter 2lvsc the three level neutral point clamped 3lnpc topology is considered for the converter optimization due to reduced voltage stress in 3lnpc the system voltage can be increased up to medium voltage while keeping the same 17 kv semiconductors used for 2lvsc potential extra efficiency due to the increased voltage cooling system and filter volume reduction are studied by simulation commercial cooling systems and capacitors are analyzed to calculate volumes while analytical expressions are used for the inductors a system level optimization is proposed achieving filter and combined volume reduction as well as efficiency increment the benefits of using three level converters to increase the application voltage without increasing the semiconductors voltage range are shown at system level
we establish the existence of nonnegative weak solutions to nonlinear reaction–diffusion system with cross‐diffusion and nonstandard growth conditions subject to the homogeneous neumann boundary conditions we assume that the diffusion operators satisfy certain monotonicity condition and nonstandard growth conditions and prove that the existence of weak solutions using galerkins approximation technique
integration of the line laser scanning system with visual slam for 3d mapping is conceptually attractive yet facing the difficulty with processing projected line laser which is not only hard to be extracted from images captured under natural light but also disrupts the feature tracking procedure in visual slam this paper proposes a method of segmenting the target object and extracting the laser line to build an accurate and realistic 3d model by using a semantic segmentation method first we introduce adaptive thresholds for the recognized objects to solve the laser extraction problem second we discard the extracted image features in the laser area for better pose estimation of visual slam finally we complement the surface of lasers with the color information in the related objects of 3d mapping in our experiments we show that the proposed method can produce a dense colored 3d mapping and has higher performance than the traditional visual slam based laser scanning system
 we have proposed an information acquisition method with detecting the reﬂector code on roadside by using infrared laser range scanner the scanner on vehicle scans the reﬂector code on roadside while traveling however we cannot change the code pattern according to the road information gps location and so on hence we propose an adaptive reﬂector code by switching infrared led to express appropriate information through some experiments we show that the vehicle can obtain diﬀerent information from roadside according to the use case
