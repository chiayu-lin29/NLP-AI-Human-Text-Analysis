clean_abstract
in the era of burgeoning electric vehicle ev popularity understanding the patterns of ev users’ behavior is imperative this paper examines the trends in household charging sessions’ timing duration and energy consumption by analyzing realworld residential charging data by leveraging the information collected from each session a novel framework is introduced for the efficient realtime prediction of important charging characteristics utilizing historical data and userspecific features machine learning models are trained to predict the connection duration charging duration charging demand and time until the next session these models enhance the understanding of ev users’ behavior and provide practical tools for optimizing the ev charging infrastructure and effectively managing the charging demand as the transportation sector becomes increasingly electrified this work aims to empower stakeholders with insights and reliable models enabling them to anticipate the localized demand and contribute to the sustainable integration of electric vehicles into the grid
in this study we apply two classification algorithm methods namely the gaussian naïve bayes gnb and the decision tree dt classifiers the gaussian naïve bayes classifier is a probabilitybased classification model that predicts future probabilities based on past experiences whereas the decision tree classifier is based on a decision tree a series of tests that are performed adaptively where the previous test affects the next test both of these methods are simulated on the iris dataset where the dataset consists of three types of iris setosa virginica and versicolor the data is divided into two parts namely training and testing data in which there are several features as information on flower characteristics furthermore to evaluate the performance of the algorithms on both methods and determine the best algorithm for the dataset we evaluate it using several metrics on the training and testing data for each method some of these metrics are recall precision f1score and accuracy where the higher the value the better the algorithms performance the results show that the performance of the decision tree classifier algorithm is the most outperformed on the iris dataset
human–robot interaction hri systems are crucial in robotics natural fastresponse and multimodal are the future trends in their development however current interaction methods have the following flaws 1 slow response of action recognition algorithms for generic scenes especially at the beginning stage 2 insufficient feature extraction and fusion capabilities for spatiotemporal graph data and 3 no good paradigm of body–hand recognition in hri to overcome these bottlenecks we propose a fastresponse graph convolutional network gcn for body–hand gesture recognition first we propose a dynamicstatic parallel network for dynamic body gestures that is responsive and accurate second we propose a spatiotemporal graph attention module to improve the graph data fusion effect in the dynamicstatic network third we implement a complete command module to form complete commands with body and hand information for interactions and control of the robot finally extensive experiments on four datasets and realworld experiments were conducted to demonstrate that our network is capable of fast response and accurate recognition of dynamic body gestures at the beginning stage verifying the effectiveness of skeletonbased bodyhand gesture recognition with a clear advantage over the stateoftheart
pvpq switching of generator buses when hitting reactive power limits is an important factor in voltage stability this article establishes the formulation and convexification of voltage stability constrained optimal power flow vscopf considering pvpq switching that cooptimizes generation dispatch and bus type profile the conventional power flow jacobian is intractable to be adopted since it has a variable size under pvpq switching to solve this issue a bigmparameterized constantsize power flow jacobian is constructed which explicitly considers pvpq switching it is proved that this new jacobian has the same minimum eigenvalue as the conventional jacobian which enables us to propose a more tractable vscopf by enforcing a lower bound for the constantsize jacobian minimum eigenvalue further a linear approximation of the voltage stability constraint is constructed by combining smoothing relaxation and eigensensitivity techniques finally we establish an semidefinite programming formulation for vscopf considering pvpq switching which can be efficiently solved by outer linearization algorithm the proposed formulation and convexification techniques are extendable to include more general pvpq switching actions triggered by hitting the capability curve of converterinterfaced generators the proposed method is verified via numerical tests on the ieee 118bus system comparing to the vscopf under a predefined bus type profile our method finds out the optimal bus type profile that helps to achieve a lowercost solution and higher precision in voltage stability assessment
the internet of things is shaping the next generation of cyber–physical systems to improve the future industry for smart cities it has created novel and essential applications that require specific network performance to enhance the quality of services since network performance requirements are applicationoriented it is of paramount importance to provide tailored solutions that seamlessly manage the network resources and orchestrate the network to satisfy user requirements in this article we propose elise a reinforcement learning rl framework to optimize the slotframe size of the time slotted channel hopping protocol in iiot networks while considering the user requirements we primarily address the problem of designing a framework that selfadapts to the optimal slotframe length that best suits the users requirements the framework takes care of all functionalities involved in the correct functioning of the network while the rl agent instructs the framework with a set of actions to determine the optimal slotframe size each time the user requirements change we evaluate the performance of elise through extensive analysis based on simulations and experimental evaluations on a testbed to demonstrate the efficiency of the proposed approach in adapting network resources at runtime to satisfy user requirements
federated clustering an essential extension of centralized clustering for federated scenarios enables multiple dataholding clients to collaboratively group data while keeping their data locally in centralized scenarios clustering driven by representation learning has made significant advancements in handling highdimensional complex data however the combination of federated clustering and representation learning remains underexplored to bridge this we first tailor a clustercontrastive model for learning clusteringfriendly representations then we harness this model as the foundation for proposing a new federated clustering method named clustercontrastive federated clustering ccfc benefiting from representation learning the clustering performance of ccfc even double those of the best baseline methods in some cases compared to the most related baseline the benefit results in substantial nmi score improvements of up to 04155 on the most conspicuous case moreover ccfc also shows superior performance in handling device failures from a practical viewpoint
this study proposes a set of generic rules to revise existing neural networks for 3d point cloud processing to rotationequivariant quaternion neural networks reqnns in order to make feature representations of neural networks to be rotationequivariant and permutationinvariant rotation equivariance of features means that the feature computed on a rotated input point cloud is the same as applying the same rotation transformation to the feature computed on the original input point cloud we find that the rotationequivariance of features is naturally satisfied if a neural network uses quaternion features interestingly we prove that such a network revision also makes gradients of features in the reqnn to be rotationequivariant wrt inputs and the training of the reqnn to be rotationinvariant wrt inputs besides permutationinvariance examines whether the intermediatelayer features are invariant when we reorder input points we also evaluate the stability of knowledge representations of reqnns and the robustness of reqnns to adversarial rotation attacks experiments have shown that reqnns outperform traditional neural networks in both terms of classification accuracy and robustness on rotated testing samples
acoustic imaging is powerful in collecting spatial information of acoustic sources into a visual representation in this paper we focus on the analysis of the exterior acoustic field captured by a circular array of microphones with a proper parametrization based on angles we map the directions of arrival of sources as a function of the microphone locations thus obtaining an acoustic image called “angular space” therefore we introduce a linear transform to enable analysis and synthesis operations for mapping the microphone pressures onto the angular space using local spacetime fourier analysis we prove the ability of this representation to combine global information coming from multiple arrays in a single acoustic image that can be processed and manipulated examples of source localization applications in simulated and measured scenarios show the effectiveness of the proposed method obtaining results comparable with stateoftheart methods
background porcine circovirus type 2 pcv2 infection is ubiquitous around the world diagnosis of the porcine circovirusassociated disease requires clinicpathological elements together with the quantification of viral loads furthermore given pig farms in regions lacking access to sufficient laboratory equipment developing diagnostic devices with high accuracy accessibility and affordability is a necessity objectives this study aims to investigate two newly developed diagnostic tools that may satisfy these criteria methods we collected 250 specimens including 170 pcv2positive and 80 pcv2negative samples the standard diagnosis and cycle threshold ct values were determined by quantitative polymerase chain reaction qpcr then two pointofcare poc diagnostic platforms convective polymerase chain reaction cpcr qualitative assay positive or negative results are shown and eztargex quantitative assay ct values are shown were examined and analyzed results the sensitivity and specificity of cpcr were 8823 and 100 respectively the sensitivity and specificity of eztargex were 8765 and 100 respectively these assays also showed excellent concordance compared with the qpcr assay κ  0828 for cpcr and κ  0820 for eztargex the statistical analysis showed a great diagnostic power of the eztargex assay to discriminate between samples with different levels of positivity conclusions the two pointofcare diagnostic platforms are accurate rapid convenient and require little training for pcv2 diagnosis these poc platforms can discriminate viral loads to predict the clinical status of the animals the current study provided evidence that these diagnostics were applicable with high sensitivity and specificity in the diagnosis of pcv2 infection in the field
interactive grasping from clutter akin to human dexterity is one of the longeststanding problems in robot learning challenges stem from the intricacies of visual perception the demand for precise motor skills and the complex interplay between the two in this work we present teacheraugmented policy gradient tapg a novel twostage learning framework that synergizes reinforcement learning and policy distillation after training a teacher policy to master the motor control based on object pose information tapg facilitates guided yet adaptive learning of a sensorimotor policy based on object segmentation we zeroshot transfer from simulation to a real robot by using segment anything model for promptable object segmentation our trained policies adeptly grasp a wide variety of objects from cluttered scenarios in simulation and the real world based on humanunderstandable prompts furthermore we show robust zeroshot transfer to novel objects videos of our experiments are available at httpsmaltemosbachgithubiograspanything
significant research work has been undertaken related to the gamebased learning approach over the last years however a closer look at this work reveals that further research is needed to examine some types of gamebased learning approaches such as virtual reality serious games and lego serious play this article examines and compares the effectiveness for learning scrum and related agile practices of a serious game based on virtual reality and a learning activity based on the lego serious play methodology the presented study used a quasiexperimental design with two groups pre and posttests and a perceptions questionnaire the sample was composed of 59 software engineering students 22 of which belonged to group a while the other 37 were part of group b the students in group a played the virtual reality serious game whereas the students in group b conducted the lego serious play activity the results show that both gamebased learning approaches were effective for learning scrum and related agile practices in terms of learning performance and motivation but they also show that the students who played the virtual reality serious game outperformed their peers from the other group in terms of learning performance
background technology is poised to bridge the gap between demand for therapies to improve gait in people with parkinsons and available resources a wearable sensor heel2toetm a small device that attaches to the side of the shoe and gives a sound each time the person starts their step with a strong heel strike has been developed and pretested by a team the objective of this study was to estimate feasibility and efficacy potential of the heel2toetm sensor in changing walking capacity and gait pattern in people with parkinsons methods a pilot study was carried out involving 27 people with parkinsons randomized 2 to 1 to train with the heel2toetm sensor and or to train with recommendations from a gaitrelated workbook results a total of 21 completed the 3month evaluation 14 trained with the heel2toetm sensor and 7 trained with the workbook thirteen of 14 people in the heel2toe group improved over measurement error on the primary outcome the six minute walk test mean change 664 m and 0 of the 7 in the workbook group mean change 194 m 4 of 14 in the heel2toe group made reliable change and 0 of 7 in the workbook group improvements in walking distance were accompanied by improvements in gait quality 40 of participants in the intervention group were strongly satisfied with their technology experience and an additional 37 were satisfied conclusions despite some technological difficulties feasibility and efficacy potential of the heel2toe sensor in improving gait in people with parkinsons was supported
the situation in which ukraine has found itself since february 2022 has also contributed to the development of digitalisation of all processes in higher education thus higher education institutions have forms of control such as course design diploma design various practices etc in this context the issue of digitalising the process of diploma design is the most relevant since explanatory notes to qualification works must be kept at the university upon graduation and go through several stages to be admitted to defence the article presents the experience of the zhytomyr polytechnic state university in digitalising the process of diploma design the procedure of preparing and defending a qualification work with a detailed description of each step is presented the article discusses the peculiarities of implementing an electronic digital signature in the zhytomyr polytechnic portal
as one of the potential explosions and inflammation compressed natural gas cng stations in urban areas cause irreparable losses and casualties estimating risk assessment in gas stress based on coherent uses can reduce accidents in urban areas the purpose of the present study was to estimate a small risk estimation at one of the cng multipurpose stations lpg using combined models of the fuzzy bayesian network bowtie diagram and consequence modeling this study was conducted based on the basic and 25 intermediate events this study formed a sevenperson safety team to identify the primary events and build the bowtie diagram then because of the lack of a proper database fuzzy theory was used to determine the probability of significant events bayesian networks were drawn based on the bowtie model using genle software finally the main events of the two bowtie bayesian network modeling and risk estimation were performed with the help of phastsafeti v822 the geographical information system software was used to zone the explosion effects the risk assessment result showed that the social risks and the bayesian network model are more than bowtie and the bowtie diagram is unacceptable therefore using incompatible land uses in the vicinity of the cng stations gives rise to the effects of accident scenarios in particular residential and administrative land uses which decisionmakers and city managers should consider based on the findings of this study the obtained results can be utilized to implement effective control measures these measures encompass devising a response plan tailored to address specific emergency conditions and conducting comprehensive training programs for the individuals and residents residing within the study area
in the last two decades alphasynuclein alphasyn assumed a prominent role as a major component and seeding structure of lewy bodies lbs this concept is driving ongoing research on the pathophysiology of parkinson’s disease pd in line with this alphasyn is considered to be the guilty protein in the disease process and it may be targeted through precision medicine to modify disease progression therefore designing specific tools to block the aggregation and spreading of alphasyn represents a major effort in the development of diseasemodifying therapies in pd the present article analyzes concrete evidence about the significance of alphasyn within lbs in this effort some dogmas are challenged this concerns the question of whether alphasyn is more abundant compared with other proteins within lbs again the occurrence of alphasyn compared with nonprotein constituents is scrutinized finally the prominent role of alphasyn in seeding lbs as the guilty structure causing pd is questioned these revisited concepts may be helpful in the process of validating which proteins organelles and pathways are likely to be involved in the damage to mesostriatal dopamine neurons and other brain regions involved in pd
the burgeoning growth of vehicular traffic fuelled by rapid urbanization and an everexpanding population has resulted in congested road networks traditional and sensorbased adaptive traffic light management systems have shown commendable progress in some scenarios but they suffer from inherent disadvantages that hinder their effectiveness and scalability to combat these challenges a dynamic and adaptable traffic control system is imperative to optimize traffic flow the present paper explores the limitations of sensorbased adaptive traffic light management and advocates for integrating a novel algorithm to overcome the existing drawbacks it proposes an intelligent traffic management algorithm called intellisignal that leverages the map service for fetching realtime traffic information to calculate the optimal green time and a penaltybased road selection to optimize traffic flow the proposed intellisignal is designed to provide equal chances for all roads while prioritizing higherdensity roads with more green time effectively mitigating traffic congestion and improving overall transportation efficiency it incorporates qlearning a reinforcement learning technique that enables the system to adapt and learn from traffic patterns the proposed intellisignal’s performance is assessed through rigorous evaluations conducted on the simulation platform sumo the acquired results demonstrate substantial enhancements across various crucial metrics including average waiting time vehicle density travel time co2 emissions and queue length furthermore the simulation results demonstrate that the proposed intellisignal algorithm exhibits a remarkable 3052 increment in system throughput compared to the traditional approach this significant enhancement underscores the efficacy of the proposed intellisignal in optimizing system performance and merits consideration for practical implementation
"background
the hypothesized link between lowdensity lipoprotein ldl and oncogenesis has garnered significant interest yet its explicit impact on lung adenocarcinoma luad remains to be elucidated this investigation aims to demystify the function of ldlrelated genes lrgs within luad endeavoring to shed light on the complex interplay between ldl and carcinogenesis


methods
leveraging singlecell transcriptomics we examined the role of lrgs within the tumor microenvironment tme the expression patterns of lrgs across diverse cellular phenotypes were delineated using an array of computational methodologies including aucell ucell singscore ssgsea and addmodulescore cellchat facilitated the exploration of distinct cellular interactions within ldllow and ldlhigh groups the findmarker utility coupled with pearson correlation analysis facilitated the identification of pivotal genes correlated with ldl indices an integrative approach to transcriptomic data analysis was adopted utilizing a machine learning framework to devise an ldlassociated signature las this enabled the delineation of genomic disparities pathway enrichments immune cell dynamics and pharmacological sensitivities between las stratifications


results
enhanced cellular crosstalk was observed in the ldlhigh group with the coxboostridge algorithm achieving the apex cindex for las formulation benchmarking against 144 extant luad models underscored the superior prognostic acuity of las elevated las indices were synonymous with adverse outcomes diminished immune surveillance and an upsurge in pathways conducive to neoplastic proliferation notably a pronounced susceptibility to paclitaxel and gemcitabine was discerned within the highlas cohort delineating prospective therapeutic corridors


conclusion
this study elucidates the significance of lrgs within the tme and introduces an las for prognostication in luad patients our findings accentuate putative therapeutic targets and elucidate the clinical ramifications of las deployment"
abstract currently workfamily conflict of college teachers is a common issue especially when their children are in the preschool stage which will directly affect the children’s problem behaviors this study aims to explore the relationship between parental workfamily conflict and children’s problem behaviors and the mediating role of parental phubbing and children’s problematic media use among chinese college teachers a total of 234 college teachers and their preschool children meanage  6029 months participated in this study the results indicate that 1 there is a significant positive correlation between college teachers’ workfamily conflict and their children’s problem behaviors 2 there is a significant mediating effect of problematic media use between workfamily conflict and preschool children’s problem behaviors and 3 parental phubbing and children’s problematic media use play a chain mediating role between workfamily conflict and children’s problem behaviors the results reveal the influencing mechanism of workfamily conflict on preschool children’s problem behaviors
abstract background understanding the genotype of pest species provides an important baseline for designing integrated pest management ipm strategies recently developed longread sequence technologies make it possible to compare genomic features of nonmodel pest species to disclose the evolutionary path underlying the pest species profiles here we sequenced and assembled genomes for 3 agricultural pest gelechiid moths phthorimaea absoluta tomato leafminer keiferia lycopersicella tomato pinworm and scrobipalpa atriplicella goosefoot groundling moth we also compared genomes of tomato leafminer and tomato pinworm with published genomes of phthorimaea operculella and pectinophora gossypiella to investigate the gene family evolution related to the pest species profiles results we found that the 3 solanaceous feeding species p absoluta k lycopersicella and p operculella are clustered together gene family evolution analyses with the 4 species show clear gene family expansions on host plant–associated genes for the 3 solanaceous feeding species these genes are involved in host compound sensing eg gustatory receptors detoxification eg abc transporter c family cytochrome p450 glucosemethanolcholine oxidoreductase insect cuticle proteins and udpglucuronosyl and digestion eg serine proteases and peptidase family s1 a gene ontology enrichment analysis of rapid evolving genes also suggests enriched functions in host sensing and immunity conclusions our results of family evolution analyses indicate that host plant adaptation and pathogen defense could be important drivers in species diversification among gelechiid moths
the gasthermal inertia and powergasheat interdependency as two fundamental features of the ies play a crucial role in ies resilience during extreme events this paper proposes a comprehensive resilience assessment framework to investigate the influence of these two factors on ies resilience performance during extreme events considering the inherent operational properties of power distribution system pds natural gas system ngs and district heating system dhs the interactions among multiple sectors and gasthermal inertia a dynamic optimal energy flow model is established to simulate the ies cascading failures and restoration process under outage scenarios on this basis the multistage resilience curves for the ies as well as each subsystem are obtained regarding the geometric features of the resilience curves a set of interdependency metrics is further proposed to quantify the sensitivity of ngs and dhs vulnerabilities that arise from their interdependency with pds comparative experiments are conducted to analyze the influence of gasthermal inertia and system interdependency on ies resilience the results demonstrated the proposed metrics can effectively assess the spatial and temporal interdependency among the resilience of power gas and heat subsystems providing useful information for operators in identifying critical links and evaluating potential upgrades to enhance the resilience of ies
multiaxis force sensors mfss are crucial for robotic manipulation and interaction in unstructured environments existing strain sensingbased mfss usually have fixed functions and sensitivities and are typically expensive and fragile requiring complex fabrication and assembly it is challenging to develop mfss to meet the requirements of application scenarios this article presents a fully integrated reconfigurable mfs rmfs which utilizes an array of printedcircuitboard pcb coils to detect the multiaxis displacementrotation of a metal target at high resolution without mechanical or electrical connections the rmfs design requires simple fabrication at a very low cost and can be assembled in a few minutes the basic characteristics design criteria and calibration method of the rmfs were investigated the rmfs prototype achieved an ultrahigh resolution of 1 mn for triaxial force sensing 0005 with a maximum error of 19 for 20 n loading a maximum crosstalk error of 57 and extremely small drifts over 10 h experiments show that the rmfs can respond rapidly and accurately to single and multiaxis force loadings a reconfigured rmfs for z axis force and xy axis torque sensing was realized by simply changing the metal target configuration the rmfs shows promising features to be implemented in diverse applications from robotics human–machine interaction and biomedical systems
this study offers valuable insights into the factors underpinning the cultivation of green innovation competitive advantage gica and the stimulation of sustainable innovative firm performance sifp while previous research has predominantly fixated on assessing the financial ramifications of green innovation endeavors the current discourse of governmental and non‐governmental authorities has witnessed a surge in discussions surrounding green innovation strategies gis to address climate change we contend that constructing gica through innovative products rather than traditional ones poses a formidable challenge to address this lacuna our research dissects gis into three dimensions ie green product process and service innovation to enhance gica and sifp to probe these intricate relationships we meticulously examined data from 545 chinese firms seeking to unravel the complex connections between gis gica sifp and moderation of csr we constructed a comprehensive model grounded in established theories resource‐based theory rbt and the theory of competitive advantage structural equation modeling via spss and amos was employed to analyze the data rigorously our findings unequivocally underscore the pivotal role of gis in nurturing gica as a moderating factor firms that integrate gis into their csr initiatives are strategically positioned to secure a substantial competitive edge moreover the primary impetus for businesses to adopt gis lies in their pursuit of gaining a competitive edge while concurrently sustaining innovative performance over the long haul these findings underscore the allure of gis as an avenue to attain sifp
existing blind image quality assessment biqa methods focus on designing complicated networks based on convolutional neural networks cnns or transformer in addition some biqa methods enhance the performance of the model in a twostage training manner despite the significant advancements these methods remarkably raise the parameter count of the model thus requiring more training time and computational resources to tackle the above issues we propose a lightweight parallel framework lpf for biqa first we extract the visual features using a pretrained feature extraction network furthermore we construct a simple yet effective feature embedding network fen to transform the visual features aiming to generate the latent representations that contain salient distortion information to improve the robustness of the latent representations we present two novel selfsupervised subtasks including a samplelevel category prediction task and a batchlevel quality comparison task the samplelevel category prediction task is presented to help the model with coarsegrained distortion perception the batchlevel quality comparison task is formulated to enhance the training data and thus improve the robustness of the latent representations finally the latent representations are fed into a distortionaware quality regression network daqrn which simulates the human vision system hvs and thus generates accurate quality scores experimental results on multiple benchmark datasets demonstrate that the proposed method achieves superior performance over stateoftheart approaches moreover extensive analyses prove that the proposed method has lower computational complexity and faster convergence speed
multiple object tracking mot a key task in image recognition presents a persistent challenge in balancing processing speed and tracking accuracy this study introduces a novel approach that leverages quantum annealing qa to expedite computation speed while enhancing tracking accuracy through the ensembling of object tracking processes a method to improve the matching integration process is also proposed by utilizing the sequential nature of mot this study further augments the tracking method via reverse annealing ra experimental validation confirms the maintenance of high accuracy with an annealing time of a mere 3 mus per tracking process the proposed method holds significant potential for realtime mot applications including traffic flow measurement for urban traffic light control collision prediction for autonomous robots and vehicles and management of products massproduced in factories
this paper introduces a shape‐based inflow scenarios reduction framework applied in long‐term hydro‐thermal scheduling this scheduling problem involves strategically managing the limited stored hydro energy in coordination with the electricity system the scenario fan problem selected as a long‐term hydro‐thermal scheduling approach can effectively represent the individual hydropower plants and their short‐term flexibility within the long‐term scheduling process making it well‐suited for capturing the disaggregated physical characteristics of hydro stations however the computational burden of scenario fan problem increases due to the complexity of the cascaded system and the large number of inflow scenarios to tackle these challenges a machine learning clustering method is developed for extracting shape‐based features to capture the energy and shape attributes of hydro inflows a comparative analysis was conducted to evaluate the effectiveness of the proposed method comparing it with four established clustering methods this evaluation was carried out on a hydro‐thermal test case utilizing both in‐sample and out‐of‐sample data analysis the findings demonstrate that the proposed framework outperforms other methods in terms of computational time by 426 and optimal reservoir trajectories by 121 considering k‐means using euclidean distance as the benchmark additionally this study showcases the feasibility of performing long‐term hydro‐thermal scheduling at a granular level with detailed topological information and a shorter time scale this expanded approach opens up new possibilities for addressing the integration of renewable energy resources within the realm of long‐term hydro‐thermal scheduling
handball is a team sport that involves fourteen players who are attempting to score more goals than their opponent within two thirtyminute halves a biomechanical analysis based on measuring the kinematics of jump throws could provide us with information on the ball’s velocity the maximal internal rotation of the trunk and the trunk’s flexion as well as the angular velocity of the ball during shoulder rotation the main aim of this study was to determine the wrist velocity during jump throws and standing throws without a runup the trunk arm rotation and wrist velocity will influence the speed of the ball during throwing this case study included a seniorgrade male handball player aged 1875 years with a body mass index bmi of 255 the biomechanical evaluation was carried out using a threedimensional vicon system the biomechanical analysis consisted of an evaluation of angular trunk velocity angular arm rotation velocity and wrist velocity during two types of throwing jump throws and standing throws without a runup the data were recorded for standing throws without a runup s1 and jump throws s2 for each situation we measured two phases due to the great variation in the kinematic parameters phase 1 f1 occurred when the elbow angle was 90° up to the moment when the wrist had an inflection of its trajectory and phase 2 f2 finished when the wrist’s velocity reached its maximum the results regarding the angular velocity of the trunk torsion showed a high value of this parameter during phase f2 compared to phase f1 for both types of throws s1 and s2 the angular velocity of the arm rotation achieved its maximum value in f2 during s2 and the wrist velocity was highest during phases f2 and s2 the correlation analysis demonstrated that there was a good correlation between the angular velocity of the trunk torsion and the angular velocity of the arm rotation for s1 in phase f1 however in phase f2 we found a good correlation between the angular velocity of the trunk torsion and wrist velocity for s2 we found that in phase f1 there was a good correlation between the angular velocity of the trunk torsion and wrist velocity however for phase f2 there was a good correlation between the angular velocity of the arm’s rotation and wrist velocity therefore the results from this case study indicate that the wrist velocity is influenced by the other two kinematic parameters especially the angular velocity of the arm’s rotation this means that the development of explosive force in the muscles of the trunk and arm could improve the wrist’s velocity and also increase the optimization of throwing in handball
abstract purchasing a tourist package for a specific tourist destination has become extremely easy and transparent through the diversity and continuously upgraded online booking platforms their content provides information through the reviews among tourists and hospitality managers being at the same time opened big data for researchers or policymakers mining the reviews of two global platforms booking and tripadvisor and a national one turistinfo the study aims to analyze the tourists’ sentiments and emotions experienced in a balneary destination such as northern oltenia romania the research addresses the overarching question of whether positive sentiments dominate in destinations led by spa tourism and consequently which emotions are definers for this purpose 10945 online reviews from 2018 to 2020 for 248 accommodation units of the studied area were collected and processed in statistical package for the social sciences spss 170 and geographical information system gis the key findings indicate that most travelers are satisfied with tourist destinations sustained by the dominance of positive sentiments 82 associated with a high rating score 89 and ‘joy’ and ‘trust’ emotions mostly positive sentiments are linked to the quality of five and fourstar accommodation units but also to the intimacy of the small family’s business the spatial framing in the landscape and the friendship of the hosts at the same time the repulsive aspects draw attention to some problems of the state of the indoor or outdoor environment and the pricequality ratio the research demonstrates the effectiveness of leveraging electronic word of mouth as a valuable resource for stakeholders in the tourism industry this approach enables a swift and sustainable assessment of tourist satisfaction providing valuable insights for accommodation service providers to make informed decisions
the electromagnetic radiation sensitivity of the electric explosive device eed and its installed use state is closely related to the size of the equipment and radiation field strength constraints the use of the traditional alllevel electromagnetic radiation method for the effect of the actual installed eed test in the electromagnetic environment simulation encountered a technical bottleneck the microwave band is difficult to effectively assess through the current standing wave distribution and skin effect the temperature rise of the eed bridge wire has no relationship with the frequency of the electromagnetic wave in this paper through the analysis of the electromagnetic effect mechanism of the eed the coupled power model of electromagnetic irradiation of the eed is obtained and the relationship between the temperature rise of the bridge wire of the eed and the electric field strength model is established under the action of highfrequency continuous waves the electromagnetic effect of the device is tested to verify the correctness of the mechanism analysis of the electromagnetic effect of the device under the action of continuous waves the results provide crucial technical support for the electromagnetic protection of the device under the harsh electromagnetic environment of the battlefield
future network architectures are expected to be autonomous intelligent and servicebased posing new security challenges to address these challenges the artificial intelligence ai security service emerges as a promising solution however the complex service configurations and performance guarantees hinder the autonomous deployment of the ai security service this paper proposes an autonomous deployment mechanism in softwaredefined networkingnetwork function virtualization sdnnfv enabled networks first our mechanism introduces user and decision planes on top of the control plane enabling hierarchical intent expression and translation from user security intent to security policies then we analyze the embedding problem of the aibased security function chain aisfc during security policy generation we formulate the aisfc embedding problem as an integer linear programming ilp task to minimize the total response delay by decomposing it into aisf placement and routing we design a heuristic algorithm with polynomial time complexity finally we validate the proposed mechanism through a prototype system and numerical simulations demonstrating its ability to autonomously translate implement and guarantee the user security intent comparative analysis shows that our approach considering the relationship between available computing resources and delay achieves smaller response delays than the baseline furthermore our algorithm achieves a gap from optimality approximately 2857 smaller than the greedy algorithm and supports networks that are 434 times larger in scale than the exact solution within a 2second execution time
changes in the clinical trials landscape have been driven by advancements in digital technology the use of electronic informed consent to inform research participants and to obtain their consent electronically has the potential to improve participant–researcher interactions over time facilitate clinical trial participation and increase efficiency in clinical trial conduct a personalized electronic informed consent platform that enables longterm interactions with the research team could function as a tool to empower participant engagement in clinical trials however significant challenges persist impeding successful and widespread implementation this perspective provides insights into the opportunities and challenges for the implementation of electronic informed consent in clinical trials it sets out key recommendations to promote the implementation of this innovative approach to the informed consent process including the creation of uniform electronic informed consent platforms at regional and national level
large numbers of accident reports are recorded in the aviation domain which greatly values improving aviation safety to better use those reports we must understand the most important events or impact factors according to the accident reports however the increasing number of accident reports requires large efforts from domain experts to label those reports to make the labeling process more efficient many researchers have started developing algorithms to automatically identify the underlying events from accident reports this article argues that we can identify the events more accurately by leveraging the event taxonomy more specifically we consider the problem to be a hierarchical classification task where we first identify the coarselevel information and then predict the finelevel information we achieve this hierarchical classification process by incorporating a novel hierarchical attention module into the bidirectional encoder representations from transformers model to further utilize the information from event taxonomy we regularize the proposed model according to the relationship and distribution among labels the effectiveness of our framework is evaluated using data collected by the national transportation safety board it has been shown that finelevel prediction accuracy is highly improved and that the regularization term can be beneficial to the rare event identification problem funding the research reported in this paper was supported by funds from nasa university leadership initiative program contract no nnx17aj86a project officer dr anupa bajwa principal investigator dr yongming liu and nsf dms 1830363 data ethics  reproducibility note the code capsule is available on code ocean at httpscodeoceancomcapsule9128124treev1 and in the ecompanion to this article available at httpsdoiorg101287ijds20220032 
abstract microorganisms inhabiting hypersaline environments have received significant attention due to their ability to thrive under polyextreme conditions including high salinity elevated temperatures and heavy metal stress they are believed to possess biosynthetic gene clusters bgcs that encode secondary metabolites as survival strategy and offer potential biotechnological applications in this study we mined bgcs in shotgun metagenomic sequences generated from lake afdera a hypersaline lake in the afar depression ethiopia the microbiome of lake afdera is predominantly bacterial with acinetobacter 186 and pseudomonas 118 being ubiquitously detected a total of 94 distinct bgcs were identified in the metagenomic data these bgcs are found to encode secondary metabolites with two main categories of functions i potential pharmaceutical applications nonribosomal peptide synthase nrps polyketide synthase others and ii miscellaneous roles conferring adaptation to extreme environment bacteriocins ectoine others notably nrps 206 and bacteriocins 106 were the most abundant furthermore our metagenomic analysis predicted gene clusters that enable microbes to defend against a wide range of toxic metals oxidative stress and osmotic stress these findings suggest that lake afdera is a rich biological reservoir with the predicted bgcs playing critical role in the survival and adaptation of extremophiles
the µopioid receptor mor is a gprotein coupled receptor involved in nociception and is the primary target of opioid drugs understanding the relationships between ligand structure receptor dynamics and efficacy in activating mor is crucial for drug discovery and development here we use coarsegrained normal mode analysis to predict ligandinduced changes in receptor dynamics with the quantitative dynamics activity relationships qdar dynasigml methodology training a lasso regression model on the entropic signatures es computed from ligandreceptor complexes we train and validate the methodology using a dataset of 179 mor ligands with experimentally measured efficacies split into strickly chemically different crossvalidation sets by analyzing the coefficients of the es lasso model we identified key residues involved in mor activation several of which have mutational data supporting their role in mor activation additionally we explored a contactsonly lasso model based on ligandprotein interactions while the model showed predictive power it failed at predicting efficacy for ligands with low structural similarity to the training set emphasizing the importance of receptor dynamics for predicting ligandinduced receptor activation moreover the low computational cost of our approach at 3 cpu seconds per ligandreceptor complex opens the door to its application in largescale virtual screening contexts our work contributes to a better understanding of dynamicsfunction relationships in the µopioid receptor and provides a framework for predicting ligand efficacy based on ligandinduced changes in receptor dynamics contact rafaelnajmanovichumontrealca
given starting and ending positions and velocities l2 bounds on the acceleration and velocity and the restriction to no more than two constant control inputs this paper provides routines to compute the minimaltime path closed form solutions are provided for reaching a position in minimum time with and without a velocity bound and for stopping at the goal position a numeric solver is used to reach a goal position and velocity with no more than two constant control inputs if a cruising phase at the terminal velocity is needed this requires solving a nonlinear equation with a single parameter code is provided on github1 extended paper version at 1
the paper presents a study of the performance and development of unmanned ground vehicles ugvs establishing mathematical and numerical models of the chassis system the model analysis is performed by 3d software package solidworks 2018 with finite element discretization the mesh modelling and analysis are focused on studying the strength and stiffness of the robotic platform chassis and the distribution of stress and deformation in the extremal condition the paper also presents an autopilot design with a new cascade control system for the autonomous motion of an unmanned ground vehicle based on proportional–integral–derivative pid and feedforward ff control the pidff controller is part of a ugv used in a hybrid control system for precise control and stabilization which is necessary to increase the vehicle motion stability and maneuver precision the hybrid pidff control system proposed for the ground vehicle model gives satisfactory control quality while maintaining the simplicity of the control system the presented tests performed in mechanical design and control analysis give good results and prove the usefulness of the designed unmanned device
introduction connections among neurons form one of the most amazing and effective network in nature at higher level also the functional structures of the brain is organized as a network it is therefore natural to use modern techniques of network analysis to describe the structures of networks in the brain many studies have been conducted in this area showing that the structure of the neuronal network is complex with a smallworld topology modularity and the presence of hubs other studies have been conducted to investigate the dynamical processes occurring in brain networks analyzing local and largescale network dynamics recently network diffusion dynamics have been proposed as a model for the progression of brain degenerative diseases and for traumatic brain injuries methods in this paper the dynamics of network diffusion is reexamined and reactiondiffusion models on networks is introduced in order to better describe the degenerative dynamics in the brain results numerical simulations of the dynamics of injuries in the brain connectome are presented different choices of reaction term and initial condition provide very different phenomenologies showing how network propagation models are highly flexible discussion the uniqueness of this research lies in the fact that it is the first time that reactiondiffusion dynamics have been applied to the connectome to model the evolution of neurodegenerative diseases or traumatic brain injury in addition the generality of these models allows the introduction of nonconstant diffusion and different reaction terms with nonconstant parameters allowing a more precise definition of the pathology to be studied
the industrial internet of things iiot is a collection of interconnected smart sensors and actuators with industrial software tools and applications iiot aims to enhance manufacturing and industrial processes by capturing and analyzing realtime industrial data however the heterogeneous and homogeneous nature of iiot networks makes them vulnerable to several security threats as data is transmitted over an insecure communication medium intruders may intercept communication among different entities and perform malicious activities consequently ensuring the security and privacy of data transmitted in iiot networks is essential motivated by the aforementioned challenges this article presents a deeplearningintegrated blockchain framework for securing iiot networks specifically first we design a private blockchainbased secure communication among the iiot entities using sessionbased mutual authentication and key agreement mechanism in this approach the proofofauthority poa consensus mechanism is used for verification of the transactions and block creation based on the voting of miners over the cloud server second we design a novel deeplearningbased intrusion detection system that combines contractive sparse autoencoder csae attentionbased bidirectional long shortterm memory abilstm networks and softmax classifier for cyberattack detection the practical implementation of blockchain and deeplearning techniques proves the effectiveness of the proposed framework
cloud storage is the technical boon which has become the realm of many companies like amazon google microsoft drop box and many more remarkably all these companies are us based and thus eventually the personal data of every other nation is stacked on the western companies this leads the dependant country to a state of vulnerable however if the dependant countries need to get autonomy they had to be provided with their own cloud storage which is impossible with the current mechanisms with the facility of high computational devices and wide infrastructure and with the aid of secured encryption the cloud storage providers strongly protect the data of the users and enable their best service to the users also the cloud storage providers use the encryption algorithm in the provider infrastructure and data from user to provider infrastructure is done using the secured transmission finally the powerful infrastructure does the work to secure the data this mechanism is certainly possible in the developed countries but it is relatively impossible in the developing countries like india as it cannot afford the huge infrastructure to protect the public data even done it might not result in generating profit because the subscription rate is comparatively lower in india than in developed countries so to pursue the less profitable module is not suggestive and hence the reliability on western country cloud storage is over whelming however to overcome this set back and to implement indigenous cloud storage this paper has attempted to implement the uk algorithm in user device environment through which mini and micro level cloud storage shall be developed resulting in the minimal cost of maintaining the cloud storage in this case cloud storage provider do not need to have either the huge infrastructure nor the computational devices to process the data and no need of separate data transmission to secure the data as well employing the encryption algorithm in user environment helps to give full authority to the user on their data the cs provider holds only encrypted data of the user and with low cost consumption this method is feasible even for the developing country like india
the most vital energy resources for modern society are oil and natural gas these two fundamental factors might have a considerable impact in the development of international relations due to their strategic importance in global economic and political situation hence this research designs a model to enhance the supply chain performance in the oil and gas companies in the kingdom of saudi arabia ksa the recent study aimed to examine the impact of the sc coordination and sc cooperation on the sc performance with the mediating effect of the sc ambidexterity in the oil and gas companies in ksa a total of 300 questionnaires selected of the oil and gas companies in ksa using convenience sampling method the study used sem with smartpls 4 to analyze the data collected the measurement model applied to analyze the validity and reliability of the model the path coefficient in the structural equation model used to test the study hypotheses the results of this study support all of five direct effect hypotheses and two mediation hypotheses the result of the study finds that sc collaboration has a significant impact on the sc performance and sc ambidexterity and sc cooperation has a significant impact on the sc performance and sc ambidexterity also the result of the study found significant impact of the sc ambidexterity on sc performance in the oil and gas companies ksa moreover the result of the study confirms the mediating role of the sc ambidexterity on the impact of the sc collaboration and sc cooperation on sc performance the study provides very important implications to the managers of the ksa to improve their supply chain performance to gain competitive advantage by applying model of sc collaboration sc cooperation and sc ambidexterity in oil and gas industry
this letter demonstrates that an inexpensive easytodeploy and portable 3dprinted alldielectric cornerlike bragg reflector can boost antenna gain notably the concept shown here works by placing the reflector around the antennas nearfield without changing its design the idea behind this work stems from the combination of quarterwave stack bragg mirrors and electric image theories providing further gain enhancements from previous metal cornerlike and 3dprinted bragg reflectors in particular we considered an operating frequency of 57 ghz for which a 3dprinted prototype was manufactured and characterized at microwave frequencies as a result an aperture efficiency of 4096 with the gain enhanced up to 1296 db were observed without disturbing the primary source impedance bandwidth in excellent agreement with fullwave numerical analysis
"abstract there are two factors that influence fomo in generation z consisting of internal factors and external factors internal is found to have a low psychological need for relatedness and self meanwhile externals include transparency of social media information age social upmanship and conditions of relative deprivation efforts to handle fomo in generation z based on a biblical perspective are to use assistance or mentoring to generation z who are fomo the method used in this research is qualitative the approach taken is a literature study through this approach researchers examine existing literature sources in order to obtain a logical thinking construct regarding fomo the bible consistently shows how god cares for his people from generation to generation this consistency of gods providence also applies to generation z especially those who experience fomo the essence of the lords prayer is a form of gods openness in presenting his providence holistically to all believers from generation to generation
abstrak ada dua faktor yang mempengaruhi fomo pada generasi z terdiri dari faktor internal dan faktor eksternal internal adalah ditemukan rendahnya kebutuhan psikologis akan relatedness dan self sementara ekstenal meliputi transparansi informasi media sosial usia socialupmanship dan kondisi deprivasi relative upaya penanganan fomo pada generasi z berdasarkan perspektif alkitab adalah menggunakan pendampingan atau mentoring kepada generasi z yang fomo metode yang digunakan dalam penelitian ini adalah kualitatif pendekatan yang dilakukan adalah studi pustaka melalui pendekatan ini peneliti mengkaji dari sumber literatur yang ada agar diperoleh konstruk berpikir yang logis berkenaan dengan fomo alkitab secara konsisten memperlihatkan cara allah memelihara umatnya dari generasi ke generasi konsistensi providensia allah ini pun berlaku bagi generasi z terkhusus mereka yang mengalami fomo esensi doa bapa kami merupakan salah satu bentuk keterbukaan allah dalam menyajikan providensianya secara holistik bagi seluruh orang percaya dari generasi ke generasi"
robotics is the science of creating and assembling tangible robots to enhance automation and creativity engineering and computer science are combined in the field of robotics which deals with the creation manufacturing and use of robots robotics is undergoing a fast evolution with groundbreaking discoveries that are changing entire sectors and societal contexts recent advances in ai have accelerated robotics progress toward more flexibility and autonomy there are many different types of robotics a robot could be an artificial intelligence ai device that looks like a person or it could be a robotic application like robotic process automation which resembles how people interact with software to carry out repetitive rulebased tasks the continued convergence of robotics with artificial intelligence materials science and other interdisciplinary domains holds the potential to open up new avenues for automation and humanrobot cooperation this study offers an overview of the most recent advancements in robotics including significant technological innovations cuttingedge applications and developing trends
the detection and cure of epilepsy and autism spectrum disorder asd are significantly complicated by their cooccurrence this survey research investigates an integrated method for identifying asd using behavioural characteristic questionnaires and epilepsy using eeg corpus inside a single system we provide an overview of all the relevant research emphasizing the difficulties in diagnosing each of these disorders separately and in combination our suggested approach combines behavioural questionnaire assessments for asd with eegbased analysis for epilepsy detection in an effort to improve diagnostic accuracy and expedite the evaluation process this study examines the approaches difficulties and developments in both domains providing perspectives on possible overlaps and prospects for further investigation so an attempt has been made to review on the pattern detection methods for epilepsy seizure detection from eeg signals more than 150 research papers have been discussed to determine the techniques for detecting epileptic seizures further the literature review confirms that the pattern recognition techniques required to detect epileptic seizures varies across the electroencephalogram eeg datasets of different conditions this is mostly owing to the fact that eeg detected under different conditions have different characteristics
prefabricated construction pc is considered to be a lowcarbon construction method implementing prefabricated construction projects pcps requires multiple industry organizations to participate and collaborate as different pcps are initiated and implemented industry organizations will gradually gather into a complex and evolving collaborative network at the industry level based on specific collaboration relationships with different project backgrounds the evolution of the collaborative network is related to how industry organizations interact with each other and how pcrelated knowledge and innovation has spread among organizations in the long term however the laws of network evolution and the micro effects that drive network evolution are still unknown this study analyzes 236 prefabricated construction projects pcps in shanghai during 2015–2023 using the stochastic actororiented models saom method to explore how the macro structure of projectbased interorganizational collaborative networks for prefabricated construction pc technology implementation evolves over time and how micro effects jointly support the evolution of the networks the macrolevel descriptive analysis of the network indicates that the collaborative network has become increasingly dense over time and continues to show a core–peripheral structure with a small number of superconnected organizations the microlevel saom analysis further reveals that the evolution of the collaborative network structure is driven by structurebased preferential attachment and geographic proximity effects as well as attributebased ownership similarity effect this exploratory effort applies a network dynamics model to investigate the micro mechanism of the evolution of interorganizational collaboration the research results provide theoretical guidance and decisionmaking references for pc industry organizations to develop efficient network action strategies in addition it can help industry managers to formulate appropriate network management strategies
this work presents a new design of miniaturized microneedle ph sensors based on agio3agpolytetrafluoroethylene ptfewo3tungsten w which can be used to measure the realtime labelfree ph of various microsized cells and smallvolume bodily fluids the fabrication process comprises multiple steps including creating w microneedles and depositing agio3agptfewo3 layers a needlelike structure was created at the end of a cleaved w rod using electrolytic polishing the sensing layer working electrode we of wo3 films was developed onto w microneedles using the thermal oxidation method a ptfe coating was applied as an insulating layer between the we and the reference electrode re agio3ag thin film the ptfe and agio3ag layers were fabricated using the hand spray and rf sputtering methods respectively the microstructural and compositional features of miniaturized microneedle sensors agio3agptfewo3w were analyzed using field emission scanning electron microscopy fesem energy dispersive xray edx xray diffractometer xrd and raman techniques the miniaturized microneedle sensors exhibited excellent analytical performance including a nernstian slope of −4936 mvph and a linear response range from ph 401 to ph 918 as verified by testing with a ph standard solution exvivo ph testing results of fish eggs demonstrated the ability of the fabricated miniature microneedle ph sensor to penetrate biological tissue and accurately measure ph values
the question of the influence of the price set by the producer on the sales volumes in a logistics system with a linear structure is considered the impact of price on sales volumes is considered from the perspective of removing the information uncertainty that occurs when analysing the balance between supply and demand of a probabilistic nature the impact factor analysis is based on the total cost pricing method which includes production costs and profits the latter is determined on the basis of a markup factor calculation besides the offered price including the markup is considered a constant over the allotted time interval and its value must be in the sellers and buyers area of interest this approach has advantages over traditional pricing methods and is consistent with the erowdebre stabilityoriented pricing model a description of the process of changing the markup rate within outlined limits in which it varies according to the proposed sales volume is presented the price and sales volume limits are justified based on the possibility of using a «cognitive» approach the demand functions for products and rate prices in the area of their possible «overlapping» when buyingselling in micromarkets are considered
the crouzeixraviart finite element method is widely recognized in the field of finite element analysis due to its nonconforming nature the main goal of this paper is to present a general strategy for enhancing the crouzeixraviart finite element using quadratic polynomial functions and three additional general degrees of freedom to achieve this we present a characterization result on the enriched degrees of freedom enabling to define a new enriched finite element this general approach is employed to introduce two distinct admissible families of enriched degrees of freedom numerical results demonstrate an enhancement in the accuracy of the proposed method when compared to the standard crouzeixraviart finite element confirming the effectiveness of the proposed enrichment strategy
digitization has permeated every aspect of industry and the integration of immersive digital technology into traditional tourism dance performances has become a core concern in both theoretical and practical fields however existing literature lacks sufficient exploration on how the integration of immersive digital technology and tourism dance performances can improve tourist loyalty therefore our research established a theoretical model based on the stimulusorganismresponse sor framework and tested our proposed hypothesis through the structural equation model the research results indicate that authenticity esthetics and entertainment have a significant promoting effect on immersive experience and meaningful experience which have a positive impact on tourists’ revisit intention in addition authenticity esthetics and entertainment promote the improvement of revisiting intention through the mediating effect of flow experience and meaningful experience this research expands the research context of authenticity theory and experience economy framework and provides a new research perspective for tourism destination loyalty research our conclusion provides theoretical guidance for the sustainable development of tourist destinations
explainable artificial intelligence xai strategies play a crucial part in increasing the understanding and trustworthiness of neural networks nonetheless these techniques could potentially generate misleading explanations blinding attacks can drastically alter a machine learning algorithms prediction and explanation providing misleading information by adding visually unnoticeable artifacts into the input while maintaining the models accuracy it poses a serious challenge in ensuring the reliability of xai methods to ensure the reliability of xai methods poses a real challenge we leverage statistical analysis to highlight the changes in cnn weights within a cnn following blinding attacks we introduce a method specifically designed to limit the effectiveness of such attacks during the evaluation phase avoiding the need for extra training the method we suggest defences against most modern explanationaware adversarial attacks achieving an approximate decrease of 99 in the attack success rate asr and a 91 reduction in the mean square error mse between the original explanation and the defended postattack explanation across three unique types of attacks
when a drone performs physical maneuvers in close proximity to ground objects at extremely low altitudes the downwash generated significantly impacts the object directly beneath it in response to this issue we propose a drone designed to operate at low altitudes while simultaneously creating a windless zone directly beneath the aircraft thus mitigating its impact on objects below this entails the development of an hshaped airframe structure capable of establishing a windless space through the manipulation of rotor angles our objective is to formulate a control model for the drone and implement stabilization control the downwash characteristics of the proposed drone will be methodically investigated through experiments employing a unique 3d airflow measurement system and an operational drone visualization of downwash will be generated from the collected wind velocity data showcasing the potential to create a windless zone when the rotor angle is set to 30° or higher additionally in formulating the control model we will conduct an extensive search for control parameters that contribute to stabilizing the drone during flight through simulation experiments we aim to demonstrate the feasibility of controlling the drone with rotor angles up to 40° using a conventional controller utilizing the capabilities of the parameter search algorithm
at present the determination of tunnel parameters mainly rely on engineering experience and human judgment which leads to the subjective decision of parameters and an increased construction risk machine learning algorithms could provide an objective theoretical basis for tunnel parameter decision making however due to the limitations of a machine learning model’s performance and parameter selection methods the prediction model had poor prediction results and low reliability for parameter research to solve the above problems based on a large number of construction parameters of a composite section subway in shenzhen this paper combined dimensionality reduction data with service analysis to optimize the selection process of shield tunneling parameters and determined the total propulsion force cutter head torque cutter head speed and advance rate as key tunneling parameters based on an lgbm algorithm and bayesian optimization the prediction model of key tunneling parameters of an earth pressure balance shield was established the results showed that the average error of the lgbm model on the test set was 818 the average error of the cutter head torque was 1393 the average error of the cutter head speed was 316 and the average error of advance rate was 1335 compared with the rf model the prediction effect and the generalization on the test set were better therefore an lgbm algorithm could be used as an effective prediction method for tunneling parameters in tunnel construction and provide guidance for the setting of tunneling parameters
selfadmitted technical debt satd encompasses a wide array of suboptimal design and implementation choices reported in software artefacts eg code comments and commit messages by developers themselves such reports have been central to the study of software maintenance and evolution over the last decades however they can also be deemed as dreadful sources of information on potentially exploitable vulnerabilities and security flaws objective this work investigates the security implications of satd from a technical and developercentred perspective on the one hand it analyses whether security pointers disclosed inside satd sources can be used to characterise vulnerabilities in opensource software oss projects and repositories on the other hand it delves into developers’ perspectives regarding the motivations behind this practice its prevalence and its potential negative consequences method we followed a mixedmethods approach consisting of i the analysis of a preexisting dataset containing 8812 satd instances and ii an online survey with 222 oss practitioners results we gathered 201 satd instances through the dataset analysis and mapped them to different common weakness enumeration cwe identifiers overall 25 different types of cwes were spotted across commit messages pull requests code comments and issue sections from which 8 appear among mitre’s top25 most dangerous ones the survey shows that software practitioners often place security pointers across satd artefacts to promote a security culture among their peers and help them spot flaky code sections among other motives however they also consider such a practice risky as it may facilitate vulnerability exploits implications our findings suggest that preserving the contextual integrity of security pointers disseminated across satd artefacts is critical to safeguard both commercial and oss solutions against zeroday attacksccs concepts•security and privacy → human and societal aspects of security and privacy software security engineering • software and its engineering → maintaining software
accurate fault diagnosis and remaining useful life rul prediction are critical for health management before fatal system failures occur reasonable stage prediction of rul can avoid the waste of computing power however the difficulttomeasure multiple metrics of real systems cause the starting and endpoint of rul to be challenging to determine effectively based on the data distribution and digital twin dt technique this article proposes a method to collect the steadystate output signal to determine the beginning and end points of the stable equilibrium state system set rul the combination of dt technology can accurately reproduce the actual operating state and measurable characteristics of the system update the component degradation model by collecting the current component parameters in realtime after the starting point of rul and inject the future parameter state of the component into dt the dt is equivalent to a multiindicator acquisition device which solves the problem of the physical system’s difficulty in acquiring multiple indicators determines the remaining life of the system remaining life prediction under the dual constraints of data and mechanism is achieved by realizing a reasonable starting point and an accurate end point the superbuck converter is used as an example for experiments and the results prove that the method can reasonably determine the system’s start and end points to predict the remaining life the comparison of the three methods proves the superior performance of the method in this article
employers sometimes use personality tests in hiring or specifically look for candidates with certain personality traits such as being social outgoing active and extraverted therefore they hire based on personality specifically extraversion in part at least the question arises whether this practice is morally permissible we argue that in a range of cases it is not the common belief is that generally it is not permissible to hire based on sex or race and the wrongness of such hiring practices is based on two widely accepted principles the relevance principle and the fairness principle the relevance principle states that hiring should be based on what is relevant to job performance while the fairness principle states that hiring should be based on features that individuals can control since hiring based on sex or race violates these principles it is wrong however we argue that in a range of cases hiring based on extraversion also violates these principles and assuming the validity of the relevance and the fairness principles it follows that personality discrimination is morally wrong in those cases
secondary frequency control systems such as automatic generation control agc are used in interconnected power grids however when a system failure causes systems to separate into zones islands agc can no longer be used and the primary frequency control is the only control available moreover load changes may cause frequency drop in some areas and overfrequency in other areas therefore the goal in this article will be to design a neural networkbased proportional integral and derivative pid controller in the primary control architecture to control the over frequency condition the proposed controller is adaptively optimized in two stages by the honey badger algorithm hba in the first stage the pid controller gain values are optimized by the hba algorithm for different values of load loss while in the second stage a feedforward artificial neural network ann is trained to match the tieline measured power to the corresponding optimized hbapid gains obtained in the first stage finally the proposed controller is implemented on a twoarea interconnected thermal power system the proposed controller results qualitatively outperform one of the best tuning methods the zieglernichols zn approach and they show that the proposed controller has better dynamic responses with minimal frequency deviations and fast settling time creating and guaranteeing a margin of stability for the closed loop
malayan krait bungarus candidus envenoming is a cause of significant morbidity and mortality in many southeast asian countries if intubation and specific antivenom administration are delayed the most significant lifethreatening outcome may be the inhibition of neuromuscular transmission and subsequent respiratory failure it is recommended that kraitenvenomed victims without indications of neurotoxicity eg skeletal muscle weakness or ptosis immediately receive 10 vials of antivenom however the administration of excess antivenom may lead to hypersensitivity or serum sickness therefore monitoring venom concentrations in patients could be used as an indicator for snake antivenom treatment in this study we aimed to develop a screenprinted gold electrode spge biosensor to detect b candidus venom in experimentally envenomed rats the gold electrodes were coated with monovalent malayan krait igg antivenom and used as venom detection biosensors electrochemical impedance spectrometry eis and square wave voltammetry swv measurements were performed to detect the electrical characterization between b candidus venom and monovalent igg antivenom in the biosensor the eis measurements showed increases in charge transfer resistance rct following igg immobilization and incubation with b candidus venom solution 01–04 mgml thus the antibody was immobilized on the electrode surface and venom was successfully detected the lowest current signal was detected by swv measurement in rat plasma collected 30 min following b candidus experimental envenoming indicating the highest level of venom concentration in blood circulation 43 ± 07 µgml the present study demonstrates the ability of the spge biosensor to detect b candidus venom in plasma from experimentally envenomed rats the technology obtained in this work may be developed as a detection tool for use along with the standard treatment of malayan krait envenoming
losing the ability to deform the palm is unthinkable for a human hand and the same is true for a humanoid robotic hand here we aim to evaluate the performance of human palms and develop a palmmovable humanoid robotic hand with a few actuators we quantified the palm morphological characteristics by analyzing the palm morphological parameters collected from 42 subjects subsequently we constructed a kinematic model of the human hand with these characteristics to quantitatively analyze how a movable palm influences the finger workspace we found that a movable palm can significantly improve fingertipreachable space at least 50 and increase the capability of fingers to oppose each other at least 14 furthermore we designed a palmmovable humanoid robotic hand with only four actuators and compared the grasping performance of this prototype with that of a palmfixed robotic hand and a human hand the results suggest that a movable palm contributes to enhancing anthropomorphism adaptability and grip stability in humanoid robotic hands hence our findings contribute to the development of dexterous robotic hands
we introduce a new attention mechanism dubbed structural selfattention structsa that leverages rich correlation patterns naturally emerging in keyquery interactions of attention structsa generates attention maps by recognizing spacetime structures of keyquery correlations via convolution and uses them to dynamically aggregate local contexts of value features this effectively leverages rich structural patterns in images and videos such as scene layouts object motion and interobject relations using structsa as a main building block we develop the structural vision transformer structvit and evaluate its effectiveness on both image and video classification tasks achieving stateoftheart results on imageneti k kinetics400 somethingsomething vi  v2 diving48 and finegym
purposethis paper aims to explore the optimization process involved in the aircraft maintenance allocation and packing problem the aircraft industry misses a part of the optimization potential while developing maintenance plans this research provides the modeling foundation for the missing part considering the failure behavior of components costs involved with all maintenance tasks and opportunity costsdesignmethodologyapproachthe study models the costeffectiveness of support against the availability to come up with an optimization problem the mathematical problem was solved with an exact algorithm experiments were performed with real field and synthetically generated data to validate the correctness of the model and its potential to provide more accurate and better engineered maintenance plansfindingsthe solution procedure provided excellent results by enhancing the overall arrangement of the tasks resulting in higher availability rates and a substantial decrease in total maintenance costs in terms of situational awareness it provides the user with the flexibility to better manage resource constraints while still achieving optimal resultsoriginalityvaluethis is an innovative research providing a stateoftheart mathematical model and an algorithm for efficiently solving a task allocation and packing problem by incorporating components’ due flight time failure probability task relationships smart allocation of common preparation tasks operational profile and resource limitations
over the years many factors have been visible that lead to unimpressive performance in biology one of the factors includes ineffectiveness and inefficiency of biology teachers which results from the difficulties faced with some biology concepts the study investigated perceived difficulty levels of basic concepts in biology by teachers in senior schools in ogbomosho oyo state the study adopted a descriptive research of the survey type the population for this study comprised all biology teachers in ogbomosho a simple random sampling technique was employed to select 262 respondents the research instrument for this study was the biology teachers’ perceptions questionnaire on difficult concepts btpqdc three research questions were raised in the study the research questions were analyzed using frequency count percentage and crosstabulation the results showed that biology teachers perceived the majority of the basic concepts as not difficult the concepts include the conservation of energy topics such as laws of thermodynamics food chain water cycle micro and macro elements and holozoic nutrition also biology teachers perceived some reasons as the cause of difficulty in the basic concepts of biology they include misconception of the concepts insufficient subjectmatter knowledge of the concepts abstractness of the concepts and complexity of the topic 
autonomous vehicles have been actively investigated over the past few decades several recent works show the potential of autonomous vehicles in urban environments with impressive experimental results however these works note that autonomous vehicles are still occasionally inferior to expert drivers in complex scenarios furthermore they do not focus on the possibilities of autonomous driving transportation services in other areas beyond urban environments this paper presents the research results and lessons learned from autonomous driving transportation services in airfield crowded indoor and urban environments we discuss how we address several unique challenges in these diverse environments we also offer an overview of remaining challenges that have not received much attention but must be addressed this paper aims to share our unique experience to support researchers who are interested in exploring autonomous driving transportation services in various realworld environments
this article presents a 3d anthropometric vision system free of ionizing radiation for measuring the medial longitudinal arch mla and longitudinal arch angle laa of the foot essential for evaluating foot arch disorders that can affect a person’s lifestyle existing 2d techniques for determining mla based on anthropometric tests have limitations because they exclude important information about foot such as pronation dorsiflexion and rotation to overcome these limitations the proposed system uses stereo vision and a landmark detection algorithm to measure the 3d mla and laa considering the z axis perpendicular to the sagittal plane the proposed system is compared with a commercial lidar using a tridimensional grid ground truth obtaining a root mean square error rmse of 12° for the proposed system and an rmse of 48° for lidar the proposed system offers several advantages over traditional techniques including low error 3d measurement capabilities and free of ionizing radiation in addition the system’s application in human subjects showed a variability range between 3d and 2d measurements for mla which is consistent with previous research overall the proposed system has the potential to improve the evaluation and treatment of foot arch disorders thereby enhancing a person’s quality of life
the small‐molecule drug fty720 fingolimod is a synthetic sphingosine 1‐phosphate s1p analogue currently used to treat relapsing–remitting multiple sclerosis in both adults and children fty720 can cross the blood–brain barrier bbb and over time accumulate in lipid‐rich areas of the central nervous system cns by incorporating into phospholipid membranes fty720 has been shown to enhance cell membrane fluidity which can modulate the functions of glial cells and neuronal populations involved in regulating behaviour moreover direct modulation of s1p receptor‐mediated lipid signalling by fty720 can impact homeostatic cns physiology including neurotransmitter release probability the biophysical properties of synaptic membranes ion channel and transmembrane receptor kinetics and synaptic plasticity mechanisms the aim of this study was to investigate how chronic fty720 treatment alters the lipid composition of cns tissue in adolescent mice at a key stage of brain maturation we focused on the hippocampus a brain region known to be important for learning memory and the processing of sensory and emotional stimuli using mass spectrometry‐based lipidomics we discovered that fty720 increases the fatty acid chain length of hydroxy‐phosphatidylcholine pcoh lipids in the mouse hippocampus it also decreases pcoh monounsaturated fatty acids mufas and increases pcoh polyunsaturated fatty acids pufas a total of 99 lipid species were up‐regulated in the mouse hippocampus following 3 weeks of oral fty720 exposure whereas only 3 lipid species were down‐regulated fty720 also modulated anxiety‐like behaviours in young mice but did not affect spatial learning or memory formation our study presents a comprehensive overview of the lipid classes and lipid species that are altered in the hippocampus following chronic fty720 exposure and provides novel insight into cellular and molecular mechanisms that may underlie the therapeutic or adverse effects of fty720 in the central nervous system
cyp3a4mediated metabolic conversion of mitragynine to 7hydroxymitragynine 7oh has been demonstrated in human liver microsomes and in rodents pharmacokinetics pk of mitragynine and 7oh in humans is still limited we aimed to examine the pharmacokinetics of mitragynine and the formation of 7oh in healthy volunteers to elucidate involvement of cyp3a4 in 7oh formation inhibition by itraconazole was implemented two study periods with pk study of mitragynine alone in period 1 followed by period 2 including itraconazole pretreatment was conducted freshly prepared kratom tea consisting of 236 mg of mitragynine was given to participants in both study periods serial blood samplings were performed for 72 hours and analyzed using a validated lcms in multiple reaction monitoring mode the median cmax for mitragynine of 15912 ± 868 ngml was attained in 084 h while median cmax for 7oh of 1281 ± 339 ngml was observed at 177 h in period 1 cmax and auc 0inf of 7oh accounted for 9 and 20  respectively of those parameters for mitragynine the geometric mean ratio of auc072 for 7ohmitragynine metabolic ratio mr was 1325 ± 107 coadministration of itraconazole 200 mg per day orally for 4 days period 2 decreased 7oh exposure by 56 for cmax and 43 for auc072 after a single oral dose of kratom tea while the cmax of mitragynine increased by 15 folds without a significant change in tmax the geometric mean metabolic ratio was 330 ± 123 period 2 indicating the attenuation for the formation of 7oh by the pretreatment with itraconazole this suggested the cyp3a4mediated formation of 7oh from mitragynine in healthy volunteers this study provides the first evidence of metabolic conversion of mitragynine to 7oh in humans
automatic font generation is a challenging and timeconsuming task particularly in languages that consist of large amounts of characters with complicated structures typical componentwise font generation methods decompose the source character into components and search for them from the reference glyph set as candidate components these candidate components are then utilized to learn the local styles of the target glyph however these methods overlook that the same component at different locations may have different profiles when the candidate components locate differently from their corresponding components in the target glyph the style of a generated glyph will look inconsistent it is observed that for arbitrary components at two specific locations the deformation patterns are similar driven by this we present a locationaware componentdeformable font generation method specifically we search for candidate components and their corresponding deformative component pairs from the reference glyph set each deformative component pair can accurately depict how to deform the candidate component to the desired profile in the target glyph hence we introduce a locationdependent deformation module to perform component warping in this way we significantly improve the component deformation ability lastly we integrate deformed components into target glyphs while enforcing their styles to be consistent with the reference ones extensive experiments demonstrate that our method produces targetfont consistent glyphs and outperforms the stateoftheart on both seen and unseen fonts
concerning classical computational models able to express all the primitive recursive functions prf there are interesting results regarding limits on their algorithmic expressiveness or equivalently efficiency namely the ability to express algorithms with minimal computational cost by introducing the reversible programming model forest at our knowledge we provide a first study of analogous properties adapted to the context of reversible computational models that can represent all the functions in prf firstly we show that forest extends matos linear reversible computational model msrl the very extension being a guaranteed terminating iteration that can be halted by means of logical predicates the consequence is that forest is prf complete because msrl is secondly we show that forest is strictly algorithmically more expressive than msrl it can encode a reversible algorithm for the minimum between two integers in optimal time while msrl cannot
we construct perfect zeroknowledge probabilistically checkable proofs pzkpcps for every language in p this is the first construction of a pzkpcp for any language outside bpp furthermore unlike previous constructions of statistical zeroknowledge pcps our construction simultaneously achieves nonadaptivity and zero knowledge against arbitrary adaptive polynomialtime malicious verifiers our construction consists of a novel masked sumcheck pcp which uses the combinatorial nullstellen satz to obtain antisymmetric structure within the hypercube and randomness outside of it to prove zero knowledge we introduce the notion of locally simulatable encodings randomised encodings in which every local view of the encoding can be efficiently sampled given a local view of the message we show that the code arising from the sumcheck protocol the reed–muller code augmented with subcube sums admits a locally simulatable encoding this reduces the algebraic problem of simulating our masked sumcheck to a combinatorial property of antisymmetric functions
understanding the impact of artificial intelligence ai on education is vital for guiding teachers in developing educational tools ai in education aied comes not only with opportunities but mostly with challenges for both educators and learners finding the proper tools to integrate ai into the learning framework represents a test for current and future generations even if most students acknowledged ai as a valuable tool their interaction with ai in education seems more limited than expected they mainly concentrated on few tools with higher awareness this paper examines ai’s support for educational activities key drivers and tools for business education survey data collected from 254 learners were analysed using multivariate binary logistic regression two research questions were formulated to verify if ai supports educational activities and what ai tools support business educational activities results show learners appreciate ai for aiding teachers in administrative tasks personalising learning plans and saving time however learners are unfamiliar with most benefits of ai tools except computer vision edge computing and ai chatbots the paper highlights the need to increase the use of ai in education to make students more familiar with ai tools and capitalise on them in business education
patientderived cells pdc mouse xenografts are increasingly important tools in glioblastoma gbm research essential to investigate casespecific growth patterns and treatment responses despite the central role of xenograft models in the field few good simulation models are available to probe the dynamics of tumor growth and to support therapy design we therefore propose a new framework for the patientspecific simulation of gbm in the mouse brain unlike existing methods our simulations leverage a highresolution map of the mouse brain anatomy to yield patientspecific results that are in good agreement with experimental observations to facilitate the fitting of our model to histological data we use approximate bayesian computation because our model uses few parameters reflecting growth invasion and niche dependencies it is well suited for case comparisons and for probing treatment effects we demonstrate how our model can be used to simulate different treatment by perturbing the different model parameters we expect in silico replicates of mouse xenograft tumors can improve the assessment of therapeutic outcomes and boost the statistical power of preclinical gbm studies
abstract significance over the past decade machine learning ml algorithms have rapidly become much more widespread for numerous biomedical applications including the diagnosis and categorization of disease and injury aim here we seek to characterize the recent growth of ml techniques that use imaging data to classify burn wound severity and report on the accuracies of different approaches approach to this end we present a comprehensive literature review of preclinical and clinical studies using ml techniques to classify the severity of burn wounds results the majority of these reports used digital color photographs as input data to the classification algorithms but recently there has been an increasing prevalence of the use of ml approaches using input data from more advanced optical imaging modalities eg multispectral and hyperspectral imaging optical coherence tomography in addition to multimodal techniques the classification accuracy of the different methods is reported it typically ranges from ∼70 to 90 relative to the current gold standard of clinical judgment conclusions the field would benefit from systematic analysis of the effects of different input data modalities trainingtesting sets and ml classifiers on the reported accuracy despite this current limitation mlbased algorithms show significant promise for assisting in objectively classifying burn wound severity
active distribution network is integrated with a large number of renewable energy sources and flexible loads such as electric vehiclesevs the unstable output of distributed generation and the random access to different phases cause three phase unbalance in the distribution network which increase the energy loss and carbon emissions three phase unbalance reduction requires high real‐time dispatch and optimization of the power flow this paper attempts to reduce the energy loss and carbon emission by using flexible multi‐state switch fmss with optimal energy dispatch strategy a multi‐objective optimization model is established based on second‐order cone relaxation to reduce the three‐phase unbalance and energy loss evs regulation strategy is proposed by analyzing their charging flexibility a fast solution method is brought up based on second‐order cone model transformation the effectiveness and rapidity of the method is verified by multiple cases © 2024 institute of electrical engineer of japan and wiley periodicals llc
hub location is crucial for resilient and uninterrupted supply chain operations particularly during disruptions or unforeseen events in this paper we propose a resilience hub location framework for third party logistics 3pl companies with two key objectives optimizing demand flows and establishing a resilient network capable of withstanding sudden disruptions the study aims to identify the key criteria that contribute to the successful implementation of the resilient center the proposed structure utilizes a twophase decisionmaking methodology the first phase presents a new multicriteria decisionmaking mcdm approach called swaraedas method that evaluates and ranks potential locations based on resiliency criteria the second phase proposes an optimization model to determine the optimal hub location to illustrate the approach a realworld case study of a 3pl company in tehran is included due to the absence of precise demand data in the case study a novel clustering approach is proposed to estimate the demand flow each individual cluster can be considered as a distinct demand point and a clustering analysis involving 122 regions within tehran is conducted taking into account various factors such as population economic index accessibility to the internet and number of business units to enhance the resiliency of the network mobile distribution centers are also deployed these mobile centers not only provide flexibility but also serve as backup capabilities in the event of a disruption or failure at the fixed hub the proposed structure offers practical insights for 3pl companies seeking to implement a resilient network structure
in optical communication the transmitter encodes information into a set of light states defined by the modulation format selected to accommodate specific channel conditions and to remain sufficiently distinguishable at the output various receiver architectures have been designed to improve the demodulation performance ultimately limited by quantum theory in this work i introduce a new receiver based on a locally optimal greedy algorithm and apply it to pulse position modulation the receiver reduces the error probabilities of previously proposed strategies in all signal strength regimes and achieves results comparable with those obtained by numerical optimization of the detection process in contrast however it is conceptually simple and therefore can be scaled to arbitrarily high modulation orders for which numerical methods become intractable in the photonstarved regime characteristic of deep space optical communication the greedy receiver approaches the quantumoptimal helstrom bound on state discrimination error probability in the regime of fewphoton pulses the error reduction offered over the other methods grows up to an order of magnitude
abstract lee h and lee n 2023 an implicit quadraturefree modal discontinuous galerkin dg scheme for shallow water equations in lee jl lee h min bi chang ji cho gt yoon js and lee j eds multidisciplinary approaches to coastal and marine management journal of coastal research special issue no 116 pp 9195 charlotte north carolina issn 07490208 even though the discontinuous galerkin dg method has gained current status as an effective numerical tool for hyperbolic conservation laws euler equations shallow water equations etc its use is limited due to computational burden and algorithmic complexity as an alternative to the conventional and standard approaches based on nodal basis schemes an implicit modal discontinuous galerkin scheme was developed for shallow water equations the developed scheme employs a quadraturefree approach with the orthogonal basis functions on a triangular element and flux integrals for riemann solvers with an edge coordinate system it is believed to be simpler and more efficient compared to the conventional discrete quadrature method in addition the use of implicit algorithm made the code robust and allowed larger time steps the model was applied to some benchmark problems including channel contraction partial dambreak flow and curved channel flow and good agreements were observed
the blood‒brain barrier bbb acts as a hindrance to drug therapy reaching the brain with an increasing incidence of neurovascular diseases and brain cancer metastases there is a need for an ideal in vitro model to develop novel methodologies for enhancing drug delivery to the brain here we established a multicellular human brain spheroid model that mimics the bbb both architecturally and functionally within the spheroids endothelial cells and pericytes localized to the periphery while neurons astrocytes and microglia were distributed throughout ultrasoundtargeted microbubble cavitation utmc is a novel noninvasive technology for enhancing endothelial drug permeability we utilized our threedimensional 3d model to study the feasibility and mechanisms regulating utmcinduced hyperpermeability utmc caused a significant increase in the penetration of 10 kda texas red dextran trd into the spheroids 100 µm beyond the bbb without compromising cell viability this hyperpermeability was dependent on utmcinduced calcium ca2 influx and endothelial nitric oxide synthase enos activation our 3d brain spheroid model with its intact and functional bbb offers a valuable platform for studying the bioeffects of utmc including effects occurring spatially distant from the endothelial barrier
cardiovascular disease cvd is a leading motive of loss of life international with threat factors together with circle of relatives history and way of life habits playing a chief position early identity of highdanger people is critical to save you and control cvd present day strategies for cvd threat evaluation rely frequently on traditional hazard elements and are restrained in their accuracy this challenge proposes to develop a system getting to know ml version to discover excessivehazard individuals for cvd using a mixture of affected person data including demographics scientific records and way of life behavior the model will use ml techniques to analyze patterns and discover hidden relationships among risk elements and cvd effects this could identify highthreat patients with high accuracy and assist healthcare companies make informed choices for personalized prevention and remedy plans with the aid of utilizing ml this challenge aims to enhance cvd hazard prediction and reduce the load of cvd by enabling early prognosis and intervention
purposethe purpose of this paper is to explore and disseminate knowledge about quantuminspired computing technologys potential to solve complex challenges faced by the operational agility capability in industry 40 manufacturing and logistics operationsdesignmethodologyapproacha multicase study approach is used to determine the impact of quantuminspired computing technology in manufacturing and logistics processes from the supplier perspective a literature review provides the basis for a framework to identify a set of flexibility and agility operational capabilities enabled by industry 40 information and digital technologies the use cases are analyzed in depth first individually and then jointlyfindingsstudy results suggest that quantuminspired computing technology has the potential to harness and boost companies operational flexibility to enhance operational agility in manufacturing and logistics operations management particularly in the industry 40 context an exploratory model is proposed to explain the relationships between quantuminspired computing technology and the deployment of operational agility capabilitiesoriginalityvaluethis is study explores the use of quantuminspired computing technology in industry 40 operations management and contributes to understanding its potential to enable operational agility capability in manufacturing and logistics operations
soluble solids content ssc is one of the main quality indicators of apples and it is important to improve the precision of online ssc detection of whole apple fruit therefore the spectral preprocessing method of spectraltospectral ratio ss as well as multiple characteristic wavelength member model fusion mcmf and characteristic wavelength and noncharacteristic wavelength member model fusion cncmf methods were proposed for improving the detection performance of apple whole fruit ssc by diffuse reflection dr diffuse transmission dt and full transmission ft spectra the modeling analysis showed that the ss partial least squares regression models for all three mode spectra had high prediction performance after competitive adaptive reweighted sampling characteristic wavelength screening the prediction performance of all three model spectra was improved the particle swarm optimization–extreme learning machine models of mcmf and cncmf had the most significant enhancement effect and could make all three mode spectra have high prediction performance dr dt and ft spectra all had some prediction ability for apple whole fruit ssc with ft spectra having the strongest prediction ability followed by dt spectra this study is of great significance and value for improving the accuracy of the online detection model of apple whole fruit ssc
abstract the article discusses the application of rpa robotic process automation technology in financial management and demonstrates its remarkable effectiveness in improving efficiency and reducing costs the article establishes an automated accounting method for enterprise finance using rpa technology through statistical analysis classification processing and feature extraction of enterprise financial data the consistency check method is used to verify the fit and reasonableness of the method practically applied to the financial accounting process of enterprise a the method dramatically reduces the time for reconciling bonds and funds from 60 minutes to 2 seconds significantly improving the accounting efficiency in addition the method ensured that the accuracy of statement filling reached 100 and reduced the labor cost of enterprise accounting from 1758 million yuan to 688 million yuan a reduction of 256 times the article emphasizes that the established financial accounting method increases the enterprise’s profitability and lays a solid foundation for the development of financial accounting automation
vision transformers vits have achieved stateoftheart accuracy on various computer vision tasks however their high computational complexity prevents them from being applied to many realworld applications weight and token pruning methods are wellknown in reducing vit model complexity however naively combining and integrating both the methods results in irregular computation patterns leading to accuracy drops and difficulties in hardware acceleration this limits the net complexity reduction offered by integrating such pruning methods to address the above challenges we propose a comprehensive algorithmhardware codesign for accelerating vit on fpga through simultaneous pruning  combining static weight pruning and dynamic token pruning for algorithm design we systematically combine a hardwareaware structured blockpruning method for pruning model parameters and a dynamic token pruning method for removing unimportant token vectors moreover we design a novel training algorithm to reduce the accuracy drop due to such simultaneous pruning for hardware design we develop a novel hardware accelerator for executing the pruned model the proposed hardware design employs multilevel parallelism with a loadbalancing strategy to efficiently deal with the irregular computation pattern presented by the two pruning approaches moreover we develop an efficient hardware mechanism for executing the onthefly token pruning we apply our codesign approach to the widely used deitsmall model we implement the proposed accelerator on a stateoftheart fpga the evaluation results show that the proposed algorithm reduces computation complexity by up to 34× with ≈ 3 accuracy drop and a model compression ratio of up to 16× compared with stateoftheart implementation on cpu gpu and fpga our codesign on fpga achieves an average latency reduction of 128× 32× and 07 – 21× respectively
introduction renewal of extinguished responses is associated with higher activity in specific extinctionrelevant brain regions ie hippocampus hc inferior frontal gyrus ifg and ventromedial pfc vmpfc hc is involved in processing of context information while ifg and vmpfc use such context information for selecting and deciding among competing response options however it is as yet unknown to what extent trials with changed versus unchanged outcome or extinction trials that evoke renewal ie extinction context differs from acquisition and test context aba trials and trials that do not ie same context in all phases aaa trials are represented differentially in extinctionrelevant brain regions methods in this study we applied representational similarity analysis rsa to determine differences in neural representations of these trial types and their relationship to extinction error rates and renewal level results overall individuals with renewal ren and those without noren did not differ significantly in their discrimination levels between aba and aaa extinction trials with the exception of right posterior hc where ren exhibited more pronounced contextrelated discrimination in addition higher dissimilarity of representations in bilateral posterior hc as well as in several ifg regions during extinction learning was linked to lower aba renewal rates both ren and noren benefitted from prediction error feedback from aba extinction errors for context and outcomerelated discrimination of trials in ifg vmpfc and hc but only the noren group also benefitted from error feedback from aaa extinction errors discussion thus while in both groups the presence of a novel context supported formation of distinct representations only in noren the expectancy violation of the surprising change of outcome alone had a similar effect in addition only in noren contextrelated discrimination was linked to error feedback in vmpfc in summary the findings show that context and outcomerelated discrimination of trials in hc vmpfc and ifg is linked to extinction learning errors regardless of renewal propensity and at the same time point towards differential context processing strategies in ren and noren moreover better discrimination of contextrelated trials during extinction learning promotes less renewal during extinction recall suggesting that renewal may be related to suboptimal contextrelated trial discrimination
although it is very easy to calculate the 1st moment and 2nd moment values of the geometric distribution with the methods available in existing books and other articles it is quite difficult to calculate moment values larger than the 3rd order because in order to find these moment values many higher order derivatives of the geometric series and convergence properties of the series are needed the aim of this article is to find new formulas for characteristic function of the geometric random variable with parameter p in terms of the apostol–bernoulli polynomials and numbers and the stirling numbers this characteristic function characterizes the geometric distribution using the eulers identity we give relations among this characteristic function the apostol–bernoulli polynomials and numbers and also trigonometric functions including cosine and sine  a relations between the characteristic function and the moment generating function is also given by using these relations we derive new moments formulas in terms of the apostol–bernoulli polynomials and numbers moreover we give some applications of our new formulas
the intent of this work was to investigate the feasibility of developing machine learning models for calculating values of airplane configuration design variables when provided timeseries missioninformed performance data shallow artificial neural networks were developed trained and tested using data pertaining to the blended wing body bwb class of aerospace vehicles configuration design parameters were varied using a latinhypercube sampling scheme these data were used by a parametricbased bwb configuration generator to create unique bwbs performance for each configuration was obtained via a performance estimation tool training and testing of neural networks was conducted using a kfold crossvalidation scheme a random forest approach was used to determine the values of predicted configuration design variables when evaluating neural network accuracy across a blended wing body vehicle survey the results demonstrated the viability of leveraging neural networks in missiondependent inverse design of blended wing bodies in particular feedforward shallow neural network architectures yielded significantly better predictive accuracy than cascadeforward architectures furthermore for both architectures increasing the number of neurons in the hidden layer increased the prediction accuracy of configuration design variables by at least 80
desmids are usually abundant in shallow peatland pools in these localities water temperature is closely linked to seasonal fluctuations in air temperature so with increasing temperature extremes in temperate ecosystems these microalgae are exposed to conditions of hightemperature stress we investigated whether the shape size and growth rates of micrasterias thomasiana a frequently occurring species are associated with varying temperatures in cultures and natural populations the research was based on parallel analysis of clonal populations in temperature levels from 13 to 33 °c as well as cells from natural populations collected during the season the effects of high temperature on morphological plasticity and fluctuating asymmetry in the shape of cellular parts were investigated by the landmarkbased geometric morphometrics the results showed that variation among individuals and fluctuating asymmetry between the lateral lobes of micrasterias cells increased at 29 °c and in natural samples taken in july and october in parallel the size of semicells growing at temperatures above 25 °c decreased compared to those grown at lower temperatures however the temperature effects on shape and size were not directly related to the growth rates the overall bilateral asymmetry between semicell halves did not change in relation to varying temperatures in general the results showed that morphological variation in natural populations of m thomasiana reflected seasonal cycles and corresponded to plasticity associated with temperature changes in clonal cultures it might therefore be possible to use these phenotypic markers as indicators of thermal stress in natural populations inhabiting shallow pools in peatlands
uncovering the public discourse on hydrogen energy is essential for understanding public behaviour and the evolving nature of conversations over time and across different regions this paper presents a comprehensive analysis of a large multilingual dataset pertaining to hydrogen energy collected from twitter spanning a decade 2013–2022 using selected keywords the analysis aims to explore various aspects including the temporal and spatial dimensions of the discourse factors influencing twitter engagement user engagement patterns and the interpretation of conversations through hashtags and ngrams by delving into these aspects this study offers valuable insights into the dynamics of public discourse surrounding hydrogen energy and the perceptions of social media users
programmable smart contracts facilitate the application of blockchain to many areas of industry eg supply chains energy and healthcare the offchain execution model for blockchain smart contracts is a promising approach to achieve scalability for industrial applications which executes each smart contract in one of the smallscale consensus groups however previous work based on such a model cannot efficiently handle crossgroup contract transactions that involve multiple crossgroup invocations between smart contracts because of the heavyweight state synchronization and repeated executions in this article we propose interactive consensusbased offchain execution icoe a lightweight groupconsensusbased offchain execution model for crossgroup contract transactions without the need of state synchronization and duplicate executions and also prove its correctness the key novelty of our icoe is to let the invoked smart contract only executed in its residing group and only return the execution result to the invoking smart contract while leveraging the twophase commit protocol to guarantee consistent commits across multiple groups within the same invoking smart contract transaction we implement icoe and the extensive experimental results show that our icoe achieves a 14× improvement of throughput on average compared with the stateoftheart model
peridynamics pd as a nonlocal theory is wellsuited for solving problems with discontinuities such as cracks however the nonlocal effect of peridynamics makes it computationally expensive for dynamic fracture problems in largescale engineering applications as an alternative this study proposes a multitimestep mts coupling model of pd and classical continuum mechanics ccm based on the arlequin framework peridynamics is applied to the fracture domain of the structure while continuum mechanics is applied to the rest of the structure the mts method enables the peridynamic model to be solved at a small time step and the continuum mechanical model is solved at a larger time step consequently higher computational efficiency is achieved for the fracture domain of the structure while ensuring computational accuracy and this coupling method can be easily applied to largescale engineering fracture problems
background this study aimed to examine the impact of interaction between body posture and the quality of movement patterns on injury frequencies in amateur athletes methods the study sample consisted of 89 young amateur athletes movement pattern quality was assessed by the functional movement screen fms test and body posture in the frontal plane was assessed by the moire method for the parameters shoulder slope angle lower scapula protrusion difference and pelvic tilt angle injury data were collected through completion of the injury history questionnaire for the past 12 months results using cluster analysis participants were allocated into a either category with good bpg body posture or poor bpp and using fms cutoff points 14 either a category of good movement pattern quality mpg or poor mpp twoway anova was performed and the bonferroni posthoc test revealed a reduction in injuries among participants from the mpgbpg group compared to the other three groups p  005 however no interaction between factors was revealed no statistically significant differences were observed among the remaining three groups in the case of injury prevalence p  005 conclusions a combination of proper body posture and highquality movement patterns is associated with a lower frequency of injuries without direct interaction between chosen factors which suggests that they impact injury risk independently practicing suitable bp and ensuring highquality mps should be regarded as a strategy in injury prevention
the mutual information characterizes correlations between spatially separated regions of a system yet in experiments we often measure dynamical correlations which involve probing operators that are also separated in time here we introduce a spacetime generalization of mutual information which by construction satisfies several natural properties of the mutual information and at the same time characterizes correlations across subsystems that are separated in time in particular this quantity that we call the spacetime mutual information  bounds all dynamical correlations we construct this quantity based on the idea of the quantum hypothesis testing as a byproduct our definition provides a transparent interpretation in terms of an experimentally accessible setup we draw connections with other notions in quantum information theory such as quantum channel discrimination finally we study the behavior of the spacetime mutual information in several settings and contrast its longtime behavior in manybody localizing and thermalizing systems
this study delves into the elements of patent rights and dispute resolution strategies in cases of patent infringement the background of the research stems from the necessity to understand the legal framework surrounding patent protection and the mechanisms available for resolving disputes in the event of infringement the primary objective is to identify the crucial components of patent rights and the various methods employed to address disputes arising from alleged patent violations employing a normative legal research approach with both conceptual and statutory analyses this study aims to provide a comprehensive overview of the subject matter the findings underscore the significance of novelty as a fundamental criterion for patent acceptance as defined by law number 13 of 2016 concerning patents novelty characterized by the absence of prior public disclosure usage or inclusion in existing knowledge standards is essential for patent registration the study reveals that patent disputes concerning novelty can be addressed through diverse legal avenues including patent removal suits trial proceedings mediation patent reexamination outofcourt settlements and appellate processes furthermore alternative mechanisms such as arbitration offer additional pathways for resolving disputes the implications of this research highlight the importance of adhering to novelty requirements during patent registration and understanding the array of dispute resolution mechanisms available to safeguard intellectual property rights
utilizing smallformat oblique camera systems to capture simultaneous nadir and oblique photographs from unmanned aerial vehicles uavs is a common practice in modern photogrammetry oblique photographs provide enhanced geometric insights into building side views terrain morphology and vegetation thereby enriching interpretation and classification however the design of camera rig parameters and their precise mathematical modeling for smallformat oblique camera systems in multiview processing is essential to ensure accurate representation of the physical camera geometry and results this study investigates the camera rig parameters of the ‘3dmv3’ smallformat oblique camera system focusing specifically on the relative relationship between nadir and oblique cameras within two prominent photogrammetric software pix4dmapper and agisoft metashape the research concludes that optimal parameterization involves fully constrained relative translation parameters t x  t y  t z  rel for the four oblique cameras while setting approximate initial estimates as free constrained for relative rotation parameters r x  r y  r z  rel  this approach aligns with the physical geometry of the camera system and yields a precise camera model as confirmed through bundle block adjustment bba computations pix4dmapper yields horizontal and vertical root mean square errors rmse of 0023 m and 0019 m respectively while agisoft metashape results in rmse of 0018 m and 0046 m these rmse values considering the ground sample distance and ground control point accuracy reflect the robustness of the approach the insights from this research offer valuable guidance for industries facilitating informed decisions regarding the selection of appropriate software and parameters for smallformat oblique camera systems mounted on uavs thus ensuring consistency between theoretical models and realworld applications
the agricultural sector is currently faced with the urgent problem of reducing the incidence of crop diseases to maintain food security and environmentally responsible farming practices among these the brassica black rot disease poses a considerable risk to crops in the brassicaceae family which includes several different cabbage and broccoli varieties this study aims to revolutionize the diagnosis and classification of brassica black rot disease at varied severity levels by tapping into the capabilities of cuttingedge deep learning dl techniques more specifically vit models this study is based on a rigorously curated dataset that contains 5000 highresolution photos of brassica leaves these images have been categorized into six different severity levels ranging from class 0 to class 5 and the researchers have annotated each of these images the accuracy and reliability of the annotations were ensured under the oversight of seasoned plant pathologists to aid in the evaluation and development of the model a training set a validation set and a test set were meticulously collected from the dataset as our research developed the vitbased model a sophisticated dl architecture became the key focus of our attention excellent accuracy was achieved in classifying the varying degrees of disease severity by meticulous training and tuning the model using the dataset containing cases of brassica black rot by incorporating cohens upsilon  a new performance indicator we gained a more nuanced understanding of the models propensity to correctly classify diseases that was possible because of the additional context provided regarding the models propensity to correctly classify diseases our research showed that the vitbased model achieved a fantastic level of classification accuracy with an average rate of 9617 in addition the significant cohens upsilon value of 094 demonstrated that the model was able to distinguish between different degrees of severity of brassica black rot with a high level of concordance
an organization is autonomous if it has the right or power of selfgovernment selfgovernment implies that autonomous organizations cannot rely on outside parties for monitoring or contract enforcement we present a model of the optimal power allocation in such an organization the organization commits to a governance structure that allocates managerial power to agents members with power “managers” can punish members without power “subordinates” this power is however limited by the subordinates’ right to exit the organization there are three main results first the goals of autonomy decentralization and efficiency conflict with one another we call this result the organizational trilemma  second there is a paradox of power  an agent can be made worse off by their own power third optimal governance structures in autonomous organizations are centralized and populist  the powerful party shows restraint in early periods only to abuse their power in later periods
abstract background in ethiopia both incidence and mortality of cervical cancer are relatively high screening services which were implemented during the past few years are currently being expanded the world health organization recommends patients with a positive via visual inspection with acetic acid result should immediately receive treatment followed by rescreening after 1 year as precancerous lesions can reoccur or become residential despite treatment materials and methods screening logbooks dating between 2017 and 2020 were retrospectively reviewed in 14 health facilities of addis ababa and oromia region data for 741 women with a viapositive result were extracted and those women were asked to participate in a questionnairebased phone interview to gain insights about adherence to treatment and followup data were analyzed using descriptive methods and then fitted into 2 generalized linear models to test variables for an influence on adherence to follow up results around 13 800 women had received a via screening of which approximately 820 59 were via positive while over 90 of women with a positive screen received treatment only about half of the treated patients returned for a followup examination after treatment 31 women had a viapositive rescreen we found that educational status age over 40 noincorrect followup appointment health facilityrelated barriers and use of reminders are important drivers of adherence to follow up conclusion our results revealed that adherence to treatment after via positive screening is relatively high whereas adherence to follow up recommendations still needs improvement reminders like appointment cards and phone calls can effectively reduce the loss of followup
efficient modeling of soft robots for design optimization and control is an active area of research when representing soft robots as an assembly of rods and rigid bodies strainbased parametrization has proved to be able to reduce the required number of degrees of freedom drastically however this reduction ability strongly depends on the choice of strain basis employed to describe the system in this letter we proposed a new implicit strain parametrization and we showed its use to represent the systems configuration manifold with no more degrees of freedom than the number of actuators coupled with standard strain bases this parametrization is applied to the dynamic simulation and control of soft robots employing a handful of generalized coordinates drastically simplifying the control design problem the approach is validated against a highorder model and exploited for shape configuration space and tippose taskspace control of a complex tendondriven soft robot
existing studies often lack a systematic solution for an unmanned aerial vehicles uav inspection system which hinders their widespread application in crack detection to enhance its substantial practicality this study proposes a formal and systematic framework for uav inspection systems specifically designed for automatic crack detection and pavement distress evaluation the framework integrates uav data acquisition deeplearningbased crack identification and road damage assessment in a comprehensive and orderly manner firstly a flight control strategy is presented and road crack data are collected using dji mini 2 uav imagery establishing highquality uav crack image datasets with ground truth information secondly a validation and comparison study is conducted to enhance the automatic crack detection capability and provide an appropriate deployment scheme for uav inspection systems this study develops automatic crack detection models based on mainstream deep learning algorithms namely fasterrcnn yolov5s yolov7tiny and yolov8s in urban road scenarios the results demonstrate that the fasterrcnn algorithm achieves the highest accuracy and is suitable for the online data collection of uav and offline inspection at work stations meanwhile the yolo models while slightly lower in accuracy are the fastest algorithms and are suitable for the lightweight deployment of uav with online collection and realtime inspection quantitative measurement methods for road cracks are presented to assess road damage which will enhance the application of uav inspection systems and provide factual evidence for the maintenance decisions made by road authorities
the aim of this paper is to develop a theoretical vocabulary that allows us to better understand not only the visible effects of digitalization on organizations but also the invisible work that arises in and around the digitalized organization to prepare maintain and repair its key features drawing on feminist science and technology studies and their classic concept of invisible work we challenge some of the dominant spatial root metaphor assumptions in current research and develop an alternative metaphoric of digital work and the digitalized organization we develop the theoretical concept of invisible digiwork as a corollary to the already established concept of digital work and flesh out three types of work that we conceptualize as invisible connecting compensating and cleaning work this analytical framework captures aspects of work that tend be out of sight and devalued in dominant accounts as such it represents a theoretical alternative to imageries of digital spaces that lead to an overemphasis on the affordances of new digital technologies establishing an alternative ground for interrogating work at margins which is essential to the constitution of digitalized organizations theorizing invisible digiwork is in line with recent calls in organization studies to go beyond the visual and investigate the indirect and less visible implications of digitalization
emerging concepts from scientific deep machine learning such as physicsinformed neural networks pinns enable a datadriven approach for the study of complex kinetic problems we present an extended framework that combines the advantages of pinns with the detailed consideration of experimental parameter variations for the simulation and prediction of chemical reaction kinetics the approach is based on truncated taylor series expansions for the underlying fundamental equations whereby the external variations can be interpreted as perturbations of the kinetic parameters accordingly our method allows for an efficient consideration of experimental parameter settings and their influence on the concentration profiles and reaction kinetics a particular advantage of our approach in addition to the consideration of univariate and multivariate parameter variations is the robust modelbased exploration of the parameter space to determine optimal reaction conditions in combination with advanced reaction insights the benefits of this concept are demonstrated for higherorder chemical reactions including catalytic and oscillatory systems in combination with small amounts of training data all predicted values show a high level of accuracy demonstrating the broad applicability and flexibility of our approach
the stable operation of the process industrial system which is integrated with various complex equipment is the premise of production which requires the condition monitoring and diagnosis of the system recently the continuous development of deep learning dl has promoted the research of intelligent diagnosis in process industry systems and the sensor system layout has provided sufficient data foundation for this task however these dldriven approaches have had some shortcomings 1 the output signals of heterogeneous sensing systems existing in process industry systems are often highdimensional coupled and 2 the fault diagnosis model built from pure data lacks systematic process knowledge resulting in inaccurate fitting to solve these problems a graph feature fusiondriven fault diagnosis of complex process industry systems is proposed in this paper first according to the system’s prior knowledge and data characteristics the original multisource heterogeneous data are divided into two categories on this basis the two kinds of data are converted to physical space graphs psg and process knowledge graphs pkg respectively according to the physical space layout and reaction mechanism of the system second the node features and system spatial features of the subgraphs are extracted by the graph convolutional neural network at the same time and the fault representation information of the subgraph is mined finally the attention mechanism is used to fuse the learned subgraph features getting the globalgraph representation for fault diagnosis two publicly available process chemistry datasets validate the effectiveness of the proposed method
a spatial survival analysis was performed to identify some of the factors that influence the survival of patients with covid19 in the states of guerrero méxico and chihuahua the data that we analyzed correspond to the period from 28 february 2020 to 24 november 2021 a cox proportional hazards frailty model and a cox proportional hazards model were fitted for both models the estimation of the parameters was carried out using the bayesian approach according to the dic waic and lpml criteria the spatial model was better the analysis showed that the spatial effect influences the survival times of patients with covid19 the spatial survival analysis also revealed that age gender and the presence of comorbidities which vary between states and the development of pneumonia increase the risk of death from covid19
the rapid advancement in selfsupervised representation learning has highlighted its potential to leverage unlabeled data for learning rich visual representations however the existing techniques particularly those employing different augmentations of the same image often rely on a limited set of simple transformations that cannot fully capture variations in the real world this constrains the diversity and quality of samples which leads to suboptimal representations in this paper we introduce a framework that enriches the selfsupervised learning ssl paradigm by utilizing generative models to produce semantically consistent image augmentations by directly conditioning generative models on a source image our method enables the generation of diverse augmentations while maintaining the semantics of the source image thus offering a richer set of data for ssl our extensive experimental results on various jointembedding ssl techniques demonstrate that our framework significantly enhances the quality of learned visual representations by up to 10 top1 accuracy in downstream tasks this research demonstrates that incorporating generative models into the jointembedding ssl workflow opens new avenues for exploring the potential of synthetic data this development paves the way for more robust and versatile representation learning techniques
abstract discontinuity in natural language is characterized by the linear disruption of a continuous string of linguistic expressions forming a constituent while dependency relations in dependency grammar dg can capture discontinuity well phrasestructurebased approaches such as phrase structure grammar psg face difficulty in accommodating discontinuity categorial grammar cg has correspondences with psg although it can handle discontinuity if equipped with wrapping operations given the existing literature on discontinuity in natural language it appears that constituency relations of psg dependency relations of dg and functorargument relations of cg are distinct and independent here we argue for a unified representation achieved by taking into account fundamental representational principles of psg dg and cg for simplicity we show this by considering an embedded clause from wan spoken in ivory coast as an illustrative case the paper then attempts to explain based on available empirical pieces of evidence the plausible connections between the unified representation and the neurocognitive representation of continuity and discontinuity in natural language
as an optical sensor lidar cannot effectively detect transparent obstacles in the environment such as glass parapets and glass doors in buildings in this article a glass detection and point cloud reconstruction method relying only on lidar is proposed which can improve the detection success rate for glass and achieve the reconstruction of glass the algorithm achieves the detection of glass point clouds by reflection intensity and local structure feature of point clouds experiments prove that the algorithm can well identify the glass position and reconstruct the point cloud of glass it lays the foundation for mobile robots to carry out simultaneous localization and mapping slam and path planning tasks smoothly in the indoor environment with glass
in highspeed optical communication systems intersymbol interferences isis and nonlinear distortions can occur due to the bandwidth limitations and nonlinear responses of optical and electrical components at the transceiver to address these issues this paper proposes the use of bahl cocke jelinek and raviv bcjr algorithm based on nonlinear channel models such as the volterra nonlinear filter and lookup table lut additionally we introduce an ultralow complexity bcjr algorithm referred to as statestruncated bcjr based on reducedsize lut rlutstbcjr to mitigate the impact of bandwidth limitations and nonlinear distortions the rlutstbcjr uses a pruned lut as the channel model emulator to assist in the calculation of the transition metric only the states with probabilities greater than a set threshold are retained in the decoding process resulting in significantly reduced computational complexity the experimental results confirm the improvement of both ber and normalized general mutual information ngmi performance using the proposed rlutstbcjr algorithm compared to the conventional linear bcjr equalization with this approach we successfully transmit a 100gbaud pam8 signal in both backtoback and 1km fiber cases under the ber threshold of 24e2 assuming 20 overhead softdecision forward error correction
this study explores the application of the fully convolutional network fcn algorithm to the field of meteorology specifically for the shortterm nowcasting of severe convective weather events such as hail convective wind gust cg thunderstorms and shortterm heavy rain sthr in gansu the training data come from the european center for mediumrange weather forecasts ecmwf and realtime ground observations the performance of the proposed fcn model based on 2017 to 2021 training datasets demonstrated a high prediction accuracy with an overall error rate of 166 furthermore the model exhibited an error rate of 186 across both severe and nonsevere weather conditions when tested against the 2022 dataset operational deployment in 2023 yielded an average critical success index csi of 243 a probability of detection pod of 626 and a false alarm ratio far of 712 for these convective events it is noteworthy that the predicting performance for sthr was particularly effective with the highest pod and csi as well as the lowest far cg and hail predictions had comparable csi and far scores although the pod for cg surpassed that for hail the fcn model’s optimal performances in terms of hail prediction occurred at the 4th 8th and 10th forecast hours while for cg the 6th hour was most accurate and for sthr the 2nd and 4th hours were most effective these findings underscore the fcn model’s ideal suitability for shortterm forecasting of severe convective weather presenting extensive prospects for the automation of meteorological operations in the future
classroom management involves all endeavors aimed at establishing an efficient and pleasant teaching and learning environment that can inspire learners to learn effectively based on their abilities it encompasses a range of intentional activities conducted by lecturers with the goal of establishing and sustaining optimal conditions for the teaching and learning process this research investigates practical management strategies for english as a foreign language efl speaking class semistructured interviews and active classroom observation were employed to collect the data three efl lecturers from the english department of universitas darussalam gontor who provided english speaking classes participated in this study the objective was to look into the lecturers viewpoints and management of speaking classes and to examine the strategies they employed and the challenges they experienced the findings show that the lecturers had implemented a variety of strategies including setting discipline physical environment encouraging more usage of english offering engaging topics speaking correction techniques and building excitement for content the results also revealed that the lecturers encountered difficulties to manage their teaching challenges such as improper pronunciation learners low selfesteem and shortage of lexical resources were among the lecturers’ obstacles 
the aus oryza sativa l varietal group comprises of aus boro ashina and rayada seasonal andor field ecotypes and exhibits unique stress tolerance traits making it valuable for rice breeding despite its importance the agromorphological diversity and genetic control of yield traits in aus rice remain poorly understood to address this knowledge gap we investigated the genetic structure of 181 aus accessions using 399115 snp markers and evaluated them for 11 morphoagronomic traits through genomewide association studies gwas we aimed to identify key loci controlling yield and plant architectural traits our population genetic analysis unveiled six subpopulations with strong geographical patterns subpopulationspecific differences were observed in most phenotypic traits principal component analysis pca of agronomic traits showed that principal component 1 pc1 was primarily associated with panicle traits plant height and heading date while pc2 and pc3 were linked to primary grain yield traits gwas using pc1 identified ossac1 on chromosome 7 as a significant gene influencing multiple agronomic traits pc2based gwas highlighted the importance of osglt1 and ospup4 big grain 3 in determining grain yield haplotype analysis of these genes in the 3000 rice genome panel revealed distinct genetic variations in aus rice in summary this study offers valuable insights into the genetic structure and phenotypic diversity of aus rice accessions we have identified significant loci associated with essential agronomic traits with glt1 pup4 and sac1 genes emerging as key players in yield determination supplementary information the online version contains supplementary material available at 101186s12284024007004
abstract background the objective of this study was to develop and validate a machine learning ml model for predict inhospital mortality among critically ill patients with congestive heart failure chf combined with chronic kidney disease ckd methods after employing least absolute shrinkage and selection operator regression for feature selection six distinct methodologies were employed in the construction of the model the selection of the optimal model was based on the area under the curve auc furthermore the interpretation of the chosen model was facilitated through the utilization of shapley additive explanation shap values and the local interpretable modelagnostic explanations lime algorithm results this study collected data and enrolled 5041 patients on chf combined with ckd from 2008 to 2019 utilizing the medical information mart for intensive care unit after selection 22 of the 47 variables collected postintensive care unit admission were identified as mortalityassociated and subsequently utilized in the development of ml models among the six models generated the extreme gradient boosting xgboost model demonstrated the highest auc at 0837 notably the shap values highlighted the sequential organ failure assessment score age simplified acute physiology score ii and urine output as the four most influential variables in the xgboost model in addition the lime algorithm explains the individualized predictions conclusions in conclusion our study accomplished the successful development and validation of ml models for predicting inhospital mortality in critically ill patients with chf combined with ckd notably the xgboost model emerged as the most efficacious among all the ml models employed
many organizations nowadays combine profits with a social mission this paper reveals a new hidden benefit of the mission its role in facilitating the emergence of efficiency wages we show that in a standard giftexchange principals highly underestimate agents’ reciprocity and thereby offer wages that are much lower than the profitmaximizing level this bias has a high social cost if principals had correct beliefs and thus offered the profitmaximizing wage efficiency would increase by 86 percent however the presence of a social mission in the form of a positive externality generated by the agent’s effort by increasing principals trust acts as a debiasing mechanism and thereby increases efficiency by 50 percent these results contribute to our understanding of behavior in missionoriented organizations to the debate about the relevance of reciprocity in the workplace and open new questions about belief formation in prosocial contexts
"abstract blueschistfacies rocks are scarce within the variscan orogen two main occurrences are known in the armorican massif nw france at île de groix and boisdecéné another glaucophane occurrence was discovered in 1988 but went unnoticed it is located on île dumet an uninhabited island off the coast of southern brittany in the estuary of the river vilaine orthogneiss occurs on the sw half of the island the original granitoid magma had intruded mica schists on the ne half where numerous 1 to 10 m long boudins of mafic rocks occur these lenses are typically retrogressed into plagioclasebearing amphibolite but a few contain remnants of glaucophanebearing eclogite which also occurs as numerous loose blocks along the ne coast of the island suggesting that the bestpreserved eclogites lie in situ offshore in that direction the glaucophane eclogites contain garnet omphacite quartz amphibole clinozoisiteepidote minor phengite paragonite rutile and rare apatite prograde metamorphic evolution is indicated by garnet crystals zoned from mnrich cores to mgricher rims typically core→rim alm44→58 prp1→12 grs33→29 sps22→1 and amphibole grains with glaucophane nuclei and ca–naamphibole overgrowths that show sharp transitions supporting evolution through a solvus as predicted by the thermodynamic modelling modelling of the p–t conditions using the p–t pseudosection technique indicates a peak of metamorphism at about 620 ∘c and 16 kbar the retrograde evolution of the metabasites is evidenced by the late formation of albite titanite and ferroactinolite the surrounding mica schists composed of quartz garnet phengite paragonite and chlorite were also largely retrogressed during exhumation the orthogneiss of the sw part of the island does not show clear evidence of highpressure metamorphism since the magmatic feldspars are still preserved similarly to the orthogneiss of les sables rouges on the island of groix île dumet and the western part of the vilaine estuary represent a blueschistfacies equivalent to île de groix brittany and boisdecéné vendée on the mainland all three occurrences occupy the centres of wide synforms whose concentric units are from rims to core ie from base to top a a hight migmatitic basement b cambroordovician metasediments and acid metavolcanites “porphyroids” and c blueschistfacies mica schists and metabasites serpentinites and minor orthogneisses derived from a prevariscan oceanic accretionary prism there are about 10 similar occurrences within the iberoarmorican arc forming a discontinuous highpressure belt but most of them have remained unnoticed due to a high degree of retrogression
"
multimodal large language models mllms are experiencing rapid growth yielding a plethora of noteworthy contributions in recent months the prevailing trend involves adopting datadriven methodologies wherein diverse instructionfollowing datasets are collected however a prevailing challenge persists in these approaches specifically in relation to the limited visual perception ability as cliplike encoders employed for extracting visual information from inputs though these encoders are pretrained on billions of imagetext pairs they still grapple with the information loss dilemma given that textual captions only partially capture the contents depicted in images to address this limitation this paper proposes to improve the visual perception ability of mllms through a mixtureofexperts knowledge enhancement mechanism specifically we introduce a novel method that incorporates multitask encoders and visual tools into the existing mllms training and inference pipeline aiming to provide a more comprehensive and accurate summarization of visual inputs extensive experiments have evaluated its effectiveness of advancing mllms showcasing improved visual perception achieved through the integration of visual experts
with the continuous emergence and development of 3d sensors in recent years it has become increasingly convenient to collect point cloud data for 3d object detection tasks such as the field of autonomous driving but when using these existing methods there are two problems that cannot be ignored 1 the bird’s eye view bev is a widely used method in 3d objective detection however the bev usually compresses dimensions by combined height dimension and channels which makes the process of feature extraction in feature fusion more difficult 2 light detection and ranging lidar has a much larger effective scanning depth which causes the sector to become sparse in deep space and the uneven distribution of point cloud data this results in few features in the distribution of neighboring points around the key points of interest the following is the solution proposed in this paper 1 this paper proposes multiscale feature fusion composed of feature maps at different levels made of deep layer aggregation dla and a feature fusion module for the bev 2 a point completion network is used to improve the prediction results by completing the feature points inside the candidate boxes in the second stage thereby strengthening their position features supervised contrastive learning is applied to enhance the segmentation results improving the discrimination capability between the foreground and background experiments show these new additions can achieve improvements of 27 24 and 25 respectively on kitti easy moderate and hard tasks further ablation experiments show that each addition has promising improvement over the baseline
background plant growthpromoting rhizobacteria pgpr have a specific symbiotic relationship with plants and rhizosphere soil the purpose of this study was to evaluate the effects of pgpr on blueberry plant growth rhizospheric soil nutrients and the microbial community methods in this study nine pgpr strains belonging to the genera pseudomonas and buttiauxella were selected and added into the soil in which the blueberry cuttings were planted all the physiological indexes of the cuttings and all rhizospheric soil element contents were determined on day 6 after the quartic root irrigation experiments were completed the microbial diversity in the soil was determined using highthroughput amplicon sequencing technology the correlations between phosphorus solubilization the auxin production of pgpr strains and the physiological indexes of blueberry plants and the correlation between rhizospheric microbial diversity and soil element contents were determined using the pearson’s correlation kendall’s tau correlation and spearman’s rank correlation analysis methods results the branch number leaf number chlorophyllcontentand plant height of the treated blueberry group were significantly higher than those of the control group the rhizospheric soil element contents also increased after pgpr root irrigation the rhizospheric microbial community structure changed significantly under the pgpr of root irrigation the dominant phyla except actinomycetota in the soil samples had the greatest correlation with phosphorus solubilization and the auxin production of pgpr strains the branch number leaf number and chlorophyllcontent had a positive correlation with the phosphorus solubilization and auxin production of pgpr strains and soil element contents in conclusion plant growth could be promoted by the root irrigation of pgpr to improve rhizospheric soil nutrients and the microenvironment with modification of the rhizospheric soil microbial community discussion plant growth could be promoted by the root irrigation of pgpr to improve rhizospheric soil nutrients and the microenvironment with the modification of the rhizospheric soil microbial community these data may help us to better understand the positive effects of pgpr on blueberry growth and the rhizosphere soil microenvironment as well as provide a research basis for the subsequent development of a rhizospherepromoting microbial fertilizer
a emph2partition of a graph g is a function fvgrightarrow 01 a 2partition f of a graph g is a emphlocallybalanced with an open neighborhood if for every vin vg leftvert vert uin ngvcolonfu0vert  vert uin ngvcolonfu1vert rightvertleq 1 a 2partition fprime of a graph g is a emphlocallybalanced with a closed neighborhood if for every vin vg leftvert vert uin ngvcolonfprimeu0vert  vert uin ngvcolonfprimeu1vert rightvertleq 1 in this paper we prove that the problem of the existence of locallybalanced 2partition with an open closed neighborhood is npcomplete for some restricted classes of graphs in particular we show that the problem of deciding if a given graph has a locallybalanced 2partition with an open neighborhood is npcomplete for biregular bipartite graphs and even bipartite graphs with maximum degree 4 and the problem of deciding if a given graph has a locallybalanced 2partition with a closed neighborhood is npcomplete even for subcubic bipartite graphs and odd graphs with maximum degree 3 last results prove a conjecture of balikyan and kamalian
one of the primary challenges in applying deep learning approaches to medical imaging is the limited availability of data due to various factors these factors include concerns about data privacy and the requirement for expert radiologists to perform the timeconsuming and laborintensive task of labeling data particularly for tasks such as segmentation consequently there is a critical need to develop novel approaches for fewshot learning tasks in this domain in this work we propose a novel cnntransformer fusion scheme to segment multiclasses pneumonia infection from limited ctscans data in total there are three main contributions i cnntransformer encoders fusion which allows to extract and fuse richer features in the encoding phase which contains local global and longrange dependencies features ii multibranches skip connection mbsc is proposed to extract and fuse richer features from the encoder features then integrate them into the decoder layers where mbsc blocks extract higherlevel features related to the finer details of different infection types and iii a multiclasses boundary aware crossentropy mbace loss function is proposed to deal with fuzzy boundaries enhance the separability between classes and give more attention to the minority classes the performance of the proposed approach is evaluated using two evaluation scenarios and compared with different baseline and stateoftheart segmentation architectures for multiclasses covid19 segmentation the obtained results show that our approach outperforms the comparison methods in both groundglass opacity ggo and consolidation segmentation on the other hand our approach shows consistent performance when the training data is reduced to half which proves the efficiency of our approach in fewshot learning in contrast the performance of the comparison methods drops in this scenario moreover our approach is able to deal with imbalanced data classes these advantages prove the effectiveness and efficiency of the proposed embtrattunet approach in a pandemic scenario where time is critical to save patient lives
abstract in this paper random optimization problems are investigated some sufficient conditions ensuring the existence of random solutions to random optimization problems are proposed
plant root imaging is crucial for progress in various domains such as plant breeding and crop optimization traditionally root tomography involves invasive methods that disrupt plant systems and yield nonreproducible results as a result noninvasive techniques particularly electrical tomography have gained significant attention despite the advantages these techniques have limitations in terms of radiation efficiency and directivity due to suboptimal antenna design this paper presents a comprehensive simulation on antenna design optimization focusing on dimensions spacing and integration of advanced algorithms a micropatch transducer antenna was engineered for an existing insilico plant root setup operating within a 3–5 mhz frequency range the optimized dimensions of the antenna are 10932 mm × 14067 mm × 255 mm and it resonates effectively within a frequency range of 31–568 mhz using scalar minimization techniques patch transducers were interconnected into an antenna array with an optimized 3 mm spacing utilizing multiobjective optimization algorithm based on sperm fertilization procedure and shuffled frog leaping algorithm optimal frequencies were obtained at 398979688 hz and 398995183 hz respectively validated using cadfeko software the proposed antenna design demonstrated distinctive voltage distribution superior directivity of 924 dbi gain of 915 dbi and 986 radiation efficiency when compared to the existing siliconbased root tomography antenna setups
3d articulated objects are inherently challenging for manipulation due to the varied geometries and intricate functionalities associated with articulated objects pointlevel affordance which predicts the perpoint actionable score and thus proposes the best point to interact with has demonstrated excellent performance and generalization capabilities in articulated object manipulation however a significant challenge remains while previous works use perfect point cloud generated in simulation the models cannot directly apply to the noisy point cloud in the realworld to tackle this challenge we leverage the property of realworld scanned point cloud that the point cloud becomes less noisy when the camera is closer to the object therefore we propose a novel coarsetofine affordance learning pipeline to mitigate the effect of point cloud noise in two stages in the first stage we learn the affordance on the noisy far point cloud which includes the whole object to propose the approximated place to manipulate then we move the camera in front of the approximated place scan a less noisy point cloud containing precise local geometries for manipulation and learn affordance on such point cloud to propose finegrained final actions the proposed method is thoroughly evaluated both using largescale simulated noisy point clouds mimicking realworld scans and in the real world scenarios with superiority over existing methods demonstrating the effectiveness in tackling the noisy realworld point cloud problem
an applicationoriented landbased hyperspectral target detection framework based on 3d–2d cnn and transfer learning is proposed integrating multiple spectral similarity evaluation indicators to extract spectral features extracting spatial and spectral information from hyperspectral images using a 3d–2d network model target detection based on hyperspectral images refers to the integrated use of spatial information and spectral information to accomplish the task of localization and identification of targets there are two main methods for hyperspectral target detection supervised and unsupervised methods supervision method refers to the use of spectral differences between the target to be tested and the surrounding background to identify the target when the target spectrum is known in ideal situations supervised object detection algorithms perform better than unsupervised algorithms however the current supervised object detection algorithms mainly have two problems firstly the impact of uncertainty in the ground object spectrum and secondly the universality of the algorithm is poor a hyperspectral target detection framework based on 3d–2d cnn and transfer learning was proposed to solve the problems of traditional supervised methods this method first extracts multiscale spectral information and then preprocesses hyperspectral images using multiple spectral similarity measures this method not only extracts spectral features in advance but also eliminates the influence of complex environments to a certain extent the preprocessed feature maps are used as input for 3d–2d cnn to deeply learn the features of the target and then the softmax method is used to output and obtain the detection results the framework draws on the ideas of integrated learning and transfer learning solves the spectral uncertainty problem with the combined similarity measure and depth feature extraction network and solves the problem of poor robustness of traditional algorithms by model migration and parameter sharing the area under the roc curve of the proposed method has been increased to over 099 in experiments on both publicly available remote sensing hyperspectral images and measured landbased hyperspectral images the availability and stability of the proposed method have been demonstrated through experiments a feasible approach has been provided for the development and application of specific target detection technology in hyperspectral images under different backgrounds in the future
progressive cerebral infarction pci is a common complication in patients with ischemic stroke that leads to poor prognosis blood pressure bp can indicate post‐stroke hemodynamic changes which play a key role in the development of pci the authors aim to investigate the association between bp‐derived hemodynamic parameters and pci clinical data and bp recordings were collected from 80 patients with cerebral infarction including 40 patients with pci and 40 patients with non‐progressive cerebral infarction npci hemodynamic parameters were calculated from the bp recordings of the first 7 days after admission including systolic and diastolic bp mean arterial pressure and pulse pressure pp with the mean values of each group calculated and compared between daytime and nighttime and between different days hemodynamic parameters and circadian bp rhythm patterns were compared between pci and npci groups using t‐test or non‐parametric equivalent for continuous variables chi‐squared test or fishers exact test for categorical variables cox proportional hazards regression analysis and binary logistic regression analysis for potential risk factors in pci and npci groups significant decrease of daytime systolic bp appeared on the second and sixth days respectively systolic bp and fibrinogen at admission daytime systolic bp of the first day nighttime systolic bp of the third day pp and the ratio of abnormal bp circadian rhythms were all higher in the pci group pci and npci groups were significantly different in bp circadian rhythm pattern pci is associated with higher systolic bp pp and more abnormal circadian rhythms of bp
dimensionality reduction methods such as principal component analysis pca and factor analysis are central to many problems in data science there are however serious and wellunderstood challenges to finding robust low dimensional approximations for data with significant heteroskedastic noise this paper introduces a relaxed version of minimum trace factor analysis mtfa a convex optimization method with roots dating back to the work of ledermann in 1940 this relaxation is particularly effective at not overfitting to heteroskedastic perturbations and addresses the commonly cited heywood cases in factor analysis and the recently identifiedcurse of illconditioningfor existing spectral methods we provide theoretical guarantees on the accuracy of the resulting low rank subspace and the convergence rate of the proposed algorithm to compute that matrix we develop a number of interesting connections to existing methods including heteropca lasso and softimpute to fill an important gap in the already large literature on low rank matrix estimation numerical experiments benchmark our results against several recent proposals for dealing with heteroskedastic noise
we construct a new partially linear regression based on a reparametrized generalized gamma distribution with two systematic components that can be easily interpreted its parameters are estimated by penalized maximum likelihood for different parameter settings sample sizes and censoring percentages some simulations are performed to examine the accuracy of the maximum likelihood estimators and the empirical distribution of the residuals compared with the standard normal distribution the methodology is applied to breast cancer data in the city of joão pessoa in the state of paraíba in brazil
recognizing smoke in satellite imagery is a critical approach in an internet of things iot system for monitoring forest fires however the task remains challenging due to false alarms of smokelike occurrences caused by complex land cover types and missing detections caused by the diversity of fire smoke some reasons are that existing methods overlook attention granularity neglect alllayerbased fusion of lowlevel features with highlevel semantic information and fail to address interferences arising from fusing different kinds of features to solve these issues this article presents an attention pyramid network with bidirectional multilevel multigranularity feature aggregation and gated fusion for smoke recognition first to guide the model sequentially extract multigranularity smoke attention clues for complementary smoke perception we design an attentionguided feature pyramid module by concatenating residual blocks and attention pyramid blocks second to leverage both lowlevel finegrained and highlevel semantic features in all network layers we design a bidirectional feature aggregation module using multilevel multigranularity feature blocks finally to selectively integrate the features with different resolutions and semantic levels to effectively achieve feature complementarity and avoid feature mutual interference we design a gated feature fusion module using gated feature fusion blocks the experimental results demonstrate that our model achieves an accuracy of 9833 on the ustcsmokers data set additionally on the eustcsmokers data set our model achieves a detection rate of 9492 a false alarm rate of 300 and an f1score of 09553 these results surpass the performance of existing satelliteimagerybased smoke recognition methods
abstract premise plant trait data are essential for quantifying biodiversity and function across earth but these data are challenging to acquire for large studies diverse strategies are needed including the liberation of heritage data locked within specialist literature such as floras and taxonomic monographs here we report floratraiter a novel approach using rule‐based natural language processing nlp to parse computable trait data from biodiversity literature methods floratraiter was implemented through collaborative work between programmers and botanical experts and customized for both online floras and scanned literature we report a strategy spanning optical character recognition recognition of taxa iterative building of traits and establishing linkages among all of these as well as curational tools and code for turning these results into standard morphological matrices results over 95 of treatment content was successfully parsed for traits with 1 error data for more than 700 taxa are reported including a demonstration of common downstream uses conclusions we identify strategies applications tips and challenges that we hope will facilitate future similar efforts to produce large open‐source trait data sets for broad community reuse largely automated tools like floratraiter will be an important addition to the toolkit for assembling trait data at scale
the effects of technologysupported behavior change interventions for reducing sodium intake on health outcomes in adults are inconclusive effective intervention characteristics associated with sodium reduction have yet to be identified a systematic review and metaanalysis were conducted searching randomized controlled trials rcts published between january 2000 and april 2023 across 5 databases prospero crd42022357905 metaanalyses using randomeffects models were performed on 24h urinary sodium 24huna systolic blood pressure sbp and diastolic blood pressure dbp subgroup analysis and metaregression of 24huna were performed to identify effective intervention characteristics eighteen rcts involving 3505 participants 515 female mean age 516 years were included technologysupported behavior change interventions for reducing sodium intake significantly reduced 24huna mean difference md −039 gm24 h 95 confidence interval ci −050 to −027 i2  24 sbp md −267 mmhg 95 ci −406 to −129 i2  40 and dbp md −139 mmhg 95 ci −231 to −048 i2  31 compared to control conditions interventions delivered more frequently ≤weekly were associated with a significantly larger effect size in 24huna reduction compared to less frequent interventions weekly other intervention characteristics such as intervention delivery via instant messaging and participantfamily dyad involvement were associated with larger albeit nonsignificant effect sizes in 24huna reduction when compared to other subgroups technologysupported behavior change interventions aimed at reducing sodium intake were effective in reducing 24huna sbp and dbp at postintervention effective intervention characteristics identified in this review should be considered to develop sodium intake reduction interventions and tested in future trials particularly for its longterm effects
simple summary accurately estimating body weight is crucial for managing water buffalo health and optimizing feeding strategies this study explored the effectiveness of machine learning models in predicting body weight based on body measurements principal component analysis was employed to reduce the dimensionality of the data and identify the most relevant features subsequently gradient boosting and random forest algorithms were utilized to predict body weight using the reduced data set the gradient boosting algorithm demonstrated superior performance compared to the random forest algorithm these findings suggest that the combination of principal component analysis and gradient boosting offers a reliable and effective method for estimating body weight in water buffaloes this approach holds promise for improving animal production and health management practices future research could focus on enhancing the applicability and generalizability of these models to diverse water buffalo populations across various geographical regions abstract this study aims to use advanced machine learning techniques supported by principal component analysis pca to estimate body weight bw in buffalos raised in southeastern mexico and compare their performance the first stage of the current study consists of body measurements and the process of determining the most informative variables using pca a dimension reduction method this process reduces the data size by eliminating the complex structure of the model and provides a faster and more effective learning process as a second stage two separate prediction models were developed with gradient boosting and random forest algorithms using the principal components obtained from the data set reduced by pca the performances of both models were compared using r2 rmse and mae metrics and showed that the gradient boosting model achieved a better prediction performance with a higher r2 value and lower error rates than the random forest model in conclusion pcasupported modeling applications can provide more reliable results and the gradient boosting algorithm is superior to random forest in this context the current study demonstrates the potential use of machine learning approaches in estimating body weight in water buffalos and will support sustainable animal husbandry by contributing to decision making processes in the field of animal science
this paper presents a unique control system to regulate power exchanges and load bus voltage in a networked microgrid nmg system comprising ac and dc microgrids during the islanding of a microgrid in this nmg system load voltage and power balance can get disturbed a control system and associated converter and inverter control methods are presented to rectify these issues an efficient model predictive control mpc method which gives a tracking error of 50 lower than a conventional proportionalintegral pi controller is used to control multiple inverters in the nmg system simulation studies are conducted to test the nmg in islanding and load change scenarios with the help of these studies it is verified that the mpccontrolled inverters can provide better tracking accuracy in achieving desired power flows in the nmg system
encryption is vital for data security converting information into an unreadable format to ensure privacy in online communication and sensitive sectors advanced encryption balances innovation and security in userfriendly applications image encryption employs techniques to protect image data from unauthorized access during transmission or storage particularly crucial for safeguarding sensitive images in various applications the goal is to prevent unauthorized access and ensure the safety of associated information in this paper i present a study on previous research related to my investigation which focuses on encryption in general and image encryption in particular the paper also discusses the methods used particularly those closely related to my work involving either sha256 md5 or a combination of both the review will look at the many strategies and techniques employed as well as how precisely the task was completed by applying a set of parameters in comparison to earlier studies
permanent magnet synchronous motors pmsms are consciously used as traction motors in electric vehicles evs and hybrid electric vehicles hevs the rotor position innerrotor outerrotor and topology of pmsms significantly impact their torque profile efficiency and demagnetization characteristics this article focuses on designing an outerrotor permanentmagnetassisted synchronous reluctance motor pmasynrm under traction motor requirements and investigating its electromagnetic performance using the finite element method fem this studys primary challenge is achieving optimal machine performance considering high maximum torque and the risk of demagnetization at low levels the design derived from analytical calculations was subjected to finite element method fem analyses these analyses investigated motor performance in terms of efficiency the ability to generate torque at different drive currents and the risk of demagnetization as a result of the study the proposed pmasynrm design was obtained that provides an adequate demagnetization performance even under 300 loading has a maximum torque exceeding 35 nm and achieves an efficiency of 90 at rated conditions
ephemeral fast radio bursts frbs must be powered by some of the most energetic processes in the universe that makes them highly interesting in their own right and as precise probes for estimating cosmological parameters this field thus poses a unique challenge frbs must be detected promptly and immediately localised and studied based only on that single millisecondduration flash the problem is that the burst occurrence is highly unpredictable and that their distance strongly suppresses their brightness since the discovery of frbs in singledish archival data in 2007 detection software has evolved tremendously pipelines now detect bursts in real time within a matter of seconds operate on interferometers buffer hightime and frequency resolution data and issue realtime alerts to other observatories for rapid multiwavelength followup in this paper we review the components that comprise a frb search software pipeline we discuss the proven techniques that were adopted from pulsar searches we highlight newer more efficient techniques for detecting frbs and we conclude by discussing the proposed novel future methodologies that may power the search for frbs in the era of big data astronomy
bibased ybmg2bi198 zintl compounds represent promising thermoelectric materials precise composition and appropriate doping are of great importance for this complex semiconductor here the influence of zn substitution for mg on the microstructure and thermoelectric properties of ptype ybmg185−xznxbi198 x  0 005 008 013 023 was investigated polycrystalline samples were prepared using induction melting and densified with spark plasma sintering xray diffraction confirmed that the major phase of the samples possesses the trigonal caal2si2type crystal structure and semeds indicated the presence of minor secondary phases the electrical conductivity increases and the lattice thermal conductivity decreases with more zn doping in ybmg185−xznxbi198 whereas the seebeck coefficient has a large reduction the band gap decreases with increasing zn concentration and leads to bipolar conduction resulting in an increase in the thermal conductivity at higher temperatures figure of merit zt values of 051 and 049 were found for the samples with x  0 and 005 at 773 k respectively the maximum amount of zn doping is suggested to be less than x  01
background low hand grip strength hgs is associated with the risk of cardiovascular diseases but the association between hgs and myocardial infarctionangina pectoris miap is unclear furthermore there have been no studies examining the associations of miap with anthropometric indices absolute hgs indices and relative hgs indices calculated by dividing absolute hgs values by body mass index bmi waist circumference wc waisttoheight ratio whtr or weight values therefore the objective of this study was to examine the associations of miap with absolute and relative hgs combined with several anthropometric indices methods in this largescale crosssectional study a total of 12963 subjects from the national health and nutrition examination survey were included odds ratios and 95 confidence intervals for the associations of miap with anthropometric indices absolute hgs indices and relative hgs indices were computed from binary logistic regression models we built 3 models a crude model a model that was adjusted for age model 1 and a model that was adjusted for other relevant covariates model 2 results for men the average age was 6155 ± 016 years in the miap group and 6649 ± 061 years in the nonmiap group for women the average age was 6199 ± 014 years in the miap group and 7048 ± 061 years in the nonmiap group for both sexes the miap group had lower diastolic blood pressure shorter stature greater wc and a greater whtr than did the nonmiap group and women tended to have greater systolic blood pressure weight and bmi than in men hgs was strongly associated with the risk of miap in the korean population in men relative hgs indices combined with wc and the whtr had greater associations with miap than did the anthropometric indices and absolute hgs indices however in women anthropometric indices including weight bmi wc and whtr were more strongly associated with miap than were absolute and relative hgs indices unlike in men when comparing absolute and relative hgs indices in women relative hgs indices combined with bmi and weight was more strongly related to miap than was absolute hgs indices conclusions miap might be better identified by relative hgs than absolute hgs in both sexes the overall magnitudes of the associations of miap with absolute and relative hgs are greater in men than in women
cardiovascular disease is known as the number one cause of death worldwide ultrasound imaging is widely used to assess and predict the risk of cardiovascular disease extensive research on robotic ultrasound systems has addressed the problems of overreliance on physician experience in the application of ultrasound unequal distribution of medical resources and mutual exposure of patients and physicians to infections the human carotid artery requires more standardization safety and intelligence in applying robotic ultrasound systems due to its sensitive location and inconspicuous skin features this article introduces an innovative robotassisted system for autonomous ultrasound scanning of the carotid artery the system uses a structured twostage approach encompassing prescanning and scanning to enhance the precision and efficiency of the procedure it uses advanced image servo control for intelligent scanning path determination significantly improving the quality and speed of scans moreover the system’s capability to alternate between transverse and longitudinal scanning modes allows it to accommodate the unique anatomical differences in individual patients experimental results on real human subjects showed a success rate of 8611 for scanning the desired carotid artery ultrasound images which is promising for clinical applications
concerning energy waste and rational use this paper studies the optimal scheduling of dayahead energy supply and the community’s demand with a combined cooling heating and power cchp system in summer from the perspective of bilateral costs and renewable energy use this paper examines the impact of energy storage systems integrated into cogeneration systems the gurobi solver is used to optimize the residential community’s supply and demand sides of the traditional cchp system tcchp and the cchp system with energy storage cchpess under insufficient solar power subsequently two optimal arrangements for energy consumption on the user side under these systems are suggested in the optimization model energy storage is added to the tcchp system on the energy supply side on the user side the energy use scheme is optimized considering the user’s comfort the innovation point of this study is that the optimization of comprehensive energy in the park involves both supply and demand the impact of increasing energy storage is discussed on the energy supply side and the impact of optimization of the energy use plan on costs is discussed on the user side
accurately understanding the distribution and changing trends of center pivot irrigation cpi farmland in the mu us region and exploring the impact of cpi farmland construction on sandy land vegetation growth hold significant importance for local sustainable development by using landsat images to extract cpi farmland information and applying buffer zone analysis to explore the impact of cpi farmland construction on the surrounding vegetation growth the results revealed the following key findings 1 the number and area of cpi farmland units showed a continuous growth trend from 2008 to 2022 spatially etoke front banner was the focal point of the cpi farmland unit construction gradually expanding outward in terms of scale smallscale cpi farmland units 0–02 km2 dominated while largescale cpi farmland units 04 km2 were primarily distributed in yulin city mu us 2 the growth rate of cpi farmland units in yulin city gradually slowed down while that in ordos city mu us continued to exhibit a high growth trend affected by waterresource pressure and policies cpi farmland units in ordos city may continue to increase in the future while they may stop growing or even show a downward trend in yulin city 3 cpi farmland mainly came from the conversion of cultivated land but over time more and more grassland was reclaimed as cpi farmland the absence of cover planting after crop harvesting and the lack of shelterbelt construction may exacerbate land desertification in the region 4 within the typical region cpi farmland unit construction promoted vegetation growth within the cpi units and the 500 m buffer zone but had an inhibitory effect on vegetation growth within the 500–3000 m buffer zone and no significant effect on vegetation growth within the 3000–5000 m buffer zone 5 the decrease in groundwater reserves caused by cpi farmland unit construction was the primary reason for inhibiting the vegetation growth within the 500–3000 m buffer zone of cpi farmland units in the mu us region this study can provide a scientific basis for the sustainable development of cpi farmland in semiarid areas
to better measure the fault tolerance and reliability of a network yuan et al introduced the noninclusive ggoodneighbour diagnosability of a network the ggoodneighbour faulty set is a special faulty set which satisfies that every faultfree vertex has at least g faultfree neighbours the noninclusive ggoodneighbour diagnosability requires every pair of ggoodneighbour faulty sets is noninclusive denoted as  tngg  tngg in this paper we study the noninclusive ggoodneighbour diagnosability of hypercubes and obtain  tngqnn1g2g12g1  tngqnn−1−g2g12g−1 for  gin 12  g∈12 under the pmc model  tn1qn3n5  tn1qn3n−5  tn2qn8n21  tn2qn8n−21 under the mm model
most of the recent works on posthoc examplebased explainable ai xai methods revolves around employing counterfactual explanations to provide justification of the predictions made by ai systems counterfactuals show what changes to the inputfeatures change the output decision however a lesserknown specialcase of the counterfacual is the semifactual which provide explanations about what changes to the inputfeatures do not change the output decision semifactuals are potentially as useful as counterfactuals but have received little attention in the xai literature my doctoral research aims to establish a comprehensive framework for the use of semifactuals in xai by developing novel methods for their computation supported by user tests
the goal of this thesis is to address knowledge graph completion tasks using neurosymbolic methods neurosymbolic methods allow the joint utilization of symbolic information defined as metarules in ontologies and knowledge graph embedding methods that represent entities and relations of the graph in a lowdimensional vector space this approach has the potential to improve the resolution of knowledge graph completion tasks in terms of reliability interpretability dataefficiency and robustness
impairment of autophagic–lysosomal pathways is increasingly being implicated in parkinsons disease pd gba1 mutations cause the lysosomal storage disorder gaucher disease gd and are the commonest known genetic risk factor for pd gba1 mutations have been shown to cause autophagic–lysosomal impairment defective autophagic degradation of unwanted cellular constituents is associated with several pathologies including loss of normal protein homeostasis particularly of αsynuclein and innate immune dysfunction the latter is observed both peripherally and centrally in pd and gd here we will discuss the mechanistic links between autophagy and immune dysregulation and the possible role of these pathologies in communication between the gut and brain in these disorders recent work in a fly model of neuronopathic gd ngd revealed intestinal autophagic defects leading to gastrointestinal dysfunction and immune activation rapamycin treatment partially reversed the autophagic block and reduced immune activity in association with increased survival and improved locomotor performance alterations in the gut microbiome are a critical driver of neuroinflammation and studies have revealed that eradication of the microbiome in ngd fly and mouse models of pd ameliorate brain inflammation following these observations lysosomal–autophagic pathways innate immune signalling and microbiome dysbiosis are discussed as potential therapeutic targets in pd and gd this article is part of a discussion meeting issue ‘understanding the endolysosomal network in neurodegeneration’
the integration of machine learning and artificial intelligence mlai into fifthgeneration 5g networks has made evident the limitations of network intelligence with everincreasing strenuous requirements for current and nextgeneration devices this transition to ubiquitous intelligence demands high connectivity synchronicity and endtoend communication between users and network operators and will pave the way towards full network automation without human intervention intentbased networking is a key factor in the reduction of human actions roles and responsibilities while shifting towards novel extraction and interpretation of automated network management this paper presents the development of a custom large language model llm for 5g and nextgeneration intentbased networking and provides insights into future llm developments and integrations to realize endtoend intentbased networking for fully automated network intelligence
the increasing prevalence of smart devices spurs the development of emerging indoor localization technologies for supporting diverse personalized applications at home given marked drawbacks of popular chirp signalbased approaches we aim at developing a novel devicefree localization system via the continuous wave of the inaudible frequency to achieve this goal solutions are developed for finegrained analyses able to precisely locate moving human traces in the roomscale environment in particular a smart speaker is controlled to emit continuous waves at inaudible 20khz with a colocated microphone array to record their doppler reflections for localization we first develop solutions to remove potential noises and then propose a novel idea by slicing signals into a set of narrowband signals each of which is likely to include at most one body segment’s reflection different from previous studies which take original signals themselves as the baseband our solutions employ the doppler frequency of a narrowband signal to estimate the velocity first and apply it to get the accurate baseband frequency which permits a precise phase measurement after iq ie inphase and quadrature decomposition a signal model is then developed able to formulate the phase with body segment’s velocity range and angle we next develop novel solutions to estimate the motion state in each narrowband signal cluster the motion states for different body segments corresponding to the same person and locate the moving traces while mitigating multipath effects our system is implemented with commodity devices in room environments for performance evaluation the experimental results exhibit that our system can conduct effective localization for up to three persons in a room with the average errors of 749cm for a single person with 2406cm for two persons with 5115cm for three persons
summary goal accurate prediction of operating room or time is critical for effective utilization of resources optimal staffing and reduced costs currently electronic health record ehr systems aid or scheduling by predicting or time for a specific surgeon and operation on many occasions the predicted or time is subject to manipulation by surgeons during scheduling we aimed to address the use of the ehr for or scheduling and the impact of manipulations on or time accuracy methods between april and august 2022 a pilot study was performed in our tertiary center where surgeons in multiple surgical specialties were encouraged toward nonmanipulation for predicted or time during scheduling the or time accuracy within 5 months before trial group 1 and within the trial period group 2 were compared accurate cases were defined as cases with total length wheelsin to wheelsout within ±30 min or ±20 of the scheduled duration if the scheduled time is ≥ or 150 min respectively the study included single and multiple current procedural terminology code procedures while procedures involving multiple surgical specialties combo cases were excluded principal findings the study included a total of 8821 operations 4243 group 1 and 4578 group 2 p  001 the percentage of manipulation dropped from 198 group 1 to 76 group 2 p  001 while scheduling accuracy rose from 417 group 1 to 479 group 2 p  0001 with a significant reduction of underscheduling percentage 387 vs 317 p  0001 and without a significant difference in the percentage of overscheduled cases 15 vs 17 p  22 inaccurate or hours were reduced by 18 during the trial period 2383 hr vs 1954 hr practical applications the utilization of ehr systems for predicting or time and reducing manipulation by surgeons helps improve or scheduling accuracy and utilization of or resources
this article studies the problem of computing a minimum zero forcing set zfs in undirected graphs and presents new approaches to reducing the size of the minimum zfs via edge augmentation the minimum zfs problem has numerous applications for instance it relates to the minimum leader selection problem for the strong structural controllability of networks defined over graphs computing a minimum zfs is an nphard problem in general we show that the greedy heuristic for the zfs computation though it typically performs well could give arbitrarily bad solutions for some graphs we provide a lineartime algorithm to compute a minimum zfs in trees and a complete characterization of the minimum zfs in the clique chain graphs we also present a gametheoretic solution for general graphs by formalizing the minimum zfs problem as a potential game in addition we consider the effect of edge augmentation on the size of the zfs adding edges could improve network robustness however it could increase the size of the zfs we show that adding a set of carefully selected missing edges to a graph may actually reduce the size of the minimum zfs finally we numerically evaluate our results on random graphs
proximity and feature similarity are two important determinants of perceptual grouping in vision when viewing visual scenes conveying both grouping options simultaneously people most usually detect proximity groups faster than similarity groups this article demonstrates that perceptual judgments of grouping orientation guided by either proximity or contrast similarity are indicative of a sequential organization of grouping operations in the visual pathway which lends a temporal processing advantage to proximity grouping experiment 1 invoking the doublefactorial paradigm latent cognitive architecture for perceptual grouping is also investigated in a task with redundant signals experiment 2 reaction time data from this task is assessed in terms of the race model inequality workload capacity analysis and interaction contrasts of means and survivor functions again empirical benchmarks indicate serial processing of proximity groups and similarity groups with a selfterminating stopping rule for processing a subset of participants exhibit atypical performance metrics hinting at possible individual differences in configural visual processing
triboelectric nanogenerators tengs utilize the synergetic effect of triboelectrification and electrostatic induction to guide electrons through an external circuit enabling low‐frequency mechanical and biomechanical energy harvesting and self‐powered sensing integrating 2d material with a high specific surface area into flexible ferroelectric polymers such as polyvinylidene difluoride pvdf has proven to be an efficient strategy to improve the performance of teng devices scalable fabrication of graphene‐integrated pvdf nanocomposite fiber using thermal drawing process is demonstrated for the first time in this study the open‐circuit voltage and short‐circuit current show 141 times and 148 times improvement with the integration of 5 graphene in the pvdf fibers respectively the teng fabric shows a maximum power output of 3214 µw at a matching load of 7 mω and a power density of 5357 mw m−2 the fibers exhibit excellent stability in harsh environmental conditions such as alkaline medium highlow temperature multi‐washing cycle and long‐time usage
this paper characterizes the sensitivity of a time domain mems accelerometer the sensitivity is defined by the increment in the measured time interval per gravitational acceleration two sensitivities exist and they can be enhanced by decreasing the amplitude and frequency the sensitivity with minor nonlinearity is chosen to evaluate the time domain sensor the experimental results of the developed accelerometer demonstrate that the sensitivities span from −6891 μsg to −12496 μsg and the 1σ noises span from 859 mg to 62 mg amplitude of 626 nm −6891 μsg and 1021 mg amplitude of 455 nm −9451 μsg and 776 mg amplitude of 342 nm −12496 μsg and 623 mg which indicates the bigger the amplitude the smaller the sensitivity and the bigger the 1σ noise the adjustable sensitivity provides a theoretical foundation for range selfadaption and all the results can be extended to other time domain inertial sensors eg a gyroscope or an inclinometer
mathematical literacy is the ability to use mathematical knowledge in reallife situations making it an essential component of education because of its importance in solving everyday problems mathematical literacy is also part of the program for international student assessment pisa global assessment because of the importance of the subject this systematic literature review slr investigated the relationship between students’ mathematical literacy and their ability to solve mathematical problems this slr uses the preferred reporting items for systematic reviews and metaanalysis prisma approach was used and articles published from january 2013 to august 2023 were obtained from databases including eric 342 proquest 1329 and scopus 27 following prisma a total of 20 articles were included in the review of the 20 articles most were conducted on junior high school students in turkey the majority of reviewed studies found students to have a low level of mathematical literacy which caused difficulties in formulating problems the examined studies also revealed several internal and external factors affecting mathematical literacy problems used by pisa were the most frequently employed to measure students’ mathematical literacy in the reviewed studies
it is challenging for models to understand complex multimodal content such as television clips and this is in part because videolanguage models often rely on singlemodality reasoning and lack interpretability to combat these issues we propose tvtrees the first multimodal entailment tree generator tvtrees serves as an approach to video understanding that promotes interpretable jointmodality reasoning by searching for trees of entailment relationships between simple textvideo evidence and higherlevel conclusions that prove questionanswer pairs we also introduce the task of multimodal entailment tree generation to evaluate reasoning quality our method’s performance on the challenging tvqa benchmark demonstrates interpretable stateoftheart zeroshot performance on full clips illustrating that multimodal entailment tree generation can be a bestofbothworlds alternative to blackbox systems
n4methylcytosine 4mc is a common dna methylation that has been implicated in epigenetic regulation and host defense accurate prediction of 4mc sites in dna sequences will help to better explore the biological processes and mechanisms for this problem computational methods based on machine learning and deep learning are faster less complex and less expensive than experimental detection methods however the existing computational methods are still unsatisfactory in terms of prediction accuracy so we propose a new method with better performance in this work we propose a weighted fuzzy system for identifying dna 4mc sites by kernel entropy component analysis keca we named it as wtskfskeca this model is improved based on the takagi–sugeuo–kang fuzzy system tskfs we use positionspecific trinucleotide propensity to construct feature vectors on representative benchmark datasets then we use keca to get the reconstruct error finally we put the calculated reconstruction error add to the regular term of tskfs as the weights to enhance the model performance comparative experiments with other methods show that it has good classification perfor mance
typhoon disasters undergo a complex evolutionary process influenced by temporal changes and investigating this process constitutes the central focus of geographical research as a key node within the typhoon disaster process the state serves as the foundation for gauging the dynamics of the disaster the majority of current approaches to disaster information extraction rely on event extraction methods to acquire fundamental elements including disastercausing factors disasterbearing bodies disasterpregnant environment and the extent of damage due to the dispersion of various disaster information and the diversity of time and space it is a challenge for supporting the analysis of the typhoon disaster process in this paper a typhoon disaster state information extraction tdsie method for chinese texts is proposed which aims to facilitate the systematic integration of fragmented typhoon disaster information first the integration of partofspeech tagging with spatiotemporal information extraction is employed to achieve the tagging of typhoon disaster texts second within the framework of spatiotemporal semantic units the typhoon disaster semantic vector is constructed to facilitate the identification of information elements of typhoon disaster states third coreferential state information fusion is performed based on spatiotemporal cues experimental analysis conducted using online news as the data source reveals that the tdsie achieves precision and recall rates consistently surpassing 85 the typhoon disaster state information derived from the tdsie allows for the analysis of spatiotemporal patterns evolutionary characteristics and activity modes of typhoon disasters across various scales therefore tdsie serves as valuable support for investigating the inherent process properties of typhoon disasters
the rising demand for autonomous quadrotor flights across diverse applications has led to the introduction of novel control strategies resulting in several comparative analyses and comprehensive reviews however existing reviews lack a comparative analysis of experimental results from published papers resulting in verbosity additionally publications featuring comparative studies often demonstrate biased comparisons by either selecting suboptimal methodologies or finetuning their own methods to gain an advantageous position this review analyzes the experimental results of leading publications to identify current trends and gaps in quadrotor tracking control research furthermore the analysis accomplished through historical insights datadriven analyses and performancebased comparisons of published studies distinguishes itself by objectively identifying leading controllers that have achieved outstanding performance and actual deployment across diverse applications crafted with the aim of assisting earlycareer researchers and students in gaining a comprehensive understanding the review’s ultimate goal is to empower them to make meaningful contributions toward advancing quadrotor control technology lastly this study identifies three gaps in result presentation impeding effective comparison and decelerating progress currently advanced control methodologies empower quadrotors to achieve a remarkable flight precision of 1 cm and attain flight speeds of up to 30 ms
rna secondary structure is modeled with the novel arbitraryorder hidden markov model alphahmm the alphahmm extends over the traditional hmm with capability to model stochastic events that may be in influenced by historically distant ones making it suitable to account for longrange canonical base pairings between nucleotides which constitute the rna secondary structure unlike previous heavyweight extensions over hmm the alphahmm has the flexibility to apply restrictions on how one event may influence another in stochastic processes enabling efficient prediction of rna secondary structure including pseudoknots
sports visualization has developed into an active research field over the last decades many approaches focus on analyzing movement data recorded from unstructured situations such as soccer for the analysis of choreographed activities like formation dancing however the goal differs as dancers follow specific formations in coordinated movement trajectories to date little work exists on how visual analytics methods can support such choreographed performances to fill this gap we introduce a new visual approach for planning and assessing dance choreographies in terms of planning choreographies we contribute a web application with interactive authoring tools and views for the dancers positions and orientations movement trajectories poses dance floor utilization and movement distances for assessing dancers real‐world movement trajectories extracted by manual bounding box annotations we developed a timeline showing aggregated trajectory deviations and a dance floor view for detailed trajectory comparison our approach was developed and evaluated in collaboration with dance instructors showing that introducing visual analytics into this domain promises improvements in training efficiency for the future
imitation can allow us to quickly gain an understanding of a new task through a demonstration we can gain direct knowledge about which actions need to be performed and which goals they have in this paper we introduce a new approach to imitation learning that tackles the challenges of a robot imitating a human such as the change in perspective and body schema our approach can use a single human demonstration to abstract information about the demonstrated task and use that information to generalise and replicate it we facilitate this ability by a new integration of two stateoftheart methods a diffusion action segmentation model to abstract temporal information from the demonstration and an open vocabulary object detector for spatial information furthermore we refine the abstracted information and use symbolic reasoning to create an action plan utilising inverse kinematics to allow the robot to imitate the demonstrated action
in real scenarios videos are usually corrupted by multiple types of noise which brings great challenges to retrieving social videos however most of the current video hashing methods for video retrieval consider the attack of a single noise model and rarely discuss when dealing with complex noise models which is not conducive to solving the above difficulties thus we describe a novel video hashing with secure antinoise model sanm to improve the robustness of noise attacks the input video is reconstructed into an sanm by lowrank representation lrr and random subspace partition rsp lrr is useful technique for capturing the global structure of data it focuses on recovering the underlying subspace in noisy environment and helps to make the proposed model robust to multiple noises in addition using chaotic mapping to control the generation of rsp can ensure the security of proposed model then a new subspace decomposition descriptor sdd is proposed sdd is obtained by calculating the invariant distances of the factor matrices obtained by tucker decomposition and is used to decompose sanm to derive a compact hash various experiments demonstrate that the sanm hashing performs better than several stateoftheart algorithms in terms of good robustness and discrimination and it can accurately retrieve social videos
quantum key distribution allows secret key generation with information theoretical security it can be realized with photonic integrated circuits to benefit the tiny footprints and the largescale manufacturing capacity continuousvariable quantum key distribution is suitable for chipbased integration due to its compatibility with mature optical communication devices however the quantum signal power control compatible with the mature photonic integration process faces difficulties on stability which limits the system performance and causes the overestimation of a secret key rate that opens practical security loopholes here a highly stable chipbased quantum signal power control scheme based on a biased machzehnder interferometer structure is proposed theoretically analyzed and experimentally implemented with standard silicon photonic techniques simulations and experimental results show that the proposed scheme significantly improves the system stability where the standard deviation of the secret key rate is suppressed by an order of magnitude compared with the system using traditional designs showing a promising and practicable way to realize a highly stable continuousvariable quantum key distribution system on chip
"
purpose
this paper aims to provide a framework regarding information technology it flexibility in supply chain and its relationship with the benefits we could see from enterprise resource planning erp systems furthermore this research explores the moderating effect of process integration capability in the relationship between it flexibility and erp benefits


designmethodologyapproach
this research model will help organizations get additional benefits from their erp systems that incurred huge costs time and multiple resources at their implementation the technique used for analyzing data is structural equation modeling sem and data is collected from 107 respondents through a questionnaire from business and it professionals


findings
the study findings reveal a positive and significant relationship between it flexibility and erp systems benefits moreover results also confirmed that the organizations process integration capability significantly increased the benefits of erp systems the findings also highlight empirical evidence about the significance of the toptobottom approach investing in it flexibility and the bottomtotop approach during the implementation of it systems for successful implementations


practical implications
this study has various implications for practitioners that help them successfully implement and longterm viability of their it infrastructure


originalityvalue
this studys findings will help it managers and strategists make effective decisions for creating it flexibility in alignment with the strategic goals to realize the desired results expected from erp systems and implementations of new it systems
"
massive open online courses moocs now offer a variety of options for everyone to obtain a highquality education the purpose of this study is to better understand the behaviours of mooc learners and provide some insights for taking actions that benefit larger learner groups accordingly 2288559 learners’ behaviours on 174 mitx courses were analysed the results show that moocs are more attractive to the elderly male and highly educated groups of learners learners’ performance improves as they register for more courses and improve their skills and experiences on moocs the findings suggest that in the long run learners’ adaptation to moocs will significantly improve the potential benefits of the moocs hence moocs should continue by better understanding their learners and providing alternative instructional designs by considering different learner groups mooc providers’ decisionmakers may take these findings into account when making operational decisions
abstract equations describing mushy systems in which solid and liquid are described as a single continuum have been extensively studied most studies of mushy layers have assumed them to be ‘ideal’ such that the liquid and solid were in perfect thermodynamic equilibrium it has become possible to simulate flows of passive porous media at the pore scale where liquid and solid are treated as separate continua in this contribution we study the simplest possible mushy layers at the pore scale modelling a single straight cylindrical pore surrounded by a cylindrical annulus representing the solid matrix heat and solute can be exchanged at the solid–liquid boundary we consider harmonic temperature and concentration perturbations and examine their transport rates due to advection and diffusion and the melting and solidification driven by this transport we compare the results of this numerical model with those of a onedimensional ideal mushy layer and with analytical solutions valid for ideal mushy layers for small temperature variations we demonstrate that for small values of an appropriately defined péclet number the results of the porescale model agree well with ideal mushy layer theory for both transport rates and melting as this péclet number increases the temperature and concentration profiles with radius within the pore differ significantly from constant and the behaviour of the porescale model differs significantly from that of an ideal mushy layer some effects of mechanical dispersion arise naturally in our porescale model and are shown to be important at high péclet number
typical convolutional neural networks cnns are widely used to recognize a users stress state using the functional nearinfrared spectroscopy fnirs which is the latest brain imaging technology fnirs signals are usually fed into cnn models in the form of highdimensional image data however this approach is not easy to achieve high classification accuracy because of physiological noises in brain signals it is also likely to overlook the process of evaluating the reliability of calculated classification accuracy to solve these problems we proposed a memristorbased cnn mcnns this models weight update process involves using stochastic gradient descent with momentum sgdm where the normalized conductances of memristors are used as weight substitutes these conductances are then adjusted to classify stress states we calculated the classification accuracies between the control and stress groups by using the mcnns and then compared them with those of the cnns we used densenet the most recent cnn model to simulate accuracy under the same conditions to ensure a fair comparison we divided the densenet into the memristorbased densenet mdensenet and the conventional densenet cdensenet as a result we discovered that the accuracy of mcnns 9333 exceeded that of cnns 8750 and is reliable by precision recall and fscore calculated from a confusion matrix likewise mdensenet 9238 has higher accuracy than cdensenet 9000 but shows lower accuracy than mcnns moreover we observed the reproducibility of mcnndensenet in various datasets therefore our study suggests a promising application of cnn by combining conductances of memristor for classifying stress states
despite its growth as a clinical activity and research topic the complex dynamic nature of advance care planning acp has posed serious challenges for researchers hoping to quantitatively measure it methods for measurement have traditionally depended on lengthy manual chart abstractions or static documents eg advance directive forms even though completion of such documents is only one aspect of acp natural language processing nlp in the form of an assisted electronic health record ehr review is a technological advancement that may help researchers better measure acp activity in this article we aim to show how nlpassisted ehr review supports more accurate and robust measurement of acp we do so by presenting three example applications that illustrate how using nlp for this purpose supports 1 measurement in research 2 detailed insights into acp in quality improvement and 3 identification of current limitations of acp in clinical settings
optical imaging with photocontrollable probes has greatly advanced biological research with superb chemical specificity of vibrational spectroscopy stimulated raman scattering srs microscopy is particularly promising for supermultiplexed optical imaging with rich chemical information functional srs imaging in response to light has been recently demonstrated but multiplexed srs imaging with reversible photocontrol remains unaccomplished here we create a multiplexing palette of photoswitchable polyynes with 16 raman frequencies by coupling asymmetric diarylethene with supermultiplexed carbow carbowswitch through optimization of both electronic and vibrational spectroscopy carbowswitch displays excellent photoswitching properties under visible light control and srs response with large frequency change and signal enhancement reversible and spatialselective multiplexed srs imaging of different organelles are demonstrated in living cells we further achieve photoselective timelapse imaging of organelle dynamics during oxidative stress and protein phase separation the development of carbowswitch for photoswitchable srs microscopy will open up new avenues to study complex interactions and dynamics in living cells with high spatiotemporal precision and multiplexing capability
we propose and experimentally demonstrate the enhancement of a longrange brillouin optical correlation domain analysis bocda system by utilizing a reconfigurable optical delay line rodl the rodl replaces the severalhundredkilometerlong delay fiber traditionally used for controlling correlation order in the bocda system with timedomain data processing the rodl comprised of 11 units of dual 1x2 optomechanical switches arranged in a cascaded switch matrix provides access to 2048 different optical paths with a maximum differential length of 4094 meters this configuration enables the adjustment of the length of the delay line allowing for the uniform shifting of all correlation peaks cps generated in the bocda system the incorporation of the rodl addresses issue related to nonuniform sensing intervals and significantly reduces localization errors in cps caused by variations in ambient temperature surrounding the delay fiber in our experimental studies we have successfully achieved a consistent sensing interval of 5084 cps along a 526 km sensing fiber and have empirically confirmed a substantial reduction in cp localization errors along the sensing fiber
multilayer neural networks can approximate endogenous disturbances with relatively high accuracy however for multilayer‐neural‐network‐based control methods of high‐order uncertain nonlinear systems hard to handle large exogenous disturbances especially for mismatched types complex controller scheme and so on make them difficult to be practical therefore a novel high‐performance multilayer neurocontroller which can simultaneously reject matched and mismatched disturbances will be proposed in this paper specially strong endogenous and exogenous disturbances will be feedforwardly compensated additionally the proposed controller not only protects from “explosion of complexity” but also owns a simple scheme
knowledge of the spatial variability and forecast of soil water in the ultra‐deep 21 m loess profile are important for understanding the chemical physical and biological processes in the cz in this study we regularly monitor soil water content swc in deep soil profile along regional transect on the loess plateau descriptive statistical analysis found that the coefficient of variation cv of mean swc in ansai and shenmu were 16036 and 13606 respectively indicating moderate variability the cv of mean swc in yangling changwu and fuxian were 4111 7951 and 6117 respectively showing low variability geo‐statistical analysis indicated that mean swc showed strong spatial dependence wavelet analysis showed that the approximative trend of mean swc in five sampling sites showed an increased trend along depth series in addition a good fit line equation r2  0329 was established by using observed values and ann‐forecast of three sites yangling fuxian and shenmu and the rmse values of five sample sites respectively were 1302 4546 2662 6231 and 4293 this study fills the gap in research ultra‐deep 21 m soil water changes and forecast evidence from soil borehole data at the same time our research provides valuable information for the vegetation restoration and modelling of deep soil water
mechanical ventilation techniques are vital for preserving individuals with a serious condition lives in the prolonged hospitalization unit nevertheless an imbalance amid the hospitalized people demands and the respiratory structure could cause to inconsistencies in the patient’s inhalation to tackle this problem this study presents an iterative learning pid controller ilcpid a unique current cycle feedback type controller that helps in gaining the correct pressure and volume the paper also offers a clear and complete examination of the primarily efficient neural approach for generating optimal inhalation strategies moreover machine learningbased classifiers are used to evaluate the precision and performance of the ilcpid controller these classifiers able to forecast and choose the perfect type for various inhalation modes eliminating the likelihood that patients will require mechanical ventilation in pressure control the suggested accurate neural categorization exhibited an average accuracy rate of 882 in continuous positive airway pressure cpap mode and 917 in proportional assist ventilation pav mode while comparing with the other classifiers like ensemble classifier has reduced accuracy rate of 695 in cpap mode and also 717 in pav mode an average accuracy of 789 rate in other classifiers compared to neutral network in cpap the neural model had an typical range of 816 in cpap mode and 8459 in pav mode for 20 cm h2o of volume created by the neural network classifier in the volume investigation compared to the other classifiers an average of 7217 was in cpap mode and 7783 was in pav mode in volume control different approaches such as decision trees optimizable bayes trees naive bayes trees nearest neighbour trees and an ensemble of trees were also evaluated regarding the accuracy by confusion matrix concept training duration specificity sensitivity and f1 score
background in peru hiv cases are highly concentrated among men who have sex with men msm despite the availability of antiretroviral therapy people living with hiv pwh have higher levels of oral diseases alcohol use disorder aud is significantly present among pwh our overarching goal was to generate foundational evidence on the association of aud and oral health in msm with hiv and enhance research capacity for future intersectional research on aud oral health and hiv our specific aim was to implement an onsite electronic data collection system through the use of a redcap mobile app in a lowmiddle income country lmic setting methods five validated surveys were utilized to gather data on demographics medical history hiv status alcohol use hiv stigma perceived oral health status and dietary supplement use these surveys were developed in redcap and deployed with the redcap mobile app which was installed on ten ipads across two medical hiv clinics in lima peru redcap app as well as the protocol for data collection were calibrated with feedback from trial participants and clinical research staff to improve clinical efficiency and participant experience results the mean age of participants n  398 was 3594 ± 913y of which 985 identified as male and 857 identified as homosexual 781 of participants binge drank and 123 reported being heavy drinkers after pilot testing significant modifications to the structure and layout of the surveys were performed to improve efficiency and flow the app was successfully deployed to replace cumbersome paper records and collected data was directly stored in a redcap database conclusions the redcap mobile app was successfully used due to its ability to a capture and store data offline b timely translate between multiple languages on the mobile app interface and c provide userfriendly interface with low associated costs and ample support trial registration 1r56de02963901
in this paper we introduce quadratic and cubic polynomial enrichments of the classical crouzeixraviart finite element with the aim of constructing accurate approximations in such enriched elements to achieve this goal we respectively add three and seven weighted line integrals as enriched degrees of freedom for each case we present a necessary and sufficient condition under which these augmented elements are welldefined for illustration purposes we then use a general approach to define twoparameter families of admissible degrees of freedom additionally we provide explicit expressions for the associated basis functions and subsequently introduce new quadratic and cubic approximation operators based on the proposed admissible elements the efficiency of the enriched methods is compared to the triangular crouzeixraviart element as expected the numerical results exhibit a significant improvement confirming the effectiveness of the developed enrichment strategy
recent progress in texttoimage t2i models promises transformative applications in art design education medicine and entertainment these models exemplified by dalle imagen and stable diffusion have the potential to revolutionize various industries however a primary concern is their operation as a ‘blackbox’ for many users without understanding the underlying mechanics users are unable to harness the full potential of these models this study focuses on bridging this gap by developing and evaluating explanation techniques for t2i models targeting inexperienced end users while prior works have delved into explainable ai xai methods for classification or regression tasks t2i generation poses distinct challenges through formative studies with experts we identified unique explanation goals and subsequently designed tailored explanation strategies we then empirically evaluated these methods with a cohort of 473 participants from amazon mechanical turk amt across three tasks our results highlight users’ ability to learn new keywords through explanations a preference for examplebased explanations and challenges in comprehending explanations that significantly shift the image’s theme moreover findings suggest users benefit from a limited set of concurrent explanations our main contributions include a curated dataset for evaluating t2i explainability techniques insights from a comprehensive amt user study and observations critical for future t2i model explainability research
minimizing the offloading latency of agricultural drip irrigation and fertilization tasks has long been a pressing issue in agricultural drip irrigation and fertilization wireless sensor networks aifwsns the introduction of edge computing as a robust and practical aid to cloud computing in aifwsns can significantly improve the execution speed of agricultural drip irrigation and fertilization tasks and effectively reduce the task offloading latency therefore this paper investigates the optimization method of drip irrigation and fertilization task offloading delay in aifwsns based on edge computing and proposes a new edge task offloading method for aifwsns namely quantum chaotic genetic optimization algorithm qcga this paper introduces a novel quantum operator in qcga comprising a quantum nongate and a quantum rotation gate to improve the algorithm’s global search capability the quantum operator accomplishes the updating of quantum rotating gates without querying the quantum rotation angle table which reduces the computational complexity of introducing quantum optimization into the task offloading problem of aifwsns this paper proposes a new chaotic operator to make the initial solution more uniformly distributed in the search space by chaotic mapping this paper’s simulation experiments compared qcga and snake optimizer so genetic algorithm ga particle swarm optimization pso sequential offloading and random offloading methods simulation results showed that compared with so ga pso sequential offloading and random offloading methods the average delay of qcga was reduced by 996 2678 2931 4467 and 6124
novel design solutions for dedicated portable brain positron emission tomography systems with improved performance facilitate emergency and interventional imageguided surgery as well as advanced diagnostics of clinically impaired patients with neurodegenerative diseases we report a novel portable mvtbased alldigital helmet pet system with a hemispherical detector arrangement based on the multi voltage threshold technology it has a transverse and axial field of view fov of 200 and 124 mm respectively it allows to scan subjects in a standing sitting and lying position we evaluated the performance of the system according to nema standards the scanner exhibits a noise equivalent count rate peak of inlineformula texmath notationlatexmathbf 151pm 2 texmathinlineformula kcps at the activity of 4065 kbqml a sensitivity of inlineformula texmath notationlatexmathbf 5524pm 005 texmathinlineformula cpskbq and a spatial resolution at the center of the fov of approximately 33 mm fwhm when using the filtered back projection algorithm for a mini derenzo phantom rods of 20mm diameter can be clearly separated timedynamic inlineformula texmath notationlatexmathbf 18textf texmathinlineformulafluorodeoxyglucose human brain imaging was performed showing the distinctive traits of cortex and thalamus uptake as well as of the arterial and venous flow with 30 s time frames we finally verified the usability of the device in the diagnostics of alzheimer’s disease by imaging human subjects with inlineformula texmath notationlatexmathbf 18textf texmathinlineformulaflorbetapir
we present disrnerf a diffusionguided framework for viewconsistent superresolution sr nerf unlike prior works we circumvent the requirement for highresolution hr reference images by leveraging existing powerful 2d superresolution models nonetheless independent sr 2d images are often inconsistent across different views we thus propose iterative 3d synchronization i3ds to mitigate the inconsistency problem via the inherent multiview consistency property of nerf specifically our i3ds alternates between upscaling lowresolution lr rendered images with diffusion models and updating the underlying 3d representation with standard nerf training we further introduce renoised score distillation rsd a novel scoredistillation objective for 2d image resolution our rsd combines features from ancestral sampling and score distillation sampling sds to generate sharp images that are also lrconsistent qualitative and quantitative results on both synthetic and realworld datasets demonstrate that our disrnerf can achieve better results on nerf superresolution compared with existing works code and video results available at the project website’
recent studies have established that exosomes exs derived from follicular fluid ff can promote oocyte development however the specific sources of these exs and their regulatory mechanisms remain elusive it is universally acknowledged that oocyte development requires signal communication between granulosa cells gcs and oocytes however the role of gcsecreted exs and their functions are poorly understood this study aimed to investigate the role of porcine granulosacellderived exosomes gcexs in oocyte development in this study we constructed an in vitro model of porcine gcs and collected and identified gcexs we confirmed that porcine gcs can secrete exs and investigated the role of gcexs in regulating oocyte development by supplementing them to cumulus–oocyte complexes cocs cultured in vitro specifically gcexs increase the cumulus expansion index cei promote the expansion of the cumulus alleviate reactive oxygen species ros and increase mitochondrial membrane potential mmp resulting in improved oocyte development additionally we conducted small rna sequencing of gcexs and hypothesized that mir148a3p the highestexpressed microrna mirna may be the key mirna our study determined that transfection of mir148a3p mimics exerts effects comparable to the addition of exs meanwhile bioinformatics prediction dual luciferase reporter gene assay and rtqpcr identified dock6 as the target gene of mir148a3p in summary our results demonstrated that gcexs may improve oocyte antioxidant capacity and promote oocyte development through mir148a3p by targeting dock6
cellular homeostasis is regulated by growth factors gfs which orchestrate various cellular processes including proliferation survival differentiation motility inflammation and angiogenesis dysregulation of gfs in microbial infections and malignancies have been reported previously viral pathogens exemplify the exploitation of host cell gfs and their signalling pathways contributing to viral entry virulence and evasion of antiviral immune responses viruses can also perturb cellular metabolism and the cell cycle by manipulation of gf signaling in some cases this disturbance may promote oncogenesis viral pathogens can encode viral gf homologues and induce the endogenous biosynthesis of gfs and their corresponding receptors or manipulate their activity to infect the host cells close investigation of how viral strategies exploit and regulate gfs a will shed light on how to improve antiviral therapy and cancer treatment in this review we discuss and provide insights on how various viral pathogens exploit different gfs to promote viral survival and oncogenic transformation and how this knowledge can be leveraged toward the design of more efficient therapeutics or novel drug delivery systems in the treatment of both viral infections and malignancies
offset boosting is a crucial passage for chaotic signal modification in chaosbased engineering searching an offset controller for a 3d chaotic system is usually complex let alone two independent nonbifurcating offset constants this paper constructs a class of chaotic systems providing a single constant posing direct offset boosting for two dimensions this offset boostable chaotic system regime has multiple typical control modes including a system variable single control synchronous common control reverse control and differential control this new type of chaotic systems also finds twodimensional offset boosting combined with amplitude control
the adsorption properties of five gases on metal oxides cuo ag2o modified ti3c2o2 are studied by the first‐principles density functional theory the gas sensitivity of nitric oxide ammonia nitrogen dioxide methane and formaldehyde on metal oxides cuo ag2o modified ti3c2o2 is systematically discussed on the aspects of adsorption energy the density of state electronic localization function and desorption time the results prove that metal oxides cuo ag2o modification can enhance the adsorption abilities of ti3c2o2 the adsorption capacity of cuo‐modified ti3c2o2 is nh3  no  no2  ch2o  ch4 and the adsorption capacity of ag2o‐modified ti3c2o2 is no  no2  ch2o nh3  ch4 the no2 adsorption capacity of ag2o‐modified ti3c2o2 is 1124 times that of pure ti3c2o2 by comparison cuo‐modified ti3c2o2 is more suitable for capturing no gas than ag2o‐modified ti3c2o2 and the former exhibits chemically adsorbed considering the results cuo‐ and ag2o‐modified ti3c2o2 can be used as oxynitride gas sensors
in terms of producing pollutionfree power renewable energy has recently had a considerable impact on the power generation industry in this study traditional economic load dispatch eld problems are solved by integrating thermal with solar power plants to reduce generation cost of generated demand power here chaosbased backtracking search algorithm cbsa has been used to determine optimal generation cost of traditional ten fifteen and forty unit thermal power generation cbsa has been also applied to reduced generation cost of three separated thermalsolar test systems  one solarten thermal one solarfifteen thermal and one solarforty thermal proposed generation model can be able to reduce generation cost ie 22278 3027 and 4664 from the generation cost of traditional generation model proposed results show the effectiveness of the presented thermalsolar generation models than traditional generation models the efficacy of the suggested cbsa has been demonstrated by comparing the generating costs of different test systems to the outcomes of other applied algorithms such as the bsa covid 19 and whale algorithms
although artificial intelligence ai and automated decisionmaking systems have been around for some time they have only recently gained in importance as they are now actually being used and are no longer just the subject of research ai to support decisionmaking is thus affecting ever larger parts of society creating technical but above all ethical legal and societal challenges as decisions can now be made by machines that were previously the responsibility of humans this introduction provides an overview of attempts to regulate ai and addresses key challenges that arise when integrating ai systems into human decisionmaking the special topic brings together research articles that present societal challenges ethical issues stakeholders and possible futures of ai use for decision support in healthcare the legal system and border control
as an emerging alternative mode of transportation an indepth understanding of autonomous shuttle as experiences among all age groups with and without disabilities may impact acceptance and adoption of the as shape industry guidelines and impact public policy therefore this study analyzed qualitative data from older n  104 younger and middleaged n  106 adults and people with disabilities n  42 the data were obtained by asking participants four openended questions from an autonomous vehicle user perception survey the result revealed seven themes safety ease of use cost availability aging as information and experience with as for older younger and middleaged adults and six themes all of the previously mentioned except for aging for people with disabilities frequency counts indicated priority attention among all groups to safety and ease of use this study provides valuable information pertaining to the experiences concerns and motivations of all potential users across age groups and disabilities—and may inform policymakers and industry partners to address their needs more adequately these findings may contribute to improving and enhancing as programming design and deployment in a safer accessible affordable and tailored way
the scale and quality of a dataset significantly impact the performance of deep models however acquiring largescale annotated datasets is both a costly and timeconsuming endeavor to address this challenge dataset expansion technologies aim to automatically augment datasets unlocking the full potential of deep models current data expansion techniques include image transformation and image synthesis methods transformationbased methods introduce only local variations leading to limited diversity in contrast synthesisbased methods generate entirely new content greatly enhancing informativeness however existing synthesis methods carry the risk of distribution deviations potentially degrading model performance with outofdistribution samples in this paper we propose distdiff a trainingfree data expansion framework based on the distributionaware diffusion model distdiff constructs hierarchical prototypes to approximate the real data distribution optimizing latent data points within diffusion models with hierarchical energy guidance we demonstrate its capability to generate distributionconsistent samples significantly improving data expansion tasks distdiff consistently enhances accuracy across a diverse range of datasets compared to models trained solely on original data furthermore our approach consistently outperforms existing synthesisbased techniques and demonstrates compatibility with widely adopted transformationbased augmentation methods additionally the expanded dataset exhibits robustness across various architectural frameworks our code is available at httpsgithubcomhaoweiz23distdiff
in this article the adaptive backstepping control problem is investigated for a class of mismatched uncertain nonlinear systems with input and state quantization all available states are generated by the static bounded quantizers which can cause the failure of the recursive backstepping design previous results are based on linearlike virtual controllers to ensure that the partial derivatives of virtual controllers are constants therefore the systems are required to be an integral form or to meet matched conditions based on a dynamic gain approach this article presents a new compensation mechanism to solve the difficulty of recursive backstepping design caused by discontinuous states the control problem is transformed into a design problem of the dynamic variable first the dynamic variable is introduced based on a coordinate transformation its derivative is used to compensate for discontinuous mismatched nonlinear terms then with the help of the lyapunov stability theorem it is strictly proved that all signals of the closedloop system are globally uniformly bounded finally numerical simulations are provided to validate the effectiveness of the developed algorithm
reconfigurable intelligent surfaceassisted full duplex risfd communication is attracting attention because of its ability to solve the issue of spectrum crunch and improve signal strength in shadowing areas however its potential has not been exploited due to various interferences like selfinterference and loopinterference this letter proposes a risfd communication where the loop interference is mitigated by employing a zero steering beamforming zsbf technique the proposed scheme also becomes effective by partitioning the single ris into two parts to support simultaneous uplink and downlink communications additionally the system has been examined under imperfect channel state information to make it more practical further the closedform analytical expressions of the outage probability ergodic capacity and energy efficiency are derived for the proposed scheme as well as corroborated with simulation results the comparative analysis and its results show that the proposed scheme yields superior outcomes compared to the existing work in the literature
the topology selection plays a key role in minimizing the losses and improving the output waveform quality of an inverter in addition increasing the switching frequency of an inverter help to reduce the size of emi filters resulting in power density improvement use of widebandgap device like sic devices enables high switching frequency operation in power electronic converters however due to low gate charge and small junction capacitance of sic devices the sicbased inverter is more likely to be influenced by sideeffects of fast switching transition like undesired ringing larger voltage overshoot and mistriggerig due to millereffect parasitic inductances of multiple power loops and gate loops within the inverter leg play a key role in these undesired effects however a proper design of pcblayout can reduce these effects in this paper a comparison is conducted between a ttype and 2level inverter topologies for motor drive applications furthermore a placement configuration of components along with the pcb layout are proposed for a ttype leg to improve its switching transition behavior by reducing the power and gate loop parasitic inductances the parasitic inductances of proposed configuration are estimated using fea simulation and the estimated values are used to simulate the switching transition behavior of a ttype leg finally a double pulse test dpt is executed to demonstrate the accurate switching transition behaviour of a ttype leg which is designed with the proposed pcb layout and component placement
in lung radiotherapy infrared cameras can record the location of reflective objects on the chest to infer the position of the tumor moving due to breathing but treatment system latencies hinder radiation beam precision realtime recurrent learning rtrl is a potential solution as it can learn patterns within nonstationary respiratory data but has high complexity this study assesses the capabilities of resourceefficient online rnn algorithms namely unbiased online recurrent optimization uoro sparse1 step approximation snap1 and decoupled neural interfaces dni to forecast respiratory motion during radiotherapy treatment accurately we use time series containing the 3d position of external markers on the chest of healthy subjects we propose efficient implementations for snap1 and dni based on compression of the influence and immediate jacobian matrices and an accurate update of the linear coefficients used in credit assignment estimation respectively the original sampling frequency was 10hz we performed resampling at 333hz and 30hz we use uoro snap1 and dni to forecast each markers 3d position with horizons the time interval in advance for which the prediction is made h21s and compare them with rtrl least mean squares and linear regression rnns trained online achieved similar or better accuracy than most previous works using larger training databases and deep learning even though we used only the first minute of each sequence to predict motion within that exact sequence snap1 had the lowest normalized root mean square errors nrmse averaged over the horizon values considered equal to 0335 and 0157 at 333hz and 100hz respectively similarly uoro had the highest accuracy at 30hz with an nrmse of 00897 dnis inference time equal to 68ms per time step at 30hz intel core i713700 cpu was the lowest among the rnn methods examined
fast and accurate infrared ir sea–sky line ssl detection could greatly benefit the efficiency of target detection under maritime however the traditional ssl detection algorithms are slightly inferior in detection accuracy while the convolutional neural network cnnbased algorithms have high standard hardware and dataset requirements which are difficult to satisfy in some practical scenes in this article a novel concise and intuitive ssl detection algorithm named bisection window and homogeneity principlebased local contrast measure bhlcm is proposed first a bisection local contrast window blcw is proposed based on the local contrast measure lcm and be used for the search of the patches that contain ssl segments along a set of variable preset vertical paths then based on the analysis of the physical characteristics of areas near ssl the homogeneity principle is proposed to remove the patches containing false ssl segments finally the midpoints of the remaining ssl segments are used to fit the final ssl through random sample consensus ransac experimental results based on four ir image sequences with more than 3000 ir images illustrate that compared with the stateoftheart algorithms bhlcm not only achieves precision comparable to the most accurate one but also is significantly ahead in speed among most of the algorithms compared in addition speed tests about bhlcm based on different hardware platforms show that realtime detection could still be achieved even without highperformance graphics cards the code and dataset are available at bhlcm httpsgithubcomfjsrepobhlcm
this research investigates the intricate interplay among the digital economy green innovation and the level of sustainable development panel data from 268 cities in china from 2011 to 2020 are used to comprehensively evaluate the level of digital economy development and investigate the digital economy’s influence on sustainable development additionally a mechanism analysis is used to investigate the contribution of green innovation the findings suggest that the digital economy significantly stimulates sustainable development and green innovation serves as a mediating intermediary and moderating effect in facilitating this relationship moreover the robustness check extends the verification of the positive effect of the “broadband china” policy on sustainable development strengthening the reliability of the results the contribution of this study provides management insights on how regions can promote sustainable development in the digital age
onestream transformer trackers have shown outstanding performance in challenging benchmark datasets over the last three years as they enable interaction between the target template and search region tokens to extract targetoriented features with mutual guidance previous approaches allow free bidirectional information flow between template and search tokens without investigating their influence on the trackers discriminative capability in this study we conducted a detailed study on the information flow of the tokens and based on the findings we propose a novel optimized information flow tracking oiftrack framework to enhance the discriminative capability of the tracker the proposed oiftrack blocks the interaction from all search tokens to target template tokens in early encoder layers as the large number of nontarget tokens in the search region diminishes the importance of targetspecific features in the deeper encoder layers of the proposed tracker search tokens are partitioned into target search tokens and nontarget search tokens allowing bidirectional flow from target search tokens to template tokens to capture the appearance changes of the target in addition since the proposed tracker incorporates dynamic background cues distractor objects are successfully avoided by capturing the surrounding information of the target the oiftrack demonstrated outstanding performance in challenging benchmarks particularly excelling in the oneshot tracking benchmark got10k achieving an average overlap of 746 the code models and results of this work are available at urlhttpsgithubcomjananikugaaoiftrack
ice surface speed increases dramatically from upstream to downstream in many ice streams and glaciers this speed‐up is thought to be associated with a transition from internal distributed deformation to highly localized deformation or sliding at the ice‐bedrock interface the physical processes governing this transition remain unclear here we argue that highly localized deformation does not necessarily initiate at the ice‐bedrock interface but could also take the form of an internal shear band inside the ice flow that connects topographic highs the power‐law exponent n in the ice rheology amplifies the feedback between shear heating and shear localization leading to the spontaneous formation of an internal shear band that can create flow separation within the ice we model the thermomechanical ice flow over a sinusoidal basal topography by building on the high‐resolution stokes solver fastice v10 we compile a regime diagram summarizing cases in which a sinusoidal topography with a given amplitude and wavelength leads to shear band formation for a given rheology we compare our model results to borehole measurements from greenland and find evidence to support the existence of an internal shear band our study highlights the importance of re‐evaluating the degree to which internal deformation contributes to total deformation in the ice column and to the flow‐to‐sliding transition
industry 40 aids organisational transformation powered by innovative technologies and connectivity in addition to navigating complex industry 40 concepts and characteristics organisations must also address organisational consequences related to fastpaced organisational transformation and resource efficacy the optimal allocation of organisational resources and capabilities to large transformational programs as well as the significant capital investment associated with digital transformation compel organisations to prioritize their efforts hence this study investigates how key industry 40 organisational capabilities could be prioritized towards organisational digital transformation data were collected from 49 participants who had completed a questionnaire containing 26 statement actions aligned to sensing seizing transforming and supporting organisational capability domains by analysing the data statement actions were prioritized and operationalized into a prototyped checklist two organisations applied the prototyped checklist illustrating unique profiles and transformative actions the operationalisation of the checklist highlighted its utility in establishing where an organisation operates in terms of digital transformation as well as what additional steps might be followed to improve its capability prioritisation based on low checklist scores by understanding the prioritisation of industry 40 capabilities organisations could ensure that resources are allocated optimally for business value creation based on organisational capabilities prioritisation
in recent years there has been a prominent focus on enhancing the quality of features derived from convolutional neural networks cnns within the field of polarimetric synthetic aperture radar polsar image classification targeting this challenge this article first visualizes the lack of discriminability and generalizability in cnn features through several empirical observations subsequently we explain why these problems arise from a causal perspective accomplished by means of a structural causal model scm constructed according to the training and testing process of cnns this scm facilitates the identification of variables that affect the quality of polsar image feature learning as well as an intervention on those variables using backdoor adjustment building upon this groundwork a novel causal inferenceguided feature enhancement framework is constructed it can be seamlessly integrated into any cnnbased polsar image classifier in a plugandplay manner enabling the enhanced classifier to filter out interference information and prevent model overfitting these two aspects bring better feature discriminability and generalizability respectively leading to improved classification performance experimental results on four widelyused polsar image datasets demonstrate the effectiveness of our proposed framework we integrate it into several mainstream methods in the field and show that the accuracy of the enhanced classifier is improved compared to the original model
this study proposes a robust optimization method for waterborne emergency resource allocation in inland waterways that addresses the uncertainties and mismatches between supply and demand to accomplish this we integrate the risk evaluation of maritime with a robust optimization model and employ the entropy weighted method ewmtechnique for order preference by similarity to ideal solution topsisanalytic hierarchy process ahp method to evaluate the risk of various areas the approach enables exploration of the relationship between maritime risk and emergency resource allocation strategy the robust optimization method is used to deal with uncertainty and derive the robust counterpart of the proposed model we establish an emergency resource allocation model that considers both the economy and timeliness of emergency resource allocation we construct an optimization model and transform it into an easily solvable robust counterpart model the results demonstrate that the proposed method can adapt to realworld scenarios and effectively optimize the configuration effect while improving rescue efficiency under reasonable resource allocation specifically the proportion of rescue time saved ranges from 2852 to 9260 and the proportion of total cost saved is 9582 our approach has significant potential to provide a valuable reference for decisionmaking related to emergency resource allocation in maritime management
the security of electric vehicle ev charging has gained momentum after the increase in the ev adoption in the past few years mobile applications have been integrated into ev charging systems that mainly use a cloudbased platform to host their services and data like many complex systems cloud systems are susceptible to cyberattacks if proper measures are not taken by the organization to secure them in this paper we explore the security of key components in the ev charging infrastructure including the mobile application and its cloud service we conducted an experiment that initiated a man in the middle attack between an ev app and its cloud services our results showed that it is possible to launch attacks against the connected infrastructure by taking advantage of vulnerabilities that may have substantial economic and operational ramifications on the ev charging ecosystem we conclude by providing mitigation suggestions and future research directions
background the willingness to trust predictions formulated by automatic algorithms is key in a wide range of domains however a vast number of deep architectures are only able to formulate predictions without associated uncertainty purpose in this study we propose a method to convert a standard neural network into a bayesian neural network and estimate the variability of predictions by sampling different networks similar to the original one at each forward pass methods we combine our method with a tunable rejectionbased approach that employs only the fraction of the data ie the share that the model can classify with an uncertainty below a userset threshold we test our model in a large cohort of brain images from patients with alzheimers disease and healthy controls discriminating the former and latter classes based on morphometric images exclusively results we demonstrate how combining estimated uncertainty with a rejectionbased approach increases classification accuracy from 086 to 095 while retaining 75 of the test set in addition the model can select the cases to be recommended for eg expert human evaluation due to excessive uncertainty importantly our framework circumvents additional workload during the training phase by using our network “turned into bayesian” to implicitly investigate the loss landscape in the neighborhood of each test sample in order to determine the reliability of the predictions conclusion we believe that being able to estimate the uncertainty of a prediction along with tools that can modulate the behavior of the network to a degree of confidence that the user is informed about and comfortable with can represent a crucial step in the direction of user compliance and easier integration of deep learning tools into everyday tasks currently performed by human operators
summarization tasks aim to summarize multiple pieces of information into a short description or representative information a text summarization task summarizes textual information into a short description whereas an image collection summarization task summarizes an image collection into images or textual representation in which the challenge is to understand the relationship between images in recent years scenegraph generation has shown the advantage of describing the visual contexts of a singleimage and incorporating external knowledge into the scenegraph generation model has also given effective directions for unseen singleimage scenegraph generation while external knowledge has been implemented in related work it is still challenging to use this information efficiently for relationship estimation during the summarization following this trend in this paper we propose a novel scenegraphbased imagecollection summarization model that aims to generate a summarized scenegraph of an image collection the key idea of the proposed method is to enhance the relation predictor toward relationships between images in an image collection incorporating knowledge graphs as external knowledge for training a model with this approach we build an endtoend framework that can generate a summarized scene graph of an image collection to evaluate the proposed method we also build an extended annotated mscoco dataset for this task and introduce an evaluation process that focuses on estimating the similarity between a summarized scene graph and groundtruth scene graphs traditional evaluation focuses on calculating precision and recall scores which involve true positive predictions without balancing precision and recall meanwhile the proposed evaluation process focuses on calculating the fscore of the similarity between a summarized scene graph and groundtruth scene graphs which aims to balance both false positives and false negatives experimental results show that using external knowledge to enhance the relation predictor achieves better results than existing methods
we introduce a new approach for the adaptation of the maximal internal envelope method extended to address the largest empty sphere problem within unstructured 3d point clouds we explore the identification of the largest empty sphere by computing convex hull vertices and employing a voidness score based on minimal distance scoring for optimal segment selection the integration of delaunay triangulation and voronoi diagrams facilitates the initial identification of potential largest empty sphere candidates our analysis reveals the methods efficacy and efficiency often locating the largest empty sphere in initial computational stages suggesting a lower complexity than initially projected
this paper discusses the relationship between robot movements and sense of agency in the teleoperation of a robot arm using a graphical user interface based on the sense of agency model this study focuses on the imagery of robot movements held by operators it verifies the relationship between imagined robot movements and preferred actual robot movements as well as the relationship between robot movements and the sense of agency during operations through two experiments these findings suggest that operators exhibit a bias toward favoring swifter motions than those depicted in the imagery additionally this study suggests that when there are significant differences in the movement distance and speed between movements based on imagined robot actions and those that consider bias the sense of agency decreases in particular when considering the bias towards preferring faster movements adjusting the control to achieve the same acceleration as movements that consider this bias even when the movement distance changes can evoke a sense of agency during the operation
"this paper introduces the new class of onebound core games where the core can be described by either a lower bound or an upper bound on the payoffs of the players named lower bound core games and upper bound core games respectively we study the relation of the class of onebound core games with several other classes of games and characterize the new class by the structure of the core and in terms of davismaschler reduced games we also provide explicit expressions and axiomatic characterizations of the nucleolus for onebound core games and show that the nucleolus coincides with the shapley value and the tau
 τ
 value when these games are convex"
detecting and magnifying imperceptible highfrequency motions in realworld scenarios has substantial implications for industrial and medical applications these motions are characterized by small amplitudes and high frequencies traditional motion magnification methods rely on costly highspeed cameras or active light sources which limit the scope of their applications in this work we propose a dualcamera system consisting of an event camera and a conventional rgb camera for video motion magnification providing temporallydense information from the event stream and spatiallydense data from the rgb images this innovative combination enables a broad and costeffective amplification of highfrequency motions by revisiting the physical camera model we observe that estimating motion direction and magnitude necessitates the integration of event streams with additional image features on this basis we propose a novel deep network tailored for eventbased motion magnification our approach utilizes the secondorder recurrent propagation module to proficiently interpolate multiple frames while addressing artifacts and distortions induced by magnified motions additionally we employ a temporal filter to distinguish between noise and useful signals thus minimizing the impact of noise we also introduced the first eventbased motion magnification dataset which includes a synthetic subset and a realcaptured subset for training and benchmarking through extensive experiments in magnifying smallamplitude highfrequency motions we demonstrate the effectiveness and accuracy of our dualcamera system and network offering a costeffective and flexible solution for motion detection and magnification
the infrared dichroic beamsplitter plays an important role in infrared multiband imaging systems especially for infrared remote sensing this paper presents the design and preparation of a dichroic beamsplitter that is capable of reflecting near infrared nir and shortwave infrared swir and transmitting medium wave infrared mwir as well as longwave infrared lwir a single crystal germanium ge sheet is used as the substrate of the dichroic beamsplitter while ge zinc sulfide zns and ytterbium trifluoride ybf3 are selected as coating materials the average reflectance of the dichroic beamsplitter is more than 95 in bands 128 to 138 μm 158 to 183 μm and 195 to 232 μm and the average transmittance is more than 92 in bands 37 to 62 μm and 75 to 125 μm at an incident angle of 45° the dichroic beamsplitter has been successfully applied in the optical system of infrared remote sensing it provides a technical approach for other optical systems to separate the optical spectrum from nir to lwir
the prevalence of pelvic organ prolapse pop has been steadily increasing over the years rendering it a pressing global health concern that significantly impacts women’s physical and mental wellbeing as well as their overall quality of life with the advancement of threedimensional reconstruction and computer simulation techniques for pelvic floor structures research on pop has progressively shifted toward a biomechanical focus finite element fe analysis is an established tool to analyze the biomechanics of complex systems with the advancement of computer technology an increasing number of researchers are now employing fe analysis to investigate the pathogenesis of pop in women there is a considerable number of research on the female pelvic fe analysis and to date there has been less review of this technique in this review article we summarized the current research status of fe analysis in various types of pop diseases and provided a detailed explanation of the issues and future development in pelvic floor disorders currently the application of fe analysis in pop is still in its exploratory stage and has inherent limitations through continuous development and optimization of various technologies this technique can be employed with greater accuracy to depict the true functional state of the pelvic floor thereby enhancing the supplementation of the pop mechanism from the perspective of computer biomechanics
org
china’s low altitude general aviation industry requires effective groundtoair broadband communication network this article analyzes the existing groundtoair communication methods and the low altitude coverage problems in the commercial tdlte networks a tdlte ground to air network solution was designed and its feasibility was analyzed
a dc microgrid’s tightly regulated dcdc converter encounters significant challenges in voltage stability primarily due to the negative incremental resistance of constant power loads cpls conventional controllers often struggle with load variations and changes in system parameters therefore there has been growing interest in adaptive machine learning algorithms such as deep reinforcement learning drl to improve voltage regulation this paper presents an endtoend drl framework based on a modified twin delayed deep deterministic policy gradient td3 algorithm the framework is designed to directly control power switches for regulating the voltage of a dcdc buck converter that supplies power to cpls realtime experiments were conducted using opalrt to validate the approach under diverse load cycles and converter parameter changescomparative analysis against other drlbased control strategies including deep qlearning dqn and deep deterministic policy gradient ddpg demonstrated the superior static and dynamic voltage response of the proposed modified td3 drl controller particularly in scenarios involving load and parametric variations
web crawlers are widely used to automatically explore and test web applications however navigating the pages of a web application can be difficult due to dynamic page generation in particular the inputs for the web form fields can affect the resulting pages and subsequent navigation therefore choosing the inputs and the order of clicks on a web page is essential for an effective web crawler to achieve high code coverage this paper proposes a set of actions to quickly fill in web form fields and uses reinforcement learning algorithms to train a convolutional neural network cnn the trained agent named irobot can autonomously select actions to guide the web crawler to maximize code coverage we experimentally compared different reinforcement learning algorithms neural networks and actions the results show that our cnn network with the proposed actions performs better than other neural networks in terms of branch coverage using the deep qlearning dqn or proximal policy optimization ppo algorithm furthermore compared to previous studies irobot can increase branch coverage by about 17 while reducing training time to 1254
differential drive mobile robots being widely used in several industrial and domestic applications are increasingly demanding when concerning precision and satisfactory maneuverability in the present paper the problem of independently controlling the velocity and orientation angle of a differential drive mobile robot is investigated by developing an appropriate two stage nonlinear controller embedded on board and also by using the measurements of the speed and accelerator of the two wheels as well as taking remote measurements of the orientation angle and its rate the model of the system is presented in a nonlinear state space form that includes unknown additive terms arising from external disturbances and actuator faults based on the nonlinear model of the system the respective io relation is derived and a twostage nonlinear measurable output feedback controller analyzed into an internal and an external controller is designed the internal controller aims to produce a decoupled inner closedloop system of linear form regulating the linear velocity and angular velocity of the mobile robot independently the internal controller is of the nonlinear pd type and uses real time measurements of the angular velocities of the active wheels of the vehicle as well as the respective accelerations the external controller aims toward the regulation of the orientation angle of the vehicle it is of a linear delayed pd feedback form offering feedback from the remote measurements of the orientation angle and angular velocity of the vehicle which are transmitted to the controller through a wireless network analytic formulae are derived for the parameters of the external controller to ensure the stability of the closedloop system even in the presence of the wireless transmission delays as well as asymptotic command following for the orientation angle to compensate for measurement noise external disturbances and actuator faults a metaheuristic algorithm is proposed to evaluate the remaining free controller parameters the performance of the proposed control scheme is evaluated through a series of computational experiments demonstrating satisfactory behavior
apiculture is an important industry closely related to the national economy and people’s livelihoods beekeepers’ behavior is an important factor affecting the yield quality and benefits of apiculture however there is a lack of a systematic understanding of the longterm changes in beekeeping decisions made by beekeepers using panel data we analyzed the dynamic trends and related influencing factors of decisions made by beekeeping models honey source plant selection and the migration flow space of beekeepers from 2009 to 2020 the results showed that the proportion of the lmb model decreased while the pab and smb models continued to increase the frequency of utilization of the main nectar source plants for honey collection decreased and the concentration of migratory flow of beekeeping increased behavior of beekeepers from 2009 to 2020 showed a certain degree of spatial contraction which seriously restricted the effective use of nectar plant resources family attributes economic status beekeeping models and disaster conditions directly or indirectly affected beekeepers’ decisions we propose a series of recommendations to facilitate the transformation and advancement of the chinese bee industry this study promotes an understanding of sustainable development of the bee industry in china and other countries worldwide
"
 aufeis are a key element in the chain of water exchange processes in the permafrost zone the hydrological role of aufeis in the formation of river flow can be comparable to that of glaciers observations of aufeis during the construction of the baikalamur mainline showed that the share of aufeis runoff in the annual river flow can reach 35 for watersheds with an area of up to 500 km2 despite the long history of studying aufeis and the results achieved there are no methods for predicting the development of aufeis processes as well as hydrological models that take into account the share of aufeis feeding in river runoff this is due to the lack of observational data on the dynamics and development factors of aufeis in the last century longterm studies were carried out on some aufeis in siberia and the far east one of these unique objects is the giant anmangynda aufeis which forms in the upper reaches of the kolyma river basin the aim of the research is to study the hydrological regime of the anmangynda river basin and develop a method for accounting of aufeis runoff module «aufeis» in the distributed hydrological model «hydrograph» the module “aufeis” takes into account two factors of aufeis destruction – under the influence of solar radiation and thermal erosion destruction the input data is the area of aufeis at the beginning of the warm season and daily meteorological data the calculation parameters are the coefficients of ice melting and evaporation from ice as well as the coefficient of the relative area of aufeis depending on the period of destruction calculated for the modern climatic period the result of the calculation is the aufeis runoff and the characteristics of aufeis for a given interval based on historical data and materials obtained during own field research the module «aufeis» was tested the deviation of the calculated and observed values was 2—10 and 1—9 for the maximum values of the area and volume of the anmangynda aufeis respectively results of numerical calculations for the period 1967–2022 revealed the dynamics of the contribution of the anmangynda aufeis to the river flow in different seasons the expanded functionality of the hydrograph model has improved the quality of modeling for rivers where aufeis form for the river basin anmangynda average nashsutcliffe coefficient and annual runoff error were 057 and 130 compared to 041 and 180 when the module «aufeis» was not used
"
regular inspections of important civil infrastructures are mandatory to ensure structural safety and reliability until today these inspections are primarily conducted manually which has several deficiencies in context of prestressed concrete structures steel tendons can be susceptible to stress corrosion cracking which may result in breakage of individual wires that is visually not observable recent research therefore suggests acoustic emission monitoring for wire break detection in prestressed concrete structures however in noisy environments such as wind turbines conventional acoustic emission detection based on userdefined amplitude thresholds may not be suitable thus we propose the use of matched filters for acoustic emission detection in noisy environments and apply the proposed method to the task of wire break detection in posttensioned wind turbine towers based on manually conducted wire breaks and rebound hammer tests on a largescale test frame we employ a bruteforce search for the most suitable query signal of a wire break event and a rebound hammer impact respectively then we evaluate the signal detection performance on more than 500 other wire break signals and approximately one week of continuous acoustic emission recordings in an operating wind turbine for a signaltonoise ratio of 0 db the matched filter approach shows an improvement in auc by up to 078 for both the wire break and the rebound hammer query signal compared to stateoftheart amplitudebased detection even for the unscaled wire break measurements originally recorded at the 12 m large laboratory test frame the improvement in auc still lies between 001 and 025 depending on the wind turbine noise recordings considered for evaluation matched filters may therefore be a promising alternative to amplitudebased detection algorithms and deserve particular consideration with regard to acoustic emission monitoring especially in noisy environments or when sparse senor networks are required
this brief studies the adaptive eventdriven control for continuous positive stochastic semimarkov jump models in the presence of cyber attacks to save the limited network resources an adaptive eventdriven mechanism is constructed in which the communication threshold can be dynamically tuned according to the error and state signals on the basis of linear copositive lyapunov function and gain matrix decomposition stochastic stability conditions are developed under the framework of deception attacks next the design strategy of adaptive eventdriven controller is proposed to guarantee the positivity and stochastic stability of the underlying system in linear programming furthermore by means of a lower bound of inter execution time it is illustrated that there will be finite triggering within a limited time resulting in no zeno phenomenon finally a communication network model is shown to verify the theoretical findings
we propose a dimensionless bendability parameter ε1hw2t11 for wrinkling of thin twisted ribbons with thickness h width w and tensional strain t bendability permits efficient collapse of data for wrinkle onset wavelength critical stress and residual stress demonstrating longitudinal wrinklings primary dependence on this parameter this parameter also allows us to distinguish the highly bendable range ε120 from moderately bendable samples ε1∈020 we identify scaling relations to describe longitudinal wrinkles that are valid across our entire set of simulated ribbons when restricted to the highly bendable regime simulations confirm theoretical nearthreshold nt predictions for wrinkle onset and wavelength
wireless power transfer has the potential to seamlessly power electronic systems such as electric vehicles industrial robots and mobile devices however the leakage magnetic field is a critical bottleneck that limits the transferable power level and heavy ferromagnetic shields are needed for transferring large amounts of power in this letter we propose a ferriteless coil design that generates an asymmetric magnetic field pattern focused on one side of the resonator which effectively reduces the leakage magnetic field the key to enabling the asymmetric field pattern is a coil winding strategy inspired by the halbach array a permanent magnet arrangement which is then tailored for wireless power using an evolutionary strategy algorithm numerical analyses and simulations demonstrated that the proposed coil structure delivers the same amount of power as spiral coils while achieving an 866 reduction in magnetic field intensity at a plane located 75 mm away from the resonator pair and a power efficiency of 960 we verified our approach by measuring the power efficiency and magnetic field intensity of a test wireless power system operating at 678 mhz these findings indicate that our approach can efficiently deliver over 50 times more power without increasing magnetic field exposure making it a promising solution for highpower wireless power transfer applications
the emergence of digital music platforms has fundamentally transformed the way we engage with and organize music as playlist creation has gained widespread popularity there is an increasing desire among music aficionados and industry experts to comprehend the factors that drive playlist success this paper presents a machine learningbased approach designed to predict the success of music playlists by analyzing various musical characteristics of songs our model achieves an impressive accuracy of 896 in predicting playlist success notably it exhibits a remarkable 920 accuracy in forecasting the success of popular playlists while also effectively identifying unpopular playlists with an accuracy of 894 these findings provide invaluable insights into playlist creation ultimately enhancing the overall musiclistening experience by harnessing the power of machine learning our proposed approach unlocks new prospects for optimizing playlist design strategies and delivering personalized music recommendations this has significant ramifications for music enthusiasts and industry professionals seeking to elevate playlist creation and enrich the music consumption experience
in this paper we put forward a proof of concept for sixth generation 6g terabit infrared ir laserbased indoor optical wireless networks we propose a novel doubletier access point ap architecture based on an array of arrays of vertical cavity surface emitting lasers vcsels to provide a seamless gridofbeam coverage with multigbs per beam we present systematic design and thorough analytical modeling of the ap architecture which are then applied to downlink system modeling using nonimaging angle diversity receivers adrs we propose static beam clustering with coordinated multibeam joint transmission combjt for network interference management and devise various clustering strategies to address interbeam interference ibi and intercluster interference ici nonorthogonal multiple access noma and orthogonal frequency division multiple access ofdma schemes are also adopted to handle intracluster interference and the resulting signaltointerferenceplusnoise ratio sinr and achievable data rate are derived the network performance is studied in terms of spatial distributions and statistics of the downlink sinr and data rate through extensive computer simulations the results demonstrate that data rates up to 15 gbs are achieved within the coverage area and a properly devised clustering strikes a balance between the sum rate and fairness depending on the number of users
the fiberfilled polymer composite is one of the best materials which provides a symmetrical superior strength and stiffness to structures with the strengthening of people’s environmental protection and resource reuse consciousness the development of renewable materials especially natural fiberfilled polymer composites is receiving great attention this study investigated the mechanical properties of polymer composites incorporating waste materials from the food processing industry and agricultural sources waste vegetal fiberfilled polymer biocomposites wvffpbs with varying fiber types and 3d printing orientations were systematically fabricated subsequently the tensile tests were executed to comprehensively assess the anisotropic mechanical behaviors of the wvffpbs the results demonstrated that wvffpbs performed excellent anisotropic mechanical properties compared to pristine matrix samples as print orientation changed as the printing angle increased from 0° to 90° the tensile mechanical properties of the wvffpbs displayed a decreasing trend moreover the print orientation–anisotropic mechanical behavior relationship of 3dprinted wvffpbs was revealed through the analysis of the material manufacturing characteristics and damage features
this article proposes a high stepup dc–dc converter based on a transinverse impedancesource structure in which the voltage gain of the converter is increased by using a lower number of turns ratio of the coupled inductors ci windings the proposed converter achieves a very high voltage gain and a very low voltage stress on the switches an active clamp is incorporated into the topology of the proposed converter helping to absorb the energy of the leakage inductances of the ci and to recycle that energy to the output of the converter to further increase the voltage gain furthermore the active clamp is used to realize soft turn on for the main switch of the converter additionally the clamp switch also turns on under zero voltage switching condition apart from the soft turn on of the switches of the proposed converter the diodes of the converter also turn off with a minimized reverse recovery power loss because of the controlled current falling rate of these diodes with the leakage inductance of the ci the operation principle steadystate analysis and comparison are presented in this article also the analysis is verified using a 200 w and 20 v to 400 v experimental setup
the internet of things iot is an emerging technological advancement with significant implications it connects a wireless sensor or node network via lowpower and lossy networks lln the routing protocol over a lowpower and lossy network rpl is the fundamental component of lln its lightweight design effectively addresses the limitations imposed by bandwidth energy and memory on both llns and iot devices notwithstanding its efficacy rpl introduces susceptibilities including the version number attack vna which underscores the need for iot systems to implement effective security protocols this work reviews and categorizes the security mechanisms proposed in the literature to detect vna against rplbased iot networks the existing mechanisms are thoroughly discussed and analyzed regarding their performance datasets implementation details and limitations furthermore a qualitative comparison is presented to benchmark this work against existing studies showcasing its uniqueness finally this work analyzes research gaps and proposes future research avenues
the understanding of largescale scientific software poses significant challenges due to its diverse codebase extensive code length and target computing architectures the emergence of generative ai specifically large language models llms provides novel pathways for understanding such complex scientific codes this paper presents s3llm an llmbased framework designed to enable the examination of source code code metadata and summarized information in conjunction with textual technical reports in an interactive conversational manner through a userfriendly interface s3llm leverages opensource llama2 models to enhance code analysis through the automatic transformation of natural language queries into domainspecific language dsl queries specifically it translates these queries into feature query language fql enabling efficient scanning and parsing of entire code repositories in addition s3llm is equipped to handle diverse metadata types including dot sql and customized formats furthermore s3llm incorporates retrieval augmented generation rag and langchain technologies to directly query extensive documents s3llm demonstrates the potential of using locally deployed opensource llms for the rapid understanding of largescale scientific computing software eliminating the need for extensive coding expertise and thereby making the process more efficient and effective s3llm is available at httpsgithubcomresponsibleailabs3llm
magnetostrictive alloys hold great promise for energy harvesting applications due to their inherent durability however their implementation often results in usable voltage ranges that fall significantly below common electronic standards like 16 33 and 5 volts consequently the utilization of electronic circuits becomes essential to amplify the voltage and enhance energy conversion efficiency over the past few decades numerous conversion techniques have been devised for other intelligent materials such as piezoelectrics some of which have even made their way into commercial products surprisingly there is a dearth of specialized techniques if not a complete absence tailored to magnetostrictive devices among potential solutions a suitable ac–dc boost converter stands out as a highly promising candidate for addressing this challenge but this solution has never been fully characterized then this paper presents thorough experimental validations of such a converter driven by a realtime arduino board equipped to measure source time periods and operate under various conditions we present several cases demonstrating the circuit’s substantial potential for enhancing energy harvesting from magnetostrictive materials
we developed a fluorescent molecular probe based on gold nanoparticles functionalized with nn′bis21piperazinoethyl34910perylenetetracarboxylic acid diimide dihydrochloride and these probes exhibit potential for applications in microscopic thermometry the intensity of fluorescence was affected by changes in temperature chemical environments such as different buffers with the same ph also resulted in different fluorescence intensities due to the fluorescence intensity changes exhibited by modified gold nanoparticles these materials are promising candidates for future technologies involving microscopic temperature measurements
the reduced‐order observer‐based finite‐time control problem for one‐sided lipschitz nonlinear switched singular systems is addressed in this paper first the design method of the reduced‐order observer is given via state transformation then based on the average dwell time adt approach some new sufficient conditions for regularity impulse‐freeness have a unique solution and finite‐time boundedness ftb of the dynamic augmented systems are obtained by exploring the reduced‐order observer‐based controller further the lower finite‐time bound can be obtained by using singular value decomposition method and the state feedback gain and the observer gain are computed by solving linear matrix inequalities lmis finally the validity of the obtained method is illustrated by means of a numerical example and a dc motor system
we study data corruption robustness for reinforcement learning with human feedback rlhf in an offline setting given an offline dataset of pairs of trajectories along with feedback about human preferences an varepsilonfraction of the pairs is corrupted eg feedback flipped or trajectory features manipulated capturing an adversarial attack or noisy human preferences we aim to design algorithms that identify a nearoptimal policy from the corrupted data with provable guarantees existing theoretical works have separately studied the settings of corruption robust rl learning from scalar rewards directly under corruption and offline rlhf learning from human feedback without corruption however they are inapplicable to our problem of dealing with corrupted data in offline rlhf setting to this end we design novel corruption robust offline rlhf methods under various assumptions on the coverage of the datagenerating distributions at a high level our methodology robustifies an offline rlhf framework by first learning a reward model along with confidence sets and then learning a pessimistic optimal policy over the confidence set our key insight is that learning optimal policy can be done by leveraging an offline corruptionrobust rl oracle in different ways eg zeroorder oracle or firstorder oracle depending on the data coverage assumptions to our knowledge ours is the first work that provides provable corruption robust offline rlhf methods
reinforcement learning rl has shown remarkable success in solving relatively complex tasks yet the deployment of rl systems in realworld scenarios poses significant challenges related to safety and robustness this paper aims to identify and further understand those challenges thorough the exploration of the main dimensions of the safe and robust rl landscape encompassing algorithmic ethical and practical considerations we conduct a comprehensive review of methodologies and open problems that summarizes the efforts in recent years to address the inherent risks associated with rl applications after discussing and proposing definitions for both safe and robust rl the paper categorizes existing research works into different algorithmic approaches that enhance the safety and robustness of rl agents we examine techniques such as uncertainty estimation optimisation methodologies explorationexploitation tradeoffs and adversarial training environmental factors including simtoreal transfer and domain adaptation are also scrutinized to understand how rl systems can adapt to diverse and dynamic surroundings moreover human involvement is an integral ingredient of the analysis acknowledging the broad set of roles that humans can take in this context importantly to aid practitioners in navigating the complexities of safe and robust rl implementation this paper introduces a practical checklist derived from the synthesized literature the checklist encompasses critical aspects of algorithm design training environment considerations and ethical guidelines it will serve as a resource for developers and policymakers alike to ensure the responsible deployment of rl systems in many application domains
urban areas worldwide are experiencing escalating temperatures due to the combined effects of climate change and urbanization leading to a phenomenon known as urban overheating understanding the spatial distribution of land surface temperature lst and its driving factors is crucial for mitigation and adaptation of urban overheating so far there has been an absence of investigations into spatiotemporal patterns and explanatory factors of lst in the city of addis ababa the study aims to determine the spatial patterns of land surface temperature analyze how the relationships between lst and its factors vary across space and compare the effectiveness of using ordinary least squares and geographically weighted regression to model these connections the findings showed that the spatial patterns of lst show statistically significant hot spot zones in the northcentral parts of the study area moran’s i  0172 the relationship between lst and its explanatory variables were modelled using ordinary least square model and thereby tested if there is spatial dependence in the model using the koenker bp statisticthe result revealed nonstationarity p  0000 and consequently geographically weighted regression was employed to compare the performance with ols the research has revealed that gwr r2  057 aic  10521 is more effective technique than ols r2  042 aic  21620 for studying the relationship lst and the selected explanatory variables the use of gwr has improved the accuracy of the model by capturing the spatial heterogeneity in the relationship between land surface temperature and its explanatory variables the relationship between lst and its explanatory variables were modelled using ordinary least square model and thereby tested if there is spatial dependence in the model using the koenker bp statistic the result revealed nonstationarity p  0000 and consequently geographically weighted regression was employed to compare the performance with ols the research has revealed that gwr r2  057 aic  10521 is more effective technique than ols r2  042 aic  21620 for studying the relationship lst and the selected explanatory variables the use of gwr has improved the accuracy of the model by capturing the spatial heterogeneity in the relationship between land surface temperature and its explanatory variables consequently localized understanding of the spatial patterns and the driving factors of lst has been formulated
longterm retention is one of the major challenges concerning the reliability of redoxbased resistive switching random access memories based on the valence change mechanism vcm the stability of the programmed state has to be ensured over several years leaving a sufficient read window between the states which is even more challenging at large statistics thus the underlying physical mechanisms have to be understood and experimental data have to be evaluated accurately here it shows that the retention behavior of the high resistive state hrs is more complex than that of the low resistive state and requires a different evaluation method in this work we experimentally investigate the retention behavior of 5m vcm devices via accelerated life testing and show the difficulties of commonly used evaluation methods in view of the hrs subsequently we present a new evaluation method focusing on the standard deviation of the hrs current distribution hereby an activation energy for the degradation process can be extracted which is essential for the prediction of the devices’ behavior under operating conditions furthermore we reproduce the experimentally observed behavior with our 3d kinetic monte carlo simulation model we confirm the plausibility of our evaluation method and are able to connect the calculated activation energy to the migration barriers of oxygen vacancies that we implemented in the model and that we believe play a key role in the experimentally observed degradation process
background this study used chatgpt for sentiment analysis to investigate the possible links between online sentiments and covid19 vaccination rates it also examines internet posts to understand the attitudes and reasons associated with vaccinerelated opinions methods we collected 500558 posts over 60 weeks from the blind platform mainly used by working individuals and 854 relevant posts were analyzed after excluding duplicates and irrelevant content attitudes toward and reasons for vaccine opinions were studied through sentiment analysis the study further correlated these categorized attitudes with the actual vaccination data results the proportions of posts expressing positive negative and neutral attitudes toward covid19 vaccines were 5 83 and 12 respectively the total post count showed a positive correlation with the vaccination rate indicating a high correlation between the number of negative posts about the vaccine and the vaccination rate negative attitudes were predominantly associated with societal distrust and perceived oppression conclusions this study demonstrates the interplay between public perceptions of covid19 vaccines as expressed through social media and vaccination behavior these correlations can serve as useful clues for devising effective vaccination strategies
this article investigates conceptions of morality within the framework of ressentimentful victimhood in the manosphere while also exploring avenues for resistance among young individuals encountering the “hatred pipeline” in study 1 we use the emotional mechanism of ressentiment to examine how incels construct narratives of victimhood rooted in the notion of sexual entitlement that remains owed and unfulfilled alongside its “black pill” variant emphasising moral and epistemic superiority through a linguistic corpus analysis and content examination of 4chan and incelis blog posts we find evidence of ressentiment morality permeating the language and communication within the incel community characterised by blame directed at women and the pervasive themes of victimhood powerlessness and injustice in study 2 we delve into young individuals’ reflections on incel morality and victimhood narratives as they engage with online networks of toxic masculinity in the manosphere drawing from semistructured interviews with young participants who have accessed the manosphere we explore their perceptions of risks attribution of blame and experiences of empathy towards individuals navigating the “hatred pipeline” our analysis underscores the significance of ressentiment in elucidating alternative conceptions of morality and victimhood while shedding light on the potential for acceptance or resistance within online environments characterised by hatred
this letter investigates the ergodic capacity of an ambient backscatter system ambs that comprises an ambient source a reader and multiple tags under radiofrequency hardware impairments rfhis the considered ambs adopts a multitag selection scheme where a selected tag backscatters the ambient source’s rf signals to communicate with the reader we assume that the ambient source and reader are fabricated using a lowcost hardware that results in residual rfhis at these nodes for this setup we derive an accurate ergodic capacity expression by considering nakagami m fading channels we present numerical analysis to unveil the associated system performance gains
nothing is simpler than binary successfailure models for reliability or any other quality of the system a lot of effort has been made to extend these models to multistates for components as well as the systems some progress has also been made with continuous state space for components and system another approach is to model the success or failure state using fuzzy sets reliability or any quality must be defined and evaluated by the customer and a good reliability measure should not only capture the state transitions of the multistate system but also capture the customers total experience with the system over time
crystallization kinetic parameter estimation is important for the classification design and scaleup of pharmaceutical manufacturing processes this study investigates the impact of supersaturation and temperature on the induction time nucleation rate and growth rate for the compounds lamivudine slow kinetics and aspirin fast kinetics adaptive bayesian optimization adbo has been used to predict experimental conditions that achieve target crystallization kinetic values for each of these parameters of interest the use of adbo to guide the choice of the experimental conditions reduced material usage up to 5fold when compared to a more traditional statistical design of experiments doe approach the reduction in material usage demonstrates the potential of adbo to accelerate process development as well as contribute to netzero and green chemistry strategies implementation of adbo can lead to reduced experimental effort and increase efficiency in pharmaceutical crystallization process development the integration of adbo into the experimental development workflows for crystallization development and kinetic experiments offers a promising avenue for advancing the field of autonomous data collection exploiting digital technologies and the development of sustainable chemical processes
the reliable operation of industrial equipment is imperative for ensuring both safety and enhanced production efficiency machine learning technology particularly the light gradient boosting machine lightgbm has emerged as a valuable tool for achieving effective fault warning in industrial settings despite its success the practical application of lightgbm encounters challenges in diverse scenarios primarily stemming from the multitude of parameters that are intricate and challenging to ascertain thus constraining computational efficiency and accuracy in response to these challenges we propose a novel innovative hybrid algorithm that integrates an arithmetic optimization algorithm aoa simulated annealing sa and new search strategies this amalgamation is designed to optimize lightgbm hyperparameters more effectively subsequently we seamlessly integrate this hybrid algorithm with lightgbm to formulate a sophisticated fault warning system validation through industrial case studies demonstrates that our proposed algorithm consistently outperforms advanced methods in both prediction accuracy and generalization ability in a realworld water pump application the algorithm we proposed achieved a fault warning accuracy rate of 90 compared to three advanced algorithms namely improved social engineering optimizerbackpropagation network iseobp long shortterm memoryconvolutional neural network lstmcnn and grey wolf optimizerlight gradient boosting machine gwolightgbm its root mean square error rmse decreased by 714 1784 and 1316 respectively at the same time its rsquared value increased by 215 702 and 373 respectively lastly the method we proposed also holds a leading position in the success rate of a water pump fault warning this accomplishment provides robust support for the timely detection of issues thereby mitigating the risk of production interruptions
preserving sensor nodes’ energy in underwater sensor networks uwsns stands as a crucial priority uwsns find key applications in ocean monitoring offshore oil and gas exploration and underwater robotic operations our study introduces a novel approach ctsp utilizing clustering alongside the traveling salesman ts protocol to optimize data routing in uwsns while minimizing energy usage by adjusting the pathways of data transmission among sensor nodes this method aims to curtail the network’s overall power consumption ctsp primarily relies on two fundamental components ts and clustering leveraging the ts protocol allows the determination of the most efficient route between any pair of sensor nodes within the network the approach ensures that each sensor node transmits data solely to its nearest neighbor thereby reducing the energy required for transmission utilizing the positions of sensor nodes as input a clustering algorithm forms larger groups enhanced communication within clusters and reduced longrange communication between clusters contribute to energy conservation simulation results demonstrate that the proposed method significantly diminishes power consumption compared to traditional routing methods like the leach algorithm precisely the ctsp method exhibits the potential to reduce energy usage by up to 50 presenting a feasible option for energyefficient data routing in underwater settings
the main purpose of this paper is to investigate the axisymmetric bending response of functionally graded porous fgp circular plates the material properties are changed continuously in the thickness direction of the plate three distinct porosity distributions uniform symmetric and monolithic are employed the effect of porosity on the axisymmetric bending analysis of circular plates is examined parametrically in this study clamped and roller supports which commonly serve to achieve ideal boundary conditions in numerous engineering applications are used the finite element method is employed for numerical analysis the principal of the potential energy is used to obtain the governing equations to generate the model of the fgp circular plates an eightnode quadratic quadrilateral element with two degrees of freedom on each node is utilized the results of this study are confirmed by the existing published literature a good agreement between the results of the presented model and the previous literature has been observed the results of the present study show that plate deflection increases with the increase of the porosity coefficient and the ratio of radius to thickness of circular plates by increasing the porosity coefficient the displacement values of the plates made of uniform porosity distribution is effected more than those of other porosity distributions
during the development of an internal combustion enginebased powertrain traditional procedures for control strategies calibration and validation produce huge amount of data that can be used to develop innovative datadriven applications such as emission virtual sensing one of the main criticalities is related to the data quality that cannot be easily assessed for such a big amount of data this work focuses on an emission modeling activity using an enhanced light gradient boosting regressor and a dedicated data preprocessing pipeline to improve data quality first thing a software tool is developed to access a database containing data coming from emissions tests the tool performs a data cleaning procedure to exclude corrupted data or invalid parts of the test moreover it automatically tunes model hyperparameters it chooses the best set of features and it validates the procedure by comparing the estimation and the experimental measurement the proposed preprocessing pipeline shows an improvement in terms of accuracy demonstrating the utility of using large training data which cover a wide set of vehicle maneuvers thus custom designed tests are performed for dataset enrichment allowing the model to predict nonconventional conditions of aftertreatment systems inefficiency real case applications of the proposed model are exposed such as emission estimation in nonmeasurable conditions virtual assessment of the impact of new control strategy calibration on emissions alignment of emission measurements with all other vehicle signals finally a principal component analysisbased algorithm is developed to assess the epistemic uncertainty of the model and the prediction reliability during inference
adpdependent glucokinase adpgk produces glucose6phosphate with adenosine diphosphate adp as the phosphate group donor in contrast to atpdependent hexokinases hks originally found in archaea adpgk is involved in glycolysis however its biological function in most eukaryotic organisms is still unclear and the molecular mechanism of action requires further investigation this paper provides a concise overview of adpgk’s origin biological function and clinical application it aims to furnish scientific information for the diagnosis and treatment of human metabolic diseases neurological disorders and malignant tumours and to suggest new strategies for the development of targeted drugs
background mobile health mhealth interventions have the potential to deliver longitudinal support to users outside of episodic clinical encounters we performed a qualitative substudy to assess the acceptability of a text message‐based mhealth intervention designed to increase and sustain physical activity in cardiac rehabilitation enrollees methods and results semistructured interviews were conducted with intervention arm participants of a randomized controlled trial delivered to low‐ and moderate‐risk cardiac rehabilitation enrollees interviews explored participants interaction with the mobile application reflections on tailored text messages integration with cardiac rehabilitation and opportunities for improvement transcripts were thematically analyzed using an iteratively developed codebook sample size consisted of 17 participants with mean age of 657 sd 82 years 29 were women 29 had low functional capacity and 12 were non‐white four themes emerged from interviews engagement health impact personalization and future directions participants engaged meaningfully with the mhealth intervention finding it beneficial in promoting increased physical activity however participants desired greater personalization to their individual health goals fitness levels and real‐time environment generally those with lower functional capacity and less experience with exercise were more likely to view the intervention positively finally participants identified future directions for the intervention including better incorporation of exercise physiologists and social support systems conclusions cardiac rehabilitation enrollees viewed a text message‐based mhealth intervention favorably suggesting the potentially high usefulness of mhealth technologies in this population addressing participant‐identified needs on increased user customization and inclusion of clinical and social support is crucial to enhancing the effectiveness of future mhealth interventions registration url httpswwwclinicaltrialsgov unique identifier nct04587882
gas turbines are thermoelectric plants with various applications such as largescale electricity production petrochemical industry and steam generation in order to optimize the operation of a gas turbine it is necessary to develop system identification models that allow for the development of studies and analyses to increase the system’s reliability current strategies for modeling complex and nonlinear systems can be based on artificial intelligence techniques using autoregressive neural networks of the narx and lstm type in this context this work aims to develop a model of a gas turbine capable of estimating the rotation speed of the turbine and simultaneously estimating the uncertainty associated with the estimation these methodologies are based on artificial neural networks and the monte carlo dropout simulation method the results were obtained from experimental data from a 215 mw gas turbine getting the best model with a mape of 002 and an uncertainty associated with the turbine rotation speed of 22 rpm
the purpose of aspectbased sentiment analysis absa is to determine the sentiment polarity of aspects in a given sentence most historical works on sentiment analysis used complex and inefficient methods to integrate external knowledge furthermore they fell short of completely utilizing bert’s potential because when trying to generate word embeddings they merely averaged the bert subword vectors to overcome these limitations we propose a knowledgeguided heterogeneous graph convolutional network for aspectbased sentiment analysis khgcn specifically we consider merging subword vectors utilizing a dynamic weight mechanism in the bert embedding layer additionally heterogeneous graphs are constructed to fuse different feature associations between words and graph convolutional networks are utilized to identify contextspecific syntactic features furthermore by embedding a knowledge graph the model can learn additional features from sources other than the corpus based on this knowledge it is consequently possible to obtain more knowledge representation for a particular aspect by utilizing the attention mechanism last but not least semantic features syntactic features and knowledge are dynamically combined using feature fusion experiments on three public datasets demonstrate that our model achieves accuracy rates of 8087 8542 and 9107 which is an improvement of more than 2 compared to other benchmark models based on hgcns and bert
in an everchanging visual world animals’ survival depends on their ability to perceive and respond to rapidly changing motion cues the primary visual cortex v1 is at the forefront of this sensory processing orchestrating neural responses to perturbations in visual flow however the underlying neural mechanisms that lead to distinct cortical responses to such perturbations remain enigmatic in this study our objective was to uncover the neural dynamics that govern v1 neurons’ responses to visual flow perturbations using a biologically realistic computational model by subjecting the model to sudden changes in visual input we observed opposing cortical responses in excitatory layer 23 l23 neurons namely depolarizing and hyperpolarizing responses we found that this segregation was primarily driven by the competition between external visual input and recurrent inhibition particularly within l23 and l4 this division was not observed in excitatory l56 neurons suggesting a more prominent role for inhibitory mechanisms in the visual processing of the upper cortical layers our findings share similarities with recent experimental studies focusing on the opposing influence of topdown and bottomup inputs in the mouse primary visual cortex during visual flow perturbations
the success or failure of a project can be influenced by the lack of planning of project activities and the supervision carried out is ineffective and has an impact on the project running inefficiently then it will result in delays and a decrease in the quality of work and have an impact on swelling the project budget project management is needed in order to manage projects starting from implementation to the end of a project this research uses a case study on the pelita ilmu foundation building construction project especially architectural work this building is located on jl rawa kuda kp rawa kuda rtrw 004002 karangharum kedungwaringin district bekasi regency west java 17540 1 the s curve is used to find out how long a project takes to complete the aim of this research is to determine time efficiency in terms of the s curve graph the results of time management analysis using the s curve on the graph showed that the realization of time delays was not in accordance with the plan
time series data is widely used for decisionmaking and advanced analytics such as forecasting however the vast data volumes make storage challenging using lossy compression can save more space compared to lossless methods but it can affect the forecasting accuracy understanding the impact of lossy compression on forecasting accuracy is a multifaceted challenge necessitating experimental evaluation across various forecasting models compression methods and time series this paper conducts such experimental evaluation by combining seven forecasting models three lossy compression algorithms and six datasets by simulating a reallife scenario where forecasting models use lossy compressed data for prediction we address three main research questions related to compression error and its effects on the time series characteristics and the forecasting models the results show that the poor man’s compression and swing filterlossycompressionalgorithmsaddlesserrorthanthe squeeze method as the error bound increases poor man’s compression provides the best balance between compression ratio and forecasting accuracy specifically we obtained an average compression ratio of 1365 556 and 1497 for pmc swing and sz with an average impact on forecasting accuracy of 556 33 and 85 respectively an analysis of several time series characteristics shows that the maximum kullbackleibler divergence between consecutive windows in the time series is the best indicator of the impact of lossy compression on forecasting accuracy finally our results indicate that simple models like arima are more resilient to lossy compression than complex deep learning models the source code and data
multitopology routing mtr provides an attractive alternative to segment routing for traffic engineering when network devices cannot be upgraded however due to a high overhead in terms of link state messages exchanged by topologies and the need to frequently update link weights to follow evolving network conditions mtr is often limited to a small number of topologies and the satisfaction of loose qos constraints to overcome these limitations we propose vmtr an mtr extension where demands are routed over virtual topologies that are silent ie they do not exchange lsa messages and that are continuously derived from a very limited set of real topologies optimizing each a qos parameter in this context we present a polynomial and exact algorithm for vmtr and as a benchmark a local search algorithm for mtr we show that vmtr helps reducing drastically the number of real topologies and that it is more robust to qos changes
the traditional rulebased energy management strategy for plugin hybrid vehicles has issues such as difficulty in online correction and limited online optimization capabilities in addition the global optimization energy management strategy cannot be applied online or in realtime considering the above difficulties this study proposes a realtime optimization energy management strategy based on the markov chain for driving condition prediction and online optimization with the minimum principle to verify the proposed control strategy the plugin hybrid vehicle dynamics model driving condition prediction model and online optimization control model were first established the initial value of the battery state of charge was set to 04 under the udds urban dynamometer driving schedule standard cycle the simulation results showed that the comprehensive fuel consumption cost was 166 yuan which was 828 better than the energy economy of the traditional rulebased energy management strategy at the same time a complete vehicle test was also conducted based on a sample vehicle test platform the experimental results indicated that the energy management strategy proposed herein exhibits better fuel economy compared to that exhibited by the traditional rulebased energy management strategy simulations and experiments have verified the effectiveness of the proposed control strategy in this study
hardware trojans malicious components that attempt to prevent a chip from operating as expected are carefully crafted to circumvent detection during the predeployment silicon design and verification stages they are an emerging threat being investigated by academia the military and industry therefore runtime hardware trojan detection is critically needed as the final layer of defense during chip deployment and in this work we focus on hardware trojans that target the processor’s performance current stateoftheart detectors watch hardware counters for anomalies using complex machinelearning models which require a dedicated offchip processor and must be trained extensively for each target processor in this work we propose a lightweight solution that uses data from a single reference run to accurately determine whether a trojan is slowing processor performance across cpu configurations without the need for new profiles to accomplish this we use an analytical model based on the application’s inherent microarchitecturally independent characteristics such models determine the expected microarchitectural events across different processor configurations without requiring reference values for each applicationhardware configuration pair by comparing predicted values to actual hardware events one can quickly check for unexpected application slowdowns that are the key signatures of many hardware trojans the proposed methodology achieves a higher true positive rate tpr compared to prior works while having no false positives the proposed detector incurs no runtime performance penalty and only adds a negligible power overhead of 0005
decisions around distributing available funds among potential land consolidation projects require a thorough analysis in order to maximize the effects of land consolidation in order to avoid choosing the wrong land consolidation projects different methods can be used generally there are two possible groups of methods one based on a qualitative approach delphi swot and one based on a quantitative approach ahp vikor saw topsis etc in this research the focus was on the sensitivity of the resulting rankings affected by varying the input data in multicriteria analysis methods with an emphasis on the variation in the weight and the choice of criteria this research was motivated by the subjective character of the choice of criteria and their weighting before applying the multicriteria analysis methods four methods were included ahp topsis vikor and saw for the multicriteria analysis with three ways of defining weights consistent modified or quasiconsistent and freely determined without taking consistency into account in order to determine the influence of the different methods on the final ranking the weights were defined only by an acceptable interval of values the sensitivity of the methods was investigated using the differences in the obtained rankings between each method a case study is provided on real data and the results are discussed the results showed a relatively small variance and possible equal rankings of projects by means of statistical analyses this finding opens up the possibility of the valuation of projects instead of simple rankings
academic achievement is a critical measure of intellectual ability prompting extensive research into cognitive tasks as potential predictors neuroimaging technologies such as functional nearinfrared spectroscopy fnirs offer insights into brain hemodynamics allowing understanding of the link between cognitive performance and academic achievement herein we explored the association between cognitive tasks and academic achievement by analyzing prefrontal fnirs signals a novel quantum annealer qa feature selection algorithm was applied to fnirs data to identify cognitive tasks correlated with csat scores twelve features signal mean median variance peak number of peaks sum of peaks slope minimum kurtosis skewness standard deviation and root mean square were extracted from fnirs signals at two time windows 10 and 60second to compare results from various feature variable conditions the feature selection results from the qabased and xgboost regressor algorithms were compared to validate the formers performance in a threestep validation process using multiple linear regression models correlation coefficients between the feature variables and the csat scores model fitness adjusted r2 and model prediction error rmse values were calculated the quantum annealer demonstrated comparable performance to classical machine learning models and specific cognitive tasks including verbal fluency recognition and the corsi block tapping task were correlated with academic achievement group analyses revealed stronger associations between tower of london and nback tasks with higher csat scores quantum annealing algorithms have significant potential in feature selection using fnirs data and represents a novel research approach future studies should explore predictors of academic achievement and cognitive ability
olfaction is understudied in neuroimaging research compared to other senses but there is growing evidence of its therapeutic benefits on mood and wellbeing olfactory imagery can provide similar health benefits as olfactory interventions harnessing crossmodal visualolfactory interactions can facilitate olfactory imagery understanding and employing these crossmodal interactions between visual and olfactory stimuli could aid in the research and applications of olfaction and olfactory imagery interventions for health and wellbeing this review examines current knowledge debates and research on olfaction olfactive imagery and crossmodal visualolfactory integration a total of 56 papers identified using the prisma method were evaluated to identify key brain regions research themes and methods used to determine the suitability of fnirs as a tool for studying these topics the review identified fnirscompatible protocols and brain regions within the fnirs recording depth of approximately 15 cm associated with olfactory imagery and crossmodal visualolfactory integration commonly cited regions include the orbitofrontal cortex inferior frontal gyrus and dorsolateral prefrontal cortex the findings of this review indicate that fnirs would be a suitable tool for research into these processes additionally fnirs suitability for use in naturalistic settings may lead to the development of new research approaches with greater ecological validity compared to existing neuroimaging techniques
there are now a range of potential data sources for wind and surface wave conditions within tropical cyclones these sources include in situ buoy data and remote sensing data from satellite altimeters scatterometers and radiometers in addition data providing estimates of tropical cyclone tracks and wind field parameters are available from best track archives the present dataset brings together this information in a single archive providing the available data for each tropical cyclone from each of the data sources in a single file the data consists of observations in a total of 2927 global tropical cyclones over the period from 1985 to 2017 global statistics of the observations are provided along with data on the geographic distribution of tropical cyclones within the database
this paper proposes a new pipeline for longtail lt recognition instead of reweighting or resampling we utilize the longtailed dataset itself to generate a balanced proxy that can be optimized through crossentropy ce specifically a randomly initialized diffusion model trained exclusively on the longtailed dataset is employed to synthesize new samples for underrepresented classes then we utilize the inherent information in the original dataset to filter out harmful samples and keep the useful ones our strategy diffusion model for longtail recognition diffult represents a pioneering utilization of generative models in longtail recognition diffult achieves stateoftheart results on cifar10lt cifar100lt and imagenetlt surpassing the best competitors with nontrivial margins abundant ablations make our pipeline interpretable too the whole generation pipeline is done without any external data or pretrained model weights making it highly generalizable to realworld longtailed settings
the issue of ssi involves how the ground or soil reacts to a building built on top of it both the character of the structure and the nature of the soil have an impact on the stresses that exist between them which in turn affects how the structure and soil beneath it move the issue is crucial particularly in earthquake regions the interaction between soil and structure is an extremely intriguing factor in increasing or reducing structural damage or movement structures sitting on deformable soil as opposed to strong soil will experience an increase in static settlement and a decrease in seismic harm the engineer must take into account that the soil liquefaction problem occurs for soft ground in seismic areas a reinforced concrete wallframe dual frameworks dynamic reaction to ssi has not been sufficiently studied and is infrequently taken into consideration in engineering practice the structures’ seismic performance when ssi effects are taken into account is still unknown and there are still some misconceptions about the ssi idea especially regarding rc wallframe dual systems the simulation study of the soil beneath the foundations significantly impacts the frameworks frequency response and dynamic properties therefore the overall significance of ssi in the structural aspect and sustainability aspects will be reviewed in this research
in the complex field of plant science knowledge of the many difficulties that plants encounter from both living and nonliving stresses is essential for maintaining biodiversity and managing natural resources in a sustainable manner in addition to guaranteeing global food security 
anomalous cosmicrays acrs are thought to be originated from the acceleration of pickup ions puis at the termination shock or interplanetary shocks and play important role for the plasma dynamics in the outer heliosphere due to limited observation the effects of acrs on the solar wind events is not well known under the approximation of spherical symmetry we have developed a threecomponent magnetohydrodynamic mhd numerical model that contains solar wind plasma interstellar neutral atoms and acrs to investigate the evolution of the solar wind within a heliocentric distance from 1 to 150 astronomical units au we use the solar wind observations from the omni database with the time from 20105 to 20160 decimal years at the inner boundary and the effect of acrs on the propagation of the solar wind events are compared with the observations from the spacecrafts of new horizons voyager 1 and 2 the results show that acrs may decrease the speed of the solar wind shocks to some extent and the effect is positively correlated with the diffusion coefficient a larger diffusion coefficient leads to a more pronounced effect moreover the acrs has a dissipation effect on the shocklike solar wind structures and may play important roles on the dynamics of solar wind in the outer heliosphere
the object of research is forecasting processes in the case of short sets of tabular data the subject of research is the data augmentation method for images achieving the goal occurs primarily from the study of existing machine learning tools and data augmentation methods for images further software development to implement various data augmentation methods and machine learning models for images approbation of the work was carried out by analyzing the effectiveness of various methods of data augmentation for images using quality metrics and statistical methods due to the results of the research an analysis of the influence of various methods of data augmentation on the effectiveness of classifiers in images was carried out
in this work an advanced 2d nonparametric correlogram method is presented to cope with outputonly measurements of linear slow timevarying systems the proposed method is a novel generalization of the kernel functionbased regularization techniques that have been developed for estimating linear timeinvariant impulse response functions in the proposed system identification technique an estimation method is provided that can estimate the timevarying auto and crosscorrelation function and indirectly the timevarying auto and crosscorrelation power spectrum estimates based on reallife measurements without measuring the perturbation signals the slow timevarying behavior means that the dynamic of the system changes as a function of time in this work a tailored regularization cost function is considered to impose assumptions such as smoothness and stability on the 2d auto and crosscorrelation function resulting in robust and uniquely determined estimates the proposed method is validated on two examples a simulation to check the numerical correctness of the method and a flutter test measurement of a scaled airplane model to illustrate the power of the method on a reallife challenging problem
in software development resolving the emergent issues within github repositories is a complex challenge that involves not only the incorporation of new code but also the maintenance of existing code large language models llms have shown promise in code generation but face difficulties in resolving github issues particularly at the repository level to overcome this challenge we empirically study the reason why llms fail to resolve github issues and analyze the major factors motivated by the empirical findings we propose a novel llmbased multiagent framework for github issue resolution magis consisting of four agents customized for software evolution manager repository custodian developer and quality assurance engineer agents this framework leverages the collaboration of various agents in the planning and coding process to unlock the potential of llms to resolve github issues in experiments we employ the swebench benchmark to compare magis with popular llms including gpt35 gpt4 and claude2 magis can resolve 1394 github issues significantly outperforming the baselines specifically magis achieves an eightfold increase in resolved ratio over the direct application of gpt4 the advanced llm
experimentation in practical endtoend e2e nextgeneration networks deployments is becoming increasingly prevalent and significant in the realm of modern networking and wireless communications research the prevalence of fifthgeneration technology 5g testbeds and the emergence of developing networks systems for the purposes of research and testing focus on the capabilities and features of analytics intelligence and automated management using novel testbed designs and architectures ranging from simple simulations and setups to complex networking systems however with the everdemanding application requirements for modern and future networks 5gandbeyond denoted as 5g testbed experimentation can be useful in assessing the creation of largescale network infrastructures that are capable of supporting e2e virtualized mobile network services to this end this paper presents a functional modular e2e 5g system complete with the integration of a radio access network ran and handling the connection of user equipment ue in realworld scenarios as well this paper assesses and evaluates the effectiveness of emulating full network functionalities and capabilities including a complete description of userplane data from ue registrations to communications sequences and leads to the presentation of a future outlook in powering new experimentation for 6g and nextgeneration networks
most of the digital soil mapping dsm products now available across the globe have been developed from the deposits of punctual soil observations inherited from several decades of soil survey activity by using these legacy data as inputs for calibrating our dsm models we implicitly make the assumption that these legacy soil data are accurate and therefore do not affect significantly our dsm products however this assumption has never been tested the objectives of this study were to evaluate the accuracies of soil property measurements retrieved from legacy soil profiles to analyse the different sources of error that may affect these measurements and to examine their impacts on the soil property predictions delivered by dsm models the study was focused on a control sampling within the coastal plain of languedoc southern france at 129 locations where legacy measured soil profiles were collected between 1955 and 1992 at each location four topsoil 0–20 cm samples were collected at increasing distances 0 5 25 and 100 m to characterize the local variabilities of the soil properties six soil properties—clay silt sand soil organic carbon calcium carbonate contents and cation exchange capacity—were determined for each sample using certified soil laboratory methods the results revealed that legacy soil property values had large overall errors and large biases biases likely induced by differences in soil analysis protocols could be corrected by linear functions calibrated onto the reference data obtained from the control sampling the contributions of the errors propagated from the manual geo‐referencing errors mean  31 m represented on average 52 of the errors after analytical bias corrections these errors exhibited large variations from one property to another due to differences in the short‐range spatial variations 0–100 m of these soil properties a dsm exercise conducted on our control sampling revealed that the errors of legacy soil data were propagated to the soil property predictions provided by the dsm models however this propagation could be largely mitigated by applying the above‐evoked corrections this study highlights the need to better control the quality of the legacy soil data used in dsm and to account for this source of uncertainty in the dsm models
connected and autonomous vehicles cav which interact with roadside units rsu as part of a smart city infrastructure are currently seeing first realworld deployments not only can cavs benefit from access to a cities’ infrastructure by obtaining data from various sensors eg video or lidar but they can also leverage the broad network coverage to offload complex computation tasks from their limited onboard hardware to scalable cloud resources furthermore a smart city supporting multiaccess edge computing mec can even provide safetyrelevant and timecritical services thanks to reduced latency and increased reliability this requires an algorithm to determine which vehicle offloads computation to which computation resource in the city this orchestration task is a challenging combinatorial problem subject to resource and quality of service constraints we present a novel and powerful yet surprisingly simple algorithm that provides a good and fast approximation to this problem this differentiable orchestrator converts a combinatorial problem into a softconstrained differentiable analog which can be solved very quickly we compare the proposed method with other heuristic methods and conclude that it significantly outperforms most competing methods in artificial examples and realistic scenarios in order to make the method as reproducible as possible and serve as a baseline for future research we make our data and simulations publicly available
gordon moores moores law suggests chip functionality demand doubles every 152 years with global semiconductor technology roadmap recommending subnanometer ranges for ic production in nanoelectronics the single electron transistor set is a promising nanoscale device that can be cointegrated with cmos technology to improve performance the paper explores the tunnelling effect between nanoparticles in single electron transistors set revealing coulomb blockade transactions and resistance increases with reduced bias it focuses on single electron transistors and preterminal devices discussing iv characteristics coulomb diamond plots and metallic quantum dots this research also explores the hybrid setfet based model focusing on developing a room temperature set in cmos comparable technology with a field effect transistor fet prediction models and exploratory studies guide the integration of setfet technology simon is a comprehensive simulator designed for singleelectron devices and circuits the output current is not impacted by capacitances up to 1 pf and fet size in the micron range are appropriate for set signal amplification up to 4 µa  this research explores the modelling of setfet technology in highlighting its ability to mitigate drawbacks in low drive current when combined with a fet it also explores device variability mitigation in nanoscale array configurations finding that setfet significantly reduces variability in larger arrays despite the impact of capacitance and resistance
in this paper we present a humanbased computation approach for the analysis of peripheral blood smear pbs images images in patients with sickle cell disease scd we used the mechanical turk microtask market to crowdsource the labeling of pbs images we then use the experttagged erythrocytesidb dataset to assess the accuracy and reliability of our proposal our results showed that when a robust consensus is achieved among the mechanical turk workers probability of error is very low based on comparison with expert analysis this suggests that our proposed approach can be used to annotate datasets of pbs images which can then be used to train automated methods for the diagnosis of scd in future work we plan to explore the potential integration of our findings with outcomes obtained through automated methodologies this could lead to the development of more accurate and reliable methods for the diagnosis of scd
this research study explores the use of mobile phone applications and smart monitoring in extended community and home rehabilitation services after stroke technically the system integrates advanced information technology such as mobile communication platforms and video information processing to improve the level of patient rehabilitation support specifically smart monitoring enables clinicians to personalize and track each patients recovery plan through realtime remote monitoring at the same time mobile phone applications provide patients with personalized and convenient rehabilitation services allowing them to receive professional guidance and therapeutic exercises anytime anywhere in addition the system also uses cloud technology to achieve the efficient mobile phone applications advanced cloud computing reduces system response time through preheating mechanisms improving efficiency and overall performance in general this system combines advanced technical means and is committed to providing more efficient personalized and convenient rehabilitation services for stroke patients helping to improve their quality of life and rehabilitation effects
implementing innovative farming practices becomes imperative for a country whose economy relies heavily on agricultural products over recent years the swift process of urbanization and the depletion of forests have influenced farmers due to the lack of rainwater harvesting and changing weather patterns many crop failure cases have been registered in the last few years to prevent loss of annual crop production many researchers propose the technologydriven smart farming method smart agriculture involves utilizing technology to create a controlled environment for the management of the crops smart farming increases crop production and provides small farmers with an alternative income source the government initiated many pilot projects to promote smart agriculture in india yet the absence of technological assistance and skilled procedures poses a challenge for most farmers aiming to thrive in this industry this paper introduces a smart freshwater recirculating aquaculture system based on iot technology the proposed system has integrated sensors and actuators the sensor system monitors the water parameters and actuators maintain the aquaculture environment an intelligent data analytics algorithm played a significant role in monitoring and maintaining the freshwater aquaculture environment the analytics derived the relationship between the water parameters and identified the relative change from the experimental evaluation we have identified that the m5 model tree algorithm has the highest accuracy for monitoring the relative change in water parameters
this study implements the knearest neighbors knn algorithm for classification tasks using the python programming language in both serial and parallel modes the execution times are measured and compared between the serial and parallel implementations demonstrating a performance improvement in the parallel execution the research aims to emphasize the significance of parallelization and computational efficiency enhancement in machine learning algorithms specifically focusing on knn using python and tools such as dask such studies are valuable for enhancing the performance of tasks related to data scientists and data analysts benefiting from the accessibility and ease of parallelization offered by python and related frameworks
wearable biosensor technology wbt has emerged as a transformative tool in the educational system over the past decade this systematic review encompasses a comprehensive analysis of wbt utilization in educational settings over a 10year span 2012–2022 highlighting the evolution of this field to address challenges in education by integrating technology to solve specific educational challenges such as enhancing student engagement monitoring stress and cognitive load improving learning experiences and providing realtime feedback for both students and educators by exploring these aspects this review sheds light on the potential implications of wbt on the future of learning a rigorous and systematic search of major academic databases including google scholar and scopus was conducted in accordance with the prisma guidelines relevant studies were selected based on predefined inclusion and exclusion criteria the articles selected were assessed for methodological quality and bias using established tools the process of data extraction and synthesis followed a structured framework key findings include the shift from theoretical exploration to practical implementation with eeg being the predominant measurement aiming to explore mental states physiological constructs and teaching effectiveness wearable biosensors are significantly impacting the educational field serving as an important resource for educators and a tool for students their application has the potential to transform and optimize academic practices through sensors that capture biometric data enabling the implementation of metrics and models to understand the development and performance of students and professors in an academic environment as well as to gain insights into the learning process
abstract cognitive decline is common among older individuals and although the underlying brain mechanisms are not entirely understood researchers have suggested using eeg frontal alpha activity during general anaesthesia as a potential biomarker for cognitive decline this is because frontal alpha activity associated with gabaergic general anaesthetics has been linked to cognitive function however oscillatoryspecific alpha power has also been linked with chronological age we hypothesize that cognitive function mediates the association between chronological age and oscillatoryspecific alpha power we analysed data from 380 participants aged over 60 with baseline screening assessments and intraoperative eeg we utilized the telephonic montreal cognitive assessment to assess cognitive function we computed total band power oscillatoryspecific alpha power and aperiodics to measure anaesthesiainduced alpha activity to test our mediation hypotheses we employed structural equation modelling pairwise correlations between age cognitive function and alpha activity were significant cognitive function mediated the association between age and classical alpha power age → cognitive function → classical alpha β  −00168 95 confidence interval −00313 to −000521 p  00016 as well as the association between age and oscillatoryspecific alpha power age → cognitive function → oscillatoryspecific alpha power β  −000711 95 confidence interval −00154 to −0000842 p  0028 however cognitive function did not mediate the association between age and aperiodic activity 1f slope p  043 offset p  00996 this study is expected to provide valuable insights for anaesthesiologists enabling them to make informed inferences about a patient’s age and cognitive function from an analysis of anaestheticinduced eeg signals in the operating room to ensure generalizability further studies across different populations are needed
image steganography and steganalysis which involve concealing and uncovering hidden data within images have gained significant attention in recent years finding applications in various fields like military medicine egovernment and social media despite their importance in realworld applications some practical aspects remain unaddressed to bridge this gap the current study compares image steganography and steganalysis tools and techniques for digital forensic investigators dfis to uncover concealed information in images we perform a thorough review of artificial intelligence statistical and signature steganalysis methods assesses both free and paid versions and experiments with various image features like size colour mean square error mse root mean square error rmse and peak signaltonoise ratio psnr using a jpegpng dataset the research provides valuable insights for professionals in cybersecurity the originality of this research resides in the fact that although previous studies have been conducted in this area none have explicitly examined the analysis of the selected tools—f5 steghide outguess for image steganography and aletheia stegexpose for image steganalysis— and their application to jpeg image analysis
a gyrokinetic threshold model for pedestal width–height scaling prediction is applied to multiple devices a shaping and aspect ratio scan is performed on national spherical torus experiment nstx equilibria finding δped092a104κ−124038δβθped105 for the widepedestal branch with pedestal width δped  aspect ratio a elongation κ triangularity δ and normalized pedestal height βθped  the width–transport scaling is found to vary significantly if the pedestal height is varied either with a fixed density or fixed temperature showing how fueling and heating sources affect the pedestal density and temperature profiles for the kineticballooningmode kbm limited profiles for an nstx equilibrium at fixed density the wide branch is δped0028qeγe−1715∼ηe15 and at fixed temperature δped031qeγe−47085 ∼ηe085  where qe and γe are turbulent electron heat and particle fluxes and ηe∇ln⁡te∇ln⁡ne for an electron temperature te and density ne  pedestals close to the kbm limit are shown to have modified turbulent transport coefficients compared to the strongly driven kbms the role of flow shear is studied as a width–height scaling constraint and pedestal saturation mechanism for a standard and lithiated wide pedestal discharge finally the stability transport and flow shear constraints are combined and examined for an nstx experiment
quantum computing is finding promising applications in optimization machine learning and physics leading to the development of various models for representing quantum information because these representations are often studied in different contexts manybody physics machine learning formal verification simulation little is known about fundamental tradeoffs between their succinctness and the runtime of operations to update them we therefore analytically investigate three widelyused quantum state representations matrix product states mps decision diagrams dds and restricted boltzmann machines rbms we map the relative succinctness of these data structures and provide the complexity for relevant query and manipulation operations further to chart the balance between succinctness and operation efficiency we extend the concept of rapidity with support for the noncanonical data structures studied in this work showing in particular that mps is at least as rapid as some dds by providing a knowledge compilation map for quantum state representations this paper contributes to the understanding of the inherent time and space efficiency tradeoffs in this area
employers commonly use cognitive ability tests in the personnel selection process although ability tests are excellent predictors of job performance their validity may be compromised when test takers engage in careless responding it is thus important for researchers to have access to effective careless responding measures which allow researchers to screen for careless responding and to evaluate efforts to prevent careless responding previous research has primarily used two types of measures to assess careless responding to ability tests—response time and selfreported carelessness in the current paper we expand the careless responding assessment toolbox by examining the construct validity of four additional measures 1 infrequency 2 instructedresponse 3 longstring and 4 intraindividual response variability irv indices expanding the available set of careless responding indices is important because the strengths of new indices may offset the weaknesses of existing indices and would allow researchers to better assess heterogeneous careless response behaviors across three datasets  n  1193 we found strong support for the validity of the responsetime and infrequency indices and moderate support for the validity of the instructedresponse and irv indices
purpose the goal of this study was to evaluate the role of texturebased baseline radiomic features fr and dynamic radiomics alterations delta fδr within multiple targeted compartments on optical coherence tomography oct scans to predict response to anti–vascular endothelial growth factor vegf therapy in neovascular agerelated macular degeneration namd methods hawk is a phase 3 clinical trial data set of active namd patients n  1082 comparing brolucizumab and aflibercept this analysis included patients receiving 6 mg brolucizumab or 2 mg aflibercept and categorized as complete responders n  280 and incomplete responders n  239 based on whether or not the eyes achievedmaintained fluid resolution on oct a total of 481 fr were extracted from each of the fluid subretinal hyperreflective material shrm retinal tissue and sub–retinal pigment epithelium rpe compartments most discriminating eight baseline features selected by the minimum redundancy maximum relevance feature selection were evaluated using a quadratic discriminant analysis qda classifier on the training set str n  363 to differentiate between the two patient groups classifier performance was subsequently validated on independent test set st n  156 results in total 519 participants were included in this analysis from the hawk phase 3 study there were 280 complete responders and 219 incomplete responders compartmental analysis of radiomics featured identified the subrpe and shrm compartments as the most distinguishing between the two response groups the qda classifier yielded areas under the curve of 078 079 and 084 respectively using fr fδr and combined fr fδr and fc on st conclusions utilizing compartmental static and dynamic radiomics features unique differences were identified between eyes that respond differently to antivegf therapy in a large phase 3 trial that may provide important predictive value translational relevance imaging biomarkers such as radiomics features identified in this analysis for predicting treatment response are needed to enhanced precision medicine in the management of namd
afforestations provide costeffective and environmentally friendly protection against natural hazards compared to technical measures in austria more than 3000 afforestation sites for hazard protection covering 9000 ha have been established between 1906 and 2017 mainly for snow avalanche protection the actual protective effect depends on avalanche predisposing factors and land cover ie whether forest is present in this study predisposing factors and land cover classes were identified and analysed in selected afforestation sites the protective effect of forest was attributed to the presence of forest cover and tree species using rgb images with a ground resolution of 20 × 20 cm nine land cover categories have been distinguished by means of supervised classification with the random forest algorithm those land cover categories were classified with an overall accuracy of 087–098 and kappavalues ranging between 081 and 093 images were filtered using a 3 pixel by 3 pixel majority filter which assigns each cell in the output grid the most commonly occurring value in a moving window centred on each grid cell this filter further increased the overall accuracy by removing noise pixels while preserving the fine elements of the classified grid our results indicate a protective effect for about half of the analysed afforestation sites the dominance of the land use class “meadow” at most sites with little avalanche protection effect suggests grazing as a limiting factor the spatial information provided with the described method allows to identify critical areas in terms of avalanche protection even years after the initial afforestation
we investigate a new task in human motion prediction which is predicting motions under unexpected physical perturbation potentially involving multiple people compared with existing research this task involves predicting less controlled unpremeditated and pure reactive motions in response to external impact and how such motions can propagate through people it brings new challenges such as data scarcity and predicting complex interactions to this end we propose a new method capitalizing differentiable physics and deep neural networks leading to an explicit latent differentiable physics ldp model through experiments we demonstrate that ldp has high data efficiency outstanding prediction accuracy strong generalizability and good explainability since there is no similar research a comprehensive comparison with 11 adapted baselines from several relevant domains is conducted showing ldp outperforming existing research both quantitatively and qualitatively improving prediction accuracy by as much as 70 and demonstrating significantly stronger generalization
this article presents a novel generalized approach to obtain rational dyadic and integer biorthogonal wavelet filter coefficients based on two conditions namely maximally flatness and nearperfect reconstruction pr in halfband polynomial hbp an incrementaliterative approach is adopted to design the coefficients based on the proposed error equation in terms of the remainder polynomial rp of lagrange hbp and the proposed hbp with maximum vanishing moments vms in addition vlsi architecture for the proposed wavelet filter banks fbs is designed and implemented on the zedboard zynq7000 apsoc zynq fpga from xilinx fieldprogrammable gate array it is found that the proposed rationalized wavelet fbs achieved significantly low digital hardware requirements with similar characteristics when compared to wellknown rationalized existing biorthogonal wavelet fbs the effectiveness of the designed wavelet fbs is verified in image compression and image retrieval on wellknown publicly available databases it is found that the designed rationalized biorthogonal wavelet fbs give better performances when compared to existing rationalized biorthogonal wavelet fbs
"motivación
 este artículo ofrece un análisis empírico sobre los principales factores que explican los movimientos del diferencial entre el tipo repo y el tipo de la facilidad de depósito dfr por sus siglas en inglés en el área del euro con especial foco en el impacto de la normalización de la política monetaria los resultados apuntan a que el notable aumento de las expectativas de tipos fue el principal elemento detrás del ensanchamiento del diferencial en 2022 en 2023 la contención de las expectativas del ritmo de subidas y la reducción del balance del eurosistema volvieron a situar el diferencial en los niveles previos al inicio del período de subidas de tipos oficiales
 ideas principales
 •el ensanchamiento del diferencial repodfr en 2022 se debió principalmente a las expectativas de subidas de tipos
 •otros elementos coyunturales como la incertidumbre en torno a la política monetaria y la demanda de activos a corto plazo también contribuyeron al aumento del diferencial
 •las tensiones temporales observadas en este mercado durante 2022 se redujeron en 2023 con la relajación de las expectativas de subidas de tipos"
selfsupervised learning has achieved remarkable success in acquiring highquality representations from unlabeled data the widely adopted contrastive learning framework aims to learn invariant representations by minimizing the distance between positive views originating from the same image however existing techniques to construct positive views highly rely on manual transformations resulting in limited diversity and potentially false positive pairs to tackle these challenges we present genview a controllable framework that augments the diversity of positive views leveraging the power of pretrained generative models while preserving semantics we develop an adaptive view generation method that dynamically adjusts the noise level in sampling to ensure the preservation of essential semantic meaning while introducing variability additionally we introduce a qualitydriven contrastive loss which assesses the quality of positive pairs by considering both foreground similarity and background diversity this loss prioritizes the highquality positive pairs we construct while reducing the influence of lowquality pairs thereby mitigating potential semantic inconsistencies introduced by generative models and aggressive data augmentation thanks to the improved positive view quality and the qualitydriven contrastive loss genview significantly improves selfsupervised learning across various tasks for instance genview improves mocov2 performance by 2522 on imagenet linearsemisupervised classification moreover genview even performs much better than naively augmenting the imagenet dataset with laion400m or imagenet21k code httpsgithubcomxiaojieli0903genview
quadrotors are increasingly used in the evolving field of aerial robotics for their agility and mechanical simplicity however inherent uncertainties such as aerodynamic effects coupled with quadrotors operation in dynamically changing environments pose significant challenges for traditional nominal modelbased control designs we propose a multitask metalearning method called encoderprototypedecoder epd which has the advantage of effectively balancing shared and distinctive representations across diverse training tasks subsequently we integrate the epd model into a model predictive control problem protompc to enhance the quadrotors ability to adapt and operate across a spectrum of dynamically changing tasks with an efficient online implementation we validate the proposed method in simulations which demonstrates protompcs robust performance in trajectory tracking of a quadrotor being subject to static and spatially varying side winds
"
 in this article we review the main results achieved by the research activities carried out at prisma lab of the university of naples federico ii where for 35 years an interdisciplinary team of experts developed robots that are ultimately useful to humans we summarize the key contributions made in the last decade in the six research areas of dynamic manipulation and locomotion aerial robotics humanrobot interaction artificial intelligence and cognitive robotics industrial robotics and medical robotics after a brief overview of each research field the most significant methodologies and results are reported and discussed highlighting their crossdisciplinary and translational aspects finally the potential future research directions identified are discussed"
moyamoya disease mmd causes cerebral arterial stenosis and hemodynamic disturbance the latter of which may disrupt glymphatic system activity the waste clearance system we evaluated 46 adult patients with mmd and 33 age and sexmatched controls using diffusivity along the perivascular space alps measured with diffusion tensor imaging alps index which may partly reflect glymphatic system activity and multishell diffusion mri to generate freewater maps twentythree patients were also evaluated via 15ogas positron emission tomography pet and all patients underwent cognitive tests compared to controls patients 384 132 years old 35 females had lower alps indices in the left and right hemispheres 194 027 vs 165 025 and 194 022 vs 165 019 p  0001 while the right alps index showed no correlation the left alps index was correlated with parenchymal freewater ρ  047 p  0001 perfusion measured with pet cerebral blood flow ρ  070 p  0001 mean transit time ρ  060 p  0003 and oxygen extraction fraction ρ  052 p  0003 and cognitive tests trail making test part b for executive function ρ  037 p  001 adult patients with mmd may exhibit decreased glymphatic system activity which is correlated with the degree of hemodynamic disturbance increased interstitial freewater and cognitive dysfunction but further investigation is needed
for the costbenefit analysis of road safety measures it is essential to estimate the national value of statistical life by calculating the updated values it is possible to assess the aggregate national value of statistical life for road traffic crashes thereby also characterizing the road safety situation in the country it is important that the values set and the methods used are compatible with the practices of the european member states it must be stressed that updating the values is of major importance both for the costbenefit analysis of the various road safety measures and for raising public and decisionmakers awareness of the huge losses the full identification and use of loss figures is an important element of road safety in this article we present possible methods for estimating the value of statistical life
research on sustainable affordable housing is evolving yet its conceptual efficacy in light of the changing needs of today’s cities and targeted lowtomiddleincome households remains unknown in today’s rapidly urbanizing world understanding the conceptual relevance and importance of land use planning tools such as vertical subdivision to the delivery of sustainable housing is tenable in response to this knowledge gap this entry inquires how can the delivery of affordable housing be configured in a manner that leverages the potential of a redefined vertical subdivision development to optimize densities and ensure that housing affordability is sustainable here this entry redefines vertical subdivision development as a housing planning and design tool that allows for the segregation of air spaces into individual volumetric land parcels that mimic the environmental features of the landonground such that housing construction within such volumetric spaces is a function of the contextually relevant needs of occupants this entry demonstrates a paradigm shift from existing housing infrastructure planning models and narratives to one that responds to and addresses all three dimensions of sustainability economic sustainable affordability environmental sustainable densities and social occupant wellness in the housing infrastructure planning and delivery process
the paper focuses on the improvement of the quality of learning for deep neural networks for a small data set in a classification task one of the possible approaches to improve the quality of learning is researched which is based on the use of data augmentation artificial reproduction of the data set by image warping the presented mathematical model and fast algorithm for warping make it possible to transform the original image while preserving its structural basis the proposed algorithm is used to augment image data sets containing a small number of training samples the augmentation consists of two stages including horizontal mirroring and warping of each of the samples the effectiveness of such augmentation is tested through the training of neural networks of various types convolutional neural networks cnn of a standard architecture and deep residual networks drn a specific feature of the implemented approach for the solution of the problem under consideration consists in the refusal to use pretrained neural networks with a large number of layers as well as further transfer learning since their application incurs costs in terms of the computational resources the paper shows that the efficiency of image classification when implementing the proposed method of augmenting training data on small and mediumsized data sets increases to statistically significant values of the metric used
abstract in this paper we propose an improved 3d modeling method based on recurdyn dynamics theory to address the poor efficiency of traditional modeling techniques in dance dynamics tracking solid manipulation of the virtual scene is realized by geometric transformation and singular value decomposition is applied to solve the transformation simulation experiments were designed to explore the tracking effect when the ms moves at a more significant speed the larger the doppler frequency corresponding to the dynamic tracking and choreography technique of modern dance the more rapidly the channel changes and the more quickly the channel correlation decreases such as when the ms moves at a speed of 13ms and the correlation coefficient drops to 01 which corresponds to the number of sampling points within 6 from the analysis of the ablation experiments the errors of the three models are reduced by 96 112 and 59 respectively from the first qualitative test analysis frames 113 should be less spatial and 1424 should be more spatial from the analysis of the first qualitative test according to the numerical value the average moving speed of the arm in the 53rd68th frames is the largest and the average moving speed of the arm in the 87th100th frames is the smallest therefore the improved algorithm can be accurately applied to modern dance dynamic tracking and choreography techniques
"
 two devastating earthquakes struck southeastern türkiye and northwestern syria on 6 february 2023 an mw 78 mainshock followed 9 hr later by an mw 76 aftershock to recover and separate the subsurface geometry and slip distributions along the two earthquake faults we jointly invert interferometric synthetic aperture radar synthetic aperture radar pixel offset tracking burst overlap interferometry boi global navigation satellite system and aftershock datasets we introduce a new kalman filterbased approach for merging spatially dense azimuth offset azo data with the more precise yet spatially sparse boi data this procedure yields improved measurements of the displacements parallel to the near northsouth satellite tracks which are critical for resolving slip along most of the mw 78 fault segments we optimize the inversion using a new metric for assessing the degree of spatial correlation between the coseismic slip gradients and early aftershocks resulting in a stable solution honoring the complementarity between the geodetic and aftershock datasets the analysis suggests that the mw 78 rupture consisted of three large segments and two short fault branches covering about 300 km along the east anatolian fault eaf whereas the mw 76 rupture consisted of three segments extending for about 160 km along the nearby sürgü fault sf on the basis of momenttostressdrop scaling relations we show that the mw 76 stress drop is four times larger than the mw 78 stress drop consistent with the larger recurrence intervals for mw  7 earthquakes on the sf than on the eaf the moment released during the 2023 mw 78 earthquake is 2–4 times larger than the sum of the moments released during individual historical mw  7 earthquakes along the three segments of the 2023 mw 78 earthquake thus when considering moment release for multisegment earthquakes one should note that the final moment of fault coalescence is likely larger than the arithmetic sum of individual segment ruptures"
this paper introduces a multiresonance feed structure to control the input impedance for achieving a dual wideimpedance bandwidth technology according to the simulation the bandwidth of the proposed antenna structure was approximately 330 mhz with the lower band ranging from 670 mhz to 1000 mhz and 1210 mhz with the higher band ranging from 1670 mhz to 2880 mhz under a 31 voltage standing wave ratio moreover the average efficiency was significantly increased from 266 reference to 465 proposed at the lower band ranging from 550 mhz to 1020 mhz and even higher at the higher band increasing from 155 reference to 612 proposed ranging from 1400 mhz to 3000 mhz the radiation pattern also showed satisfactory antenna radiation performance this proposed technology has the advantage of achieving input impedance matching more easily and covers most fifthgeneration 5g nr operating bands in fr1 n1 n3 n5 n8 n25 n30 n41 n66 indicating its promising application to 5g communication
a robust interacting multiple model approach is proposed to address the problem of accuracy and non‐gaussian measurement noise in manoeuvering target tracking the proposed approach introduces multiple fading factors into the prediction covariance matrix and adjusts each channel of the gain matrix in real time to improve the accuracy caused by model mismatch and enhance state transition capability considering the non‐gaussian noise an improved imm filter is constructed to further improve the robustness using the maximum correntropy criterion the simulation results show that the proposed approach can effectively suppress the non‐gaussian noise and improve the accuracy with adaptability and robustness
machine learning models that embed graphs in noneuclidean spaces have shown substantial benefits in a variety of contexts but their application has not been studied extensively in the biological domain particularly with respect to biological pathway graphs such graphs exhibit a variety of complex network structures presenting challenges to existing embedding approaches learning highquality embeddings for biological pathway graphs is important for researchers looking to understand the underpinnings of disease and train highquality predictive models on these networks in this work we investigate the effects of embedding pathway graphs in noneuclidean mixedcurvature spaces and compare against traditional euclidean graph representation learning models we then train a supervised model using the learned node embeddings to predict missing proteinprotein interactions in pathway graphs we find large reductions in distortion and boosts on indistribution edge prediction performance as a result of using mixedcurvature embeddings and their corresponding graph neural network models however we find that mixedcurvature representations underperform existing baselines on outofdistribution edge prediction performance suggesting that these representations may overfit to the training graph topology we provide our mixedcurvature product graph convolutional network code at httpsgithubcommcneelamixedcurvaturegcn and our pathway analysis code at httpsgithubcommcneelamixedcurvaturepathways
purposethis study aims to establish that the relationship between the risky indebtedness behavior rib of consumers and their attitude toward adopting buynowpaylater bnpl is not immediate but is mediated through impulse buying moreover it explores how perceived risk moderates the association between the attitude to adopt bnpl and its adoption intentiondesignmethodologyapproachthis study used the existing theoretical and empirical evidence to propose a model and validated it using the data collected from 339 young shoppers in india analysis of data is conducted using partial least squares structural equation modelingfindingsthe study results show that consumers’ rib is not directly related to their attitude toward bnpl however impulse buying fully mediates this relationship influencing the attitude toward bnpl impulse buying and attitude serially mediate the relationship between rib and bnpl adoption intention further in the context of bnpl perceived risk strengthens the attitudeintention gappractical implicationsthis study advises policymakers and bnpl providers to carefully assess users’ creditworthiness to prevent those already in debt from entering into a detrimental looporiginalityvaluethis study provides novel perspectives on consumer’s rib and bnpl within the indian context the study additionally identifies the mediating influence of impulse buying and the moderating effect of perceived risk on bnpl adoption intention
as a research hotspot across logistics operations research and artificial intelligence route planning has become a key technology for intelligent transportation systems recently datadriven machine learning heuristics including learning construction methods and learning improvement methods have achieved remarkable success in solving singleobjective route planning problems however many practical route planning scenarios must simultaneously consider multiple conflict objectives for example modern logistics companies often need to simultaneously minimize time budget transportation cost and vehicle pollution several learning construction methods are proposed for solving classical multiobjective route planning morp problems yet no learning improvement heuristics have been developed so far even though they are acknowledged to be more efficient in narrowing the optimality gap to fill this gap this paper proposes a learning improvement morp method pareto improver pi pi employs a populationbased mechanism to approximate the pareto front with a single deep reinforcement learning model the experimental results on various morp problems show that pi can significantly outperform other stateoftheart methods
thermal simulation plays a crucial role in various fields often involving complex partial differential equation pde simulations for thermal optimization to tackle this challenge we have harnessed neural networks for thermal prediction specifically employing deep neural networks as a universal solver for pdes this innovative approach has garnered significant attention in the scientific community while physicsinformed neural networks pinns have been introduced for thermal prediction using deep neural networks existing methods primarily focus on offering thermal simulations for predefined relevant parameters such as heat sources loads boundaries and initial conditions however any adjustments to these parameters typically require retraining or transfer learning resulting in considerable additional work to overcome this limitation we integrated pinn methods with the deeponet model creating a novel model called pideeponet for thermal simulation this model takes both relevant parameters and coordinate points as simultaneous input functions presenting a fresh computational perspective for thermal simulation based on the paddlepaddle deep learning framework our research demonstrates that after sufficient training this model can reliably and rapidly predict parameter solutions importantly it significantly surpasses traditional numerical solvers in terms of speed by several orders of magnitude without requiring additional training this groundbreaking research framework holds vast application potential and promises substantial advancements in the field of thermal simulation
this paper introduces a novel recursive distributed estimation algorithm aimed at synthesizing input and state interval observers for nonlinear boundederror discretetime multiagent systems the considered systems have sensors and actuators that are susceptible to unknown or adversarial inputs to solve this problem we first identify conditions that allow agents to obtain nonlinear boundederror equations characterizing the input then we propose a distributed intervalvalued observer that is guaranteed to contain the disturbance and system states to do this we first detail a gain design procedure that uses global problem data to minimize an upper bound on the ell1 norm of the observer error we then propose a gain design approach that does not require global information using only values that are local to each agent the second method improves on the computational tractability of the first at the expense of some added conservatism further we discuss some possible ways of extending the results to a broader class of systems we conclude by demonstrating our observer on two examples the first is a unicycle system for which we apply the first gain design method the second is a 145bus power system which showcases the benefits of the second method due to the first approach being intractable for systems with high dimensional state spaces
traditional zinc blende semiconductor materials of groups ii–vi and iii–v exhibit excellent electrical properties yet suffer from oversized lattice thermal conductivity causing poor thermoelectric performance herein we have explored an alternative metastable phase of those materials namely porous phase compared with the stable zinc blende structure which has simple crystal structure with nearly isotropic bonding feature porous compounds exhibit complex bonding hierarchy and softened acoustic phonon modes with strong anharmonicity reducing the lattice thermal conductivity by nearly two orders of magnitude as an outstanding representative of porous compound family the suppressed thermal conductivity ∼076 wm k at room temperature combined with enhanced seebeck coefficient makes porous mgte a highperformance thermoelectric material with figure of merit above unity at ntype doping and high temperature this work highlights the important role of intrinsic porosity in design of highperformance thermoelectric materials with low lattice thermal conductivity
objective the present study aimed to develop a prediction model for predicting developing debilities after optic neuritis methods the data for this research was obtained from the optic neuritis treatment trial ontt the predictive model was built based on a cox proportional hazards regression model model performance was assessed using harrell’s cindex for discrimination calibration plots for calibration and stratification of patients into lowrisk and highrisk groups for utility evaluation results a total of 416 patients participated among them 101 patients 243 experienced disability which was defined as achieving or surpassing a score of 3 on the expanded disability status scale the median followup duration was 155 years interquartile range 70 to 168 two predictors in the final predictive model included the classification of multiple sclerosis at baseline and the condition of the optic disk in the affected eye at baseline upon incorporating these two factors into the model the model’s cindex stood at 071 95 ci 066–076 with an optimism of 0005 with a favorable alignment with the calibration curve by utilizing this model the ontt cohort can be categorized into two risk categories each having distinct rates of disability development within a 15year timeframe highrisk group 41 95 ci 31–49 and lowrisk group 13 95 ci 84–17 logrank pvalue of 0001 conclusion this predictive model has the potential to assist physicians in identifying individuals at a heightened risk of experiencing disability following optic neuritis enabling timely intervention and treatment
abstract the emergence of internetbased communities popularly known as social media has transformed communication drastically due to its importance scholars have written on the subject within the ghanaian context however the literature remains fragmented without knowledge of its current state gaps and opportunities for future research to address this we synthesised 29 peerreviewed articles in academic databases from 2013 to 2023 through a systematic literature review we observed that social media research in ghana had been underpinned by the quantitative approach at the expense of the others likewise the most explored theme is social media and politics whereas the least is health communication and social media based on the gaps identified we made recommendations for future research
"abstract spaceborne formaldehyde hcho measurements constitute an excellent proxy for the sources of nonmethane volatile organic compounds nmvocs past studies suggested substantial overestimations of nmvoc emissions in stateoftheart inventories over major source regions here the qa4ecv quality assurance for essential climate variables retrieval of hcho columns from omi ozone monitoring instrument is evaluated against 1 ftir fouriertransform infrared column observations at 26 stations worldwide and 2 aircraft in situ hcho concentration measurements from campaigns conducted over the usa during 2012–2013 both validation exercises show that omi underestimates high columns and overestimates low columns the linear regression of omi and aircraftbased columns gives ωomi0651ωairc295×1015 moleccm2 with ωomi and ωairc the omi and aircraftderived vertical columns whereas the regression of omi and ftir data gives ωomi0659ωftir202×1015 moleccm2 inverse modelling of nmvoc emissions with a global model based on omi columns corrected for biases based on those relationships leads to muchimproved agreement against ftir data and hcho concentrations from 11 aircraft campaigns the optimized global isoprene emissions ∼445tgyr1 are 25  higher than those obtained without bias correction the optimized isoprene emissions bear both striking similarities and differences with recently published emissions based on spaceborne isoprene columns from the cris crosstrack infrared sounder sensor although the interannual variability of omi hcho columns is well understood over regions where biogenic emissions are dominant and the hcho trends over china and india clearly reflect anthropogenic emission changes the observed hcho decline over the southeastern usa remains imperfectly elucidated
"
federated reinforcement learning frl allows multiple agents to collaboratively build a decision making policy without sharing raw trajectories however if a small fraction of these agents are adversarial it can lead to catastrophic results we propose a policy gradient based approach that is robust to adversarial agents which can send arbitrary values to the server under this setting our results form the first global convergence guarantees with general parametrization these results demonstrate resilience with adversaries while achieving optimal sample complexity of order tildemathcaloleft frac1nepsilon2 left 1 fracf2nrightright where n is the total number of agents and f
the aldehyde dehydrogenase 1a1 aldh1a1 also known as retinal dehydrogenase is an enzyme normally involved in the cellular metabolism development and detoxification processes in healthy cells however its also considered a cancer stem cell marker and its high levels of expression in several cancers including breast lung ovarian and colon cancer have been associated with poor prognosis and resistance to chemotherapy given its crucial role in chemotherapy resistance by detoxification of chemotherapeutic drugs aldh1a1 has attracted significant research interest as a potential therapeutic target for cancer though a few synthetic inhibitors of aldh1a1 have been synthesized and their efficacy has been proved invitro and invivo studies none of them have passed clinical trials so far in this scenario we have performed an insilico study to verify whether any of the already approved drugs used for various purposes has the ability to inhibit catalytic activity of aldh1a1 so that they can be repurposed for cancer therapy keeping in mind the feasibility of repurposing in a larger population we have selected the approved drugs from five widely used drug categories such as antibiotic antiviral antifungal anti diabetic and antihypertensive for screening computational techniques like molecular docking molecular dynamics simulations and mmpbsa binding energy calculation have been used in this study to screen the approved drugs based on the logical analysis of results we propose that three drugs  telmisartan irbesartan and maraviroc can inhibit the catalytic activity of aldh1a1 and thus can be repurposed to increase chemotherapy sensitivity in cancer cellscommunicated by ramaswamy h sarma
this letter addresses the challenge of openset instance segmentation osis which segments both known objects and unknown objects not seen in training and thus is essential for enabling robots to safely work in the real world existing solutions adopt classagnostic segmentation where all classes share the same mask output layer leading to inferior performance motivated by the superiority of the classspecific mask prediction in closeset instance segmentation we propose semseg with class semantics extraction and mask prediction modulation for conducting classspecific segmentation in osis to extract class semantics for both known and unknown objects in the absence of supervision on unknown objects we use contrastive learning to construct an embedding space where objects from each known class cluster in an independent territory and the complementary region of known classes can accommodate unknown objects to modulate the mask prediction we convert class semantic embedding to convolutional parameters used to predict the mask class semantics modulated osis allows optimizing the mask output layer for each class independently without competition between each other and class semantic information is engaged in the segmentation process directly so that can guide and facilitate the segmentation task which benefits unknown objects with severe generalization challenges particularly experiments on the coco and graspnet1billion datasets demonstrate the merits of our proposed method especially the strength of instance segmentation for unknown objects
pulse wave as a message carrier in the cardiovascular system cvs enables inferring cvs conditions while diagnosing cardiovascular diseases cvds heart failure hf is a major cvd typically requiring expensive and timeconsuming treatments for health monitoring and disease deterioration it would be an effective and patientfriendly tool to facilitate rapid and precise noninvasive evaluation of the heart’s bloodsupply capability by means of powerful featureabstraction capability of machine learning ml based on pulse wave which remains untouched yet here we present an mlbased methodology which is verified to accurately evaluate the bloodsupply capability of patients with hf based on clinical data of 237 patients enabling fast prediction of five representative cardiovascular function parameters comprising left ventricular ejection fraction lvef left ventricular enddiastolic diameter lvdd left ventricular endsystolic diameter lvds left atrial dimension lad and peripheral oxygen saturation spo2 two ml networks were employed and optimized based on highquality pulse wave datasets and they were validated consistently through statistical analysis based on the summary independentsamples t test  p   005 the bland–altman analysis with clinical measurements and the errorfunction analysis it is proven that evaluation of the spo2 lad and lvdd performance can be achieved with the maximum error  15 while our findings thus demonstrate the potential of pulse wavebased noninvasive evaluation of the bloodsupply capability of patients with hf they also set the stage for further refinements in health monitoring and deterioration prevention applications
with the continued progress of the internet network security has become an increasingly significant issue that requires constant attention and research network traffic classification is a key technology used to detect and prevent malicious network activity and it has accordingly received increasing attention and research however datasets related to malicious network traffic classification often have imbalanced characteristics in conventional traffic classification problems with multiple categories the sample size characteristics of small categories are often overlooked to address this issue the focal loss function was proposed which focuses on small samples by modulating the tradeoff between the positive and negative samples through two hyperparameters and  this article uses convolutional neural networks cnn to tackle traffic classification problem and explore the optimal values of parameters in this application scenario additionally this work proposed a novel weight allocation formula to replace  which allowed small class traffic to obtain higher accuracy
understanding the energetic landscapes of large molecules is necessary for the study of chemical and biological systems recently deep learning has greatly accelerated the development of models based on quantum chemistry making it possible to build potential energy surfaces and explore chemical space however most of this work has focused on organic molecules due to the simplicity of their electronic structures as well as the availability of data sets in this work we build a deep learning architecture to model the energetics of zinc organometallic complexes to achieve this we have compiled a configurationally and conformationally diverse data set of zinc complexes using metadynamics to overcome the limitations of traditional sampling methods in terms of the neural network potentials our results indicate that for zinc complexes partial charges play an important role in modeling the longrange interactions with a neural network our developed model outperforms semiempirical methods in predicting the relative energy of zinc conformers yielding a mean absolute error mae of 132 kcalmol with reference to the doublehybrid pwpb95 method
the demand for clean and renewable wind energy as an alternative energy source to fossil fuels that produce greenhouse emissions is everincreasing however harnessing the maximum power from the wind source via the wind energy conversion system wecs requires precise estimation of aerodynamic torque and subsequently the reference generator speed in this paper the exponential disturbance observerbased sliding mode control smc for extracting the maximum power of wecs using wind speed estimation is presented as most of the existing research assumed the aerodynamic torque to be slowly varying this study presented a comprehensive analysis of the influence of higherorder fastvarying aerodynamic torque for maximum power extraction of the wecs the simulation results under different wind profiles for the zeroorder firstorder and secondorder aerodynamic torque estimations were analyzed although the maximum power extraction has increased by 17 for an extremelyvarying wind from 1625 kw for the zeroorder to 190 kw after incorporating the higherorder estimations the zeroorder estimation almost extracts the same power 1517 vs 1522 kw and 685 vs 691 kw for the fast varying wind and low magnitude wind force respectively finally the analyses showed that if the wecs is operating where the wind is not extremely fastvarying the zeroorder estimation is the ideal choice as the higherorder estimation increases the nonlinearity of the system
conversational systems often rely on embedding models for intent classification and intent clustering tasks the advent of large language models llms which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts are being viewed as a panacea for these downstream conversational tasks however traditional evaluation benchmarks rely solely on task metrics that dont particularly measure gaps related to semantic understanding thus we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks 1 intent classification 2 intent clustering and 3 a novel triplet task the triplet task gauges the models understanding of two semantic concepts paramount in realworld conversational systems negation and implicature we observe that current embedding models fare poorly in semantic understanding of these concepts to address this we propose a pretraining approach to improve the embedding model by leveraging augmentation with data generated by an autoregressive model and a contrastive loss term our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics
as the integration of internet of things devices with cloud computing proliferates the paramount importance of privacy preservation comes to the forefront this survey paper meticulously explores the landscape of privacy issues in the dynamic intersection of iot and cloud systems the comprehensive literature review synthesizes existing research illuminating key challenges and discerning emerging trends in privacy preserving techniques the categorization of diverse approaches unveils a nuanced understanding of encryption techniques anonymization strategies access control mechanisms and the burgeoning integration of artificial intelligence notable trends include the infusion of machine learning for dynamic anonymization homomorphic encryption for secure computation and aidriven access control systems the culmination of this survey contributes a holistic view laying the groundwork for understanding the multifaceted strategies employed in securing sensitive data within iotbased cloud environments the insights garnered from this survey provide a valuable resource for researchers practitioners and policymakers navigating the complex terrain of privacy preservation in the evolving landscape of iot and cloud computing
purpose to evaluate the efficacy of spironolactone in the treatment of chronic central serous chorioretinopathy csc and identify imaging characteristics that can predict the benefit of spironolactone treatment methods patients with chronic csc were treated with spironolactone 20 mgtid and followed for 6 months the primary outcome measure was complete resolution of the subretinal fluid srf and the bestcorrected visual acuity bcva the srf area the central macular thickness cmt the subfoveal choroidal thickness sfct and the density of the choriocapillaris vessel and adverse events were secondary outcome measures patients who presented complete resolution of srf were included in the responder group and the other patients who had moderate or no resolution were included in the nonresponder group imaging characteristic comparisons between the responder and nonresponder groups were performed with regression analyses to identify factors that are predictive of a good response to treatment results fortytwo eyes of 42 patients with a mean age of 4606 ± 666 years were included a total of 571 of the patients achieved a complete resolution of srf the mean srf area cmt and sfct decreased significantly all p  005 throughout the followup period and bcva improved slightly p  005 the vascular density of the choriocapillaris of the fellow eyes did not vary significantly during treatment logistic regression analysis revealed that sfct p0002 and the intact ellipsoid zone p0001 were correlated with disease resolution a relatively higher baseline sfct was a predictive factor associated with a good response to treatment according to multivariate analysis conclusions this study suggested that oral spironolactone could be an effective and safe therapy for chronic csc patients eyes with a higher baseline sfct and intact ellipsoid zone could have a good response these parameters are an important prognostic marker
large language models llms find increasing applications in many fields here three llm chatbots chatgpt35 chatgpt4 and bard are assessed  in their current form as publicly available  for their ability to recognize alzheimers dementia ad and cognitively normal cn individuals using textual input derived from spontaneous speech recordings zeroshot learning approach is used at two levels of independent queries with the second query chainofthought prompting eliciting more detailed than the first each llm chatbots performance is evaluated on the prediction generated in terms of accuracy sensitivity specificity precision and f1 score llm chatbots generated threeclass outcome adcn orunsure when positively identifying ad bard produced highest truepositives 89 recall and highest f1 score 71 but tended to misidentify cn as ad with high confidence lowunsurerates for positively identifying cn gpt4 resulted in the highest truenegatives at 56 and highest f1 score 62 adopting a diplomatic stance moderateunsurerates overall three llm chatbots identify ad vs cn surpassing chancelevels but do not currently satisfy clinical application
recently fake news such as lowquality news with intentionally false information has threatened the authenticity of news information however existing detection methods are inefficient in modeling complicated data and leveraging external knowledge to address these limitations we propose a fake news detection framework based on knowledgeguided semantic analysis which compares the news to external knowledge through triplets for fake news detection considering that equivalent elements of triplets may be presented in different forms a triplet alignment method is designed to construct the bridge between news documents and knowledge graphs then a dualbranch network is developed to conduct interaction and comparison between text and knowledge embeddings specifically text semantics is analyzed with the guidance generated by a triplet aggregation module to capture the inconsistency between news content and external knowledge in addition a triplet scoring module is designed to measure rationality in view of general knowledge as a complementary clue finally an interaction module is proposed to fuse rationality scores in aspects of text semantics and external knowledge to obtain detection results extensive experiments are conducted on publicly available datasets and several stateoftheart methods are considered for comparison the results verify the superiority of the proposed method in achieving more reliable detection results of fake news
"
 this viewpoint describes digital redlining as racialized inequities in access to technology infrastructure including access to health care education employment and social services
"
abstract artificial intelligence ai is a discipline within the field of computer science that encompasses the development and utilization of machines capable of emulating human behavior particularly regarding the astute examination and interpretation of data ai operates through the utilization of specialized algorithms and it includes techniques such as deep dl and machine learning ml and natural language processing nlp as a result ai has found its application in the study of pharmaceutical chemistry and healthcare the ai models employed encompass a spectrum of methodologies including unsupervised clustering techniques applied to drugs or patients to discern potential drug compounds or appropriate patient cohorts additionally supervised ml methodologies are utilized to enhance the efficacy of therapeutic drug monitoring further aiaided prediction of the clinical outcomes of clinical trials can improve efficiency by prioritizing therapeutic intervention that are likely to succeed hence benefiting the patient ai may also help create personalized treatments by locating potential intervention targets and assessing their efficacy hence this review provides insights into recent advances in the application of ai and different tools used in the field of pharmaceutical medicine
spatial crowdsourcing sc is finding widespread application in todays online world as we have transitioned from desktop crowdsourcing applications eg wikipedia to sc applications eg uber there is a sense that sc systems must not only provide effective task assignment but also need to ensure privacy to achieve these oftenconflicting objectives we propose a framework task assignment with federated preference learning that performs task assignment based on worker preferences while keeping the data decentralized and private in each platform center eg each delivery center of an sc company the framework includes a federated preference learning phase and a task assignment phase specifically in the first phase we build a local preference model for each platform center based on historical data we provide means of horizontal federated learning that makes it possible to collaboratively train these local preference models under the orchestration of a central server specifically we provide a practical method that accelerates federated preference learning based on stochastic controlled averaging and achieves low communication costs while considering data heterogeneity among clients the task assignment phase aims to achieve effective and efficient task assignment by considering workers’ preferences extensive evaluations on real data offer insight into the effectiveness and efficiency of the papers proposals
background individuals who sustain an acl injury and undergo reconstruction aclr are at risk for the development of osteoarthritis recent investigations have applied the englund criteria to categorize people with a history of aclr as someone with a symptomatic or asymptomatic knee purposehypothesis the purpose of this study was to examine differences in healthrelated quality of life hrql and psychological outcomes in people with a history of aclr who were categorized as symptomatic or nonsymptomatic by application of the englund criteria the authors’ hypothesized participants classified as symptomatic would have lower hrql increased fearavoidance beliefs and decreased resilience compared to participants classified as nonsymptomatic study design crosssectional survey methods participants at least oneyear after aclr were recruited for the study and completed the tegner activity scale the brief resilience scale brs the modified disablement in the physically active scale mdpa and the fearavoidance belief questionnaire fabq at one timepoint descriptive statistics were summarized using median interquartile range and differences between groups were examined using separate mannwhitney u tests results participants with symptomatic knees had a significantly higher bmi 248 64 than the nonsymptomatic group 212 43 p0013 participants in the symptomatic group had worse hrql on the physical subscale 125 163 vs 00 25 p0001 and mental subscale 20 1 vs 00 1 p0031 higher scores on the fabqsport 145 11 vs 00 6 p0001 and fabqphysical activity 20 24 vs 1 4 p0001 and less resilience 37042 vs 40 083 p0028 compared to those participants in the nonsymptomatic group there were no differences in current physical activity p0285 or change in physical activity p0124 levels between the two groups conclusions this series of differences may represent a cascade of events that can continue to negatively impact health outcomes across the lifespan for individuals with a history of aclr future research should consider longitudinal investigations of these outcomes after injury and throughout the postsurgical and postrehabilitation timeframe level of evidence level 3b
building efficient neural network architectures can be a timeconsuming task requiring extensive expert knowledge this task becomes particularly challenging for edge devices because one has to consider parameters such as power consumption during inferencing model size inferencing speed and co2 emissions in this article we introduce a novel framework designed to automatically discover new neural network architectures based on userdefined parameters an expert system and an llm trained on a large amount of opendomain knowledge the introduced framework lemonade is tailored to be used by nonai experts does not require a predetermined neural architecture search space and considers a large set of edge devicespecific parameters we implement and validate this proposed neural architecture discovery framework using cifar10 cifar100 and imagenet16120 datasets while using gpt4 turbo and gemini as the llm component we observe that the proposed framework can rapidly within hours discover intricate neural network models that perform extremely well across a diverse set of application settings defined by the user
in this article a terahertz transmitarray antenna taa with a specific beamwidth and good beamwidth uniformity is developed for the feed source of a cassegrain antenna the taa consists of a transmit surface fed by an openended waveguide oewg the polarizationconversion elements are constructed with lshaped and grated patterns on the two sides of a polyimide pi film to provide the full phase coverage of 360° and the transmission magnitudes of −55 to −26 db the pi film prototype with conductive patterns is fabricated in house and a high process precision of inlineformula texmath notationlatex5mu textm texmathinlineformula is achieved the relative permittivity and loss tangent of the pi film are measured to be inlineformula texmath notationlatexvarepsilon text r texmathinlineformula  3323 and inlineformula texmath notationlatextan delta  texmathinlineformula  0035 around 300 ghz using the free space method before design the scale of the transmit surface is determined according to the relationship between the beamwidth and aperture size of antennas the radiation patterns of the developed taa in the planes of inlineformula texmath notationlatexvarphi  texmathinlineformula  0° and inlineformula texmath notationlatexvarphi  texmathinlineformula  90° are measured with good agreement obtained between the measured and simulated results the taa achieves the desired beamwidth and the beamwidth uniformity of 15° and ±10 respectively within the frequency band of 305–317 ghz and the maximum measured gain is 1915 dbi at 312 ghz
"skin cancer is a serious health issue that affects millions of people worldwide one in every three cancers is a skin cancer and most people fail to identify and get a diagnosis which is why early detection is essential for effective treatment and improving patient outcomes in recent years computer vision and machine learning have become important tools for the automatic detection of skin cancer one of the commonly used machine learning algorithms for this task is support vector machines svms svms are a type of supervised learning algorithm that is used for classification tasks in skin cancer detection the svm classifier is trained on a dataset of dermatoscopic images of skin lesions the first step in the process is feature extraction where relevant information is extracted from the images to serve as input to the svm classifier this information can include color texture and shape features among others 
the training and testing of the svm classifier is then performed using a portion of the dataset with the remainder being used to evaluate its performance during the testing phase the svm classifier is used to predict the class label of each image which can be malignant or benign the accuracy of the classifier is evaluated by comparing its predictions to the actual class labels of the images in the evaluation dataset 
the results of using svms in skin cancer detection have been promising with high accuracy rates being achieved this highlights the potential of svms as a useful tool for skin cancer screening and early detection in conclusion the use of svms in skin cancer detection provides a fast automatic and reliable method for detecting skin cancer which can help to improve patient outcomes currently it is really very important to watch and analyse the cancer disease automatically at intervals the first stages irregular streaks square measure one in every of the foremost very important features included in most of dermoscopy algorithms that show high association with carcinoma and basal cell malignant growth malady the diagnostic test technique for the detection is most painful and harmful so we have a tendency to tend to square measure going for the machinedriven detection here we have a tendency to tend to square measure practice the glcm choices for the detection the choices of skin lesions square measure extracted normalized symmetrical grey level cooccurrence matrices glcm glcm based texture choices square measure extracted from each of the four classes and given as input to the multiclass support vector machine thats utilized for classification purpose"
the distinguishing feature of hashbased algorithms is their high confidence in security when designing electronic signature schemes proofs of security reduction to certain properties of cryptographic hash functions are used this means that if the scheme is compromised then one of these properties will be violated it is important to note that the properties of cryptographic hash functions have been studied for many years but if a specific hash function used in a protocol turns out to be insecure it can simply be replaced with another one while keeping the overall construction unchanged this article describes a new postquantum signature algorithm syrga1 based on a hash function this algorithm is designed to sign r messages with a single secret key one of the key primitives of the signature algorithm is a cryptographic hash function the proposed algorithm uses the has01 hashing algorithm developed by researchers from the information security laboratory of the institute of information and computational technologies the security and efficiency of the specified hash algorithm have been demonstrated in other articles by its authors hashbased signature schemes are attractive as postquantum signature schemes because their security can be quantified and their security has been proven
cage aquaculture makes it easier to produce highquality aquatic products and allows full use of water resources 3therefore cage aquaculture development is highly valued globally however the current digitalization level of cage aquaculture is low and the farming risks are high research and development of digital management of the fish population in cages are greatly desired realtime monitoring of the activity status of the fish population and changes in the fish population size in cages is a pressing issue that needs to be addressed this paper proposes an improved network called ccyolov5 by embedding coordconv modules to replace the original conv convolution modules in the network which improves the model’s generalization capability by using twostage detection logic the target detection accuracy is enhanced to realize prediction of the number of fish populations opencv is then used to measure fish tail lengths to establish growth curves of the fish and to predict the output of the fish population in the cages experimental results demonstrate that the mean average precision map of the improved algorithm increases by 149 compared to the original yolov5 reaching 954 this research provides an effective solution to promote the intelligentization of cage aquaculture processes it also lays the foundation for ai artificial intelligence applications in other aquaculture scenarios
strong nonlinearity between doppler measurement and target motion in doppler radar target tracking leads to the inadequate utilization of measurement information and limited tracking accuracy we solved this problem by combining converted state kalman filtering and the interacting multiple model this maneuvering target tracking method is suitable for doppler measurement first we converted the target motion in the cartesian coordinate to the polar coordinate then we expanded the measurement equation to include doppler measurement making target motion linearly related to the doppler radar observation vectors and allowing efficient utilization of measurement information next we used unscented transformation to calculate the statistical characteristics of the process noise in the polar coordinate this process helps to reduce the noise error caused by the coordinate system transformation in the original converted state kalman filter finally the system effectively tracks targets that may perform maneuvers with unknown motion during actual tracking using the converted state kalman filter with doppler measurement as a subfilter an interacting multiple model tracking method can be constructed to adjust the model probabilities without going through nonlinear transformation simulation results show that the technique can achieve effective target tracking in doppler measurement application scenarios and has higher tracking accuracy in nonmaneuvering and maneuvering scenarios
background this systematic review summarizes the development accuracy quality and clinical utility of predictive models to assess the risk of opioid use disorder oud persistent opioid use and opioid overdose methods in accordance with preferred reporting items for a systematic review and metaanalysis guidelines 8 electronic databases were searched for studies on predictive models and oud overdose or persistent use in adults until june 25 2023 study selection and data extraction were completed independently by 2 reviewers risk of bias of included studies was assessed independently by 2 reviewers using the prediction model risk of bias assessment tool probast results the literature search yielded 3130 reports after removing 199 duplicates excluding 2685 studies after abstract review and excluding 204 studies after fulltext review the final sample consisted of 41 studies that developed more than 160 predictive models primary outcomes included opioid overdose 316 of studies oud 414 and persistent opioid use 17 the most common modeling approach was regression modeling and the most common predictors included age sex mental health diagnosis history and substance use disorder history most studies reported model performance via the c statistic ranging from 0507 to 0959 gradient boosting tree models and neural network models performed well in the context of their own study one study deployed a model in real time risk of bias was predominantly high concerns regarding applicability were predominantly low conclusions models to predict opioidrelated risks are developed using diverse data sources and predictors with a wide and heterogenous range of accuracy metrics there is a need for further research to improve their accuracy and implementation
microbial communities live in diverse habitats and significantly impact our health and the environment however the principles that govern their formation and evolution remain poorly understood a crucial step in studying microbial communities is to identify the potential metabolic interactions between the community members such as competition for nutrients or crossfeeding due to the size and complexity of the metabolic network of each organism there may be a variety of connections between each pair of organisms which poses a challenge to unraveling the metabolic interactions here we present remind a computational framework to reconstruct the interaction networks in microbial communities based on the metabolic capabilities of individual organisms we applied remind to a wellstudied uraniumreducing community and the honeybee gut microbiome our results provide new perspectives on the evolutionary forces that shape these ecosystems and the tradeoff between metabolite exchange and biomass yield by enumerating alternative interaction networks we systematically identified the most likely metabolites to be exchanged and highlighted metabolites that could mediate competitive interactions we envision that remind will help characterize the metabolic capacity of individual members and elucidate metabolic interactions in diverse communities thus holds the potential to guide many applications in precision medicine and synthetic ecology
intelligent unmanned vending machines uvms based on machine vision have attracted great attention in the unmanned retail industry however due to the complexity of practical application scenarios and environments the existing visionbased intelligent uvms face challenges related to misseddetection and misdetection of product and require costly physical components such as the infrared radio frequency sensors to capture shopping behaviors in this study we propose a bpyolo the realtime model that integrates optimized yolov7 and blazepose for product detection and shopping behaviors recognition bpyolo can accurately detect the products purchased by consumers and their shopping behaviors in complex scenarios to address the problems of misseddetection and misdetection we introduce the 3d attention mechanism simam and the deformable convnets v2 dcnv2 to recombine and optimize the onestage object detection model yolov7 this method reduces the interference of the invalid information in complex scenarios by adaptively weighting each channel and 3d spatial features focuses on feature information in a sparse space and minimizes the loss of feature information during the transmission process based on multiscale feature extraction and fusion to recognize and judge the shopping behaviors of consumers we track the hand and arm key points of consumers using the pose estimation model blazepose using the map05095 as the evaluation metric for product detection the experimental results on a customized product dataset show that bpyolo achieves an average accuracy of 9617 for all product categories detection the average success rate of consumer shopping recognition reaches 92 98 and 947 under three light and noise intensity respectively therefore our bpyolo model for intelligent uvms has effectiveness in commercial deployment
abstract forest pests impose significant threats to tree productivity and ecosystem stability most studies focussed on foliage water tree defoliate and mortality which lag behind pest outbreak to address this issue we developed hyperspectral lidar hsl system based on an acoustooptic tunable filter aotf to examine the echo intensity of tree trunks first we demonstrated the echo power of the cylindrical surface is approximately equivalent to that of the plate surface within 60° incidence angle which provide the theoretical reference for trunk surface echo intensity calibration with a white panel second we propose a twostep method to detect the attacked parts from barks the first step is rapid pest detection conducted by the echo intensity ratio of damaged area and normal area while the ratio is around one the second step is pest detection based on support vector machine classifier with reflectance when the reflectivity of five spectral channels is used as input parameters 100 rapid detection accuracy of damage area is achieved we also use the radiance under seven channels to achieve 100 classification of tree species and attack features the results show that this hsl is effective in woodboring pest detection and contributes to forest applications
a laser absorption spectroscopy las fused electrical tomography et is introduced to noninvasively measure the admittivity and temperature distributions of bunsen burner flames all the operations are carried out at the same region and height at multiple fuelrich combustion states in the same cross section the closer the fueltoair ratio of the combustion state is to one the greater the values of admittivity and temperature random forest rf regression method is used to construct a mapping model for admittivity and combustion state to temperature the maximum absolute error and relative error between mean temperatures of flame region estimated from reconstructed admittivity distributions and constructed mapping model with those measured by las are 6045 k and 48 the proposed method is used to monitor dynamic acoustically excited bunsen burner flame and the flame frequency measured in the spectrums of admittivity and temperature matches the measurements of a visible light photodetector experiments demonstrate the ability of et to monitor the combustion states and temperature distribution of dynamic combustion fields
efficient and accurate building damage assessment is crucial for effective emergency response and resource allocation following natural hazards however traditional methods are often time consuming and labor intensive recent advancements in remote sensing and artificial intelligence ai have made it possible to automate the damage assessment process and previous studies have made notable progress in machine learning classification however the application in postdisaster emergency response requires an end‐to‐end model that starts with satellite imagery as input and automates the generation of large‐scale damage maps as output which was rarely the focus of previous studies addressing this gap this study integrates satellite imagery geographic information systems gis and deep learning this enables the creation of comprehensive large‐scale building damage assessment maps providing valuable insights into the extent and spatial variation of damage the effectiveness of this methodology is demonstrated in galveston county following hurricane ike where the classification of a large ensemble of buildings was automated using deep learning models trained on the xbd data set the results showed that utilizing gis can automate the extraction of subimages with high accuracy while fine‐tuning can enhance the robustness of the damage classification to generate highly accurate large‐scale damage maps those damage maps were validated against historical reports
the major intention of this study is to design and validate manipulatives for homebased physics experiments the covid19 pandemic has caused tremendous and rapid amendments to educational systems worldwide immediately shifting from traditional ontheground teaching to virtual classroom instruction or modular learning approach to name a few similarly it has made laboratory experiments problematic demonstrations are limited just to the four corners of the electronic gadget’s screen while simulations can only do so much in terms of experiential learning hence the urgency of developing these lowcost laboratory kits for students’ use at home the laboratory activities and kits went through thorough examination for validation by 3 experts in the field of physics all of them holding mastersdegree the setup was found to be valid overall mean of 467 from a 5point likert scale validation tool the effectiveness in terms of the performance of learners was established using quasiexperimental methods involving 30 students 15 males and 15 females in the control group and 30 learners 15 males and 15 females in the experimental group the control group was exposed to customary lectures via online classes with virtual simulations the students in the experimental group attended the same online lectures and were provided with the homebased manipulative kit the analysis and interpretation of data collected from the pretestposttest scores of the student participants revealed that the laboratory manipulative kits are effective and highly acceptable the normalized gain of the experimental group g  082 high gain was significantly higher than that of the control group g  045 medium gain
participatory action research par is widely acknowledged for its substantial role in the field of english language teaching and learning elt fostering collaborative and active engagement and a culture of reflective practice this study explores the perceptions held by english teachers regarding the contribution of par to the efficacy of english language teaching and learning additionally the study seeks to scrutinize the strategies employed by teachers in implementing par to augment the teaching and learning of english within the classrooms through the discussion of data obtained from indepth interviews conducted with three english language teachers form three secondary level school of kailali district and a comprehensive review of theoretical and empirical literature this paper reveals the intricate challenges associated with the implementation of par despite the positive disposition of teachers towards this approach the findings highlight the belief held by english language teachers that par not only facilitates proficient teaching and learning of english as a foreign or second language but also assumes a pivotal role in the ongoing professional development of educators moreover this paper accentuates the significance of par in elt emphasizing its capacity to empower both teachers and learners instill contextual relevance promote a sense of community address equity concerns and contribute substantively to the perpetual enhancement of language teaching and learning practices
with the gradual development of urban rail transit the passenger flow of public transportation facilities increases resulting in the rapid growth of energy consumption which brings a significant burden to the sustainable development of the city this paper explores the marching strategy to reduce the energy consumption of train operation through the study of a single train running between two stations so as to further explore the potential of energysaving and carbonreducing of the train a singletrain multiobjective energysaving optimization model is established and a multiobjective genetic algorithm is used to solve for the lowest energy consumption and shortest running time of the train under the consideration of complex road conditions and the influence of complex dynamic processes of the motor the bp neural network optimized by gray wolf algorithm is used to verify the solutions and the minimum energy consumption is 20166e12j and the shortest running time is 2116529s
this article proposes the design of a hierarchical control architecture capable of optimally coordinating multienergy systems mess a mes involves the synergetic operation of subsystems belonging to different energy domains eg thermal electrical or gas enhancing their energy efficiency and economic savings at the price of significant control challenges in fact mess imply an increased model complexity and the interaction of networked subsystems with largely different dynamics this motivates the design of a multilayer control architecture where at the upper level a model predictive control mpc regulator relies on energy models of reduced order to coordinate power exchanges among mes subsystems while at the lower layer decentralized mpc regulators locally control subsystems with different sampling rates consistently with their local dynamics on the other hand the optimal mes regulation may imply additional costs to few subsystems although the overall operational cost decreases thus benefit sharing algorithms are also proposed relying on gametheoretical methods enabling to properly share the achieved economic benefit among subsystems and guaranteeing that the mes operation is more convenient than the independent regulation for each single subsystem the designed control strategy is tested on two different mes case studies considering also the presence of referenced electrical and thermal networks showing high versatility and enhanced performances
gliomas are highly heterogeneous tumors with generally poor prognoses leveraging multiomics data and network analysis holds great promise in uncovering crucial signatures and molecular relationships that elucidate glioma heterogeneity however the complexity of the problem and the high dimensionality of the data increase the challenges of integrating information across various biological levels in this study we developed a framework comprising two steps for variable selection based on sparse network estimation from various omics subsequently we introduced mingle multiomics integrated network for graphical exploration a novel methodology designed to merge distinct multiomics information into a single network enabling the identification of underlying relations through an innovative integrated visualization applying this method to glioma data with patients grouped according to the newest glioma classification guidelines led to the selection of variables as potential candidates for novel gliomatypespecific biomarkers
situs web blockchain adalah platform yang menggunakan teknologi blockchain untuk menyediakan layanan yang aman transparan dan  terdesentralisasi penelitian ini bertujuan untuk merancang dan mengembangkan  website berbasis blockchain yang memanfaatkan keunggulan teknologi tersebut  untuk meningkatkan keamanan data dan memudahkan interaksi antar pengguna  tanpa membutuhkan pihak ketiga sebagai perantara pada tahap desain dilakukan  analisis kebutuhan dan dilakukan pemilihan platform blockchain yang sesuai  dengan tujuan website selain itu desain antarmuka pengguna yang responsif dan  intuitif juga diperhatikan untuk menjamin kenyamanan pengguna dalam berinteraksi dengan websitedalam penelitian ini kami mengimplementasikan blockchain sebagai sistem manajemen transaksi yang berfungsi untuk merekam setiap transaksi yang terjadi pada website online shop setiap transaksi dienkripsi menggunakan algoritma perhitungan hash yang kuat memastikan integritas data dan keamanan informasi pelanggan blockchain juga memberikan kemampuan untuk transparansi yang tinggi yang memungkinkan pelanggan untuk melacak sejarah transaksi mereka dengan mudah hasil dari penelitian ini menunjukkan bahwa penerapan teknologi blockchain dengan perhitungan hash pada website online shop dapat meningkatkan tingkat keamanan mengurangi risiko penipuan dan meningkatkan kepercayaan pelanggan selain itu transaksi yang dicatat dalam blockchain menjadi lebih efisien dan dapat diverifikasi secara cepat penelitian ini mengilustrasikan potensi besar teknologi blockchain dalam meningkatkan operasi bisnis online shop sambil memastikan bahwa pelanggan merasa lebih aman dan dapat mempercayai platform tersebut dengan demikian penerapan blockchain dengan perhitungan hash menjadi solusi yang relevan dalam menghadapi tantangan keamanan dan integritas data di era digital
phytoplankton sinking is a major component of vertical ocean carbon and nutrient fluxes and sinking is an integral component of phytoplankton biology and ecology much of our understanding of phytoplankton sinking derives from the settling column method setcol in which sinking speeds are calculated from the proportion of cells reaching the bottom of a waterfilled column after a set time videobased methods are a recent alternative to setcol in which sinking speeds are measured by tracking the movement of individual cells in a salinitystratified water column in this study we present the results of a metaanalysis showing that setcol produces significantly and consistently lower sinking speeds than the video method next we perform a particle image velocimetry analysis which shows that the observed discrepancy in sinking speeds between the two methods can probably be explained by weak convection currents in the setcols finally we discuss the implications of these results for the interpretation of past and future phytoplankton sinking speed measurements and models that rely on those measurements
in the field of threedimensional object design and fabrication this paper explores the transformative potential at the intersection of biomaterials biopolymers and additive manufacturing drawing inspiration from the intricate designs found in the natural world this study contributes to the evolving landscape of manufacturing and design paradigms biomimicry rooted in emulating nature’s sophisticated solutions serves as the foundational framework for developing materials endowed with remarkable characteristics including adaptability responsiveness and selftransformation these advanced engineered biomimetic materials featuring attributes such as shape memory and selfhealing properties undergo rigorous synthesis and characterization procedures with the overarching goal of seamless integration into the field of additive manufacturing the resulting synergy between advanced manufacturing techniques and natureinspired materials promises to revolutionize the production of objects capable of dynamic responses to environmental stimuli extending beyond the confines of laboratory experimentation these selftransforming objects hold significant potential across diverse industries showcasing innovative applications with profound implications for object design and fabrication through the reduction of waste generation minimization of energy consumption and the reduction of environmental footprint the integration of biomaterials biopolymers and additive manufacturing signifies a pivotal step towards fostering ecologically conscious design and manufacturing practices within this context inanimate threedimensional objects will possess the ability to transcend their static nature and emerge as dynamic entities capable of evolution selfrepair and adaptive responses in harmony with their surroundings the confluence of biomimicry and additive manufacturing techniques establishes a seminal precedent for a profound reconfiguration of contemporary approaches to design manufacturing and ecological stewardship thereby decisively shaping a more resilient and innovative global milieu
acute lymphoblastic leukemia all is a fatal blood disorder characterized by the excessive proliferation of immature white blood cells originating in the bone marrow an effective prognosis and treatment of all calls for its accurate and timely detection deep convolutional neural networks cnns have shown promising results in digital pathology however they face challenges in classifying different subtypes of leukemia due to their subtle morphological differences this study proposes an improved pipeline for binary detection and subtype classification of all from blood smear images at first a customized 88 layers deep cnn is proposed and trained using transfer learning along with googlenet cnn to create an ensemble of features furthermore this study models the feature selection problem as a combinatorial optimization problem and proposes a memetic version of binary whale optimization algorithm incorporating differential evolutionbased local search method to enhance the exploration and exploitation of feature search space the proposed approach is validated using publicly available standard datasets containing peripheral blood smear images of various classes of all an overall best average accuracy of 9915 is achieved for binary classification of all with an 85 decrease in the feature vector together with 99 precision and 988 sensitivity for ball subtype classification the best accuracy of 9869 is attained with 987 precision and 9957 specificity the proposed methodology shows better performance metrics as compared with several existing studies
during the coronavirus disease 2019 covid19 pandemic nonfacetoface ecommerce has become a significant consumer channel for customers to buy fresh food however little is known about customer opinion changes in fresh food ecommerce ffec products and services during covid19 this study investigated the changes in expectations and preferences of ffec customers on products and services before and during the pandemic from online reviews through a text mining approach we divided the pandemic into two phases acute and recovery and found that eight attributes affect customers’ opinions some logistic servicerelated attributes gained customer attention during the acute phase but productrelated attributes gained more attention in the recovery phase customers showed a great level of forgiveness on many attributes during the acute phase but customers’ dissatisfaction was expressed during the recovery phase finally the results of the comparative importance–performance analysis provide improvement strategies for ffec and help optimize their resource allocation of ffec and enhance sustainable operation capacity in the case of a crisis
modern distributed keyvalue kv stores increasingly adopt erasure coding to reliably store data to adapt to the changing demands on access performance and reliability requirements distributed kv stores perform redundancy transitioning by tuning the redundancy schemes with different coding parameters however redundancy transitioning incurs extensive network ios which impair the performance of distributed kv stores we propose a new family of erasure codes called elastic reedsolomon ers codes whose primary goal is to mitigate network ios in redundancy transitioning ers codes eliminate data block relocation while limiting network ios for parity block updates via the new codesign of encoding matrix construction and data placement ers codes achieve such gains in both forward and backward transitioning scenarios we realize ers codes in a distributed kv store prototype based on memcached and show via testbed experiments in both local and cloud environments that ers codes significantly reduce the latency of redundancy transitioning compared with stateofthearts
physics research complements traditional approaches such as mathematical stochastic finance and econometrics in quantitative economics and finance in the early years of this millennium we embarked on an interdisciplinary research endeavor in lithuania applying concepts from statistical physics to understand complex financial and social systems here we provide a short review of investigations in lithuania spanning from 2008 to 2022 undertaken by our research group
ionic liquids ils with dimethyl sulfoxide dmso and water act as a promising solvent medium for the dissolution of cellulose in an efficient manner to develop a proper solvent system it is really important to understand the thermodynamics of the molecular solutions consisting of ils dmso and water the ionpairing propensity of the ils in the presence of dmso and water plays a crucial role in governing the property of the solvent mixtures employing allatom molecular dynamics simulations we estimate the potentials of mean force between bmim and cl ions in dmsowater mixtures analysis reveals a significant increase in the thermodynamic stability of both contact ion pair cip and solventassisted ion pair saip states with a rising dmso mole fraction thermodynamic assessments highlight the entropic stabilization of cip states and saip states in pure water in dmsowater mixtures and in pure dmso the structural analysis reveals that in comparison to the dmso local density the local water density is relatively very high around ion pairs more specifically in the solvation shell of a chloride ion preferential binding coefficients also consistently indicate exclusion of dmso from the ion pair in dmsowater mixtures to enhance our understanding regarding the solvent molecules kinetics around the ion pairs the survival probabilities of dmso and water are computed the calculations reveal that the water molecules prefer a prolonged stay in the solvation shell of cl ions
humanoid reaction synthesis is pivotal for creating highly interactive and empathetic robots that can seamlessly integrate into human environments enhancing the way we live work and communicate however it is difficult to learn the diverse interaction patterns of multiple humans and generate physically plausible reactions the kinematicsbased approaches face challenges including issues like floating feet sliding penetration and other problems that defy physical plausibility the existing physicsbased method often relies on kinematicsbased methods to generate reference states which struggle with the challenges posed by kinematic noise during action execution constrained by their reliance on diffusion models these methods are unable to achieve realtime inference in this work we propose a forward dynamics guided 4d imitation method to generate physically plausible humanlike reactions the learned policy is capable of generating physically plausible and humanlike reactions in realtime significantly improving the speedx33 and quality of reactions compared with the existing method our experiments on the interhuman and chi3d datasets along with ablation studies demonstrate the effectiveness of our approach
discreteevent simulation models generate random variates from input distributions and compute outputs according to the simulation logic the input distributions are typically fitted to finite realworld data and thus are subject to estimation errors that can propagate to the simulation outputs an issue commonly known as input uncertainty iu this paper investigates quantifying iu using the output confidence intervals cis computed from bootstrap quantile estimators the standard direct bootstrap method has overcoverage due to convolution of the simulation error and iu however the bruteforce way of washing away the former is computationally demanding we present two new bootstrap methods to enhance direct resampling in both statistical and computational efficiencies using shrinkage strategies to downscale the variabilities encapsulated in the cis our asymptotic analysis shows how both approaches produce tight cis accounting for iu under limited input data and simulation effort along with the simulation samplesize requirements relative to the input data size we demonstrate performances of the shrinkage strategies with several numerical experiments and investigate the conditions under which each method performs well we also show advantages of nonparametric approaches over parametric bootstrap when the distribution family is misspecified and over metamodel approaches when the dimension of the distribution parameters is high history accepted by bruno tuffin area editor for simulation funding this work was supported by the national science foundation career cmmi1834710 career cmmi2045400 dms1854659 and iis1849280 supplemental material the software that supports the findings of this study is available within the paper and its supplemental information  httpspubsonlineinformsorgdoisuppl101287ijoc20220044  as well as from the ijoc github software repository  httpsgithubcominformsjoc20220044  the complete ijoc software and data repository is available at httpsinformsjocgithubio 
long text matching is widely used in various subtasks of natural language processing however conducting research in this field can be challenging due to excessive redundant and distracting information the complex semantic context and the limited availability of high quality public datasets existing long text matching methods generally do not fully use the rich local features embedded in text information and focus more on encoding long text as fixed length vectors to calculate the semantic distance disregarding the importance of feature interaction in the text matching process therefore the performance of the relevant models needs to be improved to address these problems a hierarchical and multipleperspective interaction network hmin is proposed in this paper first the long text is encoded at the word and sentence levels to extract global features while onedimensional convolutional neural networks and attention mechanisms are used to focus on important local features in long texts second the different types of features are compared separately using the comparison function and then the comparison results are aggregated finally whether long texts are matched is determined in the prediction layer we have conducted comparative experiments on two datasets the results show that hmin has an improvement in accuracy and f1 values compared with the same type of existing algorithms and the related experimental analysis demonstrates the effectiveness of the proposed method
introduction fluid teams have become increasingly prevalent and necessary for modernday issues yet they differ from more traditional teams on which much of the current teams literature is based for example fluid teams are often comprised of members from different disciplines or organizational divisions who do not have a shared history or future as they come together to perform a critical timesensitive task and then disband for these reasons the mechanisms through which they function and perform may differ from those of more traditional teams and research is needed to better understand these differences methods to this end this study utilized critical incident techniques and thematic analysis to examine fluid teams within healthcare one of the primary contexts in which they are prevalent interdisciplinary faculty and students in the medical field who encounter fluid teams within simulationbased education were prompted to reflect on key factors that facilitate or hinder fluid team effectiveness results primary themes extracted pertained to the conditions fluid teams operate within eg highstress the behaviors and emergent states that contribute to their success eg communication and the ksao’s of value for members of fluid teams to possess eg readiness these themes were then compared to existing literature yielding the identification of some similarities but also many important differences between fluid and traditional teams discussion a series of practical recommendations for how to promote fluid team effectiveness is then presented
this research investigates the impact of data augmentation and noise injection on various image filtering techniques employed for static image denoising and image recognition tasks the study utilizes the mnist dataset comprising 60000 handwritten digit images for testing and 10000 for training purposes several filtering methods including mean filtering median filtering laplace’s filtering gaussian filtering canny edge filtering spatial filtering normalized box blur filtering quadratic image filtering and bilateral image filtering are evaluated the evaluation involves visually demonstrating the effects of these filters on sample images both before and after training the model subsequently accuracy metrics are computed for each filtering technique the performance of the filters are measured using peak signal to noise ration psnr and normalized mean square error nmse
functional brain network organization measured by functional connectivity fc reflects key neurodevelopmental processes for healthy development early exposure to adversity eg undernutrition affects neurodevelopment observable via disrupted fc and leads to poorer outcomes from preschool age onward we assessed longitudinally the impact of early growth trajectories on developmental fc in a rural gambian population from age 5 to 24 months to investigate how these early trajectories relate to later childhood outcomes we assessed cognitive flexibility at 35 years we observed that early physical growth before the fifth month of life drove optimal developmental trajectories of fc that in turn predicted cognitive flexibility at preschool age in contrast to previously studied developmental populations this gambian sample exhibited longrange interhemispheric fc that decreased with age our results highlight the measurable effects that poor growth in early infancy has on brain development and the subsequent impact on preschool age cognitive development underscoring the need for early life interventions throughout global settings of adversity
abstract research on digital transformation activities on top managers of businesses has revealed that a most executives believes that they can buy digital technology apply it across the business and see the benefits within a few months in similar studies it was determined that the rate of cases where digital transformation failed in such businesses reached up to 70 therefore considering the depth and scope of change required for companies to be successful in the digital transformation process these thoughts and expectations of company executives are unrealistic in addition it expresses another misconception that it would be sufficient for the digital transformation process to be carried out only by the information technology managers cio and simply the it team in this study the application challenges and opportunities that arise in the digital transformation process will be explained within the framework of changing company business models and the factors that determine success and failure will be emphasized based on managerial perspective
floods impact communities worldwide resulting in loss of life damaged infrastructure and natural assets and threatened livelihoods climate change and urban development in flood‐prone areas will continue to worsen flood‐related losses increasing the urgency for effective tools to monitor recovery many earth observation eo applications exist for flood‐hazard monitoring and provide insights on location timing and extent in near real‐time and historically to estimate flood risk less attention has been paid to flood recovery even though differing recovery rates and outcomes can have immediate and enduring distributional effects within communities eo data are uniquely positioned to monitor post‐flood recovery and inform policy on hazard mitigation and adaptation but remain underutilized we encourage the eo and flood research community to refocus on developing flood recovery applications to address growing risk translation of eo insights on flood recovery among flood‐affected communities and decision‐makers is necessary to address underlying social vulnerabilities that exacerbate inequitable recovery outcomes and advocate for redressing injustices where disparate recovery is observed we identify an unequivocal need for eo to move beyond mapping flood hazard and exposure toward post‐flood recovery monitoring to inform recovery across geographic contexts this commentary proposes a framework for remote sensing scientists to engage community‐based partners to integrate eo with non‐eo data to advance flood recovery monitoring characterize inequitable recovery redistribute resources to mitigate inequities and support risk reduction of future floods
the field of soft robotics is rapidly evolving and there is a growing interest in developing soft robots with bioinspired features for use in various applications this research presented the design and development of 3dprinted origami actuators for a soft robot with amphibious locomotion and tongue hunting capabilities two different types of programmable origami actuators were designed and manufactured namely zshaped and twist tower actuators in addition two actuator variations were developed based on the zshaped actuator including the pelvic fin and the coilinguncoiling types the zshaped actuators were used for the rear legs to facilitate the locomotion of the waterlike frogs meanwhile the twisted tower actuators were used for the rotation joints in the forelegs and for locomotion on land the pelvic fin actuator was developed to imitate the land locomotion of the mudskipper and the coilinguncoiling actuator was designed for tongue hunting motion the origami actuators and soft robot prototype were tested through a series of experiments which showed that the robot was capable of efficiently moving in water and on land and performing tongue hunting motions our results demonstrate the effectiveness of these actuators in producing the desired motions and provide insights into the potential of applying 3dprinted origami actuators in the development of soft robots with bioinspired features
we consider the problem of jointly optimizing the daily production planning and energy supply management of an industrial complex with manufacturing processes renewable energies and energy storage systems it is naturally formulated as a mixedinteger multistage stochastic problem this problem is challenging for three main reasons there is a large number of time steps typically 24 renewable energies are uncertain and uncontrollable and we need binary variables modeling hard constraints we discuss various solution strategies in particular model predictive control dynamic programming and heuristics based on the stochastic dual dynamic programming algorithm we compare these strategies on two variants of the problem with or without dayahead energy purchases
purpose according to the world health organization classification for tumors of the central nervous system mutation status of the isocitrate dehydrogenase idh genes has become a major diagnostic discriminator for gliomas therefore imagingbased prediction of idh mutation status is of high interest for individual patient management we compared and evaluated the diagnostic value of radiomics derived from dual positron emission tomography pet and magnetic resonance imaging mri data to predict the idh mutation status noninvasively methods eightyseven glioma patients at initial diagnosis who underwent pet targeting the translocator protein tspo using 18fge180 dynamic amino acid pet using 18ffet and t1t2weighted mri scans were examined in addition to calculating tumortobackground ratio tbr images for all modalities parametric images quantifying dynamic 18ffet pet information were generated radiomic features were extracted from tbr and parametric images the area under the receiver operating characteristic curve auc was employed to assess the performance of logistic regression lr classifiers to report robust estimates nested crossvalidation with five folds and 50 repeats was applied results tbrge180 features extracted from tspopositive volumes had the highest predictive power among tbr images auc 088 with age as cofactor 094 dynamic 18ffet pet reached a similarly high performance 094 with age 096 the highest lr coefficients in multimodal analyses included tbrge180 features parameters from kinetic and early static 18ffet pet images age and the features from tbrt2 images such as the kurtosis 097 conclusion the findings suggest that incorporating tbrge180 features along with kinetic information from dynamic 18ffet pet kurtosis from tbrt2 and age can yield very high predictability of idh mutation status thus potentially improving early patient management supplementary information the online version contains supplementary material available at 101007s00259024066545
as the scope of plant edna metabarcoding diversifies so do the primers markers and methods a wealth of primers exists today but their comparative evaluation is lacking behind similarly multimarker approaches are recommended but debates persist regarding barcode complementarity and optimal combinations after a literature compilation of used primers we compared in silico 102 primer pairs based on amplicon size coverage and specificity followed by an experimental evaluation of 15 primer pairs on a mock community sample covering 268 plant species and genera and about 100 families the analysis was done for the four most common plant metabarcoding markers rbcl trnl its1 and its2 and their complementarity was assessed based on retrieved species by focusing on existing primers we identify common designs promote alternatives and enhance priorsupported primers for immediate applications the its2 was the bestperforming marker for flowering vascular plants and was congruent to its1 however the combined taxonomic breadth of its2 and rbcl surpassed any other combination highlighting their high complementarity across streptophyta overall our study underscores the significance of comprehensive primer and barcode evaluations tailored to metabarcoding applications
spectral loopholes in the 10002100 nm range have been the focus of attention of quantum hacking research aimed at searching and elimination of vulnerabilities in quantum key distribution systems in this work we concentrate on shorter wavelengths ranged from 400 nm up to 800 nm and experimentally study spectra of insertion losses for a number of fiber optical components to assess potentials for their implementation as countermeasures we show that efficiency of the elements commonly used as countermeasures against hacking attacks in the telecom range can be significantly impaired in the visible range it is found that the vulnerabilities detected in this range play an increasingly important role for hacking strategies such as the inducedphotorefraction attack whose efficiency improves when the wavelength decreases
abstract accurate information on agricultural field boundaries is important for precision agriculture contour detection combining local cues presents a high performance on nature images image sparse representation describes an image is reconstructed by using as few basic functions as possible the number of farmland parcel boundaries is small and unbalanced for the whole agricultural fields it fits the application category of sparse representation in this research we investigate an approach based on contour detection and sparse representation for the extraction of farmland parcel boundaries first field boundaries have an obvious brightness contrast with the internal parts of the farmland parcels we capture the cue to describe per pixel then we use efficient sparse coding algorithm to represent every pixel for boundary determination experimental results show that the proposed method achieves a sensitivity specificity accuracy f1f1and auc of 06089 09055 08865 04073 and 07552 respectively the purpose of this paper is to demonstrate the potential of combining local features with sparse representation for a fast and accurate farmland parcel boundary extraction approach from remote sensing images comparison results with existing methods on two datasets demonstrate that the proposed method is able for accurate discrimination of the farmland parcel boundaries
as assessment becomes more important in education teachers must be advanced in both assessment and teaching course materials the most recent way of assessing the performance of students in listening classrooms is valueadded assessment using google sites as eportfolios one of among the strongest instruments used to assess the efficacy of teachers as well as recognise growth in students nonetheless the use of eportfolios in efl students listening classrooms and how it influences their learning enjoyment remains undetermined due to neither researchers nor students have addressed these issues in depth and studies on using google sites as an eportfoliobased assessment for teaching listening comprehensions have also been underutilised thus this research aims to gain insight towards efl students views on using google sites as an eportfoliobased valueadded assessment tool in listening classrooms it made use of a qualitative case study and included secondyear university students students course reflections and semistructured interviews were both utilised to collect data which was thereafter analysed qualitatively using coding and thematic analysis the results of the study revealed that creating eportfolios with google sites proves helpful in assessing students learning achievement tracking and measuring students progress and diagnosing deficiencies that need to be addressed this study renders a recommendation for the use of other welldeveloped assessment tools in future studies
cancer spatial and temporal heterogeneity fuels resistance to therapies to realize the routine assessment of cancer prognosis and treatment we demonstrate the development of an intelligent disease detection tool iddt a microfluidicbased tumor model integrated with deep learningassisted algorithmic analysis iddt was clinically validated with liquid blood biopsy samples n  71 from patients with various types of cancers eg breast gastric and lung cancer and healthy donors requiring low sample volume ∼200 μl and a highthroughput 3d tumor culturing system ∼300 tumor clusters to support automated algorithmic analysis intelligent decisionmaking and precise segmentation we designed and developed an integrative deep neural network which includes mask regionbased convolutional neural network mask rcnn vision transformer and segment anything model sam our approach significantly reduces the manual labeling time by up to 90 with a high mean intersection over union miou of 0902 and immediate results 2 s per image for clinical cohort classification the iddt can accurately stratify healthy donors n  12 and cancer patients n  55 within their respective treatment cycle and cancer stage resulting in high precision ∼993 and high sensitivity ∼98 we envision that our patientcentric iddt provides an intelligent labelfree and costeffective approach to help clinicians make precise medical decisions and tailor treatment strategies for each patient
"
purpose
firms are increasingly pressured to comply with mandatory supply chain transparency sct regulations drawing on information processing theory ipt this study aims to show how blockchain technology can address information uncertainty and equivocality in assuring regulatory compliance in an interorganizational network ion


designmethodologyapproach
ipt is applied in a single case study of an ion in the mining industry that aimed to implement blockchain to address mandatory sct regulations the authors build on a rich proprietary data set consisting of interviews and substantial secondary material from actors along the supply chain


findings
the case shows that blockchain creates equality between actors enables compliance and enhances efficiency in an ion reducing information uncertainty and equivocality arising from conflict minerals regulation the system promotes engagement and data sharing between parties while protecting commercial sensitive information the lack of central authority prevents larger partners from taking control the system provides mineral provenance and a regulationcompliant record system cost analysis shows that the system is efficient as it is inexpensive relative to volumes and values of metals transacted issues were identified related to collecting richer human rights data for assurance and compliance with due diligence regulations


originalityvalue
the authors provide some of the first evidence in the operations and supply chain management literature of the specific architecture costs and limitations of using blockchain for sct using an ipt lens in an ion setting the authors demonstrate how blockchainbased systems can address two key ipt challenges environmental uncertainty and equivocality
"
amygdala atrophy has been found in frontotemporal dementia ftd yet the specific changes of its subregions across different ftd phenotypes remain unclear the aim of this study was to investigate the volumetric alterations of the amygdala subregions in ftd phenotypes and how they evolve with disease progression patients clinically diagnosed with behavioral variant ftd bvftd n  20 semantic dementia sd n  20 primary nonfluent aphasia pnfa n  20 alzheimers disease ad n  20 and 20 matched healthy controls underwent whole brain structural mri the patient groups were followed up annually for up to 35 years amygdala nuclei were segmented using freesurfer corrected by total intracranial volumes and grouped into the basolateral superficial and centromedial subregions linear mixed effects models were applied to identify changes in amygdala subregional volumes over time at baseline bvftd sd and ad displayed global amygdala volume reduction whereas amygdala volume appeared to be preserved in pnfa asymmetrical amygdala atrophy left  right was most pronounced in sd longitudinally sd and pnfa showed greater rates of annual decline in the right basolateral and superficial subregions compared to bvftd and ad the findings provide comprehensive insights into the differential impact of ftd pathology on amygdala subregions revealing distinct atrophy patterns that evolve over disease progression the characterization of amygdala subregional involvement in ftd and their potential role as biomarkers carry substantial clinical implications
multiview clustering has gained great progress recently which employs the representations from different views for improving the final performance in this paper we focus on the problem of multiview clustering based on the markov chain by considering lowrank constraints since most existing methods fail to simultaneously characterize the relations among different entries in a tensor from the global perspective and describe local structures of similarity matrices of a tensor we propose a novel flexible tensor learning for multiview clustering with the markov chain ftlmcm to solve this problem we also construct transition probability matrices based on the markov chain to fully utilize the connection between the markov chain and spectral clustering specifically the lowrank constraints of the tensor the frontal slices and the lateral slices of the tensor are imposed on the objective function of the proposed method to achieve these goals besides these three constraints can be optimized jointly to achieve mutual refinement ftlmcm also uses the tensor rotation to better explore the relationships among different views we formulate ftlmcm as a problem of lowrank tensor recovery and solve it with the augmented lagrangian multiplier experiments on six different benchmark data sets under six metrics demonstrate that the proposed method is able to achieve better clustering performance
"composite materials are actively used as protection against electromagnetic fields as conductors and to remove excess heat from microcircuits and boards such products are often subjected to mechanical stress during operation and stretched polymer‐film composite materials suffer from deterioration in electrical conductivity which leads to loss of electrically conductive properties in this work the effect of mechanical deformation stretching on electrical conductivity properties was studied a predictive model was built based on boltzmann statistics which allows predicting the loss of electrically conductive properties when materials are under mechanical stress the resulting model was refined by introducing coefficients calculated using an ensemble regression machine learning method the mean absolute percentage error of the predicted electrical conductivity value for the materials under mechanical stress is about 5–20
stretching influence on conductivity and shielding
an expression for description and prediction resistance during stretching
machine learning for predictive modeling
"
nowadays electricity theft is a major issue in many countries and poses a significant financial loss for global power utilities conventional electricity theft detection etd models face challenges such as the curse of dimensionality and highly imbalanced electricity consumption data distribution to overcome these problems a hybrid system multilayer perceptron mlp approach with gated recurrent units gru is proposed in this work the proposed hybrid system is applied to analyze and solve electricity theft using data from the chinese national grid corporation cngc in the proposed hybrid system first preprocess the data second balance the data using the kmeans synthetic minority oversampling technique smote technique third apply the gtu model to the extracted purified data fourth apply the mlp model to the extracted purified data and finally evaluate the performance of the proposed system using different performance measures such as graphical analysis and a statistical test to verify the consistency of our proposed hybrid system we use three different ratios for training and testing the dataset the outcomes show that the proposed hybrid system for etd is highly accurate and efficient compared to the other models like alexnet gru bidirectional gated recurrent unit bgru and recurrent neural network rnn
near realtime monitoring of faecal indicator bacteria fib in waters is currently not feasible and current monitoring methods require field sampling and laboratory testing that inhibits decisionmaking within a relevant timeframe while recent studies identified the potential of using specific fluorescence regions for fib monitoring sufficient accuracy often requires sitespecific calibration due to minor variations in fluorescence peak locations in this study a series of lab experiments were completed to address some of the selectivity issues specifically the study explored correlations between wavelengthspecific fluorescence signals acquired through fluorescence excitationemission matrices eem and the amount of e coli k12 e coli and e faecalis enterococci in exponential and stationary phase broth cultures subsequently the experiments quantified how the addition of known concentrations of ltryptophan amplifies an indole pulse specifically its concentration and the corresponding fluorescence properties results show unique peak excitationemission λexλem wavelengths ± 5 nm in eems for e coli cell pellet and in m9 broth  280  327 nm enterococci cell pellet  276  324 nm ltryptophan  278  343 nm and  298  344 nm and indole  232  321 nm the findings demonstrate that ltryptophan concentrations in e coli broth were reduced at the same time the indole content increased throughout the initiation phase to the stationary phase of the bacteria growth curve with the peak indole pulse occurring approximately at the time of transition from the exponential to stationary phase such unique fluorescence signatures for not only fib but also indole whose pulse can be triggered by ltryptophan provide foundations for developing reliable and near realtime in situ fib sensors
codecompose is an aiassisted code authoring tool powered by large language models llms that provides inline suggestions to 10s of thousands of developers at meta in this paper we present how we scaled the product from displaying singleline suggestions to multiline suggestions this evolution required us to overcome several unique challenges in improving the usability of these suggestions for developers first we discuss how multiline suggestions can have a jarring effect as the llms suggestions constantly move around the developers existing code which would otherwise result in decreased productivity and satisfaction second multiline suggestions take significantly longer to generate hence we present several innovative investments we made to reduce the perceived latency for users these modelhosting optimizations sped up multiline suggestion latency by 25x finally we conduct experiments on 10s of thousands of engineers to understand how multiline suggestions impact the user experience and contrast this with singleline suggestions our experiments reveal that i multiline suggestions account for 42 of total characters accepted despite only accounting for 16 for displayed suggestions ii multiline suggestions almost doubled the percentage of keystrokes saved for users from 9 to 17 multiline codecompose has been rolled out to all engineers at meta and less than 1 of engineers have opted out of multiline suggestions
purpose of review many patients with migraine report their attacks are triggered by various weather anomalies studies have shown mixed results regarding the association of migraine to weather changes the purpose of the current review is to compile the most uptodate research studies on how weather may affect migraine in addition we explore the association between weather and other inflammatory disease states as well as neurotransmitters recent findings migraine attacks can be related to weather variables such as barometric pressure humidity and wind however the results of recent studies are inconsistent weathers’ effect on migraine attacks is around 20 however very strong weather factors have a more significant effect on migraine attack variables summary many individuals identify weather as a migraine attack trigger yet we see no causative relationship between weather and migraine patterns the outcomes of studies indicate mixed results and reflect individual variation in how weather can impact migraine patterns similar relationships can be seen with other rheumatologic and pain conditions in general overall the combination of weather plus other factors appears to be a more significant migraine trigger
neurosymbolic ai is a growing field of research aiming to combine neural networks learning capabilities with the reasoning abilities of symbolic systems this hybridization can take many shapes in this paper we propose a new formalism for supervised multilabel classification with propositional background knowledge we introduce a new neurosymbolic technique called semantic conditioning at inference which only constrains the system during inference while leaving the training unaffected we discuss its theoritical and practical advantages over two other popular neurosymbolic techniques semantic conditioning and semantic regularization we develop a new multiscale methodology to evaluate how the benefits of a neurosymbolic technique evolve with the scale of the network we then evaluate experimentally and compare the benefits of all three techniques across model scales on several datasets our results demonstrate that semantic conditioning at inference can be used to build more accurate neuralbased systems with fewer resources while guaranteeing the semantic consistency of outputs
recent studies have demonstrated that the ability of dense retrieval models to generalize to target domains with different distributions is limited which contrasts with the results obtained with interactionbased models prior attempts to mitigate this challenge involved leveraging adversarial learning and query generation approaches but both approaches nevertheless resulted in limited improvements in this paper we propose to combine the querygeneration approach with a selfsupervision approach in which pseudorelevance labels are automatically generated on the target domain to accomplish this a t53b model is utilized for pseudopositive labeling and meticulous hard negatives are chosen we also apply this strategy on conversational dense retrieval model for conversational search a similar pseudolabeling approach is used but with the addition of a queryrewriting module to rewrite conversational queries for subsequent labeling this proposed approach enables a model’s domain adaptation with real queries and documents from the target dataset experiments on standard dense retrieval and conversational dense retrieval models both demonstrate improvements on baseline models when they are finetuned on the pseudorelevance labeled data
in this article a condition monitoring approach is proposed based on vibration signal aiming at improving the adaptability of feature extraction and the accuracy of classification first the original vibration signal acquired under certain working condition is preprocessed by dividing it into multiple segments followed by the signal decomposition then the features of each decomposed signal are extracted based on the theory of diversity entropy de two parameters in the de are optimized considering the fact that these parameters are crucial for the classification result the optimization objective is to make the different segments of the signal collected in the same working condition have approximate feature characterization by this means the feature of the signal is captured adaptively and accurately using the optimized entropy value finally the support vector machine is used to identify the extracted feature vectors to realize condition classification the experiments on three representative platforms including a crystal liftingrotation system in our laboratory are conducted to verify the effectiveness of the proposed method
the goal of the present work is to characterize the hygroscopic expansion properties of epoxy materials relevant for sensor package components to allow for an appropriate thermomechanical finite elements analysis fea simulation and for the selectioncontrol of lens and encapsulation materials material samples of several epoxybased components used in optical sensor packages are exposed to humidity and variations of ambient conditions leading to changes in their dimensionsa suitable experimental approach for hygroscopic characterization has been identified and is proposed within the present work the resulting properties were implemented in finite elements simulations of relevant lens and package structures
context software professionals learned from their experience during the pandemic that most of their work can be done remotely and now software companies are expected to adopt hybrid work models to avoid the resignation of talented professionals who require more flexibility and worklife balance however hybrid work is a spectrum of flexible work arrangements and currently there are no wellestablished hybrid work configurations to be followed in the postpandemic period goal we investigated how software engineers are experiencing the postpandemic hybrid work landscape aiming to understand the factors that influence their choices between remote and inoffice work method we explored a large south american company by collecting quantitative and qualitative data from 545 software professionals who are currently navigating diverse hybrid work arrangements tailored to their individual and team requirements findings our study revealed an array of factors that significantly impact hybrid work within the software industry including individual preferences worklife balance commute time social interactions productivity and more team dynamics project demands client expectations and organizational strategies also play an important role in shaping the complex landscape of hybrid work configurations in software engineering conclusions in summary the success of hybrid work models depends on balancing individual preferences team dynamics and organizational strategies our study demonstrated that at present there is no onesizefitsall individuals approach to hybrid work in the software industry ccs concepts • social and professional topics rightarrow professional topics
predictive coding is a prominent theoretical framework for understanding the hierarchical sensory processing in the brain yet how it could be implemented in networks of cortical neurons is still unclear while most existing works have taken a handwiring approach to creating microcircuits that match experimental results recent work in applying an optimisation approach to ratebased artificial neural networks revealed that cortical connectivity might result from selforganisation given some fundamental computational principle such as energy efficiency as no corresponding approach has studied this in more plausible networks of spiking neurons we here investigate whether predictive coding properties in a multicompartment spiking neural network can emerge from energy optimisation we find that a model trained with an energy objective in addition to a taskrelevant objective is able to reconstruct internal representations given topdown expectation signals alone additionally neurons in the energyoptimised model also show differential responses to expected versus unexpected stimuli qualitatively similar to experimental evidence for predictive coding these findings indicate that predictivecodinglike behaviour might be an emergent property of energy optimisation providing a new perspective on how predictive coding could be achieved in the cortex
this paper investigates an event‐triggered adaptive resilient control problem for nonlinear systems against unknown false data injection and actuator saturation the sensors of the controlled system are subject to unknown false data injection attacks so that all original states can not be directly applied in the controller design process to address the negative effects of the false data injection attacks attack compensators are introduced in control design simultaneously a dynamic event‐triggering mechanism is set up to reduce the communication burden of the network furthermore an auxiliary system is constructed to cope with the actuator saturation that commonly exists in practical systems based on the lyapunov stability theory it is demonstrated that the designed adaptive controller can guarantee the stability of the controlled system and the boundedness of all signals the validity of the proposed control scheme is evidenced by simulation examples
our focus lies at the intersection between two broader research perspectives 1 the scientific study of algorithms and 2 the scholarship on race and racism many streams of research related to algorithmic fairness have been born out of interest at this intersection we think about this intersection as the product of work derived from both sides from 1 algorithms to 2 racism the starting place might be an algorithmic question or method connected to a conceptualization of racism on the other hand from 2 racism to 1 algorithms the starting place could be recognizing a setting where a legacy of racism is known to persist and drawing connections between that legacy and the introduction of algorithms into this setting in either direction meaningful disconnection can occur when conducting research at the intersection of racism and algorithms the present paper urges collective reflection on research directions at this intersection despite being primarily motivated by instances of racial bias research in algorithmic fairness remains mostly disconnected from scholarship on racism in particular there has not been an examination connecting algorithmic fairness discussions directly to the ideology of colorblind racism we aim to fill this gap we begin with a review of an essential account of colorblind racism then we review racial discourse within algorithmic fairness research and underline significant patterns shifts and disconnects ultimately we argue that researchers can improve the navigation of the landscape at the intersection by recognizing ideological shifts as such and iteratively reorienting towards maintaining meaningful connections across interdisciplinary lines
in an era where micro small and mediumsized businesses are developing and growing proficiency in smes financial reporting management is becoming increasingly important this expansion is not unrelated to indonesias micro small and mediumsized enterprise smes growth which is primarily concentrated in the greek district and keeps growing every year the supporting activities and training management of the financial report presentation of micro small and mediumsized enterprises smes in the village of semampir cerme district of gresik are described in this article this course aims to assist smes micro small and mediumsized businesses in becoming more proficient in handling financial reporting the recording of financial transactions about cash inputs and cash outputs the division of personal and corporate finances financial protection and financial planning are the four key topics covered in the supporting and training materials in smes
abstract lee jh woo hj jung hs jeong jb jang y ryu jh and kim k 2023 mean particle size organic carbon and porewater salinity distribution of surface sediments using a dataset from the hwangdo tidal flat taean western korea in lee jl lee h min bi chang ji cho gt yoon js and lee j eds multidisciplinary approaches to coastal and marine management journal of coastal research special issue no 116 pp 245249 charlotte north carolina issn 07490208 the hwangdo tidal flats are located in cheonsu bay in taeangun chungcheongnam on the west coast of korea cheonsu bay is a semiclosed bay characterized by various geomorphic features including channels sand bars small islands and tidal flats this study assessed the field measurement of ellipsoid height using a realtime kinematics global positioning system m and used laboratory analyses to obtain geological data on sediment type in a pilot study area using data collected in october 2022 n  107 samples analyzed surface sediment characteristics including the mean particle size total organic carbon toc total nitrogen tn total carbon tc porewater salinity specific dry bulk density porosity and water content etc the sedimentary facies were classified following folk and ward 1957 the sedimentary facies ranged from coarse sand sand mud ratio  91 to sandy mud as sandy silt sz  muddy sand ms  slightly gravelly sandy mud gsm organic matter was characterized based on particle size analysis each experimental result was verified the toctn ratio was consistently ≤ 10 which suggests the influence of marine rather than terrestrial organisms the porewater salinity distribution was correlated with the sedimentary phase tidal flat elevation latitude value and organic matter content the salinity was higher in highland tidal flats where the sedimentary facies was to fine silty and the exposure time was long the positive correlations were detected between mean particle size clay content toc and sediment porosity using correlation matrix and factor analysis these data will be useful for understanding the changes in the sedimentary environment of the hwangdo tidal flat and establishing conservation management plans for this area in the future the geological environment characteristics dataset will be used as basic data to assess changes in the tidal flat topography and sedimentation environments
cavityenhanced direct frequency comb spectroscopy cedfcs is widely used as a highly sensitive gas sensing technology in various gas detection fields for the onaxis coupling incidence scheme the detection accuracy and stability are seriously affected by the cavitymode noise and therefore stable operation inevitably requires external electronic modelocking and sweeping devices substantially increasing system complexity to address this issue we propose offaxis cavityenhanced optical frequency comb spectroscopy from both theoretical and experimental aspects which is applied to the detection of single and dualgas of carbon monoxide co and carbon dioxide co2 in the nearinfrared an erbiumdoped fiber frequency comb with a repetition frequency of ∼41709 mhz is coupled into a resonant cavity with a length of ∼360 mm in an offaxis manner exciting numerous highorder modes to effectively suppress cavitymode noise the performance of multiple machine learning models is compared for the inversion of a singledual gas concentration a few absorbance spectra are collected to build a sample data set which is then utilized for model training and learning the results demonstrate that the particle swarm optimization support vector machine psosvm model achieves the highest predictive accuracy for gas concentration and is ultimately applied to the detection system based on allan deviation the detection limit for co in singlegas detection can reach 8247 parts per million by volume ppmv by averaging 87 spectra meanwhile for simultaneous co2co measurement with highly overlapping absorbance spectra the lod can be reduced to 13196 and 4658 ppmv respectively the proposed optical gas sensing technique indicates the potential for the development of a fielddeployable and intelligent sensor system capable of simultaneous detection of multiple gases
yıllar boyunca bütün devletler kendi yapılarına göre kadastro sistemleri oluşturmuştur ve kanun yoluyla düzenleme yollarına gitmiştir bundan dolayıdır ki benzer yönler taşıyabilmek ile birlikte her bir devletin kendine münhasır bir kadastro sistemi bulunmakta ve kadastral işlemleri bu sisteme dayalı olarak yürütmektedir bu çalışmada i̇ran’daki kadastro sistemi ile türkiye’deki kadastro sistemi karşılaştırmalı olarak incelenmiştir ve farklılıklar ortaya konulmaya çalışılmıştır çalışmada i̇ran ve türkiye’deki tapu ve kadastro sistemleri tarihsel boyutlarıyla irdelenerek kadastronun tarihsel gelişimi hukuki kurumsal ve teknik yapısı gibi konular yönünden incelemeler üzerinde durulmuştur ve her iki ülkenin kadastral sistemindeki farklı yönleri vurgulanarak ele alınmıştır
power generation technologies based on water movement and evaporation use water which covers more than 70 of the earth’s surface and can also generate power from moisture in the air studies are conducted to diversify materials to increase power generation performance and validate energy generation mechanisms in this study a waterbased generator was fabricated by coating cellulose acetate with carbon black to optimize the generator fouriertransform infrared spectroscopy specific surface area zeta potential particle size and electrical performance analyses were conducted the developed generator is a cylindrical generator with a diameter of 75 mm and length of 20 mm which can generate a voltage of 015 v and current of 82 μa additionally we analyzed the power generation performance using three factors physical properties cation effect and evaporation environment and proposed an energy generation mechanism furthermore we developed an ecofriendly and lowcost generator using natural fibers with a simple manufacturing process the proposed generator can contribute to the identification of energy generation mechanisms and is expected to be used as an alternative energy source in the future
from the perspective of control theory convolutional layers of neural networks are 2d or nd linear timeinvariant dynamical systems the usual representation of convolutional layers by the convolution kernel corresponds to the representation of a dynamical system by its impulse response however many analysis tools from control theory eg involving linear matrix inequalities require a state space representation for this reason we explicitly provide a state space representation of the roesser type for 2d convolutional layers with cmathrminr1  cmathrmoutr2 states where cmathrmincmathrmout is the number of inputoutput channels of the layer and r1r2 characterizes the widthlength of the convolution kernel this representation is shown to be minimal for cmathrmin  cmathrmout we further construct state space representations for dilated strided and nd convolutions
available transfer capability atc is an important measurement index to evaluate the security margin of interconnected power grids and serve as a reference for the transmission right transaction in modern power systems atc is affected by the transmission network topology renewable power output uncertainty and load demand uncertainty traditional works usually model the power sourceload uncertainty by using robust optimization interval optimization or chanceconstraint optimization which cannot fully reflect the probabilistic distribution of the daily sourceload uncertainty this paper proposes an atc assessment methodology based on the typical stochastic scenarios of renewable output and load demand of multiarea power systems furthermore the conditional generative adversarial network cgan algorithm is adopted to generate and select representative scenario sets based on historical raw data which can fully reflect the usual operating condition of a system with high renewable energy penetration the scenario set that is fed into the atc assessment model can fully characterize the impact of sourceload uncertainty on daily atc finally the proposed method is verified by a modified threearea ieee 9bus system and a realworld provincial power system
large language models llms powered conversational search systems have already been used by hundreds of millions of people and are believed to bring many benefits over conventional search however while decades of research and public discourse interrogated the risk of search systems in increasing selective exposure and creating echo chambers  limiting exposure to diverse opinions and leading to opinion polarization little is known about such a risk of llmpowered conversational search we conduct two experiments to investigate 1 whether and how llmpowered conversational search increases selective exposure compared to conventional search 2 whether and how llms with opinion biases that either reinforce or challenge the users view change the effect overall we found that participants engaged in more biased information querying with llmpowered conversational search and an opinionated llm reinforcing their views exacerbated this bias these results present critical implications for the development of llms and conversational search systems and the policy governing these technologies
background cancer therapeutics have a low success rate in clinical trials an interdisciplinary approach is needed to translate basic clinical and remote fields of research knowledge into novel cancer treatments recent research has identified high dietary phosphate intake as a risk factor associated with cancer incidence a model of tumor dynamics predicted that reducing phosphate levels sequestered in the tumor microenvironment could substantially reduce tumor size coincidently a lowphosphate diet is already in use to help patients with chronic kidney disease manage high serum phosphate levels methods a groundedtheory literaturereview method was used to synthesize interdisciplinary findings from the basic and clinical sciences including oncology nephrology nutritional epidemiology and dietetic research on cancer results findings of tumor remission associated with fasting and a ketogenic diet which lower intake of dietary phosphate support the hypothesis that a lowphosphate diet will reduce levels of phosphate sequestered in the tumor microenvironment and reduce tumor size additionally longterm effects of a lowphosphate diet may reverse dysregulated phosphate metabolism associated with tumorigenesis and prevent cancer recurrence conclusions evidence in this article provides the rationale to test a lowphosphate diet as a dietary intervention to reduce tumor size and lower risk of cancer recurrence
in the change detection cd task the substantial variation in feature distributions across different cd datasets significantly limits the reusability of supervised cd models to alleviate this problem we propose an illumination–reflection decoupled change detection multiscale unsupervised domain adaptation model referred to as irdcduda irdcduda maintains its performance on the original dataset source domain and improves its performance on unlabeled datasets target domain through a novel cduda structure and methodology irdcduda synergizes midlevel global feature marginal distribution domain alignment classifier layer feature conditional distribution domain alignment and an easytohard sample selection strategy to increase the generalization performance of cd models on crossdomain datasets extensive experiments conducted on the levir sysu and gz optical remote sensing image datasets demonstrate that the irdcduda model effectively mitigates feature distribution discrepancies between source and target cd data thereby achieving optimal recognition performance on unlabeled target domain datasets
a fabrypérot fp antenna with wideband dualpolarization and radar cross section rcs reconfigurability adopting waterbased frequency selective surface fss is presented the feed antenna is designed to switch between horizontal polarization hp and vertical polarization vp by controlling the onoff states of two pin diodes by optimizing the height of the fp resonant cavity the proposed fp antenna achieves wideband and highgain properties in addition the incident wave can be absorbed when the polymethyl methacrylate pmma container is filled with water which reduces the monostatic rcs this design can be selected to be in stealth mode or radiation mode by injecting water into or extracting water from the water container the measured results indicate that in stealth mode the 10db rcs reduction band is from 45 ghz to 64 ghz and the peak rcs reduction is 315 db at 52 ghz in the radiation mode the realized gains of the fp antenna in both hp and vp at 52 ghz reach 166 dbi simulated and measured results verify the realizability and operational concept of the designed fp antenna
the peripheral immune system is important in neurodegenerative diseases both in protecting and inflaming the brain but the underlying mechanisms remain elusive alzheimer’s disease is commonly preceded by a prodromal period here we report the presence of large aβ aggregates in plasma from patients with mild cognitive impairment n  38 the aggregates are associated with low level alzheimer’s diseaselike brain pathology as observed by 11cpib pet and 18fftp pet and lowered cd18rich monocytes we characterize complement receptor 4 as a strong binder of amyloids and show aβ aggregates are preferentially phagocytosed and stimulate lysosomal activity through this receptor in stem cellderived microglia kim127 integrin activation in monocytes promotes size selective phagocytosis of aβ hydrodynamic calculations suggest aβ aggregates associate with vessel walls of the cortical capillaries in turn we hypothesize aggregates may provide an adhesion substrate for recruiting cd18rich monocytes into the cortex our results support a role for complement receptor 4 in regulating amyloid homeostasis
within this paper we consider the existence and uniqueness of solutions for fractional integrodiﬀerential equations with statedependent delay on the lipschitz continuous function space our results are obtained by using the resolvent operator theory and the generalized banach contraction mapping principle the regularity of solutions of fractional integrodiﬀerential equations with statedependent delay is also discussed finally an example is provided as an application
aidriven tmd diagnostic system uses ai segmentation method to diagnose temporomandibular joint disorders tmd by using segmentation three important parts temporal bone temporomandibular joint tmj disc and the condyle can be identified the location and the size of each segment are used as the basic information to determine if the patient has a high chance of having temporomandibular joint disorders tmd
echocardiography is a medical examination that uses ultrasound to assess and diagnose the structure and function of the cardiac through the use of ultrasound waves this examination allows medical professionals to create visualizations of the cardiac muscle enabling them to diagnose and monitor conditions such as cardiac diseases abnormalities and functional disorders an echocardiogram plays a crucial role in the early detection and diagnosis of various cardiac issues such as hypertension myocardial infarction valvular cardiac disease and myocardial hypertrophy it significantly contributes to determining treatment and management strategies to achieve accurate disease diagnosis and develop appropriate treatment plans through echocardiography it is essential to have a thorough understanding of proper probe usage the precise acquisition of echocardiographic images and the ability to interpret various echocardiographic examinations such as twodimensional mmode doppler etc to enhance the skills required for echocardiography medical educational institutions conduct theoretical classes practical sessions using patient models and clinical practice sessions with actual patients however issues such as inadequate practical adaptation due to theorycentric education limitations in practical opportunities due to insufficient practice equipment ethical or safety concerns arising during clinical practice and a lack of educators leading to insufficient feedback are currently being encountered hence there is a need for new educational methods that can address the existing challenges in echocardiography education in this paper as part of these efforts we propose a virtual realitybased immersive simulator for practical echocardiography training the proposed echocardiography simulator allows users to explore a virtual echocardiography examination space by wearing a headmounted display hmd this simulator consists of 3d virtual space models interactive models manipulated by interaction devices and 3d patient models containing normal or abnormal anatomical cardiac models using interactive devices such as hmd controllers and haptic devices users can manipulate 3d models related to echocardiography within the simulator and interact with 3d patient models containing normal or abnormal anatomical cardiac models allowing for the practice of echocardiography examinations ultimately a performance evaluation of the developed immersive virtual reality simulator and usability validation targeting medical university students were conducted the evaluation and validation results confirmed the potential efficacy of the proposed echocardiography vr simulator
the accurate and timely identification of crops holds paramount significance for effective crop management and yield estimation unmanned aerial vehicle uav with their superior spatial and temporal resolution compared to satellitebased remote sensing offer a novel solution for precise crop identification in this study we evaluated a methodology that integrates objectoriented method and random forest rf algorithm for crop identification using multispectral uav images the process involved a multiscale segmentation algorithm utilizing the optimal segmentation scale determined by estimation of scale parameter 2 esp2 eight classification schemes s1–s8 were then developed by incorporating index inde textural glcm and geometric geom features based on the spectrum spec features of segmented objects the besttrained rf model was established through three steps feature selection parameter tuning and model training subsequently we determined the feature importance for different classification schemes and generated a prediction map of vegetation for the entire study area based on the besttrained rf model our results revealed that s5 spec  glcm  inde outperformed others achieving an impressive overall accuracy oa and kappa coefficient of 9276 and 092 respectively whereas s4 spec  geom exhibited the lowest performance notably geometric features negatively impacted classification accuracy while the other three feature types positively contributed the accuracy of ginger luffa and sweet potato was consistently lower across most schemes likely due to their unique colors and shapes posing challenges for effective discrimination based solely on spectrum index and texture features furthermore our findings highlighted that the most crucial feature was the inde feature followed by spec and glcm with geom being the least significant for the optimal scheme s5 the top 20 most important features comprised 10 spec 7 inde and 3 glcm features in summary our proposed method combining objectoriented and rf algorithms based on multispectral uav images demonstrated high classification accuracy for crops this research provides valuable insights for the accurate identification of various crops serving as a reference for future advancements in agricultural technology and crop management strategies
choreographic programming is a paradigm for writing distributed applications it allows programmers to write a single program called a choreography that can be compiled to generate correct implementations of each process in the application although choreographies provide good static guarantees they can exhibit high latency when messages or processes are delayed this is because processes in a choreography typically execute in a fixed deterministic order and cannot adapt to the order that messages arrive at runtime in nonchoreographic code programmers can address this problem by allowing processes to execute out of order  for instance by using futures or reactive programming however in choreographic code outoforder process execution can lead to serious and subtle bugs called communication integrity violations civs in this paper we develop a model of choreographic programming for outoforder processes that guarantees absence of civs and deadlocks as an application of our approach we also introduce an api for safe nonblocking communication via futures in the choreographic programming language choral the api allows processes to execute out of order participate in multiple choreographies concurrently and to handle unordered data messages we provide an illustrative evaluation of our api showing that outoforder execution can reduce latency and increase throughput by overlapping communication with computation
this article attempts to summarize the effort by the particle physics community in addressing the tedious work of determining the parameter spaces of beyondthestandardmodel bsm scenarios allowed by data these spaces typically associated with a large number of dimensions especially in the presence of nuisance parameters suffer from the curse of dimensionality and thus render naive sampling of any kind  even the computationally inexpensive ones  ineffective over the years various new sampling from variations of markov chain monte carlo mcmc to dynamic nested sampling and machine learning ml algorithms have been adopted by the community to alleviate this issue if not all we discuss potentially the most important among them and the significance of their results in detail
comprehensively mapping the genetic basis of human disease across diverse individuals is a longstanding goal for the field of human genetics1–4 the all of us research program is a longitudinal cohort study aiming to enrol a diverse group of at least one million individuals across the usa to accelerate biomedical research and improve human health56 here we describe the programme’s genomics data release of 245388 clinicalgrade genome sequences this resource is unique in its diversity as 77 of participants are from communities that are historically underrepresented in biomedical research and 46 are individuals from underrepresented racial and ethnic minorities all of us identified more than 1 billion genetic variants including more than 275 million previously unreported genetic variants more than 39 million of which had coding consequences leveraging linkage between genomic data and the longitudinal electronic health record we evaluated 3724 genetic variants associated with 117 diseases and found high replication rates across both participants of european ancestry and participants of african ancestry summarylevel data are publicly available and individuallevel data can be accessed by researchers through the all of us researcher workbench using a unique data passport model with a median time from initial researcher registration to data access of 29 hours we anticipate that this diverse dataset will advance the promise of genomic medicine for all
in a recent article nicasiocollazo et al 2023 j phys condens matter 35 425401 explore the viscosity of the pseudohardsphere phs model in this comment we highlight some discrepancies with expected behavior and compare their results to new simulations of the same model as well as to true hard spheres in contrast to the results of nicasiocollazo et al our results follow the relation between shear bulk and longitudinal viscosity expected for isotropic fluids moreover we observe clear differences in behavior between phs and true hard sphere and encourage future hardsphere studies to focus on the true hard sphere model whenever possible
although the thermal infrared remote sensing camera plays a pivotal role in earth observation and impacts the target detection surface temperature inversion and subsequent space missions significantly the imaging quality of the camera is constrained by its optics image sensors and electronics during onorbit operation at the same time the traditional blind recovery algorithms which require extensive time for estimating intricate blur kernels encounter challenges due to varying atmospheric conditions and other factors leading to dissimilar blur kernels across different observation scenes in this context this article introduces a rapid image recovery algorithm rooted in the concept of the invariant modulation transfer function imtf specific to onorbit cameras the imtf model remains stable and impervious to influences stemming from ground targets atmospheric conditions and orbital or environmental fluctuations contingent upon the camera’s inherent characteristics the extraction of the imtf involves subjecting the transfer function’s region to a modified edge methodology followed by image recovery through a hyperlaplacian prior inverse convolution approach the resolution of the inverse problem is achieved by employing an alternating minimization scheme this method addresses the mitigation of imaging artifacts originating from the camera’s limitations comparative analysis against the stateoftheart image recovery techniques establishes the competitiveness of the method proposed in this article both in terms of recovery efficacy and operational efficiency substantiating this experimental validation using inorbit thermal infrared remote sensing images reveals a notable improvement in the average gradient ag by a factor of 32 edge intensity ei by a factor of 25 and modulation transfer function by a factor of 13 of the restored images consequently this approach introduces a novel perspective for enhancing the restoration of inorbit remote sensing images
people in complex systems exhibit varying capacities for social interaction because of differences in personal psychology educational attainment and social class it is true that people often use different social networks and show different enthusiasm for obtaining information but their interest in collecting information will decrease over time our study on personal fashion psychology pfp has shown that when people get information they behave in a way known as the diminishing marginal effect dmepfp behavior next we created a sar propagation dynamics model on a multilayer contact network to represent dmepfp behavior using a threshold function then to assess and uncover the transmission mechanism of individual behaviour we use partition theory a boundary phenomena is displayed by the propagation mode as demonstrated by both theoretical analysis and simulated tests the final size can exhibit either discontinuous firstorder phase transitions or continuous secondorder phase changes in individual dmepfp behaviour meanwhile through the unit transmission probability changed the ideal dmepfp parameters occur at the largest final adoption size moreover the promotion of the propagation pattern and behaviour from continuous secondorder to discontinuous firstorder is facilitated by interpersonal contact the numerical analysis and the actual models may eventually agree
this study investigates the perceptions of k12 science teachers regarding the implications of chatgpt on school assessments and the quality of students’ education a questionnaire was administered to k12 science teachers from public and private schools across the five regions of brazil north northeast south southeast and midwest more than 400 teachers responded to the questionnaire the conclusions regarding the opinions of the teachers who responded to the questionnaire were based on the likerttype scale the investigation covered various aspects including the types and frequency of assessment methods used with their students additionally the research delved into their opinions on whether chatgpt would impact the quality of education and assessment methods as well as their views on whether chatgpt use should be deemed plagiarism or a similar infraction the potential benefits of its use as well as the challenges arising from it are discussed in the context of the evolution of teaching and learning as one of the results it was found that among k12 science teachers there remains some skepticism regarding whether chatgpt will enhance the quality of students’ education and whether its use constitutes plagiarism or a similar infraction
dance and music are well known to improve sensorimotor skills and cognitive functions to reveal the underlying mechanism previous studies focus on the brain plastic structural and functional effects of dance and music training however the discrepancy training effects on brain structurefunction relationship are still blurred thus proficient dancers musicians and controls were recruited in this study the graph signal processing framework was employed to quantify the regionlevel and networklevel relationship between brain function and structure the results showed the increased coupling strength of the right ventromedial putamen in the dance and music groups distinctly enhanced coupling strength of the ventral attention network increased coupling strength of the right inferior frontal gyrus opercular area and increased function connectivity of coupling function signal between the right and left middle frontal gyrus were only found in the dance group besides the dance group indicated enhanced coupling function connectivity between the left inferior parietal lobule caudal area and the left superior parietal lobule intraparietal area compared with the music groups the results might illustrate dance and music trainings discrepant effect on the structurefunction relationship of the subcortical and cortical attention networks furthermore dance training seemed to have a greater impact on these networks
depression harms however due to a lack of mental health awareness and fear of stigma many patients do not actively seek diagnosis and treatment leading to detrimental outcomes depression detection aims to determine whether an individual suffers from depression by analyzing their history of posts on social media which can significantly aid in early detection and intervention it mainly faces two key challenges 1 it requires professional medical knowledge and 2 it necessitates both high accuracy and explainability to address it we propose a novel depression detection system called doris combining medical knowledge and the recent advances in large language models llms specifically to tackle the first challenge we proposed an llmbased solution to first annotate whether highrisk texts meet medical diagnostic criteria further we retrieve texts with high emotional intensity and summarize critical information from the historical mood records of users socalled mood courses to tackle the second challenge we combine llm and traditional classifiers to integrate medical knowledgeguided features for which the model can also explain its prediction results achieving both high accuracy and explainability extensive experimental results on benchmarking datasets show that compared to the current best baseline our approach improves by 0036 in auprc which can be considered significant demonstrating the effectiveness of our approach and its high value as an nlp application
a comprehensive evaluation of an organizations performance throughout its existence is known as its corporate reputation kircova  esen 2018 businesses with a good corporate reputation have a greater chance of influencing the attitudes and actions of their customers giving them a competitive edge kircova 2018 because there are so many comments and opinions on the internet that are easily available to nonspecialists the evaluation of company reputation is becoming a more relevant topic in the field of business studies shayaa 2018 in the past surveys sample groups and qualitative interviews were considered reliable methods for assessing a companys reputation smith 2010 newburry and fombrun  fonzy have already brought attention to a novel approach to assessing a companys reputation the invention of a reputationmeasuring tool and a predictive model that shows how corporate reputation affects stakeholder outcomes were the main goals of fombruns most recent research fombrun fonzy newburry 2015 sentiment analysis using internet reviews as a means of assessing corporate reputation has not received much attention up to this point in order to assess the reputation of the company this study uses sentiment analysis as a suitable tool to look at the opinions that employees have about their firms with our tool businesses can respond to stakeholder requests and market changes in an efficient manner additionally it might serve as a means of informing companies that are unaware of unfavorable internet reviews
mixedinteger optimal control problems arise in many practical applications combining nonlinear dynamic and combinatorial features to cope with the resulting complexity several approaches have been suggested in the past some of them rely on solving a reformulated and relaxed control problem referred to as partial outer convexification poc inspired by an efficient algorithm for switching time optimization by stellato and coworkers switchtimeoptjl we developed an algorithmic approach for poc implemented in a julia package both approaches are based on linearization and exponential integration to obtain second derivatives we show the efficiency and applicability of the novel approach by comparing it to switchtimeoptjl by extending the concept and calculations to the treatment of constraints and by investigating warm starting of switching time optimization an additional comparison to a casadibased standard single shooting approach shows a significant reduction in computational time despite an increase in iterations the new solver facilitates the reliable and fast solution of mixedinteger optimal control problems
the doublerow active omni wheel daow has been proposed to achieve omnidirectional mobility with low vibration and a simple structure compared to the conventional design according to a kinetic analysis the daow was found to have a stabilizing effect on singletrack vehicles in the roll direction but this phenomenon has not yet been confirmed in the real world with this in mind this paper aims to experimentally verify the posturestabilizing effect of the daow we first analyze the results of a previous study and discuss the conditions for establishing the stabilizing effect then we design a modified prototype vehicle and conduct an experiment the result indicates that the prototype vehicle maintains its balance for a certain period and exhibits the behavior of the stabilizing effect
expert demonstrations have proven an easy way to indirectly specify complex tasks recent algorithms even support extracting unambiguous formal specifications eg deterministic finite automata dfa from demonstrations unfortunately these techniques are generally not sample efficient in this work we introduce llm an algorithm for learning dfas from both demonstrations and natural language due to the expressivity of natural language we observe a significant improvement in the data efficiency of learning dfas from expert demonstrations technically llm leverages large language models to answer membership queries about the underlying task this is then combined with recent techniques for transforming learning from demonstrations into a sequence of labeled example learning problems in our experiments we observe the two modalities complement each other yielding a powerful fewshot learner
large language models are limited by challenges in factuality and hallucinations to be directly employed offtheshelf for judging the veracity of news articles where factual accuracy is paramount in this work we propose dell that identifies three key stages in misinformation detection where llms could be incorporated as part of the pipeline 1 llms could emphgenerate news reactions to represent diverse perspectives and simulate usernews interaction networks 2 llms could emphgenerate explanations for proxy tasks eg sentiment stance to enrich the contexts of news articles and produce experts specializing in various aspects of news understanding 3 llms could emphmerge taskspecific experts and provide an overall prediction by incorporating the predictions and confidence scores of varying experts extensive experiments on seven datasets with three llms demonstrate that dell outperforms stateoftheart baselines by up to 168 in macro f1score further analysis reveals that the generated reactions and explanations are greatly helpful in misinformation detection while our proposed llmguided expert merging helps produce bettercalibrated predictions
"



social media and the internet technologies are affecting society the social media environment in which we live has a significant influence on our perceptions and beliefs specifically social media plays a significant role in how information is accessed and how social and cultural environments are organized in the modern world social media has never failed to impress publics from education and information to entertainment and leisure social media has never disappointed its users similarly it has transformed the way content is produced distributed and consumed journalism has now become citizen journalism and reliance on traditional means of news have declined this extensive incorporation of social media into society has encouraged number of perceptions these views are often divided into positive and negative or mixed public opinions about social media traditional society like pakistan is more critical of social media and its integration into other means of communication therefore this study examines the public view on social media platforms a case study of pakistan it uses survey method and online questionnaire is sent to 50 university students 50 housewives 50 working males and 50 media practitioners demographics and other factor impact the view of social media among public



"
the literature on ecg delineation algorithms has seen significant growth in recent decades however several challenges still need to be addressed this work aims to propose a lightweight rpeakdetection algorithm that does not require presetting and performs classification on a samplebysample basis the novelty of the proposed approach lies in the utilization of the typicality eccentricity detection anomaly teda algorithm for rpeak detection the proposed method for rpeak detection consists of three phases firstly the ecg signal is preprocessed by calculating the signal’s slope and applying filtering techniques next the preprocessed signal is inputted into the teda algorithm for rpeak estimation finally in the third and last step the rpeak identification is carried out to evaluate the effectiveness of the proposed technique experiments were conducted on the mitbih arrhythmia database mitad for rpeak detection and validation the results of the study demonstrated that the proposed evolutive algorithm achieved a sensitivity se in  positive predictivity p in  and accuracy acc in  of 9545 9961 and 9509 respectively with a tolerance tol of 100 milliseconds one key advantage of the proposed technique is its low computational complexity as it is based on a statistical framework calculated recursively it employs the concepts of typicity and eccentricity to determine whether a given sample is normal or abnormal within the dataset unlike most traditional methods it does not require signal buffering or windowing furthermore the proposed technique employs simple decision rules rather than heuristic approaches further contributing to its computational efficiency
this study is to focus on the problem of finitetime control design for a class of networked control systems ncss with mixed delays first a novel hybrid dropout compensation control method is proposed to reasonably use available system feedback information which can actively compensate for data packet dropouts in two communication channels second an improved augmented switched system model is constructed based on the mixed delays and the hybrid parameter then the gain matrices that vary with the data packet dropout are formulated to ensure stochastic finitetime stability sfts and improve the transient control performance of the resulting closedloop system the slack variables svsbased algorithm is presented to reduce the computational time and the average dwelltime switching law is determined finally the obtained theoretical results are verified by a numerical example
robots in multiuser environments require adaptation to produce personalized interactions in these scenarios the user’s feedback leads the robots to learn from experiences and use this knowledge to generate adapted activities to the user’s preferences however preferences are userspecific and may suffer variations so learning is required to personalize the robot’s actions to each user robots can obtain feedback in human–robot interaction by asking users their opinion about the activity explicit feedback or estimating it from the interaction implicit feedback this paper presents a reinforcement learning framework for social robots to personalize activity selection using the preferences and feedback obtained from the users this paper also studies the role of user feedback in learning and it asks whether combining explicit and implicit user feedback produces better robot adaptive behavior than considering them separately we evaluated the system with 24 participants in a longterm experiment where they were divided into three conditions i adapting the activity selection using the explicit feedback that was obtained from asking the user how much they liked the activities ii using the implicit feedback obtained from interaction metrics of each activity generated from the user’s actions and iii combining explicit and implicit feedback as we hypothesized the results show that combining both feedback produces better adaptive values when correlating initial and final activity scores overcoming the use of individual explicit and implicit feedback we also found that the kind of user feedback does not affect the user’s engagement or the number of activities carried out during the experiment
the trajectory tracking problem is a fundamental control task in the study of mechanical systems a key construction in tracking control is the error or difference between an actual and desired trajectory this construction also lies at the heart of observer design and recent advances in the study of equivariant systems have provided a template for global error construction that exploits the symmetry structure of a group action if such a structure exists hamiltonian systems are posed on the cotangent bundle of configuration space of a mechanical system and symmetries for the full cotangent bundle are not commonly used in geometric control theory in this paper we propose a group structure on the cotangent bundle of a lie group and leverage this to define momentum and configuration errors for trajectory tracking drawing on recent work on equivariant observer design we show that this error definition leads to error dynamics that are themselves eulerpoincare like and use these to derive simple almost global trajectory tracking control for fullyactuated eulerpoincare systems on a lie group state space
behavioral neuroscience aims to provide a connection between neural phenomena and emergent organismlevel behaviors this requires perturbing the nervous system and observing behavioral outcomes and comparing observed postperturbation behavior with predicted counterfactual behavior and therefore accurate behavioral forecasts in this study we present fabel a deep learning method for forecasting future animal behaviors and locomotion trajectories from historical locomotion alone we train an offline pose estimation network to predict animal bodypart locations in behavioral video then sequences of pose vectors are input to deep learning timeseries forecasting models specifically we train an lstm network that predicts a future food interaction event in a specified time window and a temporal fusion transformer that predicts future trajectories of animal bodyparts which are then converted into probabilistic label forecasts importantly accurate prediction of food interaction provides a basis for neurobehavioral intervention in the context of compulsive eating we show promising results on forecasting tasks between 100 milliseconds and 5 seconds timescales because the model takes only behavioral video as input it can be adapted to any behavioral task and does not require specific physiological readouts simultaneously these deep learning models may serve as extensible modules that can accommodate diverse signals such as invivo fluorescence imaging and electrophysiology which may improve behavior forecasts and elucidate invervention targets for desired behavioral change
background psoriasis is a chronic immunemediated inflammatory disease n6methyladenosine m6a is involved in numerous biological processes in both normal and diseased states herein we aimed to explore the potential role of m6a regulators in the diagnosis of psoriasis and predict molecular mechanisms by which m6a regulators impact psoriasis methods gse30999 170 human skin tissue samples and gse13355 180 human skin tissue samples were downloaded as the training analysis dataset and validation dataset respectively m6arelated genes were obtained from the literature and their expression levels in gse30999 samples were measured to identify m6arelated degs between psoriasis lesions ls and nonlesional lesions nl we identified m6arelated degs using differential expression analysis and assessed their interactions through correlation analysis and network construction a logistic regression analysis followed by lasso optimization was employed to select m6arelated degs for the construction of a diagnostic model the performance of the model was validated using support vector machine svm methodology with sigmoid kernel function and extensive crossvalidation additionally the correlation between m6arelated degs and immune cell infiltration was analyzed as well as the association of these degs with psoriasis subtypes functional analysis of the m6arelated degs included the construction of regulatory networks involving mirnas transcription factors tfs and smallmolecule drugs the m6a modification patterns were also explored by examining the gene expression differences between psoriasis subtypes and their enriched biological pathways finally the expression of significant m6a regulators involved in the diagnostic model was examined by rtqpcr results in this study ten optimal m6arelated degs were identified including fto igf2bp2 mettl3 ythdc1 zc3h13 hnrnpc igf2bp3 lrpprc ythdc2 and hnrnpa2b1 a diagnostic model based on these m6arelated degs was constructed demonstrating high diagnostic accuracy with an area under the curve auc in gse30999 and gse13355 of 0974 and 0730 respectively meanwhile the expression level of m6a regulators verified by rtqpcr was consistent with the results in gse30999 the infiltration of activated mast cells and nk cells was significantly associated with all ten m6arelated degs in psoriasis among them ythdc1 hnrnpc and fto were targeted by most mirnas and were regulated by nine related tfs therefore patients may benefit from dorsomorphin and cyclosporine therapy between the two subgroups 1592 degs were identified including lrpprc and mettl3 these degs were predicted to be involved in neutrophil activation cytokinecytokine receptor interactions and chemokine signaling pathways conclusions a diagnostic model based on ten m6arelated degs in patients with psoriasis was constructed which may provide early diagnostic biomarkers and therapeutic targets for psoriasis
pushing tasks performed by aerial manipulators can be used for contactbased industrial inspections underactuated aerial vehicles are widely employed in aerial manipulation due to their widespread availability and relatively low cost industrial infrastructures often consist of diverse oriented work surfaces when interacting with such surfaces the coupled gravity compensation and interaction force generation of underactuated aerial vehicles can present the potential challenge of nearsaturation operations the blind utilization of these platforms for such tasks can lead to instability and accidents creating unsafe operating conditions and potentially damaging the platform in order to ensure safe pushing on these surfaces while managing platform saturation this work establishes a safety assessment process this process involves the prediction of the saturation level of each actuator during pushing across variable surface orientations furthermore the assessment results are used to plan and execute physical experiments ensuring safe operations and preventing platform damage
the purpose of the research was to improve the quality of information process control in the interaction of mobile object grouping this proposal is based on developing and implementing new combined models with algorithms for optimal planning and scheduling of information processes during interaction among mobile objects the proposed approach would guarantee the required level of aircraft maintenance at the airport the novel aspect of the proposed solution is that it divides the spatially distributed task of selecting a plan and a schedule for information processes in mobile objects into two subtasks describing the spatial and temporal features of the original task the spatial component of the original problem is described as a static model using this model the optimal distribution of information operations over the resources of mobile objects is carried out without binding these operations to time the time component is described as a dynamic model within which the information operations are tied to time and the corresponding resources examples of construction and research of synthesized plans and schedules are given abbreviations dm dynamic model fifo firstin firstout gr grouping hsc hardware and software complex its intelligent transport system ip information processes mo mobile object pc programmed control sm static model sms special mathematic support ldm logicaldynamic models
overhead lines act as the basic media for electricity transmission and their stable operation remains really important to the whole power systems as a result realtime monitoring towards the operation status and fast discovery towards the device fault are in urgent demand to deal with such issue this work presents design and optimization of an intelligent monitoring system for overhead lines based on common information model cim firstly tree format data structure is established using cim and analysis of cimxml data files are completed based on cim then the necessary relational tables without affecting database performance are established in order to ensure that the mapped relational database can fully express all kinds of relationships in cim finally the abnormal temperature rise at the connection of overhead lines in traction power supply systems is selected as the object and some simulation is conducted to evaluate performance of the designed prototype system the results show that the system can accurately detect the voltage value from normal working state to the limit state of human safety voltage with relatively small errors about 36 compared with inductive highvoltage detection technology this system has higher detection capability the intelligent monitoring of overhead lines based on the common information model cim optimizes the combined contact force characteristic values of pantographs and dynamic schemes of overhead lines reducing the average risk loss
this paper introduces a novel approach to the creation and application of confusion matrices for error pattern discovery in spellchecking for the croatian language the experimental dataset has been derived from a corpus of mistyped words and user corrections collected since 2008 using the croatian spellchecker available at ispravime the important role of confusion matrices in enhancing the precision of spellcheckers particularly within the diverse linguistic context of the croatian language is investigated common causes of spelling errors emphasizing the challenges posed by diacritic usage have been identified and analyzed this research contributes to the advancement of spellchecking technologies and provides a more comprehensive understanding of linguistic details particularly in languages with diacriticrich orthographies like croatian the presented userdatadriven approach demonstrates the potential for custom spellchecking solutions especially considering the everchanging dynamics of language use in digital communication
rapid growth in publication of clinical trial reports has made it extremely challenging to conduct systematic reviews automatic extraction of population intervention comparator and outcome pico from clinical trial reports can alleviate the traditionally adopted timeconsuming process of systematic reviews in this paper we propose a novel approach for automatically detecting the picorelated terms from clinical trial reports our techniques use biolinkbert as our base model with two bilstm layers our proposed model when trained on either coarsegrained datasets like ebmnlp ebmcomet and redhot or finegrained datasets such as ebmnlprev and ebmnlph irrespective of the nature of the dataset achieved stateoftheart sota results we have also shown with our framework that ensembling technique can be highly effective for the token span prediction task and empirical findings demonstrate that our ensemble approach when applied on ebmnlp dataset achieves new stateoftheart
introduction renal cell carcinoma rcc is a heterogeneous group of diseases the most common type of rcc is clear cell rcc tumor biopsy is the «gold» standard for verifying the diagnosis however it can be unsatisfactory due to the characteristic heterogeneity of the rcc structure noninvasive diagnostic methods — computed tomography and magnetic resonance imaging — in combination with the use of texture analysis can potentially provide a large amount of information about the structure of the kidney tumor and the presumed degree of its differentiation gradeobjective тo analyze publications devoted to texture analysis in rcc the possibilities and prospects of using this method to increase the information content of ct and mr studiesmaterials and methods our review presents data obtained from available sources pubmed scopus and web of science published up to march 2022 inclusive found using the keywords renal cell carcinoma ct mri texture analysis radiomics in russian and englishresults the literature review describes the methods of texture analysis selection of the region of interest modality and contrast phase of the study diagnostic aim based on the results of published scientific papers the authors conclude that the use of texture analysis makes it possible to predict the grade of rcc with high sensitivity specificity and accuracy as well as to make a differential diagnosis of rcc with other kidney neoplasias primarily lipid poor angiomyolipomasconclusion the use of texture analysis based on published materials is extremely promising for noninvasive prediction of rcc grade and its differential diagnosis however the difference in methods and the lack of standardization of texture analysis requires additional research
panax notoginseng p notoginseng is a valuable herbal medicine as well as a dietary food supplement known for its satisfactory clinical efficacy in alleviating blood stasis reducing swelling and relieving pain however the ability of p notoginseng to absorb and accumulate cadmium cd poses a significant environmental pollution risk and potential health hazards to humans in this study we employed laserinduced breakdown spectroscopy libs for the rapid detection of cd it is important to note that signal uncertainty can impact the quantification performance of libs hence we proposed the crater–spectrum feature fusion method which comprises ablation crater morphology compensation and characteristic peak ratio correction cprc to explore the feasibility of signal uncertainty reduction the crater morphology compensation method namely adding variables using multiple linear regression mlr analysis decreased the rootmeansquare error of the prediction set rmsep from 70233 μgg to 54043 μgg the prediction results were achieved after cprc pretreatment using the calibration curve model with an rmsep of 34980 μgg a limit of detection of 192 μgg and a limit of quantification of 641 μgg the crater–spectrum feature fusion method reached the lowest rmsep of 28556 μgg based on a leastsquares support vector machine lssvm model the preliminary results suggest the effectiveness of the crater–spectrum feature fusion method for detecting cd furthermore this method has the potential to be extended to detect other toxic metals in addition to cd which significantly contributes to ensuring the quality and safety of agricultural production
ducted propellers present broad applicability in urban air mobility vehicles due to their enhanced operational safety improved aerodynamic performance and potential to mitigate noise emissions this study proposes a numerical approach for designing adequate duct geometries focusing on the ducts lip profile expansion ratio and tip clearance aiming to provide valuable design guidance for ducted propellers the simulations are validated through experimental data showing reasonable agreement in terms of thrust generation and farfield noise the mean flow and generated thrust are characterized with a parametric study using steady simulations while delayed detached eddy simulations are employed to capture transient flow characteristics and investigate noise generation the noise levels were computed using the integral solution of the ffowcswilliams and hawkings equation the lip geometry impacts the flow distribution and generated thrust modifying the tonal noise furthermore slightly divergent ducts can increase the total thrust by minimizing flow separation on the duct wall while increasing the suction on the duct lip the primary noise sources are identified at the propellers leading edge and tip the results reveal that divergent ducts effectively reduce tonal noise at all observer angles but increase broadband noise attributed to the noise sources at the leading edge of the propeller and the interaction with the duct lip additionally reducing the tip clearance from 2 to 1 mm enhances the total thrust by more than 20 without causing extra noise generation
land use plays an important role in maintaining carbon stock balance ecosystem sustainability and the environment massive land use changes in forest areas peatlands mangroves and greenways result in an increase in co2 release this research aimed to analyze the impact of land use changes on the value of the carbon stock around yogyakarta international airport the data used were pleiades images in 2014 2018 and 2022 image analysis was carried out visually to produce detailed and accurate land use classification meanwhile multitemporal map overlays were carried out to find out land use changes changes in carbon stock were obtained from the land use formula multiplied by the value of the greenhouse gas constant ggc the results showed that the construction of an airport and its supporting infrastructure triggered land use changes that had implications for the decreasing carbon stock the decrease in the area of vegetation cover in fields community plantations and mixed plantations from 2014 to 2022 amounting to 64099 ha increased carbon emissions the results of the analysis showed that there had been changes in carbon stock in 2014 the value was 15028657 t cha in 2018 it decreased to 13663156 t cha and in 2022 it reduced to 13355436 t cha massive economic activity and infrastructure development trigger reduced vegetation cover resulting in increased carbon and increased carbon being released into the atmosphere the problem of land conversion that affects changes in carbon stock and impacts climate change requires mitigation among which is proper land use management and sustainable spatial planning
in modern highfidelity computational fluid dynamic simulations the primary vortex system in hover often breaks down into secondary vortices the sources of numerical error influencing the prediction of the vortex system were studied by performing highfidelity simulations of the wake of a twobladed rotor and comparing the predictions to stereoscopic particle image velocimetry measurements in different measurement planes various numerical inputs including subiteration convergence blade pitch offset and grid resolution were varied to resolve discrepancies between the measured and predicted vortex characteristics from a previous study done by the authors a parametric study on near and offbody solver subiteration convergence demonstrated that although the secondary vortex characteristics converged as the subiteration convergence of both solvers increased a large discrepancy in the number of secondary vortices remained this discrepancy was investigated by varying the thrust where it was found that the breakdown of the primary vortex is directly linked to the number of secondary vortices dissimilarities in the blade pitch angle which could not be avoided in the experiment were modeled by intentionally using an offset in the blade pitch angle of the two blades it was shown that as blade pitch angle offset increases vortex pairing becomes more distinct when vortex pairing occurred in both the experiment and simulation the decay of secondary vortices in the experiment and simulation agreed best to better match the experimental resolution grid resolution was increased and comparing the two simulations the finer mesh simulation agreed best with the measured primary and secondary vortex characteristics
ground‐coupled heat exchangers gche have received significant attention over several decades as a result of increasing the worlds energy demand and the need for reducing fossil fuels consumption prior studies have demonstrated the effectiveness of utilizing gche with refrigeration and heating systems however optimizing the performance of gche coupled with chillers for heat rejection especially in extreme hot‐humid climates where cooling towers are not very effective is lacking in the literature in this work a ground borehole fitted with a coaxial‐tubes heat exchanger bhe is numerically simulated based on a wide range of data collected for the soil of dubai its real in situ thermophysical properties are characterized the soils upper layer thickness is relatively small and dry that operates in conduction mode while the lower one is water‐saturated that works in coupled conduction‐advection mode the study aims at optimizing the parameters advancing heat rejection into ground considering the actual properties of the soil of dubai the results indicate that the more feasible high‐density polyethylene pipes can perform as good as the steel ones also a great finding based on the presented novel design is that insulating the inner pipe can increase the temperature duty by 55 the proposed design of bhe is relatively inexpensive more feasible and efficient which is achieved for the first time based on a deep analysis of dubai climate and soil this makes the technology ready to be implemented for industrial applications in dubai and other regions having a similar climate and soil nature
wind speed fluctuation generally shortens the lifetime of the insulated gate bipolar transistor igbt module in the doubly fed induction generator dfig which leads to frequent component replacement and cost increase of wind power generation conventional power control strategies consider solely the wind turbine output power maximization and existing junction temperature thermal management strategies focus on igbt lifetime extension both of which are not effective under turbulent wind speed conditions and ignore the tradeoff between the device reliability and output power for the overall dfig economic benefit to bridge the gap between the igbt reliability and the output power under turbulent wind speed conditions we propose a novel igbt thermal management strategy for optimal economic benefit specifically we first smooth the output power through a linear power control lpc method with conditions on the wind speed and turbine speed to reduce the igbt lowfrequency junction temperature fluctuation which extends igbt lifetime we then propose a grid encirclement and suppression search gess method that maximizes the economic benefit defined by lpcsmoothed output power and igbt lifetime simulation shows that our proposed lpcgess strategy outperforms a competing strategy at economic benefit by a large margin up to 153 our proposed strategy can potentially be used in wind power stations for fast and reliable economic benefitoriented igbt thermal management
background and purpose radiation oncology is an essential component of therapeutic oncology and necessitates welltrained personnel multicatheter brachytherapy mcbt is one radiotherapeutic option for earlystage breast cancer treatment however specialized handson training for mcbt is not currently included in the curriculum for residents a recently developed handson brachytherapy workshop has demonstrated promising results in enhancing knowledge and practical skills nevertheless these simulationbased teaching formats necessitate more time and financial resources our analyses include computational models for the implementation and delivery of this workshop and can serve as a basis for similar educational initiatives methods this study aimed to assess the costeffectiveness of a previously developed and evaluated breast brachytherapy simulation workshop using a microcosting approach we estimated costs at a detailed level by considering supplies soft and hardware and personnel time for each task this method also allows for a comprehensive evaluation of the costs associated with implementing new medical techniques the workshop costs were divided into two categories development and workshop execution the cost analysis was conducted on a perparticipant basis and the impact on knowledge improvement was measured using a questionnaire results the total workshop costs were determined by considering the initial workshop setup expenses including the development and conceptualization of the course with all involved collaborators as well as the costs incurred for each individual course the workshop was found to be financially efficient with a perparticipant cost of € 39 considering the industrial sponsorship provided for brachytherapy equipment in addition we assessed the workshop’s efficacy by analyzing participant feedback using likert scale evaluations the findings indicated a notable enhancement in both theoretical and practical skills among the participants moreover the costtobenefit ratio cbfr analysis demonstrated a cbfr of € 1353 for each likert point increment conclusion the handson brachytherapy workshop proved to be a valuable and approximately costeffective educational program leading to a significant enhancement in the knowledge and skills of the participants without the support of industrial sponsorship the costs would have been unattainable supplementary information the online version of this article 101007s00066024022186 contains supplementary material which is available to authorized users
background catheter ablation for atrial fibrillation is recommended for symptomatic patients after failed medical therapy ablation has a higher failure rate in obese patients and both the prevalence of atrial fibrillation and obesity are increasingly globally the outcome of ablation can be improved if obese patients can achieve goaloriented weight reduction prior to ablation conventional weight loss strategies however can be difficult to access and can delay ablation thereby risking a lower chance of maintaining sinus rhythm effective weightloss medications such as the glucagonlike peptide inhibitor1 drugs offer the potential for incremental impact on weight loss over a shorter period of time as a bridging therapy the aim of this study is to assess the feasibility of using liraglutide a glucagonlike peptide inhibitor1 in producing weight loss in obese patients before catheter ablation methods the study is an openlabel uncontrolled prospective singlecentre feasibility study of daily liraglutide injections in the treatment of obese patients for at least 13 weeks before and 52 weeks after af ablation adult patients with symptomatic af whose body mass index ≥ 30 will be recruited from those planning to undergo ablation feasibility will be determined based on the recruitment rate adherence to the medication and the amount of weight loss achieved over the study period exploratory outcomes include changes in atrial structure function and fibrosis with weight loss evaluated by cardiac magnetic resonance imaging electroanatomic mapping and patientreported outcome measure discussion this study will allow us to determine whether the use of liraglutide in obese patients with atrial fibrillation undergoing ablation is feasible with adequate recruitment the additional information on adherence and average weight loss over the study period will inform the design of a future definitive randomized controlled trial trial registration clinicaltrialsgov nct05221229 registered on 2 february 2022 trial funding metchley park medical society and university of birmingham starter fellowship british heart foundation accelerator grant abbott investigatorinitiated study grant supplementary information the online version contains supplementary material available at 101186s4081402401454y
transition metal atom mencapsulating silicon cage nanoclusters msi16 exhibit a superatomic nature depending on the central m atom owing to the number of valence electrons and charge state on organic substrates since msi16 superatom featuring group 4 and 5 transition metal atoms exhibit raregaslike and alkalilike characteristics respectively group 6 transition metal atoms are expected to show alkaline earthlike behavior in this study msi16 comprising a central atom from group 6 mvi  cr mo and w were deposited on c60 substrates and their electronic and chemical stabilities were investigated in terms of their charge state and chemical reactivity against oxygen exposures in comparison to alkalilike tasi16 the extent of charge transfer to the c60 substrate is approximately doubled while the oxidative reactivity is subdued for mvisi16 on c60 especially for wsi16 the results show that a divalent state of mvisi162 appears on the c60 substrate which is consistently calculated to be a symmetrical cage structure of wsi162 in c3v revealing insights into the “periodic law” of msi16 superatoms pertaining to the characteristics of alkaline earth metals
the authors show that neuronal populations in the human prefrontalmotor network interact via two discernible communication modes – ramping dynamics and neural oscillations these modes operate in concert to facilitate ruleguided behavior contextual cues and prior evidence guide human goaldirected behavior the neurophysiological mechanisms that implement contextual priors to guide subsequent actions in the human brain remain unclear using intracranial electroencephalography ieeg we demonstrate that increasing uncertainty introduces a shift from a purely oscillatory to a mixed processing regime with an additional ramping component oscillatory and ramping dynamics reflect dissociable signatures which likely differentially contribute to the encoding and transfer of different cognitive variables in a cueguided motor task the results support the idea that prefrontal activity encodes rules and ensuing actions in distinct coding subspaces while theta oscillations synchronize the prefrontalmotor network possibly to guide action execution collectively our results reveal how two key features of largescale neural population activity namely continuous ramping dynamics and oscillatory synchrony jointly support ruleguided human behavior
recently new semantic segmentation and object detection methods have been proposed for the direct processing of threedimensional 3d lidar sensor point clouds lidar can produce highly accurate and detailed 3d maps of natural and manmade environments and is used for sensing in many contexts due to its ability to capture more information its robustness to dynamic changes in the environment compared to an rgb camera and its cost which has decreased in recent years and which is an important factor for many application scenarios the challenge with highresolution 3d lidar sensors is that they can output large amounts of 3d data with up to a few million points per second which is difficult to process in real time when applying complex algorithms and models for efficient semantic segmentation most existing approaches are either only suitable for relatively small point clouds or rely on computationally intensive sampling techniques to reduce their size as a result most of these methods do not work in real time in realistic field robotics application scenarios making them unsuitable for practical applications systematic point selection is a possible solution to reduce the amount of data to be processed although our approach is memory and computationally efficient it selects only a small subset of points which may result in important features being missed to address this problem our proposed systematic sampling method called sys3ds systematic sampling for 3d semantic segmentation incorporates a technique in which the local neighbours of each point are retained to preserve geometric details sys3ds is based on the graph colouring algorithm and ensures that the selected points are nonadjacent in order to obtain a subset of points that are representative of the 3d points in the scene to take advantage of the ensemble learning method we pass a different subset of nodes for each epoch this leverages a new technique called autoensemble where ensemble learning is proposed as a collection of different learning models instead of tuning different hyperparameters individually during training and validation sys3ds has been shown to process up to 1 million points in a single pass it outperforms the state of the art in efficient semantic segmentation on large datasets such as semantic3d we also present a preliminary study on the validity of the performance of lidaronly data ie intensity values from lidar sensors without rgb values for semiautonomous robot perception
training image captioning models using teacher forcing results in very generic samples whereas more distinctive captions can be very useful in retrieval applications or to produce alternative texts describing images for accessibility reinforcement learning rl allows to use crossmodal retrieval similarity score between the generated caption and the input image as reward to guide the training leading to more distinctive captions recent studies show that pretrained crossmodal retrieval models can be used to provide this reward completely eliminating the need for reference captions however we argue in this paper that ground truth gt captions can still be useful in this rl framework we propose a new image captioning model training strategy that makes use of gt captions in different ways firstly they can be used to train a simple mlp discriminator that serves as a regularization to prevent reward hacking and ensures the fluency of generated captions resulting in a textual gan setup extended for multimodal inputs secondly they can serve as additional trajectories in the rl strategy resulting in a teacher forcing loss weighted by the similarity of the gt to the image this objective acts as an additional learning signal grounded to the distribution of the gt captions thirdly they can serve as strong baselines when added to the pool of captions used to compute the proposed contrastive reward to reduce the variance of gradient estimate experiments on mscoco demonstrate the interest of the proposed training strategy to produce highly distinctive captions while maintaining high writing quality
due to the growing need for bandwidth starving multiview videos mvv in virtual reality tv and education effectively allocating the resources of nextgeneration wireless technologies for mvv streams becomes increasingly crucial to achieve high utility for mvv users this article proposes a crosslayer resource allocation mechanism to leverage video synthesizing schemes such as depthimagebased rendering dibr for efficient mvv streaming with massive mimo first we formulate a new problem antenna allocation with video synthesis aavs and prove its nphardness then we design an approximation algorithm named utilitybased multiview synthesis umvs with the analytical performance provided and dynamic scenarios are addressed by augmenting umvs with deep reinforcement learning datadriven simulation results show that umvs outperforms existing antenna allocation schemes by at least 10 and the drl extension provides an additional 6 improvement in system utility under congested scenarios
"

to synthesize the proper control signal while guaranteeing the necessary performance indices speed resilience accuracy etc mathematical models were frequently used to represent physical systems these descriptions were utilized for control monitoring and detection in these kinds of systems quality and performance of the process may suffer if the model is inaccurate or incomplete as a result conformable systems cs may be used to make these mathematical models more near to the real world however nonpowerelectronics experts who need to model and simulate complex systems may find the task of modeling static converters to be rather challenging researchers have just recently outlined the properties of the general conformable systems gcs this innovative approach built upon the principle of the classical integer order systems employing the same mathematical foundations for its derivation with the introduction of this novel description of systems a fresh array of differential equations emerged specifically tailored for the realm of direct current to direct current dcdc static converters gcs has been proved to be more flexible and profitable than the traditional integerorder one for representing dcdc static converters this advancement paved the way for more effective control techniques based on the lyapunov method with practical applications in photovoltaic pv systems and beyond

"
accurate identification of building structure frequencies forms the basis for damage detection the structural dynamic response signal under ambient excitation can be transformed into a superposition of multiple singlefrequency exponentially damped sinusoids combined with random white noise however the peak power spectrum of the response signal tends to exhibit line splitting compromising the precision of frequency identification this study examines the accuracy characteristics of the singlefrequency free damping vibration signal sffdvs and derives the cramer–rao lower bound for the frequency estimator it thoroughly analyzes the factors influencing the accuracy of sffdvs frequency identification the study reveals that the primary cause of spectral line splitting is the random delay inherent in sffdvs based on the maximum likelihood method mlm this research introduces the mlm algorithm for sffdvs and provides a simulation analysis the findings indicate that the mlm estimation algorithm for frequency parameters effectively addresses spectral line splitting and offers robust noise resistance and recognition accuracy
in the resourceconstrained world of the digital landscape lightweight cryptography plays a critical role in safeguarding information and ensuring the security of various systems devices and communication channels its efficient and resourcefriendly nature makes it the ideal solution for applications where computational power is limited in response to the growing need for platformspecific implementations nist issued a call for standardization of lightweight cryptography algorithms in 2018 ascon emerged as the winner of this competition nist initially established general evaluation criteria for a standard lightweight scheme including security strength mitigation against sidechannel and faultinjection attacks and implementation efficiency to verify the security claims evaluating the individual components used in any cryptographic algorithm is a crucial step the quality of a substitution box sbox significantly impacts the overall security of a cryptographic primitive this paper analyzes the sboxes of six finalists in the nist lightweight cryptography lwc standardization process we evaluate them based on wellestablished cryptographic properties our analysis explores how these properties influence the sboxes resistance against known cryptanalytic attacks and potential implementationspecific vulnerabilities thus reflecting on their compliance with nists security requirements
in this paper we introduce the detachment problem it can be seen as a generalized vaccination problem the aim is to optimally cut the individuals ties to circles that connect them to others to minimize the overall information transfer in a social network when an individual is isolated from a particular circle it leads to the elimination of the connections to all the members of that circle yet the connections to other circles remain this approach contrasts with the conventional vaccination problem in which a subset of vertices is totally eliminated in our case the connections of individuals to their circles are selectively rather than entirely eliminated contextually this article focuses on private information flows specifically within networks formed by memberships in circles of insiders in companies our quasiempirical study uses simulated information flows on an observable network and the statistical properties of the simulated information flows are matched with realworld data in a broader context this paper presents the detachment problem as a versatile approach for optimal social distancing applicable across various scenarios we propose and define a concept of expected proportional outside influence or epoi as measure of how widespread information leak is we also implement a greedy algorithm for finding a set of detachments to minimize epoi for comparison we devise a simple heuristic based on minimal cut to separate the most influential circles from each other we provide evidence that the greedy algorithm is not optimal and it is sometimes outperformed by the simple heuristic minimum cut algorithm however the greedy algorithm outperforms the cut algorithm in most cases further avenues of research are discussed
to solve the problems of high computational cost and the long time required by the simulation and calculation of aeroengines’ exhaust systems a method of predicting the characteristics of infrared radiation based on the hybrid kernel extreme learning machine hkelm optimized by the improved dung beetle optimizer idbo was proposed firstly the levy flight strategy and variable spiral strategy were introduced to improve the optimization performance of the dung beetle optimizer dbo algorithm secondly the superiority of idbo algorithm was verified by using 23 benchmark functions in addition the wilcoxon signedrank test was applied to evaluate the experimental results which proved the superiority of the idbo algorithm over other current prominent metaheuristic algorithms finally the hyperparameters of hkelm were optimized by the idbo algorithm and the idbohkelm model was applied to the prediction of characteristics of infrared radiation of a typical axisymmetric nozzle the results showed that the rmse and mae of the idbohkelm model were 2064 and 883 respectively which verified the high accuracy and feasibility of the proposed method for predictions of aeroengines’ infrared radiation characteristics
"onion routing and mix networks are a central technology to enable anonymous communication on the internet as such a large number of protocols and model variants have been explored in the field which offer differing levels of privacy exhibit vulnerabilities or even supersede each other these factors make discovering the appropriate formalization for new developments difficult and some model variants have not been formalized at all
 we address this issue by creating one parametrized framework that encompasses the onion routing and mix network models and functionalities with a global adversary in the related work in doing so we create a categorization of the variants of onion routing models in use in the related work and map common or and mix network protocols to their variants for each identified variant our framework offers i an ideal functionality in the universal composability framework and ii gamebased properties that imply realization of the ideal functionality when a protocol satisfies them in effect our framework both unifies and extends previous formalization efforts in the field"
in the realm of network science determining crucial nodes within a social network is an ongoing concern as a result it garners a lot of attention and various centrality measures for the identification of crucial nodes have been proposed thus far degree and kshell decomposition are the classic centrality measures that rely on neighboring nodes however degree kshell and combination of degree and kshell measures assign the identical value to the vast count of nodes which creates a problem in distinguishing these nodes therefore in this paper for the purpose of solving the above problem we propose an index based on three different components degree improved kshell measure and eigenvector centrality called the degree kshell eigenvector dke index in addition we propose an enhanced gravity model called the dkebased gravity model dkegm on the basis of universal gravity law and the proposed index for determining crucial nodes in social networks the proposed gravity model incorporates different aspects of nodes which include count of neighbors location of nodes influence of neighbors and path information between the nodes numerous experiments are executed on eight real networks using the sir model kendall tau ranking monotonicity and distinct metric to examine the effectiveness of the dkegm with respect to the other measures the empirical outcomes show the effectiveness of the dkegm in terms of accuracy distinguishing ability and efficiency
this paper presents a measurement study on the propagation characteristics of a 28ghz communication link in an indoor environment the research aims to analyze the lineofsight los propagation effects by examining the impact of various materials commonly found in indoor settings such as human body glass wood and drywall etc the measurements were conducted using specialized setup including commercial beamforming transceivers at mathbf4 m distance to emulate reallife scenario instead of extracting the performance of the materials in an anechoic chamber the results highlight some key figures of merit including evm spectrum and signaltonoise ratio snr introduced by these materials at 28ghz providing valuable insights into the behavior of the communication link in typical indoor setting unlike standard material loss measurements that employ cw signal this setup utilizes a modulated signal to simulate realworld scenarios the results make a valuable contribution by facilitating an assessment of signal distortion and the sensitivity of modulation schemes in uncontrolled indoor settings potentially enhancing the design and optimization of forthcoming wireless communication systems operating at mmwave frequencies
prevailing sociolegal structures create a state of personhood limbo for undocumented workers where broader society undermines various aspects of their personhood in a way that prevents them from fully representing and embracing all dimensions of their selves in and around the workplace but how do undocumented workers cope with personhood limbo drawing on interviews with undocumented workers and civil society workers in italy we identify specific forms of what we call “personhood anchoring work” that undocumented workers engage in to claim aspects of personhood that are meaningful to them our theorization suggests that workers’ experiences of personhood are influenced not only by sociolegal structures but also by their own agentic acts in response to external conditions as well as their aspirations past experiences and future plans a key finding of our study is that these practices do not aim to create or disrupt social orders even in subtle or hidden forms of resistance instead they enable undocumented workers to temporarily position themselves within the social order in doing so we also introduce a new way of conceptualizing the integration of undocumented workers that can account for the possibilities and limits of retaining rather than redefining personhood in the face of prevailing constraints
 the integration of educational games has significantly transformed pedagogical approaches emphasizing the importance of constantly updating educational programs highquality games motivate students and foster engagement surpassing traditional methods addressing this need this protocol study aimed to provide practical instruction on developing a memory game educational memory games have been found to enhance intrinsic motivation in learners and are particularly beneficial for clinical subjects the design of memory games involves three components pedagogical level design level and modeling of learning content adjusting elements such as the number and nature of items time limits feedback mechanisms and task difficulty can finetune the effectiveness of a memory game the memory game consists of 2 main components learning and scoring and it is designed using articulate storyline software the learning phase focuses on mastering correct methods however the scoring phase evaluates learning retention and comprehension memory games can be adapted to different formats supported by educational websites and learning management systems they are an effective tool for enhancing cognitive development and memory retention however it is important to consider the audience’s age learning goals genre and assessment approaches when determining the ideal content level memory games can be used to identify learning challenges promote active student engagement and collaboration and improve learning outcomes educators must continually evolve their methods to resonate with new generations of learners and success in implementing memory games depends on considering lesson characteristics student needs and instructor expertise
the existence of abandoned channel represents the end of the evolution history of single river channel taking the c layer in b area a oilfield as an example synthesizing the logging response characteristics plane distribution pattern and the relative depth map method of sand top of abandoned channels abandoned channels were identified under the guidance of modern deposition theory in the threedimensional window a multiangle observation and analysis of connected well profiles was carried out by using the underground dense well pattern data and a total of 10 parts were identified in the research area including stype and penmoon abandoned channel
 complete inventory and human resource management are becoming more prevalent in the enterprise management platform when it comes to rethinking the role of information and communication technologies ict in driving change in global banking the current financial crisis and its effects have us thinking again the inventory management system is a resource allocation platform based on information technology applications that benefit from sophisticated and comprehensive management concepts to offer planning and operational applications for company managers and staff virtual reality and augmented reality vrar technologies are examined in this chapter for their potential to assist the dynamics of financial systems and to overcome the problems provided by unforeseen events and crises nodebased mobility in vr locomotion systems mimics hypertext system exploration and browsing behaviors as a result measurements of hypertext usability such as lostness can be used to determine how bewildered users are while performing assignments in a financial management system enterprise systems have some inefficiencies and other situations management accounting ma responsibilities need highquality data which information systems is supply although is can vary greatly between companies it could impact the logical decisionmaking foundation ie data quality the goal of the information management system based on transfer learning imstl is to research the effect of is quality on data quality in ma and examine the variables that might affect is quality in ma due to the rapid development of the information and knowledge economies in todays situation this paper examines the application of enterprise systems in businesses to examine the erp systems planning and control ideas ideas and the idea of internal control for businesses virtual and augmented reality vr and ar are used in various financial sectors consists of a virtual reality character displaying international financial data from multiple worldwide financial institutions educationally
the flashover characteristics at the gas–solid interface are crucial for secure and reliable application of insulating gases and must be carefully evaluated when selecting environmentally friendly insulating gas this study investigates the flashover characteristics of the epoxy resin surface in various gas mixtures including c4f7nn2 c4f7nco2 and c4f7nair with a focus on the analysis of the generation and impact of solid products using macroscopic and microscopic methods the results show that when n2 is used as the buffer gas the deep decomposition of c4f7n generates carbonbased solid particles on the surface of the epoxy resin forming conductive channel after multiple flashovers and therefore causing surface insulation failure in case of c4f7nco2 and c4f7nair the oxygen element can react with the carboncontaining solid products inhibiting the formation of solid products moreover under high gas pressure  ge 03 mpa in this article and high c4f7n content the surface of the epoxy resin sample in the c4f7nair mixture gas environment can form erosion points containing fluorinecontaining solid particles leading to a lower flashover voltage for c4f7nair than c4f7nco2 these findings provide a better understanding of the flashover characteristics of epoxy resin surfaces in different gas mixtures and contribute to the development of effective insulation medium for gasinsulated equipment
background this study investigated whether music training led to better length estimation andor rightward bias by comparing the performance of musicians pianists and nonmusicians on performance of line sections and line extensions methods one hundred and sixteen participants among them 62 musicians and 54 nonmusicians participated in the present study completed line section and line extension task under three conditions 12 13 and 23 results the mixed repeated measures anova analysis revealed a significant group × condition interaction that the musicians were more accurate than nonmusicians in all the line section tasks and showed no obvious pseudoneglect while their overall performance on the line extension tasks was comparable to the nonmusicians and only performed more accurately in the 12 line extension condition conclusion these findings indicated that there was a dissociation between the effects of music training on line section and line extension this dissociation does not support the view that music training has a general beneficial effect on line estimation and provides insight into a potentially important limit on the effects of music training on spatial cognition
the paper proposes a twolevel scheme for image processing by two independent neural networks with different classification systems the possibility of the proposed twolevel scheme application in video surveillance systems is considered for prohibited items detection the better correction ability is shown compared to different variants of a singlelevel scheme
recent work in humanrobot interaction hri has shown that robots can leverage implicit communicative signals from users to understand how they are being perceived during interactions for example these signals can be gaze patterns facial expressions or body motions that reflect internal human states to facilitate future research in this direction we contribute the react database a collection of two datasets of humanrobot interactions that display users natural reactions to robots during a collaborative game and a photography scenario further we analyze the datasets to show that interaction history is an important factor that can influence human reactions to robots as a result we believe that future models for interpreting implicit feedback in hri should explicitly account for this history react opens up doors to this possibility in the futureccs concepts• humancentered computing → empirical studies in hci • computing methodologies → artificial intelligence
traffic prediction plays an important role in network resource management existing deep learning based traffic prediction methods have huge data requirements and limited accuracy it is difficult to be used in emerging cellular networks eg 6g hence this article proposes a multifeature prediction network mfpnet based on signaling information to overcome the existing problems firstly after exploiting the correlation between traffic and signaling data ie evolved radio access bearer radio resource control etc mfpnet is presented for a single base station bs moreover multiinput convolutional neural network is introduced into mfpnet for feature extraction of signaling information and a shortterm memory network is used for learning temporal correlations secondly mfpnet is extended for multiple bss using both transfer learning and federated learning to achieve accurate traffic prediction using only data from a few base stations the former uses the finetune method for knowledge transfer between bss and the latter uses the doubleheaded federated prediction model to increase the accuracy of traffic prediction finally the proposed method is validated with realworld traffic data obtained from shenzhen the experimental results show the superiority of the proposed models over other benchmark schemes in terms of accuracy and data requirements
abstract the secondary motion caused by turbulence anisotropy is one of the crucial factors for determining the size of cornerflow separation in a sidewall interference flow field therefore through a wallresolved largeeddy simulation les of a sidewall interference flow field this study investigates the effects of the secondary motion on the cornerflow separation and explores the turbulence modelling that can reproduce the secondary flow motion the momentum transport analysis using the les results shows that the secondary vortex has twofold effects on delaying the cornerflow separation the convective transport of the streamwise momentum towards the corner and the enhanced production of turbulence by increasing the shear also the vorticity transport analysis reconfirms that the secondary motion is caused primarily by turbulence anisotropy in the outer layer of the turbulent boundary layer furthermore a quadratic constitutive relation qcr is proposed based on the analysis of the relationship between the reynolds stress and velocity gradient the proposed qcr consists of two quadratic terms and three constant parameters the a priori analysis using the les data shows that the proposed qcr represents the anisotropy of the reynolds stress overall better than the existing qcr reynoldsaveraged navier–stokes simulation using the proposed qcr with the spalart–allmaras turbulence model shows improvements in the prediction of the cornerflow separation compared to the results obtained by the existing qcr with the same turbulence model
in this study the application of open‐source digital elevation model dem is explored for regional landslide susceptibility mapping lsm and the potential impact of different dem choices on the mapping accuracy is also examined with the advancements in remote sensing technology an increasing number of global open‐source dems have been available with improvement in the accuracy however the latest released data are rarely evaluated in lsm research in this paper dem‐based factors including elevation aspect slope plan curvature and profile curvature were generated from seven open‐source dems including advanced spaceborne thermal emission and reflection aster v2 asterv3 alos world 3d‐30 m aw3d30 copernicus dem 30 m cop forest and buildings removed copernicus dem fabdem nasadem and shuttle radar topography mission srtm dem‐based factors were coupled with the distance to road distance to river land use lithology rain and normalized difference vegetation index ndvi the significant difference between dems is determined by comparing the area proportion slope plane curvature and profile curvature are found to have a maximum difference of 15–20 then k‐nearest neighbours knn and random forest rf were used to predict landslide susceptibility with two sampling methods namely 70 for training and 30 for testing s1 67 for training and 33 for testing s2 for knn with s1 the prediction rate is range from 08299 to 08701 with a difference of 00402 the difference of prediction rate is decreased to 00207 for s2 and 00258 for rf cop has the highest prediction rate of 08701 09254 and 09461 for knn with s1 and rf with s1 and s2 respectively asterv2 is the worst with prediction rate of 08897 and 08996 for knn with s2 and rf with s1 respectively the research result provides valuable insights for the selection of open‐source dems in future lsm
mobilenetv2d is a modified version of mobilenetv2 which is the novelty of this article the algorithm is used to classify swiftlet nests into seven classes in 2023 pt waleta asia jaya is required to achieve a 7fold increase in the export quota of swiftlet nests to meet the quota the company made a machine that can recognize swiftlet nest objects which are classified into seven classes based on feather intensity namely brs br bst bs bbt bb and bb2 for the light feathers to the heavy feathers respectively the input image is a combination of four images from four cameras with different positions which adds to the novelty of mobilenetv2d for the particular problem here from the evaluation that has been carried out the accuracy value of the mobilenetv2d model was better than the mobilenetv2 model ie the accuracy value of the mobilenetv2d model was 999928 for the training dataset and 940723 for the testing dataset moreover the speed of mobilenetv2d is better than mobilenetv2 architecture
silicon oxynitride sion is a lowloss and versatile material for linear and nonlinear photonics applications controlling the oxygentonitrogen on ratio in sion provides an effective way to engineer its optical and mechanical properties making it a great platform for the investigation of onchip optomechanical interactions especially the stimulated brillouin scattering sbs here we report the brillouin nonlinearity characterization of a sion platform with a specific on ratio characterized by a refractive index of n165 first we introduce this particular sion platform with fabrication details subsequently we discuss various techniques for the onchip brillouin nonlinearity characterizations in particular we focus on the intensitymodulated pumpprobe lockin amplifier technique which enables ultrasensitive characterization finally we analyze the brillouin nonlinearities of this sion platform and compare them with other sion platforms this work underscores the potential of sion for onchip brillouinbased applications moreover it paves the way for brillouin nonlinearity characterization across various material platforms
while visual literacy has been identified as a foundational skill in life science education there are many challenges in teaching and assessing biomolecular visualization skills among these are the lack of consensus about what constitutes competence and limited understanding of student and instructor perceptions of visual literacy tasks in this study we administered a set of biomolecular visualization assessments developed as part of the biomolviz project to both students and instructors at multiple institutions and compared their perceptions of task difficulty we then analyzed our findings using a mixedmethods approach quantitative analysis was used to answer the following research questions 1 which assessment items exhibit statistically significant disparities or agreements in perceptions of difficulty between instructors and students 2 do these perceptions persist when controlling for raceethnicity and gender and 3 how does student perception of difficulty relate to performance qualitative analysis of openended comments was used to identify predominant themes related to visual problem solving the results show that perceptions of difficulty significantly differ between students and instructors and that students’ performance is a significant predictor of their perception of difficulty overall this study underscores the need to incorporate deliberate instruction in visualization into undergraduate life science curricula to improve student ability in this area accordingly we offer recommendations to promote visual literacy skills in the classroom
the brain extracellular space ecs an irregular extremely tortuous nanoscale space located between cells or between cells and blood vessels is crucial for nerve cell survival it plays a pivotal role in highlevel brain functions such as memory emotion and sensation however the specific form of molecular transport within the ecs remain elusive to address this challenge this paper proposes a novel approach to quantitatively analyze the molecular transport within the ecs by solving an inverse problem derived from the advectiondiffusion equation ade using a physicsinformed neural network pinn pinn provides a streamlined solution to the ade without the need for intricate mathematical formulations or grid settings additionally the optimization of pinn facilitates the automatic computation of the diffusion coefficient governing longterm molecule transport and the velocity of molecules driven by advection consequently the proposed method allows for the quantitative analysis and identification of the specific pattern of molecular transport within the ecs through the calculation of the péclet number experimental validation on two datasets of magnetic resonance images mris captured at different time points showcases the effectiveness of the proposed method notably our simulations reveal identical molecular transport patterns between datasets representing rats with tracer injected into the same brain region these findings highlight the potential of pinn as a promising tool for comprehensively exploring molecular transport within the ecs
this concept paper explores the conceptual framework for developing and implementing communication strategies tailored to the unique challenges of highrisk environments drawing insights from the energy sector the paper identifies the importance of clear communication stakeholder engagement and proactive risk management by establishing a robust conceptual framework organizations can enhance safety mitigate risks and improve overall performance effective communication strategies are paramount in highrisk industries particularly in the energy sector where safety and operational efficiency are nonnegotiable this paper delves into the intricacies of crafting and executing communication strategies that address the specific challenges prevalent in such environments leveraging insights gleaned from the energy sector it underscores the pivotal role of lucid communication active stakeholder engagement and preemptive risk management through the establishment of a sturdy conceptual framework organizations stand to bolster safety protocols mitigate potential risks and elevate their overall performance in highrisk industries like energy effective communication serves as the linchpin that binds together various operational facets ensuring seamless coordination and alignment with overarching objectives amidst the complex operational dynamics characteristic of the energy sector clarity in communication emerges as a foundational element for averting potential hazards and facilitating smooth operations this paper navigates through the nuances of communication strategies tailored to highrisk environments advocating for a strategic approach that not only disseminates information but also fosters understanding and accountability across all levels of the organization drawing from experiences within the energy sector it becomes evident that effective communication transcends mere transmission of messages it embodies a holistic approach that encompasses proactive engagement with stakeholders and meticulous risk assessment in the energy sector stakeholders encompass a diverse spectrum ranging from employees and regulatory bodies to local communities and environmental advocates effective communication entails not only disseminating information but also soliciting feedback fostering transparency and nurturing trust among stakeholders by fostering open channels of communication organizations can cultivate a culture of collaboration thereby enhancing operational efficiency and engendering stakeholder buyin for initiatives aimed at risk mitigation and sustainability
as the frequency and complexity of natural disasters increase effective monitoring and early warning have become important for the protection of life and property this paper discusses the role of unmanned aerial vehicle uav remote sensing technology in natural disaster monitoring and early warning the research in this paper finds that the advantages of uavs which carry multiple sensors and are convenient and dexterous are conducive to natural disaster monitoring the article then provides examples of uav applications in different natural disaster scenarios including monitoring early warning postdisaster assessment and ecological restoration in floods mudslides and earthquakes in flood monitoring uavs equipped with various sensors such as multispectral sensors and infrared thermal imagers can quickly scan floodprone areas and transmit data in real time for timely warning and rescue in mudslide monitoring drones can collect data such as surface temperature soil moisture and vegetation health to help identify signs of potential danger in earthquake monitoring drones can provide highresolution images and video to assess earthquake damage and the condition of infrastructure the future technological innovation and industry development of uav remote sensing will continue to progress in terms of sensor technology innovation application of machine learning and artificial intelligence range extension and convergence of communication technologies the significance of this paper is to highlight the importance of uav remote sensing in natural disaster management and to provide a vision for future research and applications drones will continue to play a key role in facilitating the efficiency and accuracy of natural disaster monitoring and early warning to better address potential threats
underwater images suffer from severe color attenuation and contrast reduction due to the poor and complex lighting conditions in the water most mainstream methods employing deep learning typically require extensive underwater paired training data resulting in complex network structures long training time and high computational cost to address this issue a novel zeroreference parameter estimation network zerouae model is proposed in this paper for the adaptive enhancement of underwater images based on the principle of light attenuation curves an underwater adaptive curve model is designed to eliminate uneven underwater illumination and color bias a lightweight parameter estimation network is designed to estimate dynamic parameters of underwater adaptive curve models a tailored set of nonreference loss functions are developed for underwater scenarios to finetune underwater images enhancing the network’s generalization capabilities these functions implicitly control the learning preferences of the network and effectively solve the problems of color bias and uneven illumination in underwater images without additional datasets the proposed method examined on three widely used realworld underwater image enhancement datasets experimental results demonstrate that our method performs adaptive enhancement on underwater images meanwhile the proposed method yields competitive performance compared with stateoftheart other methods moreover the zerouae model requires only 17k parameters minimizing the hardware requirements for underwater detection tasks what’more the adaptive enhancement capability of the zerouae model offers a new solution for processing images under extreme underwater conditions thus contributing to the advancement of underwater autonomous monitoring and ocean exploration technologies
accurate distribution system models are becoming increasingly critical for grid modernization tasks and inaccurate phase labels are one type of modeling error that can have broad impacts on analyses using the distribution system models this work demonstrates a phase identification methodology that leverages advanced metering infrastructure ami data and additional data streams from sensors relays in this case placed throughout the mediumvoltage sector of distribution system feeders intuitive confidence metrics are employed to increase the credibility of the algorithm predictions and reduce the incidence of falsepositive predictions the method is first demonstrated on a synthetic dataset under known conditions for robustness testing with measurement noise meter bias and missing data then four utility feeders are tested and the algorithm’s predictions are proven to be accurate through field validation by the utility lastly the ability of the method to increase the accuracy of simulated voltages using the corrected model compared to actual measured voltages is demonstrated through quasistatic timeseries qsts simulations the proposed methodology is a good candidate for widespread implementation because it is accurate on both the synthetic and utility test cases and is robust to measurement noise and other issues
the rise of multimodal misinformation on social platforms poses significant challenges for individuals and societies its increased credibility and broader impact compared to textual misinformation make detection complex requiring robust reasoning across diverse media types and profound knowledge for accurate verification the emergence of large vision language model lvlm offers a potential solution to this problem leveraging their proficiency in processing visual and textual information lvlm demonstrates promising capabilities in recognizing complex information and exhibiting strong reasoning skills in this paper we first investigate the potential of lvlm on multimodal misinformation detection we find that even though lvlm has a superior performance compared to llms its profound reasoning may present limited power with a lack of evidence based on these observations we propose lemma lvlmenhanced multimodal misinformation detection with external knowledge augmentation lemma leverages lvlm intuition and reasoning capabilities while augmenting them with external knowledge to enhance the accuracy of misinformation detection our method improves the accuracy over the top baseline lvlm by 7 and 13 on twitter and fakeddit datasets respectively
the nand flash memory channel is corrupted by different types of noises such as the data retention noise and the wearout noise which lead to unknown channel offset and make the flash memory channel nonstationary in the literature machine learningbased methods have been proposed for data detection for flash memory channels however these methods require a large number of training samples and labels to achieve a satisfactory performance which is costly furthermore with a large unknown channel offset it may be impossible to obtain enough correct labels in this paper we reformulate the data detection for the flash memory channel as a transfer learning tl problem we then propose a modelbased deep tl dtl algorithm for flash memory channel detection it can effectively reduce the training data size from 106 samples to less than 104 samples moreover we propose an unsupervised domain adaptation udabased dtl algorithm using moment alignment which can detect data without any labels hence it is suitable for scenarios where the decoding of errorcorrecting code fails and no labels can be obtained finally a udabased threshold detector is proposed to eliminate the need for a neural network both the channel raw error rate analysis and simulation results demonstrate that the proposed dtlbased detection schemes can achieve nearoptimal bit error rate ber performance with much less training data andor without using any labels
the integrated sensing and communication isac system merged with reconfigurable intelligent surface ris has recently received much attention this paper proposes an intelligent metaheuristic version of enhanced artificial ecosystem optimizer eaeo for a suggested beamforming optimization framework in isac systems with ris two ris are utilized in the presented model to enhance the received signaltonoise ratio snr for multipleinput multipleoutput mimo communication systems also each element of each ris scatters the incoming signal with a controllable phaseshift without increasing its power the signal is transmitted by a dualfunction base station dfbs that is integrated with the ris which performs both communication and sensing functions simultaneously the isac system is designed to optimize the signal transmission through the ris by maximizing the snr to increase the overall performance in the proposed eaeo version a fitnessdistancebalance model fdbm is combined with the standard artificial ecosystem optimizer aeo to improve the quality of the solutions in multidimensional and nonlinear optimization scenarios the simulation results show that the proposed eaeo algorithm improves the snr of different users for different numbers of ris elements the snr reaches 25 db when using 200 ris elements moreover the proposed eaeo is tested on the ieee congress on evolutionary computation 2017 ieee cec’17 test suite a comparative analysis is conducted compared to the standard aeo and several recent algorithms the proposed eaeo derives great effectiveness and robustness over the others as it provides a higher rate of success in achieving the global optimum point
recent studies have shown that biased search results can produce substantial shifts in the opinions and voting preferences of undecided voters – a phenomenon called the “search engine manipulation effect” seme one of the most powerful list effects ever discovered we believe this is so because unlike other list effects seme is supported by a daily regimen of operant conditioning when people conduct searches for simple facts 86 of searches the correct answer invariably turns up in the top position which teaches users to attend to and click on highranking search results as a result when people are undecided they tend to formulate opinions based on web pages linked to top search results we tested this hypothesis in a controlled experiment with 551 us voters participants in our hightrust group conducted routine searches in which the correct answer always appeared in the first search result in our lowtrust group the correct answer could appear in any search position other than the first two in all participants had to answer five questions during this pretraining and we focused our analysis on people who answered all the questions correctly  n  355 – in other words on people who were maximally impacted by the pretraining contingencies a difference consistent with our hypothesis emerged between the groups when they were subsequently asked to search for information on political candidates voting preferences in the hightrust group shifted toward the favored candidate at a higher rate 346 than voting preferences in the lowtrust group 171 p  0001
in order to approximate the common solution of quasinonexpansive fixed point and pseudomonotone variational inequality problems in real hilbert spaces this paper presented three new modified subgradient extragradienttype methods our algorithms incorporated viscosity terms and double inertial extrapolations to ensure strong convergence and to speed up convergence no line search methods of the armijo type were required by our algorithms instead they employed a novel selfadaptive step size technique that produced a nonmonotonic sequence of step sizes while also correctly incorporating a number of wellknown step sizes the step size was designed to lessen the algorithms reliance on the initial step size numerical tests were performed and the results showed that our step size is more effective and that it guarantees that our methods require less execution time we stated and proved the strong convergence of our algorithms under mild conditions imposed on the control parameters to show the computational advantage of the suggested methods over some wellknown methods in the literature several numerical experiments were provided to test the applicability and efficiencies of our methods in solving realworld problems we utilized the proposed methods to solve optimal control and image restoration problems
message passing graph neural networks are known to suffer from two problems that are sometimes believed to be diametrically opposed oversquashing and oversmoothing the former results from topological bottlenecks that hamper the information flow from distant nodes and are mitigated by spectral gap maximization primarily by means of edge additions however such additions often promote oversmoothing that renders nodes of different classes less distinguishable inspired by the braess phenomenon we argue that deleting edges can address oversquashing and oversmoothing simultaneously this insight explains how edge deletions can improve generalization thus connecting spectral gap optimization to a seemingly disconnected objective of reducing computational resources by pruning graphs for lottery tickets to this end we propose a more effective spectral gap optimization framework to add or delete edges and demonstrate its effectiveness on large heterophilic datasets
the aim of our study was to predict the occurrence of distant metastases in nonsmallcell lung cancer nsclc patients using machine learning methods and texture analysis of 18flabeled 2deoxydglucose positron emission tomographycomputed tomography 18ffdg petct images in this retrospective and singlecenter study we evaluated 79 patients with advanced nsclc who had undergone 18ffdg petct scan at diagnosis before any therapy patients were divided into two independent training n  44 and final testing n  35 cohorts texture features of primary tumors and lymph node metastases were extracted from 18ffdg petct images using the lifex program six machine learning methods were applied to the training dataset using the entire panel of features dedicated selection methods were used to generate different combinations of five features the performance of selected machine learning methods applied to the different combinations of features was determined using accuracy the confusion matrix receiver operating characteristic roc curves and area under the curve auc a total of 104 and 78 lesions were analyzed in the training and final testing cohorts respectively the support vector machine svm and decision tree methods showed the highest accuracy in the training cohort seven combinations of five features were obtained and introduced in the models and subsequently applied to the training and final testing cohorts using the svm and decision tree the accuracy and the auc of the decision tree method were higher than those obtained with the svm in the final testing cohort the best combination of features included shape sphericity gray level run length matrixrun length nonuniformity glrlmrlnu total lesion glycolysis tlg metabolic tumor volume mtv and shape compacity the combination of these features with the decision tree method could predict the occurrence of distant metastases with an accuracy of 744 and an auc of 063 in nsclc patients
representation learning in neural networks may be implemented with supervised or unsupervised algorithms distinguished by the availability of feedback in sensory cortex perceptual learning drives neural plasticity but it is not known if this is due to supervised or unsupervised learning here we recorded populations of up to 90000 neurons simultaneously from the primary visual cortex v1 and higher visual areas hva while mice learned multiple tasks as well as during unrewarded exposure to the same stimuli similar to previous studies we found that neural changes in task mice were correlated with their behavioral learning however the neural changes were mostly replicated in mice with unrewarded exposure suggesting that the changes were in fact due to unsupervised learning the neural plasticity was concentrated in the medial hvas and obeyed visual rather than spatial learning rules in task mice only we found a ramping reward prediction signal in anterior hvas potentially involved in supervised learning our neural results predict that unsupervised learning may accelerate subsequent task learning a prediction which we validated with behavioral experiments
this paper presents a novel point cloud compression method cotpcc by formulating the task as a constrained optimal transport cot problem cotpcc takes the bitrate of compressed features as an extra constraint of optimal transport ot which learns the distribution transformation between original and reconstructed points specifically the formulated cot is implemented with a generative adversarial network gan and a bitrate loss for training the discriminator measures the wasserstein distance between input and reconstructed points and a generator calculates the optimal mapping between distributions of input and reconstructed point cloud moreover we introduce a learnable sampling module for downsampling in the compression procedure extensive results on both sparse and dense point cloud datasets demonstrate that cotpcc outperforms stateoftheart methods in terms of both cd and psnr metrics source codes are available at httpsgithubcomcognacleepcccot
in service of the goal of examining how cognitive science can facilitate human–computer interactions in complex systems we explore how cognitive psychology research might help educators better utilize artificial intelligence and ai supported tools as facilitatory to learning rather than see these emerging technologies as a threat we also aim to provide historical perspective both on how automation and technology has generated unnecessary apprehension over time and how generative ai technologies such as chatgpt are a product of the discipline of cognitive science we introduce a model for how higher education instruction can adapt to the age of ai by fully capitalizing on the role that metacognition knowledge and skills play in determining learning effectiveness finally we urge educators to consider how ai can be seen as a critical collaborator to be utilized in our efforts to educate around the critical workforce skills of effective communication and collaboration
fewshot learning fsl is a challenging task that aims to train a classifier to recognize novel categories where only a few annotated examples are available in each category recently many fsl approaches have been proposed based on the metalearning paradigm which attempts to learn transferable knowledge from similar tasks by designing a metalearner however most of these approaches only exploit the information from visual modality and do not utilize ones from additional modalities eg textual description since the labeled examples in fsl are limited increasing the information on the examples is a probable solution to improve the classification performance this motivates us to propose a novel metalearning method termed textual enhanced adaptive metafusion fsl tamffsl which leverages both the visual information from the visual image and semantic information from language supervision specifically tamffsl exploits the semantic information of textual description to improve the visualbased models we first employ a text encoder to learn the semantic features of each visual category and then design a modality alignment module and metafusion module to align and fuse the visual and semantic features for final prediction extensive experiments show that the proposed method outperforms many recent or competitive fsl counterparts on two popular datasets
"
 the factors that affect the severity of crashes must be identified for pedestrian and traffic safety in urban roads specifically in the case of urban road crashes these crashes occur due to the complex interaction of various factors therefore it is necessary to collect highquality data that can derive these various factors accordingly this study collected crash data which included detailed crash factor data on the huge urban and midlevel roads using this various crash factors including driver vehicle road environment and crash characteristics are constructed to develop a crash severity prediction model through this this study identified more detailed factors affecting the severity of urban road crashes the crash severity model was developed using both machine learning and statistical models because the insights that can be obtained from the latest technology and traditional methods are different therefore the binary logit model a support vector machine and extreme gradient boosting were developed using key variables derived from the multiple correspondence analysis and borutashapley additive explanations the main result of this study shows that the crash severity decreased at fourstreet intersections and when traffic segregation facilities were installed the findings of this study can be used to establish a traffic safety management strategy to reduce the severity of crashes on urban roads
"
transportation significantly influences economically underdeveloped arid regions impacting economic growth and social progress analyzing transport superiority degree tsd and its implications in such regions is crucial a new arid regionspecific evaluation framework addresses traditional limitations by considering indicators like route connectivity and desert interference this article conducts an empirical study using southern xinjiang as a research case it combines comprehensive evaluation methods spatial autocorrelation methods spatial durbin models and coupling coordination models to depict transport support capability tsc transport access capacity tac and transport guarantee capacity tgc at different scales in southern xinjiang from 2000 to 2020 the study reveals spatial patterns evolutionary characteristics economic impacts and social effects of tsd at various scales key findings include 1 rapid expansion of transportation infrastructure in southern xinjiang the levels of tsd at different scales have gradually increased and spatial and temporal pattern differences are evident at the county level tsd forms a “coreperiphery” spatial pattern centered around the southern xinjiang railway with “highhigh” agglomeration areas centered around kashgar city and “lowlow” agglomeration areas centered around qiemo county 2 prefecturallevel tsd improvements have limited impacts on regional development while countylevel tsc tac tgc and tsd positively affect economic and social growth but also exhibit competitive effects 3 tsd is transitioning from noncoordinated to coordinated development with economic and social progress at different scales this research informs transportation facility evaluation in arid regions
digital signatures are one of the simplest cryptographic building blocks that provide appealing security characteristics such as authenticity unforgeability and undeniability in 1984 shamir developed the first identitybased signature ibs to simplify public key infrastructure and circumvent the need for certificates it makes the process uncomplicated by enabling users to verify digital signatures using only the identifiers of signers such as email phone number etc nearly all existing ibs protocols rely on several theoretical assumptionbased hard problems unfortunately these hard problems are unsafe and pose a hazard in the quantum realm thus designing ibs algorithms that can withstand quantum attacks and ensure longterm security is an important direction for future research quantum cryptography qc is one such approach in this paper we propose an ibs based on qc our schemes security is based on the laws of quantum mechanics it thereby achieves longterm security and provides resistance against quantum attacks we verify the proposed designs correctness and feasibility by simulating it in a prototype quantum device and the ibm qiskit quantum simulator the implementation code in qiskit with jupyternotebook is provided in the annexure moreover we discuss the application of our design in secure email communication
abstract background decolonization treatment of mrsa carriers is recommended in denmark except in households with mrsapositive children 2 years old waitandsee approach objectives to investigate a waitandsee approach in children 2–5 years old and the effect of decolonization treatment of mrsa carriage in all children 6 years old patients and methods in this retrospective followup study we included mrsa carriers 6 years old in the capital region of denmark from 2007 to 2021 data were collected from laboratory information systems and electronic patient records we divided children into age groups of 2 years or 2–5 years and decolonization treatment versus no treatment treatment was chlorhexidine body washes and nasal mupirocin sometimes supplemented with systemic antibiotics children were followed until becoming mrsa free or censoring the probability of becoming mrsa free was investigated with cox regression higher hrs indicate faster decolonization results of 348 included children 226 were 2 years old 56226 25 received treatment and 122 were 2–5 years old 90122 74 received treatment multivariable analyses did not show a larger effect of decolonization treatment versus no treatment in 2yearolds hr 092 95 ci 052–165 or 2–5yearolds hr 054 95 ci 026–112 without treatment 2–5yearolds tended to clear mrsa faster than 2yearolds hr 181 95 ci 098–337 conclusions we did not find a larger effect of decolonization treatment versus no treatment in children 6 years old and 2–5yearolds tended to become mrsa free faster than 2yearolds these results support a waitandsee approach for all children 6 years old but further studies are needed
renewable energy certificates recs serve as vital instruments in incentivizing and verifying the production and consumption of clean and sustainable energy sources the traditional rec management processes are often burdened with challenges related to transparency security and efficiency this research paper presents an innovative approach to addressing these challenges by leveraging blockchain technology the paper explores the design and implementation of a blockchainbased system for the issuance transfer and realtime ownership verification of recs by employing smart contracts and decentralized ledger technology this system enhances rec management in several key aspects blockchains transparent and immutable nature ensures that the entire lifecycle of a rec from its creation to transfer is securely recorded reducing the potential for fraud and errors the proposed system introduces a decentralized approach granting users partial control over rec transfers while maintaining a centralized issuance process this decentralization enhances user benefits and ensures a comprehensive record for each rec in leveraging blockchain technology the system transforms rec management providing a secure transparent and efficient solution for issuance transfer and ownership verification to gain more control over rec issuance the proposed system encourages using decentralized identifiers dids and iot devices to ensure a more decentralized rec issuance process
instantngp is the stateoftheart sota algorithm of neural radiance field nerf and shows great potential to be adopted in arnr however the high cost of memory and computation limits instantngp’s implementation on edge devices in light of this we propose a novel fpgabased accelerator to reduce power consumption called boothnerf boothnerf adopts a fullypipelined technique and is built upon the booth algorithm in addition it introduces a new instruction set to accommodate multilayer perceptrons mlps of different sizes ensuring flexibility and efficiency moreover we propose an fpgafriendly multiplier architecture for matrix multiplication which is capable of performing exact or approximate multiplication using the booth algorithm and the selectshiftadd technique evaluations with a xilinx kintex xc7k325t board show that boothnerf achieves 220times speedup and 131times energy efficiency compared with nvidia jetson xavier nx16g gpu
reconfigurable intelligent surface risaided localization systems have attracted extensive research attention due to their accuracy enhancement capabilities however most studies primarily utilized the base stations bs received signal ie bs information for localization algorithm design neglecting the potential of ris received signal ie ris information compared with bs information ris information offers higher dimension and richer feature set thereby significantly improving the ability to extract positions of the mobile users mus addressing this oversight this letter explores the algorithm design based on the highdimensional ris information specifically we first propose a ris information reconstruction risir algorithm to reconstruct the highdimensional ris information from the lowdimensional bs information the proposed risir algorithm comprises a data processing module for preprocessing bs information a convolution neural network cnn module for feature extraction and an output module for outputting the reconstructed ris information then we propose a transfer learning based fingerprint tfbf algorithm that employs the reconstructed highdimensional ris information for mu localization this involves adapting a pretrained densenet121 model to map the reconstructed ris signal to the mu’s threedimensional 3d position empirical results affirm that the localization performance is significantly influenced by the highdimensional ris information and maintains robustness against unoptimized phase shifts
artificial intelligence ai plays an important role in the dynamic landscape of wireless communications solving challenges unattainable by traditional approaches this paper discusses the evolution of wireless ai emphasizing the transition from isolated taskspecific models to more generalizable and adaptable ai models inspired by recent successes in large language models llms and computer vision to overcome taskspecific ai strategies in wireless networks we propose a unified wireless physicallayer foundation model wpfm challenges include the design of effective pretraining tasks support for embedding heterogeneous time series and humanunderstandable interaction the paper presents a strategic framework focusing on embedding wireless time series selfsupervised pretraining and semantic representation learning the proposed wpfm aims to understand and describe diverse wireless signals allowing human interactivity with wireless networks the paper concludes by outlining next research steps for wpfms including the integration with llms
joint hyperresistance is a common symptom in neurological disorders it has both neural and nonneural origins but it has been challenging to distinguish different origins based on clinical tests alone combining instrumented tests with parameter identification based on a neuromechanical model may allow us to dissociate the different origins of joint hyperresistance in individual patients however this requires that the model captures the underlying mechanisms here we propose a neuromechanical model that in contrast to previously proposed models accounts for muscle shortrange stiffness srs and its interaction with muscle tone and reflex activity we collected knee angle trajectories during the pendulum test in 15 children with cerebral palsy cp and 5 typically developing children we did the test in two conditions – hold and premovement – that have been shown to alter knee movement we modeled the lower leg as an inverted pendulum actuated by two antagonistic hilltype muscles extended with srs reflex activity was modeled as delayed linear feedback from muscle force we estimated neural and nonneural parameters by optimizing the fit between simulated and measured knee angle trajectories during the hold condition the model could fit a wide range of knee angle trajectories in the hold condition the model with personalized parameters predicted the effect of premovement demonstrating that the model captured the underlying mechanism and subjectspecific deficits our model may help with the identification of neural and nonneural origins of joint hyperresistance and thereby opens perspectives for improved diagnosis and treatment selection in children with spastic cp but such applications require further studies to establish the method’s reliability
"recently dense retrieval dr models which represent queries and documents with fixedwidth vectors and retrieve relevant ones via nearest neighbor search have drawn increasing attention from the ir community 
however previous studies have shown that the effectiveness of dr critically relies on sufficient training signals which leads to severe performance degradation when applied in outofdomain scenarios where largescale training data are usually unavailable
to solve this problem existing studies adopt a dataaugmentationplusjointtraining paradigm to construct weakpseudo supervisions on the target domain and combine them with the largescale human annotated data on the source domain to train the dr models however they dont explicitly distinguish the data and the supervision signals in the training process and simply assume that the dr models are mighty enough to capture and memorize different domain knowledge and relevance matching patterns without guidance which as shown in this paper is not true
based on this observation we propose a robust multisupervision combining strategy rmsc that
decouples the domain and supervision signals by explicitly telling the dr models how the domain data and supervision signals are combined in the training data with specially designed soft tokens 
with the extra soft tokens to store the domainspecific and supervisionspecific knowledge rmsc allows the dr models 
to conduct retrieval based on humanlike relevance matching patterns and targetspecific language distribution on the target domain without human annotations
extensive experiments on zeroshot dr benchmarks show that rmsc significantly improves the ranking performance on the target domain compared to strong dr baselines and domain adaptation methods while being stable during training and can be combined with query generation or secondstage pretraining"
cannabidiol is a major component of cannabis but without known psychoactive properties a wide range of properties have been attributed to it such as antiinflammatory analgesic anticancer antiseizure and anxiolytic however being a fairly new compound in its purified form little is known about cannabidiol brain entry especially during development sprague dawley rats at four developmental ages embryonic day e19 postnatal day p4 and p12 and nonpregnant adult females were administered intraperitoneal cannabidiol at 10 mgkg with 3h labelled cannabidiol to investigate the extent of placental transfer the drug was injected intravenously into e19 pregnant dams levels of 3hcannabidiol in blood plasma cerebrospinal fluid and brain were estimated by liquid scintillation counting plasma protein binding of cannabidiol was identified by polyacrylamide gel electrophoresis and its bound and unbound fractions measured by ultrafiltration using available rnasequencing datasets of e19 rat brain choroid plexus and placenta as well as p5 and adult brain and choroid plexus expression of 13 main cannabidiol receptors was analysed results showed that cannabidiol rapidly entered both the developing and adult brains entry into csf was more limited its transfer across the placenta was substantially restricted as only about 50 of maternal blood plasma cannabidiol concentration was detected in fetal plasma albumin was the main but not exclusive cannabidiol binding protein at all ages several transcripts for cannabidiol receptors were expressed in age and tissuespecific manner indicating that cannabidiol may have different functional effects in the fetal compared to adult brain
this study utilizes both fieldwork and deskbased discourse analysis of newspaper reports to investigate the concerning number of suicides among graduates in bangladesh according to some reports a majority of suicide cases involve young adults who are either currently studying at university or have recently completed their degree between the ages of 20 and 32 this research contends that patriarchal social expectations in bangladesh place significant pressure on young adults to secure wellpaying jobs to support their families and uphold their family’s status which can have a negative impact on their mental health furthermore this article identifies additional risk factors that contribute to the high suicide rates among graduates in bangladesh these factors include unemployment poverty relationship problems drug addiction political marginalization and the stigma of shame all of which can cause low selfesteem and suicidal thoughts moreover the research suggests that families in bangladesh have not been providing adequate support to their young members when facing challenges in life on the contrary families have added to the pressure on young adults which can be attributed to joiner’s theory of the effect of industrialization on family norms and values
patients admitted for acute medical conditions and major noncardiac surgery are at risk of myocardial injury this is frequently asymptomatic especially in the context of concomitant pain and analgesics and detection thus relies on cardiac biomarkers continuous single‐lead st‐segment monitoring from wireless electrocardiogram ecg may enable more timely intervention but criteria for alerts need to be defined to reduce false alerts this study aimed to determine optimal st‐deviation thresholds from wireless single‐lead ecg for detection of myocardial injury following major abdominal cancer surgery and during acute exacerbation of chronic obstructive pulmonary disease patients were monitored with a wireless single‐lead ecg patch for up to 4 days and had daily troponin measurements single‐lead st‐segment deviations of 0255 mv andor 0245 mv based on previous study comparison with 01 mv 12‐lead ecg and variation in single‐lead ecg were analyzed for relation to myocardial injury defined as hstnt elevation of 20–64 ngl with an absolute change of ≥5 ngl or a hstnt level ≥ 65 ngl in total 528 patients were included for analysis of which 155 had myocardial injury for corrected st‐thresholds lasting ≥10 and ≥ 20 min we found specificities of 91 and 94 and sensitivities of 17 and 13 with odds ratios of 20 95 ci 11 39 and 24 95 ci 11 51 for myocardial injury in conclusion wireless single‐lead ecg monitoring with corrected st thresholds detected patients developing myocardial injury with specificities 90 and sensitivities 20 suggesting increased focus on sensitivity improvement
human activity recognition poses a significant challenge within active and assisted living aal systems relying extensively on ubiquitous environmental sensorbased acquisition devices to detect user situations in their daily living environmental measurement systems deployed indoors yield multiparametric data in heterogeneous formats which presents a challenge for developing machine learningbased aal models we hypothesized that anomaly detection algorithms could be effectively employed to create datadriven models for monitoring home environments and that the complex multiparametric indoor measurements can often be represented by a relatively small number of latent variables generated through manifold learning mnl techniques we examined both linear principal component analysis and nonlinear autoencoders techniques for generating these latent spaces and the utility of core domain detection techniques for identifying anomalies within the resulting lowdimensional manifolds we benchmarked this approach using three publicly available data sets hh105 aruba and tulum and one proprietary data set elioth for home environmental monitoring our results demonstrated the following key findings 1 nonlinear manifold estimation techniques offer significant advantages in retrieving latent variables when compared to linear techniques 2 the quality of the reconstruction of the original multidimensional recordings serves as an acceptable indicator of the quality of the generated latent spaces 3 domain detection identifies regions of normality consistent with typical individual activities in these spaces and 4 the system effectively detects deviations from typical activity patterns and labels anomalies this study lays the groundwork for further exploration of enhanced methods for extracting information from mnl data models and their application within the aal and possibly other sectors
devices that safely interface with the brain are critical to advancing neuroengineering thin and flexible neural implants show great promise alongside established silicon technologies they therefore require a physical stiffener to allow their insertion into brain tissue bioresorbable polymer shanks are novel transient guides enabling accurate implantation using biocompatible materials that will be absorbed by the body over time the development of materials with optimized stiffness and degradation is needed to provide minimally invasive probes with precise insertion capability under surgical conditions a microfabrication protocol for the patterning of polyvinyl alcohol and its physical cross‐linking is presented resulting in insertion guides with precise shapes and tunable degradation and stiffness the results demonstrate a remarkable improvement in batch fabricating micro‐scale neural shanks with designed crystallinity it results in their prolonged degradation time evaluated in agarose gel and remarkably improved penetrability due to the increase in mechanical stiffness in vitro and in vivo studies support the high acceptability of this combination in interfacing with neural cells and tissue this work represents a novel approach to the material and process engineering of bioresorbable polymers for developing fully organic and safe implants
recent years have seen a growing interest in the use of smart glasses mounted with microphones to solve the cocktail party problem using beamforming techniques or machine learning many such approaches could bring substantial advances in hearing aid or augmented reality ar research to validate these methods the easycom donley et al 2021 dataset introduced highquality multimodal recordings of conversations in noise including egocentric multichannel microphone array audio speech source pose and headset microphone audio while providing comprehensive data easycom lacks diversity in the acoustic environments considered and the degree of overlapping speech in conversations this work therefore presents the group in noise gin dataset of over 2 hours of group conversations in noisy environments recorded using binaural microphones and a pair of glasses mounted with 5 microphones the recordings took place in 3 rooms and contain 6 seated participants as well as a standing facilitator the data also include closetalking microphone audio and headpose data for each speaker an audio channel from a fixed reference microphone and automatically annotated speaker activity information a baseline method is used to demonstrate the use of the data for speech enhancement the dataset is publicly available in dolne et al 2023
whistler waves are a type of lowfrequency electromagnetic wave common in nature which is usually associated with energetic electron phenomena this study presents experimental observations of lowfrequency whistler wave instabilities driven by energetic electrons through wave–particle interactions on exl50 the energetic electrons are generated by electron cyclotron waves ecws through stochastic heating wang et al j plasma phys 89 905890603 2023 and do not match the characteristics of the runaway electrons shi et al nucl fusion 62 086047 2022 in the steadystate plasma of the energy innovation xuanlong50 exl50 whistler waves within the 30–120 mhz frequency range were observed during electron cyclotron resonance heating these waves displayed multiple frequency bands and the frequencies of waves were directly proportional to the alfvén velocity furthermore it was interesting to find that superposition of lower hybrid wave into ecw resulted in the suppression of these whistler waves the experimental results may indicate that the whistler waves are driven by energetic electrons excluding runaway electrons these discoveries carry significant implications for several areas of research including the investigation of wave–particle interactions the development of radio frequency wave current drivers their potential impact on the electron dynamics in future fusion devices and even the presence of unusually lowfrequency whistler waves in earths radiation belts
many vehicular applications especially safetyrelated ones rely on spatialtemporal messages periodically broadcast by vehicles in the absence of a secure authentication scheme invalid spatialtemporal messages may be sent out by malicious vehicles meanwhile malicious applications may also collect a lot of personal information from spatialtemporal messages since intervehicle connections are often deployed in highmoving traffic any authentication must be implemented in realtime to meet all these properties we propose a fast and anonymous spatialtemporal trust fasttrust scheme for intervehicle connections in contrast to most authentication protocols which rely on fixed infrastructures fasttrust is mostly designed on hash chains and an entropybased commitment and is able to secure periodic spatialtemporal messages fasttrust also protects vehicles’ privacy by deploying a pseudonymvarying scheduling mechanism to satisfy the anonymity and unlinkability requirements finally in order to efficiently isolate malicious vehicles a lightweight certificate management scheme is proposed for the limited bandwidth of vehicular networks we provide analytical evaluations to show that our fasttrust achieves the security and privacy properties extensive validations are done to show that fasttrust can authenticate dozens of times faster than the existing signature algorithms and isolate malicious vehicles at a low cost in terms of communication and computational resources
edgegirthregular graphs abbreviated as egr graphs are a class of highly regular graphs more specifically for integers v k g and lambda an egrvkglambda graph is a kregular graph with girth g on v vertices such that every edge is contained in exactly lambda cycles of length g the central problem in this paper is determining nkglambda which is defined as the smallest integer v such that an egrvkglambda graph exists or infty if no such graph exists as well as determining the corresponding extremal graphs we propose a linear time algorithm for computing how often an edge is contained in a cycle of length g given a graph with girth g we use this as one of the building blocks to propose another algorithm that can exhaustively generate all egrvkglambda graphs for fixed parameters v k g and lambda we implement this algorithm and use it in a largescale computation to obtain several new extremal graphs and improvements for lower and upper bounds from the literature for nkglambda among others we show that n36224 n38840 n39660 n39860 n45130 n46935 n652042 and we disprove a conjecture made by araujopardo and leemans discrete math 34510112991 2022 for the cubic girth 8 and girth 12 cases based on our computations we conjecture that n376n3810n3812n3814infty
the presence of weather and water whiplash in mediterranean regions of the world is analyzed using historical streamflow records from 1926 to 2023 depending on the region streamflow from the united states california italy australia chile and south africa is analyzed using publicly available databases water whiplash—or the rapid shift of wet and dry periods—are compared wet and dry periods are defined based on annual deviations from the historical record average and whiplash occurs when there is an abrupt change that overcomes an accommodated deficit or surplus of all the stations there are more dry years 56 than wet years 44 in these regions along with similarities in the variances and shifts in extremes ie whiplash on average 35 of the years were defined as water whiplash years in all countries with the highest levels in the us california where 42–53 of the years were whiplash years the influence of the el niño–southern oscillation enso influences chile and south africa strongest during the first quarter of the year this study found that smaller extreme wet periods and larger and less extreme dry periods are prevalent in mediterranean regions this has implications for water management as adaptation to climate change is considered
we consider random simple temporal graphs in which every edge of the complete graph kn appears once within the time interval 01 independently and uniformly at random our main result is a sharp threshold on the size of any maximum deltaclique namely a clique with edges appearing at most delta apart within 01 in random instances of this model for any constantdelta in particular using the probabilistic method we prove that the size of a maximum deltaclique is approximately frac2lognlogfrac1delta with high probability whp what seems surprising is that even though the random simple temporal graph contains thetan2 overlapping deltawindows which when viewed separately correspond to different random instances of the erdosrenyi random graphs model the size of the maximum deltaclique in the former model and the maximum clique size of the latter are approximately the same furthermore we show that the minimum interval containing a deltaclique is deltaodelta whp we use this result to show that any polynomial time algorithm for deltatemporal clique is unlikely to have very large probability of success
malicious attacks are often inevitable in cyberphysical systems cps accuracy in cyber physical system for position tracking of servos is the major concern now a days in high precision industrial automation it is very hard to achieve accuracy in tracking especially under malicious cyberattacks control saturations parametric perturbations and external disturbances in this paper we have designed a novel predefined time pdt convergence sliding mode adaptive controller ptcsmac for such kind of cyber physical control system main key feature of our control is to cope these challenges that are posed by cps systems such as parameter perturbation control saturation and cyberattacks and the whole system then upgrade to a thirdorder system to facilitate adaptive control law then we present an adaptive controller based on the novel pdt convergent sliding mode surface sms combined with a modified weight updated extreme learning machine elm which is used to approximate the uncertain part of the system another significant advantage of our proposed control approach is that it does not require detailed model information guaranteeing robust performance even when the system model is uncertain additionally our proposed ptcsmac controller is nonsingular regardless of initial conditions and is capable of eradicating the possibility of singularity problems which are frequently a concern in numerous cps control systems finally we have verified our designed ptcsmac control law through rigorous simulations on cps seeker servo positioning system and compared the robustness and performance of different existing techniques
fewshot event detection fsed is a subtask of event detection that aims to accurately identify event types with limited training instances and enable smooth transfer to newlyemerged event types recently the dominant works have used the prototypical network to accomplish this task and employ contrastive learning to alleviate the issue of semanticallyclose categories nevertheless these methods still suffer from two serious problems 1 inadequate learning of prototype representations resulting from limited training data 2 hardeasy sample imbalance and categories imbalance caused by the large number of nontrigger wordo tags in the tokenlevel classification task to address the problems this paper proposes the multichannels prototype and contrastive learning method with conditional adversarial attack which introduces the improved multichannels prototype and contrastive networks to alleviate the categories and hardeasy samples imbalance moreover we devise a constrained adversarial attack to improve the problem of limited training data extensive experimental results show that our model performs better than other fsed methods all the code and data will be available for online public access
ensuring the accuracy of responses provided by large language models llms is crucial particularly in clinical settings where incorrect information may directly impact patient health to address this challenge we construct kqa a dataset containing 1212 patient questions originating from realworld conversations held on a popular clinical online platform we employ a panel of inhouse physicians to answer and manually decompose a subset of kqa into selfcontained statements additionally we formulate two nlibased evaluation metrics approximating recall and precision 1 comprehensiveness measuring the percentage of essential clinical information in the generated answer and 2 hallucination rate measuring the number of statements from the physiciancurated response contradicted by the llm answer finally we use kqa along with these metrics to evaluate several stateoftheart models as well as the effect of incontext learning and medicallyoriented augmented retrieval schemes developed by the authors our findings indicate that incontext learning improves the comprehensiveness of the models and augmented retrieval is effective in reducing hallucinations we will make kqa available to to the community to spur research into medically accurate nlp applications
amphibious buildings use the buoyancy principle in the design of their foundation systems to mitigate flood impact in some cases amphibious buildings are fitted with mechanical systems that further aid the buoyancy element to temporarily raise the building and guide its descent to natural ground level these mechanical systems require external operation preventing the amphibious building from passively responding during flood events as is one of the requirements of a robust flood mitigation measure additionally buildings in flood environments are often left with stains on the exterior facade from floodwater contamination from sewage and chemicals among others this paper distinguishes three main components of an amphibious foundation the buoyancy element vertical guidance post and structural subframe and discusses their functionality the natural world provides solutions to tackling environmental issues such as flooding when systematically studied and transferred nature can inspire innovative ideas for functional and sustainable designs for the built environments although there are many existing designs and a small number of constructed amphibious buildings there are very few studies that discuss how the designs are derived and even fewer on a framework emulating natural systems for transfer into amphibious building design in that context this research uses the biomimetic transfer process to abstract relevant biological systems illustrating their potential for transfer into amphibious foundation design the aim is to understand how these biological systems passively and continuously respond and adapt to their environment organisms such as the venus flower basket giant kelp and red mangrove among others are discussed to understand how they perform the identified functions the steps of the biomimetic transfer process are used to integrate functions of amphibious buildings and processes of the studied biological systems the final output of this paper is a discussion of the ways in which these derived relationships can be adopted in amphibious building design
informational videos serve as a crucial source for explaining conceptual and procedural knowledge to novices and experts alike when producing informational videos editors edit videos by overlaying textimages or trimming footage to enhance the video quality and make it more engaging however video editing can be difficult and timeconsuming especially for novice video editors who often struggle with expressing and implementing their editing ideas to address this challenge we first explored how multimodality—natural language nl and sketching which are natural modalities humans use for expression—can be utilized to support video editors in expressing video editing ideas we gathered 176 multimodal expressions of editing commands from 10 video editors which revealed the patterns of use of nl and sketching in describing edit intents based on the findings we present expressedit a system that enables editing videos via nl text and sketching on the video frame powered by llm and vision models the system interprets 1 temporal 2 spatial and 3 operational references in an nl command and spatial references from sketching the system implements the interpreted edits which then the user can iterate on an observational study n10 showed that expressedit enhanced the ability of novice video editors to express and implement their edit ideas the system allowed participants to perform edits more efficiently and generate more ideas by generating edits based on user’s multimodal edit commands and supporting iterations on the editing commands this work offers insights into the design of future multimodal interfaces and aibased pipelines for video editing
deep reinforcement learning drl is being investigated as a competitive alternative to traditional techniques for solving network optimization problems a promising research direction lies in enhancing traditional optimization algorithms by offloading lowlevel decisions to a drl agent in this study we consider how to effectively employ drl to improve the performance of local search algorithms ie algorithms that starting from a candidate solution explore the solution space by iteratively applying local changes ie moves yielding the best solution found in the process we propose a local search algorithm based on lightweight deep reinforcement learning deepls that given a neighborhood queries a drl agent for choosing a move with the goal of achieving the best objective value in the long term our drl agent based on permutationequivariant neural networks is composed by less than a hundred parameters requiring only up to ten minutes of training and can evaluate problem instances of arbitrary size generalizing to networks and traffic distributions unseen during training we evaluate deepls on two illustrative nphard network routing problems namely ospf weight setting and routing and wavelength assignment training on a single small network only and evaluating on instances 2x10x larger than training experimental results show that deepls outperforms existing drlbased approaches from literature and attains competitive results with stateoftheart metaheuristics with computing times up to 8x smaller than the strongest algorithmic baselines
quantum state designs by enabling an efficient sampling of random quantum states play a quintessential role in devising and benchmarking various quantum protocols with broad applications ranging from circuit designs to black hole physics symmetries on the other hand are expected to reduce the randomness of a state despite being ubiquitous the effects of symmetry on quantum state designs remain an outstanding question the recently introduced projected ensemble framework generates efficient approximate state tdesigns by hinging on projective measurements and manybody quantum chaos in this work we examine the emergence of state designs from the random generator states exhibiting symmetries leveraging on translation symmetry we analytically establish a sufficient condition for the measurement basis leading to the state tdesigns then by making use of the trace distance measure we numerically investigate the convergence to the designs subsequently we inspect the violation of the sufficient condition to identify bases that fail to converge we further demonstrate the emergence of state designs in a physical system by studying the dynamics of a chaotic tilted field ising chain with translation symmetry we find faster convergence of the trace distance during the early time evolution in comparison to the cases when the symmetry is broken to delineate the general applicability of our results we extend our analysis to other symmetries we expect our findings to pave the way for further exploration of deep thermalization and equilibration of closed and open quantum manybody systems
due to the influence of the asian southwest monsoon seasonal drought is serious and water resources are scarce in the yunnan province of southwest china more effective watersaving irrigation methods should be developed to solve the problem of water scarcity in the dry season in this study a subsurface drip irrigation method was used to improve the water productivity of tomato cultivation deficit irrigation was conducted we controlled the lower limit of soil moisture at three different levels 5565 6575 and 7585 of the field capacity the results indicated that the subsurface drip irrigation treatment significantly increased tomato height in the later stage of tomato growth due to the buried pipes the rootshoot ratio was 818 higher for subsurface drip irrigation than for surface drip irrigation methods though the yields using subsurface drip irrigation methods were slightly lower than those obtained using surface drip irrigation methods the tomato quality and water productivity improved significantly the subsurface drip irrigation methods improved the water productivity by 85218 at different soil moisture levels and improved the chlorophyll content by 91173 the vc soluble sugar soluble solids and the ratio of sugar to acid increased by 65152 73216 4166 and 32208 this study also indicated that by optimizing the irrigation methods and patterns water productivity and fruit quality could be improved by more than 50 this research will be helpful for guiding irrigation during the drought season in the southwest monsoon area in asia
rivers with alluvial bars store more wood than those without supplied through channel shifting however wood dynamics arrival or new deposits departure or entrainment and stable or immobile pieces can vary substantially over time in response to critical hydrological drivers that are largely unknown to evaluate them we studied the dynamics of large wood pieces and logjams along a 12‐km reach of the lower allier river using six series of aerial images of variable resolution acquired between 2009 and 2020 during which maximum river discharge fluctuated around the dominant flood discharge q15 that is potentially the bankfull discharge along this well‐preserved not incised reach individual wood departure was best correlated with water levels exceeding dominant flood discharge the duration of the highest magnitude flood was best correlated with wood depositions with shorter floods resulting in a higher number of deposits finally most of the wood remained stable when river discharge did not exceed 60 of q15 over a long period of time changes in inter‐annual wood budget reach‐scale depend on the duration over which discharge exceeded 60 of q15 hydrological conditions driving jam build‐up and removal were similar to those controlling individual wood piece dynamics the results suggest that specific hydrological conditions influence the dynamics of large wood and log jams in the allier river understanding the dynamics of large wood and its impact on river morphology is fundamental for successful river management and habitat restoration initiatives
in this study sedimentation in the seyhan dam reservoir was investigated by analyzing a longterm bathymetric survey dataset the focus was on understanding how reservoir capacity changes impact reservoir operation rules the findings revealed that sedimentation from the main branch of the seyhan river has largely stopped since the construction of the çatalan dam but sedimentation from the çakıt branch continues furthermore the study observed changes in the upper basin between 1990 and 2018 based on the corine land cover clc dataset the forest area increased from 2371 to 2436 and water bodies expanded from 03 to 09 this increase in forest and water bodies contributed to a reduction in sediment flow into the seyhan dam reservoir the researchers used regression analysis and found a high correlation r2096 between water storage capacity changes and time in years at a 675meter water level in the seyhan dam reservoir overall the studys results suggest that the decrease in sediment entering from the main branch of the seyhan river has significantly improved the sedimentation issue
purposealthough user stickiness has been studied for several years in the field of live ecommerce little attention has been paid to the effects of streamer attributes on user stickiness in this field rooted in the stimulusorganismresponse sor theory this study investigated how streamer attributes influence user stickinessdesignmethodologyapproachthe authors obtained 496 valid samples from chinese live ecommerce users and explored the formation of user stickiness using partial least squaresstructural equation modeling plssem artificial neural network ann was used to capture linear and nonlinear relationships and analyze the normalized importance ranking of significant variables supplementing the plssem resultsfindingsthe authors found that attractiveness and similarity positively impacted parasocial interaction psi expertise and trustworthiness positively impacted perceived information quality moreover streamerbrand preference mediated the relationship between psi and user stickiness as well as the relationship between perceived information quality and user stickiness compared to plssem the predictive ability of ann was more robust further the results of plssem and ann both showed that attractiveness was the strongest predictor of user stickinessoriginalityvaluethis study explained how streamer attributes affect user stickiness and provided a reference value for future research on user behavior in live ecommerce the exploration of the linear and nonlinear relationships between variables based on ann supplements existing research moreover the results of this study have implications for practitioners on how to improve user stickiness and contribute to the development of the livestreaming industry
we introduce a continuoustime ct pipeline analogtodigital converter adc featuring a timeinterleaved subadcdigitaltoanalog converter dac path in its first stage the proposed subadcdac path enhances the adc’s bandwidth by improving the signal cancellation at the summing node of the first stage in addition we have developed an inductorless delay line for the first stage improving the amplitude and phase matching thus minimizing the input signal leakage into the backend adc furthermore the theoretical jitter limitations in the ct pipeline architecture have been explored and the proposed theory is compared against the measured results the prototype adc was fabricated in a 16nm finfet process the adc achieves a peak signaltonoise ratio snr of 617 db at low frequencies and 608 db at high frequencies across a 1ghz bandwidth the active area of the adc is 077 mm2 and it consumes 240 mw the schreier figure of merit fom s  is 1579 db which is amongst the best for ct adcs with bandwidth greater than 500 mhz
marine heatwaves mhws are a type of widespread persistent and extreme marine warming event that can cause serious harm to the global marine ecology and economy this study provides a systematic analysis of the longterm trends of mhws in the eastern china marginal seas ecms during summer spanning from 1982 to 2022 and occurrence mechanisms of extreme mhw events the findings show that in the context of global warming the frequency of summer mhws in the ecms has increased across most regions with a higher rate along the coast of china areas exhibiting a rapid surge in duration predominantly reside in the southern yellow sea sys and southern east china sea ecs south of 28°n in contrast the longterm trends of mean and maximum intensities exhibit both increases and decreases rising trends primarily occur in the bohai sea bs and yellow sea ys whereas descending trends are detected in the northern ecs north of 28°n influenced jointly by duration and mean intensity cumulative intensity cumint exhibits a notable positive growth off the yangtze river estuary in the sys and southern ecs by employing the empirical orthogonal function the spatiotemporal features of the first two modes of cumint and their correlation with summer mean sea surface temperature sst and sst variance are further examined the first mode of cumint displays a positive anomalous pattern throughout the ecms with notable upward trend in the corresponding time series and the rising trend is primarily influenced by summer mean sst warming moreover both of the first two modes show notable interannual variability extreme mhw events in the sys in 2016 and 2018 are examined using the mixed layer temperature equation the results suggest that these extreme mhw events originate primarily from anomalous atmospheric forcing and oceanic vertical mixing these processes involve an anomalous highpressure system over the sys splitting from the western pacific subtropical high augmented atmospheric stability diminished wind speeds intensified solar radiation and reduced oceanic mixing thereby leading to the accumulation of more heat near the sea surface and forming extreme mhw events
accurate biphasic flow pattern recognition is essential in the design of coatings for the oil and gas sector because it enables engineers to create materials that are tailored to specific flow conditions this results in enhanced corrosion protection erosion resistance flow efficiency and overall performance of equipment and infrastructure in the challenging environments of the oil and gas industry the development of flow maps has been based on empirical correlations that incorporate characteristics such as superficial velocities volume fractions and physical properties such as the density and viscosity of the analyzed substances in addition geometric parameters such as the inclination and the internal diameter of the pipes are considered however due to the difficult working conditions on offshore platforms and the limitations in monitoring internal flow patterns technological advances have been implemented to improve this process using artificial intelligence techniques in this context this study proposes using a longterm memory lstm recurrent neural network to predict the flow patterns generated in vertical pipes this lstm network was trained and validated using data obtained from a literature database the results obtained showed that the model has a prediction error of less than 1 these technological advances represent an important step towards optimizing the flow pattern identification process in the hydrocarbons industry by leveraging the capabilities of artificial intelligence more accurate and reliable forecasts can be obtained enabling informed decisions and improving the efficiency and safety of operations
this paper concerns the learning problem of state space models with unknown nonlinear responses a state space model with unknown nonlinear responses has a linear state equation while the observation equation consists of linear and nonlinear parts the model structure of the nonlinear part is unknown this paper uses the neural network model to approximate the unknown nonlinear part of the observation equation a residual analysisbased algorithm is proposed to iteratively improve the performance of hidden state inference and parameter estimation for state space models with unknown nonlinear responses the essence of the proposed algorithm is to use the residual of the linear model to learn the unknown nonlinear part we show that the proposed algorithm can improve the model iteratively by applying the minorizationmaximization principle a numerical example and a battery capacity estimation case study have been conducted to validate the proposed method the results show that the proposed method can perform better on parameter estimation and hidden state inference than previously developed tools
a concept of a contactless inline transition between a monolithic microwave integrated circuit mmic and a ridge gap waveguide rgw is proposed and investigated at wband the transition employs an eplane waveguide bifurcation obtained by mounting a gaas mmic on a supporting printed circuit board in the opening of an rgw top metal lid designed this way multiple contactless transitions can be placed in a row with an electrically small spacing that makes the transition idea suitable for array antenna front ends a transition equivalent circuit is constructed employing a singlemode transmission line model which is verified through a fullwave simulation an 85–105 ghz transition design is then developed and experimentally investigated in the backtoback configuration indicating a 05–075 db transition insertion loss finally the performance of a 1bit phase shifter mmic integrated into the rgw using two proposed transitions is demonstrated
in the context of multichannel retailing the phenomenon of webrooming where consumers research online and purchase offline has become widespread therefore we consider supply chain consisting of a manufacturer an ecommerce platform and an offline retailer we study the impact of consumer webrooming effect on the three sales modes of online channels which are directly operated by the manufacturer resold by the ecommerce platform or coexist with direct operation and resale and comparatively analyze the pricing demand and profit of each channel under the three modes the conditions for optimal pricing decisions are further explored through numerical simulation it is found that profit always increases whether the online channel is opened by the manufacturer or the ecommerce company the coexistence of direct sales and resale does not always increase profit for offline retailers which must be discussed in the context of the sales model before channel expansion the existence of webrooming not only affects the decisionmaking of manufacturers and ecommerce platforms but also always harms the interests of ecommerce platforms and as the intensity of webrooming deepens the revenue of ecommerce platforms becomes smaller offline retailers benefit from webrooming and experience a slowdown in profit growth as the intensity of the phenomenon increases for manufacturers the impact of changes in the intensity of the webrooming is analyzed in relation to platform commission rates and online retail prices
the paper dwells on the contributions of apos theory to the development of teaching and learning of mathematics in school apos is an acronym for action process object and schema the theory emerges as an extension to constructivism but with a more focused and robust learnercentered approach to the teaching and learning of mathematics proponents of the theory believed that learning occurs initially as an action or activity in learners’ cognitive settings independent of learners’ environment triggered by cognitive coherence then it is transformed to process where learner now waits for internalization of the earlier activity preparatory to the occurrence of learning at object level learner now considers what has been learnt earlier to have been fully internalized into mathematical objects lastly at schema level the object learnt is assumed to have been embedded in the learners’ schema–a cognitive structure formed as a result of accumulated learning experience and a complete mental image of what has been learnt is said to have been formed against the backdrop of this the paper looks at how this theory had changed the narrative about teaching and learning of mathematics visàvis the bearing of the theory to other cognitive abilities of the learner such as intelligence and creativity in the end the paper suggests the application of apos theory in teaching and learning mathematics at all levels of learning in nigeria and beyond
"
 
 rolling resistance and aerodynamic losses cause a significant part of a truck’s energy consumption therefore there is an interest from both vehicle manufacturers and regulators to measure these losses to understand quantify and reduce the energy consumption of vehicles onroad measurements are particularly interesting because it enables testing in various ambient conditions and road surfaces with vehicles in service
 
 
 common driving loss measurement devices require unique instrumented measurement wheels which hinders effective measurements of multiple tyre sets or measurements of vehicles in service for this purpose the objective is to develop a novel loadsensing device for measuring braking or driving torque
 
 
 the strength of the measurement device is calculated using finite element methods and the output signal is simulated using virtual strain gauge simulations in addition to the signal simulation the device is calibrated using a torsional test rig
 
 
 the simulation results confirm that the device fulfils the strength requirements and is able to resolve low torque levels the output signal is simulated for the novel cascaded multiwheatstone bridge using the strains extracted from the finite element analysis the simulations and measurements show that the measurement signal is linear and not sensitive to other load directions the device is tested on a truck and the effort of mounting the device is comparable to a regular tyre change
 
 
 a novel driving loss measurement device design is presented with an innovative positioning of strain gauges decoupling the parasitic loads from the driving loss measurements the design allows onroad testing using conventional wheels without requiring special measurement wheels or instrumentation of drive shafts enabling more affordable and accurate measurements
"
abstract aims right heart disease rhd characterized by right ventricular rv and atrial ra hypertrophy and cardiomyocytes’ cm dysfunctions have been described to be associated with the incidence of atrial fibrillation af right heart disease and af have in common an inflammatory status but the mechanisms relating rhd inflammation and af remain unclear we hypothesized that right heart disease generates electrophysiological and morphological remodelling affecting the cm leading to atrial inflammation and increased af susceptibility methods and results pulmonary artery banding pab was surgically performed except for sham on male wistar rats 225–275 g to provoke an rhd twentyone days d21 postsurgery all rats underwent echocardiography and electrophysiological studies eps optical mapping was performed in situ on langendorffperfused hearts the contractility of freshly isolated cm was evaluated and recorded during 1 hz pacing in vitro histological analyses were performed on formalinfixed ra to assess myocardial fibrosis connexin43 levels and cm morphology right atrial levels of selected genes and proteins were obtained by qpcr and western blot respectively pulmonary artery banding induced severe rhd identified by rv and ra hypertrophy pulmonary artery banding rats were significantly more susceptible to af than sham compared to sham ra cm from pab rats were significantly elongated and hypercontractile right atrial cm from pab animals showed significant augmentation of mrna and protein levels of proinflammatory interleukin il6 and il1β sarcoplasmic–endoplasmic reticulum ca2atpase2a serca2a and junctophilin2 were decreased in ra cm from pab compared to sham rats conclusions right heart diseaseinduced arrhythmogenicity may occur due to dysfunctional serca2a and inflammatory signalling generated from injured ra cm which leads to an increased risk of af
abstract artificial intelligence has been widely applied to water depth retrieval across various environments deemed essential for habitat modeling hydraulic structure design and watershed management however most of these models have been developed for deep waters with the critical impact of the gradient descent algorithm often not evaluated to address this gap in current research this study adopted the artificial neural network with seven gradient descent methods including step momentum quick propagation deltabardelta conjugate gradient levenberg–marquardt and resilient backpropagation rprop for shallow water depth modeling shallow water depths in taiwan’s mountainous rivers were then modeled using multispectral imagery taken by drone and vegetation indices from our results it was revealed that methods optimizing weight updates were outperformed by those based on gradient information such as rprop the selection of gradient descent algorithm was identified as pivotal an inappropriate selection might even result in performance inferior to a traditional linear regression model in the sensitivity analysis nearinfrared and normalized difference water index were classified as highly sensitive by leveraging multispectral data and vegetation indices with ann the optimal gradient descent algorithm and the critical model input for shallow water modeling were successfully identified offering invaluable insights for future studies
programmable logic controllers plcs constitute the basis of industrial control systems icss underpinning sectors ranging from nuclear up to energy and manufacturing currently plc vulnerability assessment practices employed by ics operators are limited due to their reliance on empirical observations of visible code crashes prompted by plc compilers in parallel the prevalent plc firmware dependency on proprietary vendor routines restricts the composition of generic vulnerability detection or discovery schemes for zeroday threat vectors in this work we propose sizzler a novel vendorindependent vulnerability discovery framework specific to plc applications operating with logic realised through ladder diagrams sizzler extends the current state of the art by proposing the optimal synergy of a mutationbased fuzzing strategy using sequential generative adversarial network seqgan by virtue of critical vendor restrictions on emulating plc firmware we also refine the quick emulator qemu’s general purpose io gpio and the interintegrated circuit i2c protocols to evaluate and compare sizzler across 30 plc ladder diagram programs compiled from ldmicro and openplc projects over five widely used microcontroller units mcus it is noteworthy that sizzler has successfully identified vulnerabilities in ladder diagrams within a relatively short time frame based on our proprietary dataset and secured a cveid moreover through a comparison of sizzler with prevalent fuzzing techniques over the commonly used magma and lavam datasets we exhibit its wider applicability on embedded systems and identify its limitations
background with the rapid development of artificial intelligence prediction of warfarin dose via machine learning has received more and more attention since the dose prediction involve both linear and nonlinear problems traditional machine learning algorithms are ineffective to solve such problems at one time objective based on the characteristics of clinical data of chinese warfarin patients an improved stacking ensemble learning can achieve higher prediction accuracy methods information of 641 patients from southern china who had reached a steady state on warfarin was collected including demographic information medical history genotype and comedication status the dataset was randomly divided into a training set 90 and a test set 10 the predictive capability is evaluated on a new test set generated by stacking ensemble learning additional factors associated with warfarin dose were discovered by feature selection methods results a newly proposed heuristicstacking ensemble learning performs better than traditionalstacking ensemble learning in key metrics such as accuracy of ideal dose 7344 7188 mean absolute errors 011 mgday 013 mgday root mean square errors 018 mgday 020 mgday and r2 087 082 conclusions the developed heuristicstacking ensemble learning can satisfactorily predict warfarin dose with high accuracy a relationship between hypertension a history of severe preoperative embolism and warfarin dose is found which provides a useful reference for the warfarin dose administration in the future
this paper introduces an automatic affordance reasoning paradigm tailored to minimal semantic inputs addressing the critical challenges of classifying and manipulating unseen classes of objects in household settings inspired by human cognitive processes our method integrates generative language models and physicsbased simulators to foster analytical thinking and creative imagination of novel affordances structured with a tripartite framework consisting of analysis imagination and evaluation our systemanalyzesthe requested affordance names into interactionbased definitionsimaginesthe virtual scenarios andevaluatesthe object affordance if an object is recognized as possessing the requested affordance our method also predicts the optimal pose for such functionality and how a potential user can interact with it tuned on only a few synthetic examples across 3 affordance classes our pipeline achieves a very high success rate on affordance classification and functional pose prediction of 8 classes of novel objects outperforming learningbased baselines validation through real robot manipulating experiments demonstrates the practical applicability of the imagined user interaction showcasing the systems ability to independently conceptualize unseen affordances and interact with new objects and scenarios in everyday settings
simulation models are a valuable tool for exoskeleton development especially for system optimization and evaluation it allows an assessment of the performance and effectiveness of exoskeletons even at an early stage of their development without physical realization due to the closed physical interaction between the exoskeleton and the user accurate modeling of the human–exoskeleton interaction in defined scenarios is essential for exoskeleton simulations this paper presents a novel approach to simulate exoskeleton motion in response to human motion and the interaction forces at the physical interfaces between the human and the exoskeleton our approach uses a multibody model of a shoulder exoskeleton in matlab r2021b and imports human motion via virtual markers from a digital human model to simulate human–exoskeleton interaction to validate the humanmotionbased approach simulated exoskeleton motion and interaction forces are compared with experimental data from a previous lab study the results demonstrate the feasibility of our approach to simulate human–exoskeleton interaction based on human motion in addition the approach is used to optimize the support profile of an exoskeleton indicating its potential to assist exoskeleton development prior to physical prototyping
purposefew resources exist to incorporate principles of modular approach to course design this research aimed to help instructors by presenting principles for practical and empirically informed modular course design in engineering educationdesignmethodologyapproachin the first phase a systematic literature review was completed to identify categories addressing a modular course design search and screening procedures resulted in 33 qualifying articles describing the development of a modular course in the second phase 6 expert interviews were conducted to elaborate on the identified categoriesfindingsguided by the interview results and the addie analyze design develop implement and evaluate course design model the categories were compiled into six design principles to present the design principles in relation to the guiding principles of modular approach an overarching conceptual model was developedoriginalityvaluehere we present our innovation a foundation for an evidencebased systematic approach to modular course design implications have value for supporting flexibility and autonomy in learning
"

around 333bc diogenes laertius cited greek quote οὐκ ἐν τῷ μεγάλῳ τὸ εὖ… ἀλλ᾿ ἐν τῷ εὖ τὸ μέγα which means that volume big data tera peta velocity and variety doesnt guarantee quality furthermore protagorass doctrine about homo mensura and the socratic maieutic method evidences demonstrated the way that queries sql on big data can play on specific needs when crafted such as turning big data into knowledge and a competitive advantage in this research an alternative to the datadriven approach will be propounded with the development of the strategic alignment model sam based on the principles of greek philosophy the sam model will support decisions of big data analytics structure with strategies to fit and align in a way to create a competitive advantage mathematical formulation of the model will help to optimize a competitive advantage through the economic value added eva to guide the proper strategy and big data analytics structure in a holistic framework conclusions will be drawn on how strategy and the structure of big data analytics can be aligned

"
diabetic retinopathy dr is a global visual indicator of diabetes that leads to blindness and loss of vision manual testing presents a more difficult task when attempting to detect dr due to the complexity and variances of dr early detection and treatment prevent the diabetic patients from visual loss also classifying the intensity and levels of dr is crucial to provide necessary treatment this study develops a novel deep learning dl approach called he weighted bidirectional long shortterm memory hwblstm with an effective transfer learning technique for detecting dr from the rfi the collected fundus images initially undergo preprocessing to improve their quality which includes noise removal and contrast enhancement using a hybrid gaussian filter and probability density functionbased gamma correction hgfpdfgc technique the segmentation procedure divides the image into subgroups and is crucial for accurate detection and classification the segmentation of the study initially removes the optical disk od and blood vessels bvs from the preprocessed images using mathematical morphological operations next it segments the retinal lesions from the od and bv removed images using the enhanced grasshopper optimizationbased region growing algorithm egorga then the features from the segmented retinal lesions are learned using a squeeze net sqn and the dimensionality reduction of the extracted features is done using the modified singular value decomposition msvd approach finally the classification is performed by employing the hwblstm approach which classifies the dr abnormalities in datasets as nondr ndr nonproliferative dr npdr moderate npdr mdnpdr and severe dr also known as proliferative dr pdr the proposed approach is implemented on aptos as well as messidor datasets the outcomes proved that the proposed technique accurately identifies the dr with minimal computation overhead compared to the existing approachescommunicated by ramaswamy h sarma
autonomous vehicles rely on accurate trajectory prediction to inform decisionmaking processes related to navigation and collision avoidance however current trajectory prediction models show signs of overfitting which may lead to unsafe or suboptimal behavior to address these challenges this paper presents a comprehensive framework that categorizes and assesses the definitions and strategies used in the literature on evaluating and improving the robustness of trajectory prediction models this involves a detailed exploration of various approaches including data slicing methods perturbation techniques model architecture changes and posttraining adjustments in the literature we see many promising methods for increasing robustness which are necessary for safe and reliable autonomous driving
regular expressions have applications in fields from network intrusion detection to bioinformatics this has led to the widespread development of fpgabased hardware accelerators however reprogramming these accelerators for different regular expressions is slow and difficult due to fpga toolchain overheads translation overlays that enable fast repurposing of existing accelerator layouts have been proposed however these assume the underlying accelerator design exists and is welldesigned for many domains this is impossible to do byhand requiring simple patterns and significant programmer effortwe present secco a compiler targeting symbolonly reconfigurable architectures which codesigns the fastreconfigured translation overlay and the slowreconfigured underlying accelerator layout by reusing the same hardware when simple overlay translations are available and generating new hardware otherwise this allows significant efficiency improvements secco enables 59x more expressions using the same resources across all anmlzoo benchmarks compared to regular expression tool chains like reapr this enables large numbers of diverse regular expressions to be accelerated with contextswitching overheads in the milliseconds
this paper addresses the preprocessing of event sequences issued from cyclic discrete event processes which perform activities continuously whose delimitation of jobs or cases is not explicit the sequences include several occurrences of the same events due to the iterative behaviour such that discovery methods conceived for workflow nets wfn cannot process such sequences in order to handle this issue a novel technique for splitting a set of long event traces s  sk s ≥ 1 exhibiting the behaviour of cyclic processes is presented the aim of this technique is to obtain from s a log λ  σi of event traces representing the same behaviour which can be processed by methods that discover wfn the procedures derived from this technique have polynomialtime complexity
the traditional multiline laser calibration method relies primarily on calibrating the spatial plane equations of multiple laser lines and reconstructing the multiline laser in three dimensions through the plane equation and camera parameters in traditional methods the multiline laser is projected as a straight line by default but in reality the laser line projected by the multiline laser is not a plane due to the diffractive optical element doe instead it is a surface with a certain curvature in space during the production process the multiline laser experiences significant distortion because of the doe projecting at a large angle consequently if traditional methods are used accurate calibration of the multiline laser cannot be achieved to address this issue this paper introduces a new calibration algorithm for multiline laser based on a spatial quadric equation by calibrating the spatial quadric equation of the multiline laser the spatial characteristics of the laser can be accurately captured the algorithm involves projecting a crossed 7line laser onto a calibration plate and changing the position of the calibration plate while capturing images this allows for obtaining the spatial threedimensional coordinates of each laser line in the camera coordinate system next spatial quadric equations are fitted for each laser line based on the threedimensional coordinates finally 3d reconstruction is performed using the quadratic surface parameters of the multiline laser and camera parameters the results from 3d reconstruction demonstrate that compared to traditional laser calibration methods based on planes the proposed algorithm achieves higher calibration accuracy it also improves the number of successful binocular multiline laser matches and overall accuracy
depth information opens up new opportunities for video object segmentation vos to be more accurate and robust in complex scenes however the rgbd vos task is largely unexplored due to the expensive collection of rgbd data and timeconsuming annotation of segmentation in this work we first introduce a new benchmark for rgbd vos named depthvos which contains 350 videos over 55k frames in total annotated with masks and bounding boxes we futher propose a novel strong baseline model  fused colordepth network fusedcdnet which can be trained solely under the supervision of bounding boxes while being used to generate masks with a bounding box guideline only in the first frame thereby the model possesses three major advantages a weaklysupervised training strategy to overcome the highcost annotation a crossmodal fusion module to handle complex scenes and weaklysupervised inference to promote ease of use extensive experiments demonstrate that our proposed method performs on par with top fullysupervised algorithms we will opensource our project on httpsgithubcomyjybuaadepthvos to facilitate the development of rgbd vos
quadruped robots have gained attention for their potential to navigate various terrains however the stability of these robots in different gait sequences remains an open question this study investigates the relationship between different gait sequences and the motion stability of quadruped robots assuming a flat terrain for the purpose of the analysis utilizing mathematical models based on spiral theory we examine the stability margins associated with different leg movement sequences notably our findings confirm that the most commonly observed sequence in both natural and robotic contexts indeed offers optimal stability the study also scrutinizes the influence of the robots structural parameters and gait configuration on its motion stability these results provide a theoretical foundation for the design and stability control of quadruped robots setting the stage for future work on more complex terrains
a comprehensive review of recent challenges faced by pregnant women and how artificial intelligence can use to overcome these challenges is provided in this paper in this paper we explore various ai technologies and methodologies that contribute to the development of personalized pregnancy care system pregnancy is a complex vital period in a woman’s life with potential impact on her physical and psychological healthaidriven personalized pregnancy care includes prediction of complications during pregnancy proper diet for pregnant women and optimization of treatment plans there are so many existing ai technologies related pregnancy care however these technologies are still facing many challenges to improve existing techniques further research is required
the problem of picking a sequence of items that maximizes a given submodular function over the item sequence exists in many practical applications existing studies on this problem only consider uniform costs on items however it is more general that items have nonuniform costs taking this cue in this article we study the problem of budgeted sequence submodular maximization bssm which considers both uniform and nonuniform item costs in the sequence selection this problem widely appears in many applications such as movie recommendation and course learning for bssm with uniform costs we first prove a new approximation ratio for an existing algorithm omega and then propose an anytime randomized iterative algorithm pobm which converges faster than a stateoftheart method poseqsel in obtaining approximate solutions we also develop some optimization techniques for improving the efficiency of pobm for bssm with nonuniform costs we design a greedy algorithm gbm with a guaranteed error bound and prove that pobm can find approximate solutions in a reasonable time the results of the experimental study on both synthetic and real datasets demonstrate the excellent performance of our algorithms
text plagiarism detection task is a common natural language processing task that aims to detect whether a given text contains plagiarism or copying from other texts in existing research detection of high level plagiarism is still a challenge due to the lack of high quality datasets in this paper we propose a plagiarized text data generation method based on gpt35 which produces 32927 pairs of text plagiarism detection datasets covering a wide range of plagiarism methods bridging the gap in this part of research meanwhile we propose a plagiarism identification method based on faiss with bert with high efficiency and high accuracy our experiments show that the performance of this model outperforms other models in several metrics including 9886 9890 9886 and 09888 for accuracy precision recall and f1 score respectively at the end we also provide a userfriendly demo platform that allows users to upload a text library and intuitively participate in the plagiarism analysis
this paper is focused on the optimalization of methods for the synthesis isolation and purification of 2mercaptobenzothiazolebased acrylic and methacrylic monomers the structures of the newly synthesized compounds were confirmed through infrared ir and nuclear magnetic resonance spectroscopy nmr spectroscopic properties of the resulting 2mercaptobenzothiazole derivatives were determined based on their absorption spectra and molar absorption coefficients in solvents with varying polarities a correlation was established between the calculated density functional theory dft energies and frontier molecular orbitals and the experimental observations confirming their consistency the practical utility of the synthesized compounds particularly in future polymerization processes hinges on a thorough understanding of these properties
quantitatively assessing the performance of free space optical fso communication systems requires an accurate description of turbulence induced fading however it is well known that common irradiance distributions inaccurately model intensity tail regions recently a general fading model as the product of arbitrary number of gamma with inverse gamma was proposed and shown to accurately predict measurements besides containing many well‐known models as special cases this paper is concerned with analyzing the effect of discrepancies among different fading distributions on optical system performance in terms of bit error rates and capacity based on the general fading model a unified analysis of the channel structure effects on fso communications in the presence of turbulence and pointing error is presented the outage probability is investigated for both homodyne and direct detection schemes closed form expressions for the average probability of bit error for intensity modulation phase shift keying and polarization shift keying single‐input‐single‐output systems are given and verified by monte‐carlo simulations spatial diversity is also studied through single‐input‐multiple‐output systems also the ergodic capacity is derived comparisons between the proposed distributions and other widely used distributions are carried out and show significant differences among existing fading models this suggests possible improvements in the spectral efficiency and error performance design of optical wireless communication systems
we study the problem of estimating the score function of an unknown probability distribution rho from n independent and identically distributed observations in d dimensions assuming that rho is subgaussian and has a lipschitzcontinuous score function s we establish the optimal rate of tilde thetanfrac2d4 for this estimation problem under the loss function hat s  s2l2rho that is commonly used in the score matching literature highlighting the curse of dimensionality where sample complexity for accurate score estimation grows exponentially with the dimension d leveraging key insights in empirical bayes theory as well as a new convergence rate of smoothed empirical distribution in hellinger distance we show that a regularized score estimator based on a gaussian kernel attains this rate shown optimal by a matching minimax lower bound we also discuss extensions to estimating betaholder continuous scores with beta leq 1 as well as the implication of our theory on the sample complexity of scorebased generative models
"
 this research probes the boundarywork of communities of practice cop by examining china’s active efforts in advancing a statecentric approach in managing cyberspace in the international arena boundaries separating the global north and south are reproduced in the expansion of what i call the “community of cyber sovereignty” this phenomenon points to the cop literature’s omission of the proactive aspects of boundarywork undertaken by core members and its surrounding and confounding historical contexts i argue that boundary reproduction arises from both deliberate and unreflective moves in cultivating cop first emerging from reflexivity in interacting with the external environment practitioners in cops that are vigorously contested will likely replicate historically successful boundary drawing to acquire or maintain credibility second cop members engage in the habitual selection of strategies anchoring the community toward sustaining relationships with actors that its members traditionally associate with these proactive boundarywork dynamics are enabled by inclusive and exclusive mechanisms such as “brokering” varied practices to include potential members and “gatekeeping” others my analysis suggests that scholarship must attend to actors’ proactive efforts to establish communities preferentially as well as their linkages with prevailing social structures"
in response to contemporary challenges and the imperative to adapt medical education to a distance learning format particularly for international students there arises a pressing need to develop and implement effective teaching methodologies this article is aimed at investigating and discussing modern approaches to distance learning tailored for medical students especially those from abroad the highlights recent literature sources exploring current teaching methods in distance education for international medical students and outlining the most promising strategies for ensuring highquality medical education through online platforms for this analysis a variety of sources including scientific articles pedagogical reports and personal experiences in teaching medical disciplines such as general surgery nursing practice in surgery and patient care with a surgical focus in an online format were consulted it is imperative to underscore that the success of distance education for medical professionals hinges not only on the utilization of technology but also on the cultivation of interactive medical communities collaborative problemsolving of clinical tasks and case discussions foster the development of professional identity the analysis indicates that employing interactive virtual lectures online simulation workshops and virtual clinical cases enables medical students to acquire practical skills in realtime based on this analysis it can be inferred that effective distance education for foreign medical practitioners demands a comprehensive approach interactive methodologies teacher support and the integration of virtual tools facilitate not only the assimilation of theoretical knowledge but also the development of practical skills essential for future medical professionals the conclusions of this work can be used for the further development of distance medical education ensuring the high quality of training of specialists in the field of medicine
curcumin and omega‐3 polyunsaturated fatty acids ω‐3 pufa are multifunctional compounds which play an important role in alzheimers disease ad and little has been addressed about the role of these two compounds together in the progression of the disease there is evidence of the beneficial effect of combined administration of ω‐3 pufa and other dietary supplements such as vitamins and polyphenols in the prevention of ad although much remains to be understood about their possible complementary or synergistic activity therefore the objective of this work is to review the research focused on studying the effect and mechanisms of action of curcumin ω‐3 pufa and the combination of these nutraceutical compounds particularly on ad and to integrate the possible ways in which these compounds can potentiate their effect the most important pathophysiologies that manifest in ad will be addressed in order to have a better understanding of the mechanisms of action through which these bioactive compounds exert a neuroprotective effect
wireless video offloading in mobileedgecomputing mecenabled industrial internet of things imposes a risk of exposing users private data to eavesdroppers it is difficult for existing secure video offloading schemes to simultaneously guarantee security reduce latency and energy consumption in privacysensitive multicell scenarios where users are unwilling to offload data to other cells in this article a secure video offloading scheme based on multicell federated mcf deep reinforcement learning drl is proposed to facilitate a secure realtime and efficient mec network by efficient orchestration of limited resources we formulate a collaborative optimization problem of video frame resolution and resources to minimize latency and energy consumption while maximizing the security rate subject to analytic accuracy and limited resources to solve the formulated nphard problem a mcf drl algorithm based on the frameworks of multicell horizontal federated learning fl and hierarchical reward functionbased twin delayed deep deterministic policy gradient td3 is proposed first of all hierarchical reward functionbased td3 is employed to solve the collaborative optimization nphard problem formulated for each single cell where the optimal solution can be efficiently approached by the agent under the guidance of the innovatively designed hierarchical reward function then multicell horizontal fl is applied on td3 to obtain a model with higher model quality by averagely aggregating multiple individual td3 models simulation results reveal that the proposed algorithm outperforms comparison algorithms in terms of utility cost latency energy consumption and security rate
breeding for improved reliable cultivars despite growing environmental irregularity can be challenging unoccupied aircraft systems uas are a popular high‐throughput phenotyping technology that has been shown to help interpret the mechanisms associated with crop productivity and environmental response creating potential for improved breeding strategies spectral reflectance indices sris encompassing both vegetation and water indices like normalized difference vegetation index ndvi normalized difference red‐edge index and normalized water index were employed to assess 4094 winter wheat genotypes across 11593 breeding plots at washington state university from 2019 through 2022 sris were then used with genomic data in univariate models as covariates and multivariate models as secondary response variables for predictions of grain yield the prediction accuracy of models was evaluated using a leave‐one‐year‐out validation strategy against a base genomic prediction method including sri data as fixed effects in univariate genomic prediction models can improve prediction accuracy over the control but is unreliable across years when used in multivariate models sris improve prediction performance across years but require high‐performance computational resources that could limit feasibility in univariate models when test year ndvi data were available and used to calculate breeding values prediction performance was at least 16 better than the control ranging in prediction accuracy from 054 in 2019 to 093 in 2020 this study highlights the limited reliability of sri use in genomic prediction of untested environments and locations however a significant application for the technology can be found in early‐season uas data collection to aid accurate predictions in late season a helpful tool in tight turnaround times commonly experienced in winter crop breeding programs
in this work for the first time compact and monotone difference schemes of the 4th order of accuracy are constructed and studied preserving the property of conservation divergence for a quasilinear stationary reactiondiffusion equation to linearize the nonlinear difference scheme an iterative method of the newtonseidel type is used which also preserves the idea of conservation and monotonicity of the iteration the main idea of implementing the proposed difference scheme on a threepoint stencil of the sweep method is based on the possibility of parallelizing the computational process first the solution is at the even nodes and then at the odd ones in this case all equations remain threepoint with respect to the unknown function the arising problems of finding additional boundary conditions at the boundary nodes are solved using the newton interpolation polynomial of the 4th order of accuracy the presented results of the computational experiment illustrate the effectiveness of the proposed algorithm the possibility of generalizing this method to more difficult problems is also indicated
a common network inference problem arising from realworld data constraints is how to infer a dynamic network from its timeaggregated adjacency matrix and timevarying marginals ie row and column sums prior approaches to this problem have repurposed the classic iterative proportional fitting ipf procedure also known as sinkhorns algorithm with promising empirical results however the statistical foundation for using ipf has not been well understood under what settings does ipf provide principled estimation of a dynamic network from its marginals and how well does it estimate the network in this work we establish such a setting by identifying a generative network model whose maximum likelihood estimates are recovered by ipf our model both reveals implicit assumptions on the use of ipf in such settings and enables new analyses such as structuredependent error bounds on ipfs parameter estimates when ipf fails to converge on sparse network data we introduce a principled algorithm that guarantees ipf converges under minimal changes to the network structure finally we conduct experiments with synthetic and realworld data which demonstrate the practical value of our theoretical and algorithmic contributions
currently transformer is the most popular architecture for image dehazing but due to its large computational complexity its ability to handle longrange dependency is limited on resourceconstrained devices to tackle this challenge we introduce the ushaped vision mamba uvmnet an efficient singleimage dehazing network inspired by the state space sequence models ssms a new deep sequence model known for its power to handle long sequences we design a bissm block that integrates the local feature extraction ability of the convolutional layer with the ability of the ssm to capture longrange dependencies extensive experimental results demonstrate the effectiveness of our method our method provides a more highly efficient idea of longrange dependency modeling for image dehazing as well as other image restoration tasks the url of the code is urlhttpsgithubcomzzridamuvmnet our method takes only textbf0009 seconds to infer a 325 times 325 resolution image 100fps without io handling time
this systematic literature review attempts to answer the question of what methods to use to analyze how blockchain technology plays a role in the development of digital advertising so what is the role of blockchain technology in developing digital advertising in other industries blockchain plays a huge role in digital marketing as the digital advertising industry has historically relied heavily on third parties blockchain can eliminate the middleman between two parties helping brands and customers one approach to literature study is a systematic literature review slr this research uses a systematic literature review using the prism method with the help of uake vos viewer and the mendeley watase application this research was limited by only searching for scopus journal articles q1 q2 q3
entropy is arguably one of the most powerful concepts to understand the world from the behavior of molecules to the expansion of the universe from how life emerges to how hybrid complex systems like cities come into being and continue existing yet despite its widespread application it is also one of the most misunderstood concepts across the sciences this chapter seeks to demystify entropy and its main interpretations along with some of its explorations in the context of cities it first establishes the foundations of the concept by describing its trajectory since its inception in thermodynamics and statistical mechanics in the 19th century its different incarnations from boltzmanns pioneering formulation and shannons information theory to its absorption in biology and the social sciences until it reaches a nascent urban science in the 1960s the chapter then identifies some of the main domains in which entropy has been explored to understand cities as complex systems from entropymaximizing models of spatial interaction and applications as a measure of urban form diversity and complexity to a tool for understanding conditions of selforganization and urban sustainability
the aim of this article is to investigate the existence of traveling waves of a diffusive model that represents the transmission of a virus in a determined population composed of the following populations susceptible s infected i asymptomatic a and recovered r an analytical study is performed where the existence of solutions of traveling waves in a bounded domain is demonstrated we use the upper and lower coupled solutions method to achieve this aim the existence and local asymptotic stability of the endemic ee and diseasefree e0 equilibrium states are also determined the constructed model includes a discretetime delay that is related to the incubation stage of a virus we find the crucial basic reproduction number r0 which determines the local stability of the steady states we perform numerical simulations of the model in order to provide additional support to the theoretical results and observe the traveling waves the model can be used to study the dynamics of sarscov2 and other viruses where the disease evolution has a similar behavior
considering the implications of netflix’s role as a content producer for cultural diversity in europe this methodological article investigates how to define and measure the locality of netflix originals we employ a threefold methodological study based on industry data analysis audience reception research and content analysis this replicable and scalable methodological design provides a solid analytical framework for future studies examining netflix originals from the normative perspective of cultural diversity demonstrating the steps of our exploratory study we also find that netflix’s locallyproduced but globallyoriented content uses culture as window dressing warranting further research
thermoresponsive wound dressings with realtime monitoring and ondemand drug delivery have gained significant attention recently however such smart systems with stable temperature adjustment and drug release control are still lacking here a novel smart fabric is designed for wound management with thermoresponsive drug delivery and simultaneously temperature monitoring the triple layers of the fabrics are composed of the drugloaded thermoresponsive nanofiber film the mxeneoptimized joule heating film and the fpcb control chip the precise and stable temperature stimulation can be easily achieved by applying a low voltage 04 v to the heating film achieving the temperature control ranging from 25 to 130 °c and the temperature of the wound region can be monitored and adjusted in real time demonstrating an accurate and lowvoltage joule heating capability based on that the drugloaded film achieved precise thermoresponsive drug release and obtained significant antibacterial effects in vitro the in vivo experiments also proved the hybrid fabric system with a notable antibacterial effect and accelerated wound healing process about 30 faster than the conventional gauze group
"vehicle automation and smartphone appbased ridesplitting are commonly discussed topics in the transportation literature while these technologies have been examined for their role in transportation decarbonization through simulation study the motivation for such work is rarely made explicit in this commentary we provide a motivation for research in this area based on our own simulation research as well as land use and vehicle operational factors specifically land use factors such as density and the speed of its adjustment make traditional transit operations using large vehicles costprohibitive in most us communities and many other communities around the world automation and ridesplitting technologies may offer digitized transportation solutions that can match vehicle size to local land development density and passenger demand in addition we highlight a difference in the supplydemand relationship for freight transportation that causes additional challenges for decarbonizing that sector finally we emphasize that fleet ownership is key to ensuring timely vehicle fleet turnover as safer and more efficient technologies enter the market
"
mobile crowdsensing mcs has rapidly emerged as a popular paradigm for sensory data collection and benefited various locationbased services and applications like road monitoring smart transportation and environmental monitoring in practice there often exist datamissing regions in the target sensing area due to factors like limited budget large area size and scarcity of participants this poses a demand for data recovery which is commonly done based on the compressive sensing cs technique however csbased data recovery requires access to sensory data tagged with locations raising critical concerns on participants’ location privacy while a plethora of location privacy techniques exist most of them breach the data correlation inherently required by csbased data recovery meanwhile existing works mostly focus on protecting locations and overlook sensory data which may also indirectly lead to location leakages in this paper we propose secdr a new system design supporting secure efficient and accurate data recovery for locationbased mcs applications secdr protects both locations and sensory data and is built from a delicate synergy of csbased data recovery and lightweight cryptography techniques extensive evaluations demonstrate that secdr achieves promising performance and even with stronger security guarantees outperforms the stateoftheart with accuracy close to the plaintext domain
indoor air quality is essential in improving indoor wellbeing since most people spend their time there the breathing architecture concept promotes a strategy for designing indoor space to maintain the indoor air quality this concept has various challenges such as indoor pollutants determining the indoor environment to be healthy and decent this study aims to identify the relationship between iaq variables related to breathing architecture and air pollutants based on secondary sources of past and existing research ten years ago the review identified 16 articles collected by selecting several relevant keywords in sciencedirect and screening the title and abstract content analysis was conducted to analyze the relationship between iaq variables and pollutants and their potential association with the breathing architecture concept the results show that air exchange rate volume of spaces relative humidity and indooroutdoor temperature are the most potential variables and have a possible relationship while implementing the ba concept using these variables the optimal condition might be perceived since the possibility of lower pollutant contamination might be lower and the purpose of breathing architecture might be obtained however the relationship between other variables and pollutants requires future studies since it is undescribed or may impact other variables
this work presents new and practical methods for constructing multistandard adaptive architectures for address generators and interleavers the main objective is to reuse hardware resources whenever feasible and maximize hardware resources for support of multiple standards the study provides a reasonably cost reconfigurable architecture for address generation and interleaving that meets with wlan 80211abg and 80211n wimax 80216e and 3gpp lte requirements in order to reduce hardware cost and implementation complexity the study recommends a straightforward algorithm and a reconfigurable architecture that do away with costly computations like the floor function for wlanwimax and both the mod and floor functions for lte
this perspective article aims to emphasize the crucial role of uncertainty quantification uq in understanding magnetic phase transitions which are pivotal in various applications especially in the transportation and energy sectors d c jiles acta mater 51 5907–5939 2003 and gutfleisch et al adv mater 23 821–842 2011 magnetic materials undergoing phase transitions particularly due to high temperatures pose challenges related to the loss of their inherent properties however pinpointing a definitive phase transition temperature proves challenging due to the diverse and uncertain nanostructure of materials deterministic approaches are limited when seeking a precise threshold as a result there is a need to develop probabilistic methods to improve the understanding of this physical problem in this study uq is explored within the context of magnetic phase transitions in addition the broader applications of uq in relation to microstructures and hamiltonian systems are discussed to highlight its significance in materials science furthermore this study discusses the potential future work on the integration of quantum computing to achieve more efficient uq solutions of magnetic phase transitions using ising models
the energy flow optimization of the integrated energy system ies can not only decrease the operating cost of the ies but also maximize the utilization rate of renewable energy to deal with the problem that the optimization results fall into the local optimal solution due to the overlimit of ies state variables based on the ies energy circuit theory this paper established an energy flow optimization model of the electricgasheat integrated energy system by taking the weighted sum of the operating cost state variables overlimit penalty cost and wind and light abandonment penalty cost as the optimization objective the growth optimizer go algorithm can avoid the local convergence of the optimization results and accelerate the solving speed of the model therefore the effectiveness and feasibility of the model and the optimization method are confirmed by the simulation example
we present a monte carlo model of einstein–podolsky–rosen experiments that may be implemented on two independent computers and resembles the measurements of the clauser–aspect–zeilingertype which are performed in two distant stations sa and sb our computer model is local deterministic because we show that a theorist in station sb is able to conceive the products of the measurement outcomes of both stations conditional to any possible equipment configuration in station sa we show that the monte carlo model violates belltype inequalities and approaches the results of quantum theory for certain relationships between the number of measurements performed and the number of possible different physical properties of the entangled photon pairs these relationships are clearly linked to vorob’ev cyclicities which always enforce belltype inequalities the realization of this cyclicity depends however on combinatorial symmetry considerations that in turn depend on the mathematical properties of einstein’s elements of physical reality because these mathematical properties have never been investigated and therefore may be free to be chosen in the models for all published experiments einstein’s physics does not contradict the experimental findings instantaneous influences at a distance are put into question and the paradox of einstein–podolsky–rosen and bell is thus resolved
background surgical site infection ssi is a common and serious complication of elective clean orthopedic surgery that can lead to severe adverse outcomes however the prognostic efficacy of the current staging systems remains uncertain for patients undergoing elective aseptic orthopedic procedures this study aimed to identify highrisk factors independently associated with ssi and develop a nomogram prediction model to accurately predict the occurrence of ssi methods a total of 20960 patients underwent elective clean orthopedic surgery in our hospital between january 2020 and december 2021 of whom 39 developed ssi we selected all 39 patients with a postoperative diagnosis of ssi and 305 patients who did not develop postoperative ssi for the final analysis the patients were randomly divided into training and validation cohorts in a 73 ratio univariate and multivariate logistic regression analyses were conducted in the training cohort to screen for independent risk factors of ssi and a nomogram prediction model was developed the predictive performance of the nomogram was compared with that of the national nosocomial infections surveillance nnis system decision curve analysis dca was used to assess the clinical decisionmaking value of the nomogram results the ssi incidence was 0186 univariate and multivariate logistic regression analysis identified the american society of anesthesiology asa class odds ratio or 1564 95 confidence interval ci 1029–599 p   0046 operative time or 1003 95 ci 1006–1019 p   0001 and ddimer level or 1055 95 ci 1022–129 p   0046 as risk factors for postoperative ssi we constructed a nomogram prediction model based on these independent risk factors in the training and validation cohorts our predictive model had concordance indices cindices of 0777 95 ci 0672–0882 and 0732 95 ci 0603–0861 respectively both of which were superior to the cindices of the nnis system 0668 and 0543 respectively calibration curves and dca confirmed that our nomogram model had good consistency and clinical predictive value respectively conclusions operative time asa class and ddimer levels are important clinical predictive indicators of postoperative ssi in patients undergoing elective clean orthopedic surgery the nomogram predictive model based on the three clinical features demonstrated strong predictive performance calibration capabilities and clinical decisionmaking abilities for ssi
iter diagnostics include an extensive set of laser and microwave diagnostics to give access to a wealth of information on the core and edge plasma and to support high performance operation of iter for example core and edge thomson scattering systems build detailed density and temperature profiles on time scales much faster than τ e to follow transient events ece and reflectometry add time resolution to follow mhd events implementing these diagnostics is challenging needing a panoply of technologies to keep them functioning reliably for thousands of hours despite extreme events such as disruptions and wall conditioning cycles shielding shutters and cleaning systems protect the forward elements of most optical systems from the buildup of deposits and damage still plasmafacing mirrors must survive laser loads and endure erosion deposition and insitu rf cleaning calibration and monitoring systems ensure accurate and driftfree operation these support systems are also not straightforward and required specific rd access also drives the design to deal with the neutron and gamma sources yet allow maintenance of activated components iter uses large multipurpose ports that couple otherwise distinct systems into modules for maintenance machine movement requires provisions to maintain alignment and calibration from these port plugs shown in figure 1 to the accessible areas 10–50 m away a final complication comes from the difficulty of employing electronics near the plugs extensive qualification for radiation resistance is needed this paper examines design adaptations that iter adopted for its nearreactor environment consider the lessons learnt from the iter design activity specifically for laser and microwave systems and lays out some possible evolution paths for the reactor diagnostician that must follow a more industrial approach
the internet of things iot and realtime media streaming have increased due to the rapid development of wireless communication technologies and the enormous growth of computation and data transmission tasks edgecloud computing ecc combines the benefits of mobile cloud computing mcc and mobile edge computing mec to meet energy consumption and delay requirements and achieve more stable and affordable task execution the most significant challenge in ecc is making realtime task offloading decisions in order to generate offloading decisions in ecc environments in an efficient and near optimal manner a deep reinforcement learning drlbased distributed task offloading drldo framework is proposed the keras ml library is used to implement and evaluate the proposed drldo and other offloading algorithms in python experiments experimental results demonstrate the accuracy of the drldo framework it achieves a high gain ratio gr of about 223 and greatly reduces the energy consumption response time and system utility by about 76 43 and 262 respectively while attaining moderate time cost compared with other offloading algorithms
data annotation remains the sine qua non of machine learning and ai recent empirical work on data annotation has begun to highlight the importance of rater diversity for fairness model performance and new lines of research have begun to examine the working conditions for data annotation workers the impacts and role of annotator subjectivity on labels and the potential psychological harms from aspects of annotation work this paper outlines a critical genealogy of data annotation starting with its psychological and perceptual aspects we draw on similarities with critiques of the rise of computerized labbased psychological experiments in the 1970s which question whether these experiments permit the generalization of results beyond the laboratory settings within which these results are typically obtained do data annotations permit the generalization of results beyond the settings or locations in which they were obtained psychology is overly reliant on participants from western educated industrialized rich and democratic societies weird many of the people who work as data annotation platform workers however are not from weird countries most data annotation workers are based in global south countries social categorizations and classifications from weird countries are imposed on nonweird annotators through instructions and tasks and through them on data which is then used to train or evaluate ai models in weird countries we synthesize evidence from several recent lines of research and argue that data annotation is a form of automated social categorization that risks entrenching outdated and static social categories that are in reality dynamic and changing we propose a framework for understanding the interplay of the global social conditions of data annotation with the subjective phenomenological experience of data annotation work
although ai algorithms and applications become more and popular in the healthcare sector only few institutions have an operational ai strategy identifying the best suited processes for ml algorithm implementation and adoption is a big challenge also raising human confidence in ai systems is elementary to building trustworthy socially beneficial and responsible ai a commonly agreed ai auditing framework that provides best practices and tools could help speeding up the adoption process in this paper we first highlight important concepts in the field of ai auditing and then restructure and subsume them into an ml auditing core criteria catalog we conducted a scoping study where we analyzed sources being associated with the term “auditable ai” in a qualitative way we utilized best practices from mayring 2000 miles and huberman 1994 and bortz and döring 2006 based on referrals additional relevant white papers and sources in the field of ai auditing were also included the literature base was compared using inductively constructed categories afterwards the findings were reflected on and synthesized into a resulting ml auditing core criteria catalog the catalog is grouped into the categories conceptual basics data  algorithm design and assessment metrics as a practical guide it consists of 30 questions developed to cover the mentioned categories and to guide ml implementation teams our consensusbased ml auditing criteria catalog is intended as a starting point for the development of evaluation strategies by specific stakeholders we believe it will be beneficial to healthcare organizations that have been or will start implementing ml algorithms not only to help them being prepared for any upcoming legally required audit activities but also to create better wellperceived and accepted products potential limitations could be overcome by utilizing the proposed catalog in practice on real use cases to expose gaps and to further improve the catalog thus this paper is seen as a starting point towards the development of a framework where essential technical components can be specified
periodontitis is a high prevalence dental disease caused by bacterial infection of the bone that surrounds the tooth early detection and precision treatment can prevent more severe symptoms such as tooth loss traditionally periodontal disease is identified and labeled manually by dental professionals the task requires expertise and extensive experience and it is highly repetitive and timeconsuming the aim of this study is to explore the application of ai in the field of dental medicine with the inherent learning capabilities ai exhibits remarkable proficiency in processing extensive datasets and effectively managing repetitive tasks this is particularly advantageous in professions demanding extensive experiential knowledge such as dentistry by harnessing ai the potential arises to amplify process efficiency and velocity in this study bitewing radiographs are used as the image source and there are two major steps to detect the dental symptoms including 1 tooth position identification and 2 symptom identification the study combines image enhancement techniques and tooth position identification using gaussian filtering and adaptive binarization for data preprocessing facilitated by the yolov4 model to precisely mark tooth positions the subsequent step enhances symptom area visibility via contrast enhancement utilizing a cnn model particularly the alexnet model with significant improvements in caries recognition accuracy 9285 and restorations recognition accuracy 9655 compared to prior research moreover the inclusion of periodontal disease symptoms achieves an accuracy of 9113 by harnessing deep learning techniques based on cnn models this research enhances diagnostic precision reduces errors and increases efficiency for dentists thereby providing meticulous and swift patient care this innovation not only saves time but also has the potential for widespread implementation in remote and preventive medicine aligning with the aspiration of universal health care accessibility
 successful imaging of electrical activity in newborn infants is highly dependent on accurate andor adequate representation of head representation from structural point of view namely the electrical activity and the corresponding electroencephalography eeg measurements are dependant on electrical properties of brain and skull tissue ie corresponding conductivities and geometry of the skull and brain automated procedure for geometrystructural analysis are sparse even for adults and almost nonexistent for neonates and newborn infants in this paper we propose to develop automatic procedures for analyzing skull geometry and potentially other shapessizes that are relevant for electrical imaging of the cortex activity to this purpose we propose to estimate the thickness of the skull using magnetic resonance mr images as a preliminary step in obtainingestimating relevant structural parameters since the number of mr images is rather limited due to the age of the patients we develop a semisupervised machine learning algorithm in which certain number of mr slices is used for training we demonstrate applicability of our preliminary results using real mr images obtained from the university children’s hospital university of belgrade serbia
the use of games to model and understand complex systems has an extensive history in military circles gaming is used by commanders at all levels to inform their understanding of the operational environment evaluate disparate courses of action and refine future concepts of operation the current article examines the use of gaming by the us air force to mature and promote its agile combat employment operational concept the case is made that this approach is extensible to allow leaders with varying problem sets a tool to develop a deeper understanding of their leadership options
a number of recent studies have shown that wastage and inefficiency are a significant problem in all global healthcare systems one initiative that could radically improve the operational efficiency of health systems is to make a paradigm shift in data ownership—that is to transition such systems to a patientcentric model of data management by deploying blockchain technology such a development would not only make an economic impact by radically cutting wastage but would deliver significant social benefits by improving patient outcomes and satisfaction however a blockchainbased solution presents considerable challenges this research seeks to understand the principal factors which act as barriers to the acceptance of a blockchainbased patientcentric data management infrastructure in the healthcare systems of the gcc gulf cooperation council countries the study represents an addition to the current literature by examining the perspectives and views of healthcare professionals and users this approach is rare within this subject area and is identified in existing systematic reviews as a research gap a qualitative investigation of motivations and attitudes among these groups is a critical need the results of the study identified 12 key barriers to the acceptance of blockchain infrastructures thereby adding to our understanding of the challenges that need to be overcome in order to benefit from this relatively recent technology the research is expected to be of use to healthcare authorities in planning a way forward for system improvement particularly in terms of successfully introducing patientcentric systems
abstract recent work using neuroimaging has shown that brain responses to a movie are similar across viewers these similar responses emerge because the movie recruits brain systems involved in sensory eg responding to the flickering lights on screen perceptual eg identifying the characters’ faces and socialcognitive processing eg following and understanding the story social and affective responses – separately in each individual brain but collectively across the audience here we compare brain response similarities during an engaging social and nonverbal 5minute pixar movie across two levels first we show that at a macrolevel the movieevoked brain responses among the current audience from australia are correlated with the brain responses to the same movie watched by an audience from the usa second we investigate whether twins who maximize the preexisting similarity two individual audience members can have exhibit more similar brain responses to the same movie we find that shared responses measured in an audience from australia were highly correlated with responses from an audience watching the same movie in the usa second we find that twins who are genetically more similar and usually raised in a similar environment exhibit more strongly aligned brain responses compared to nontwin participants these results support our predictions about the role of preexisting similarities among audiences for braintobrain coupling during movie reception moreover they suggest that braintobrain similarities in response to movies contain information about similarities at the social level
various studies have been carried out to establish the key drivers impacting small enterprise sustainable performance in developing countries despite many policyoriented studies to uncover the factors influencing sme resilience in emerging markets these firms continue to register high failure rate which has been further exacerbated by the covid19 pandemic guided by a history of linear and loglinear econometric model estimation that ignores potential network effects our study extends the literature by implicating smme resilience as a production network utilising data from both incubated and nonincubated smmes marking a departure from traditional linear econometric models radial basis function artificial neural network algorithm was invoked to establish the drivers of smme resilience during covid19 regime the study extends the literature by implicating explainable artificial intelligence xai methods specifically optimal shapley additive explanations values shap values were computed to enhance the prediction output from the machine learning algorithm the xai analytics provide insightful findings on the key drivers which influenced the resilience of smmes during the covid19 pandemic the importance of innovation through introduction of new products company age and higher number of marketing mediums is confirmed however total assets analytics educational level and number of workers surfaced as a threat to these enterprises’ sustainable performance the study recommends that both the government and smes should leverage xai to identify their heterogeneous attributes and inform intelligent decisionmaking which necessities their resilient performance
goal recognition is the task by which an observer aims to discern the goals that correspond to plans that comply with the perceived behavior of subject agents given as a sequence of observations research on goal recognition as planning encompasses reasoning about the model of a planning task the observations and the goals using planning techniques resulting in very efficient recognition approaches in this article we design novel recognition approaches that rely on the operatorcounting framework proposing new constraints and analyze their constraints properties both theoretically and empirically the operatorcounting framework is a technique that efficiently computes heuristic estimates of costtogoal using integerlinear programming iplp in the realm of theory we prove that the new constraints provide lower bounds on the cost of plans that comply with observations we also provide an extensive empirical evaluation to assess how the new constraints improve the quality of the solution and we found that they are especially informed in deciding which goals are unlikely to be part of the solution our novel recognition approaches have two pivotal advantages first they employ new iplp constraints for efficiently recognizing goals second we show how the new iplp constraints can improve the recognition of goals under both partial and noisy observability
digital developments through new media trace progress in the transformation of knowledge in digital format the transformation of media into public consumption is a catalyst for many changes in the digital world besides that social media innovation has also become fertile ground for the spread of fake news throughout the world thereby catalyzing the formation of the post truth era the aim of the research is to determine the impact of the intensification of social media on the spread of fake news in the post truth era this research uses a qualitative method with a literature review approach where the data source is adopted from several data that have been verified and have continuity with the research object based on the analysis results there are 35 articles with related objects the data is transferred to the ms file excel and the main aspects lie in authorship patterns contribution distribution by country author ranking affiliate contributions and country distribution the results of the analysis of 35 articles from 2017 to 2020 revealed that there were 44 authors using journals with related topics the most widely used research method in this topic is quantitative the keywords used by the author are new media fake news and post truth the contribution of this journal identification can help researchers and people around the world to be more careful in consuming information
the claustrum is thought to be one of the most highly interconnected forebrain structures but its organizing principles have yet to be fully explored at the level of single neurons here we investigated the identity connectivity and activity of identified claustrum neurons to understand how the structure’s unique convergence of input and divergence of output support binding information streams we found that neurons in the claustrum communicate with each other across efferent projectiondefined modules which were differentially innervated by sensory and frontal cortical areas individual claustrum neurons were responsive to inputs from more than one cortical region in a celltype and projectionspecific manner particularly between areas of frontal cortex in vivo imaging of claustrum axons revealed responses to both unimodal and multimodal sensory stimuli finally chronic claustrum silencing specifically reduced animals’ sensitivity to multimodal stimuli these findings support the view that the claustrum is a fundamentally integrative structure consolidating information from around the cortex and redistributing it following local computations
the aim of this research is to refine knowledge transfer on audioimage temporal agreement for audiotext cross retrieval to address the limited availability of paired nonspeech audiotext data learning methods for transferring the knowledge acquired from a large amount of paired audioimage data to shared audiotext representation have been investigated suggesting the importance of how audioimage cooccurrence is learned conventional approaches in audioimage learning assign a single image randomly selected from the corresponding video stream to the entire audio clip assuming their cooccurrence however this method may not accurately capture the temporal agreement between the target audio and image because a single image can only represent a snapshot of a scene though the target audio changes from moment to moment to address this problem we propose two methods for audio and image matching that effectively capture the temporal information i nearest match wherein an image is selected from multiple time frames based on similarity with audio and ii multiframe match wherein audio and image pairs of multiple time frames are used experimental results show that method i improves the audiotext retrieval performance by selecting the nearest image that aligns with the audio information and transferring the learned knowledge conversely method ii improves the performance of audioimage retrieval while not showing significant improvements in audiotext retrieval performance these results indicate that refining audioimage temporal agreement may contribute to better knowledge transfer to audiotext retrieval
instruction tuning on a mixture of tasks has improved zeroshot capabilities in natural language processing nlp nevertheless existing methods often learn features that exhibit correlations between instructionformatted samples and target labels rather than causal relationships termed as spurious correlation in statistics such a correlation may change drastically in a new task making the effect from the learned features to be misleading to this end we develop a meta structural causal model metascm to integrate different nlp tasks under a single causal structure of the data specifically the metascm introduces multiple latent factors that represent properties of source context only some of which causally influence the target labels for a specific task the key idea is to learn taskrequired causal factors and only use those to make predictions for a given task theoretically we prove the causal factor can be identified without mixing information from others guided by the identifiability we propose a structural instruction tuning sit method to learn the taskrequired causal representations that can mimic the causal factors for each task the utility of our approach is verified by improvements of zeroshot ability on a range of unseen datasets and tasks
we propose multiloop vacuum amplitudes in the looptree duality ltd as the optimal building blocks for efficiently assembling theoretical predictions at highenergy colliders this hypothesis is strongly supported by the manifestly causal properties of the ltd representation of a vacuum amplitude the vacuum amplitude in ltd acting as a kernel encodes all the final states contributing to a given scattering or decay process through residues in the onshell energies of the internal propagators gauge invariance and the wave function renormalization of the external legs are naturally incorporated this methodological approach dubbed ltd causal unitary leads to a novel differential representation of cross sections and decay rates that is locally free of ultraviolet and infrared singularities at all orders in perturbation theory threshold singularities also match between different phasespace residues most notably it allows us to conjecture for the first time the local functional form of initialstate collinear splitting functions the fulfillment of all these properties provides a theoretical description of differential observables at colliders that is well defined in the four physical dimensions of the spacetime
purposethe purpose of this study is to investigate the impact of audit committee characteristics on firm performance in particular the authors employ the randomeffects variant of the hunter–schmidt metaanalyze procedure to analyze the effects of key audit committee attributes namely audit committee independence audit committee expertise audit committee size audit committee meeting along with big four impact on firm performance the authors hope to gain a better understanding of the function of audit committees in enhancing firm performance and to uncover potential discrepancies in prior findings due to varying economic levels or performance metricsdesignmethodologyapproachthis study uses the hunter–schmidt method to conduct a metaanalysis of 39 previous studies published between 2012 and 2022 to investigate the relationship between audit committee characteristics and firm performancefindingsthe results indicate that audit committee independence expertise size and affiliation with the big four have a significant and positive effect on firm performance while audit committee meetings have a nonsignificant effect furthermore findings suggest that companies should carefully consider the contextual factors that may impact the effectiveness of their corporate governance structures such as economic level when designing and implementing governance mechanismsoriginalityvaluethis study is significant as it is the first to combine and analyze previous research on this topic and highlights the importance of certain audit committee characteristics in enhancing financial reporting quality and corporate governance
background hypoxicischemic encephalopathy hie appears in neurological conditions where some brain areas are likely to be injured such as deep grey matter basal ganglia area and white matter subcortical periventricular áreas moreover modeling these brain areas in a newborn is challenging due to significant variability in the intensities associated with hie conditions this paper aims to evaluate functional measurements and 3d machine learning models of a given hie case by correlating the affected brain areas with the pathophysiology and clinical neurodevelopmental case presentation a comprehensive analysis of a term infant with perinatal asphyxia using longitudinal 3d brain information from machine learning models is presented the clinical analysis revealed the perinatal asphyxia diagnosis with apgar 5 at 5 and 10 minutes umbilical arterial ph of 70 be of 212 mmol  l neonatal seizures and invasive ventilation mechanics therapeutic interventions physical occupational and language neurodevelopmental therapies epilepsy treatment vagus nerve stimulation levetiracetam and phenobarbital furthermore the 3d analysis showed how the volume decreases due to age exhibiting an increasing asymmetry between hemispheres the results of the basal ganglia area showed that thalamus asymmetry caudate and putamen increase over time while globus pallidus decreases clinical outcomes spastic cerebral palsy microcephaly treatmentrefractory epilepsy conclusions slight changes in the basal ganglia and cerebellum require 3d volumetry for detection as standard mri examinations cannot fully reveal their complex shape variations quantifying these subtle neurodevelopmental changes helps in understanding their clinical implications besides neurophysiological evaluations can boost neuroplasticity in children with neurological sequelae by stimulating new neuronal connections
the use of magnetic sensors for vehicle detection is proposed as a solution for monitoring traffic owing to its advantages including ease of setup low cost strong resistance to interference and privacy preservation in this study we proposed a vehicle detection system using dualaxis magnetoimpedance mi sensors which are placed on the roadside the bar magnet and magnetic dipole models were proposed for simulating the movement of driving vehicles and the applicability of both models was discussed the experiment was conducted on a straight road in aichi japan and six common vehicle types on urban roads were used a detection algorithm based on the realtime geomagnetic field was used to automatically detect moving vehicles and extract valid signals the algorithm achieved a detection accuracy of 916 for vehicles driven under normal conditions the impact of various experimental parameters including the sensor placement height measurement distance and actual driving conditions on the measurement results was evaluated the best detection results were achieved when the sensor was placed at a height of 66 cm and was approximately 09 m away from the vehicle five magnetic features in the time and frequency domains were used to analyze the driving state of vehicles in the frequency domain the bandwidth and dominant frequency were confirmed to have a strong correlation with the driving speed whereas their relationship with the vehicle type and other experimental conditions was relatively weak making them suitable for speed estimation
the article is devoted to the problem of transformation of the future primary school teachers vocational training taking into account the sociocultural challenges existing in the country and the world the authors note the relevance of this problem both from a theoretical point of view and at the practical level based on the theoretical and methodological analysis of normative documents the authors reveal the features of innovations in the general education system and present variants for training a “new” primary school teacher depending on technological development in particular bioengineering the article presents a model of criteriabased assessment of the educational outcomes in primary school which was developed taking into account the requirements of the updated federal state educational standard and the federal educational program in this regard the authors pay attention to the formation of new metaprofessional competencies of the teacher contributing to the organization of the educational process in accordance with the technological progress summarizing and interpreting the opinions of scholars the authors come to the conclusion that in the course of training future primary school teachers it is necessary to develop a set of pedagogical and methodological measures that will become the basis for improving the quality of education in the russian federation comparative analysis of various aspects of vocational training allowed the authors to develop a model of transformation of vocational training of primary school teachers that meets modern sociocultural challenges the authors are convinced that the involvement of students – future primary school teachers  in scientific and methodological activities contributes to the formation of their conscious teaching position corresponding to new sociocultural requirements and the methodology of engineering the article is addressed to specialists of general secondary vocational and higher education as well as those who are interested in the problems of modern education
background in traditional mongolian or tibetan medicine in china chebulae fructus cf is widely used to process or combine with aconitums to decrease the severe toxicity of aconitums researches in this area have predominantly focused on tannins with few research on other major cf components for cardiotoxicity mitigation the present study aimed to clarify whether triterpenoids can attenuate the cardiotoxicity caused by mesaconitine ma and investigate the mechanism of cardiotoxicity attenuation methods firstly the pharmacophore model molecular docking and 3dqsar model were used to explore the mechanism of cf components in reducing the toxicity of ma mediated by the trpv1 channel then three triterpenoids were selected to verify whether the triterpenoids had the effect of lowering the cardiotoxicity of ma using h9c2 cells combined with mtt hoechst 33258 and jc1 finally western blot fluo3am and mtt assays combined with capsazepine were used to verify whether the triterpenoids reduced h9c2 cardiomyocyte toxicity induced by ma was related to the trpv1 channel results seven triterpenoids in cf have the potential to activate the trpv1 channel and they exhibited greater affinity for trpv1 compared to other compounds and ma however their activity was relatively lower than that of ma cell experiments revealed that ma significantly reduced h9c2 cell viability resulting in diminished mitochondrial membrane potential and nuclear pyknosis and damage in contrast the triterpenoids could improve the survival rate significantly and counteract the damage of ma to the cells we found that ma arjungenin ar and maslinic acid msa except corosolic acid cra upregulated the expression of trpv1 protein ma induced a significant influx of calcium whereas all three triterpenoids alleviated this trend blocking the trpv1 channel with capsazepine only increased the cell viability that had been simultaneously treated with ma and ar or msa however there was no significant difference in the cra groups treated with or without capsazepine conclusion the triterpenoids in cf can reduce the cardiotoxicity caused by ma the msa and ar function as trpv1 agonists with comparatively reduced activity but a greater capacity to bind to trpv1 receptors thus antagonizing the excessive activation of trpv1 by ma
the covid19 pandemic has caused major disruptions in global supply chains with unforeseen and unpredictable consequences however the pandemic was not the only reason why supply chain risk management has become more crucial than ever before in the last decade the occurrence of previously merely theoretical risks has emphasised the importance of risk management in supply chains this has increased interest in risk assessment and management covid19 and other disaster impact studies and proposals for more stable and resilient supply chains this article addresses the problem of transport risk in supply chains in the context of covid19 particular attention is paid to quantitative approaches identifying and quantifying risks and modelling their interdependencies contribute to the stability of the supply chains the analysis presents the current state of knowledge and can serve as a guide for further research it highlights transport risk management in supply chain management as an important area of investigation in light of the challenges of the covid19 pandemic the article proposes an approach to transportation risk assessment based on quantitative assessment and interconnection of risk factors
the objective of this study is to identify the best practices of facebook use for municipalities looking to communicate and interact with their citizens with a particular impact for rural municipalities a narrative review was conducted to identify the scientific and gray literature on research databases and google respectively a thematic analysis of the data was conducted to summarize the main strengths challenges and recommendations to improve municipalities’ facebook use our results showed many benefits of facebook use for municipalities and elected officials such as communicating efficiently with citizens the main challenge identified was developing an effective communication strategy finally several recommendations were found such as making facebook posts that appeal to citizens and promote discussion these results will be useful in helping municipalities develop an effective facebook communication strategy to improve online engagement and citizen participation for local governments
we present a formalization of higherorder logic in the isabelle proof assistant building directly on the foundational framework isabellepure and developed to be as small and readable as possible it should therefore serve as a good introduction for someone looking into learning about higherorder logic and proof assistants without having to study the much more complex isabellehol with heavier automation to showcase our development and approach we explain a sample proof describe the axioms and rules of our higherorder logic and discuss our experience with teaching the subject in a classroom setting
"jatspin this paper we address an open problem posed by bai and xia in 2 we study polynomials of the form jatsinlineformulajatsalternativesjatstexmathfxx4q1lambda 1x5qlambda 2xq4jatstexmathmmlmath xmlnsmmlhttpwwww3org1998mathmathml
                mmlmrow
                  mmlmifmmlmi
                  mmlmrow
                    mmlmommlmo
                    mmlmixmmlmi
                    mmlmommlmo
                  mmlmrow
                  mmlmommlmo
                  mmlmsup
                    mmlmixmmlmi
                    mmlmrow
                      mmlmn4mmlmn
                      mmlmiqmmlmi
                      mmlmommlmo
                      mmlmn1mmlmn
                    mmlmrow
                  mmlmsup
                  mmlmommlmo
                  mmlmsub
                    mmlmiλmmlmi
                    mmlmn1mmlmn
                  mmlmsub
                  mmlmsup
                    mmlmixmmlmi
                    mmlmrow
                      mmlmn5mmlmn
                      mmlmiqmmlmi
                    mmlmrow
                  mmlmsup
                  mmlmommlmo
                  mmlmsub
                    mmlmiλmmlmi
                    mmlmn2mmlmn
                  mmlmsub
                  mmlmsup
                    mmlmixmmlmi
                    mmlmrow
                      mmlmiqmmlmi
                      mmlmommlmo
                      mmlmn4mmlmn
                    mmlmrow
                  mmlmsup
                mmlmrow
              mmlmathjatsalternativesjatsinlineformula over the finite field jatsinlineformulajatsalternativesjatstexmathmathbb f5kjatstexmathmmlmath xmlnsmmlhttpwwww3org1998mathmathml
                mmlmsub
                  mmlmifmmlmi
                  mmlmsup
                    mmlmn5mmlmn
                    mmlmikmmlmi
                  mmlmsup
                mmlmsub
              mmlmathjatsalternativesjatsinlineformula which are not quasimultiplicative equivalent to any of the known permutation polynomials in the literature we find necessary and sufficient conditions on jatsinlineformulajatsalternativesjatstexmathlambda 1 lambda 2 in mathbb f5kjatstexmathmmlmath xmlnsmmlhttpwwww3org1998mathmathml
                mmlmrow
                  mmlmsub
                    mmlmiλmmlmi
                    mmlmn1mmlmn
                  mmlmsub
                  mmlmommlmo
                  mmlmsub
                    mmlmiλmmlmi
                    mmlmn2mmlmn
                  mmlmsub
                  mmlmo∈mmlmo
                  mmlmsub
                    mmlmifmmlmi
                    mmlmsup
                      mmlmn5mmlmn
                      mmlmikmmlmi
                    mmlmsup
                  mmlmsub
                mmlmrow
              mmlmathjatsalternativesjatsinlineformula so that jatsitalicfjatsitalicjatsitalicxjatsitalic is a permutation monomial binomial or trinomial of jatsinlineformulajatsalternativesjatstexmathmathbb f52kjatstexmathmmlmath xmlnsmmlhttpwwww3org1998mathmathml
                mmlmsub
                  mmlmifmmlmi
                  mmlmsup
                    mmlmn5mmlmn
                    mmlmrow
                      mmlmn2mmlmn
                      mmlmikmmlmi
                    mmlmrow
                  mmlmsup
                mmlmsub
              mmlmathjatsalternativesjatsinlineformulajatsp"
manufacturers are facing challenges in achieving high productivity and quality in manufacturing through machining pvdcoated tools can control several machining challenges by enhancing hardness and abrasion resistance of the cutting tool these tools facilitate turning operations in terms of efficiency accuracy and productivity by extending cutting performance and tool life aluminum bronze a copper alloy valued for its mechanical thermal corrosion and wearresistant properties finds application in diverse industries such as aerospace automobile marine and electrical engineering as well as in the creation of sculptures decorative elements and thermal devices however machining aluminum bronze presents common challenges including achieving a smooth surface finish and minimizing high cutting force due to its inherent strength and abrasiveness this research aims at identifying the optimal levels of cutting velocity feed and depth of cut to minimize surface roughness and cutting force during dry turning of wearresistant highstrength cual10fe5ni5c pvd altincoated tools were utilized which offer many advantages over others experiments were conducted through taguchi’s l27 oa orthogonal array of factors the results indicate that coated tools have superior performance in reducing surface roughness and cutting force when it comes to designing and optimizing experiments integrating pca with taguchi method is a potent strategy again it was observed that feed is the most influential factor affecting responses
the modular multilevel converter mmc can integrate distributed energy systems des such as a battery energy storage system to expand its functionalities and carry out multiple simultaneous tasks however a des induces power imbalances within the mmc which affects the operating currents and voltages of the converter this phenomenon has been partially covered in recent works but an analytical analysis has not yet been carried out to see the behavior and implications in different mmcdes applications this article introduces a novel analytical analysis of the power imbalances between mmc clusters it pioneers the development of general equations and imbalance capability metrics enabling the assessment of maximum currents and voltages supported by the mmc clusters the developed tools allow the evaluation of any mmcdes application regarding the current and voltage rating requirements of mmc clusters the analysis shows that the mmc operating mode can substantially restrain or enlarge its imbalance capacity affecting its suitability for different des applications while it needs around 33 current overrating in the worst imbalances under some operating modes it can reach most imbalances without requiring current overrating the ac compensation mode is much more capable of achieving imbalances than the dc compensation mode reaching 8837 and 1674 of the imbalance points respectively without requiring any overrating
recent efforts in cognitive neuroscience have focused on leveraging restingstate functional connectivity rsfc data from fmri to predict human traits and behaviors more accurately traditional methods typically analyze rsfc by correlating averaged timeseries data between regions of interest rois or networks a process that may overlook critical spatial signal patterns to address this limitation we introduced a novel linear regression technique that estimates rsfc by predicting spatial brain activity patterns in a target roi from those in a seed roi we applied both traditional and our novel rsfc estimation methods to a largescale dataset from the human connectome project analyzing restingstate fmri data to predict sex age personality traits and psychological task performance additionally we developed an ensemble learner that integrates these methods using a weighted average approach to enhance prediction accuracy our findings revealed that hierarchical clustering of rsfc patterns using our novel method displays distinct wholebrain grouping patterns compared to the traditional approach importantly the ensemble model outperformed the traditional rsfc method in predicting human traits and behaviors notably the predictions from the traditional and novel methods showed relatively low similarity indicating that our novel approach captures unique and previously undetected information about human traits and behaviors through finegrained local spatial patterns of neural activation these results highlight the potential of combining traditional and innovative rsfc analysis techniques to enrich our understanding of the neural basis of human traits and behaviors
a formal consideration of user tactics during product evaluation in earlystage product development trenton brady owens department of mechanical engineering byu master of science frequent and effective design evaluation is foundational to the success of any product development effort products used installed or otherwise handled by humans would benefit from an evaluation of the product while formally considering both the physical embodiment of the technology termed technology and the steps a user should take to use that technology termed tactics formal and simultaneous evaluations of both technology and tactics are not widespread in the product design literature although informal evaluation methods have advantages formal methods are also known to be effective in this paper we propose a formal method for evaluating tactics and technology simultaneously unlike the published literature this evaluation involves explicitly defined tactics in the form of a written description of the actor environment and series of steps it also involves the use of stageappropriate explicitly defined tacticsdependent criteria which include criteria from a broad range of impact categories such as impacts on the user environment project and technology
objective machine learning methods hold the promise of leveraging available data and generating higher quality data while alleviating the data collection burden on healthcare professionals international classification of diseases icd diagnoses data collected globally for billing and epidemiological purposes represents a valuable source of structured information however icd coding is a challenging task while numerous previous studies reported promising results in automatic icd classification they often describe input data specific model architectures that are heterogeneously evaluated with different performance metrics and icd code subsets this study aims to explore the evaluation and construction of more effective computer assisted coding cac systems using generic approaches focusing on the use of icd hierarchy medication data and a feed forward neural network architecture methods we conduct comprehensive experiments using the mimiciii clinical database mapped to the omop data model our evaluations encompass various performance metrics alongside investigations into multitask hierarchical and imbalanced learning for neural networks results we introduce a novel metric rer tailored to the icd coding task which offers interpretable insights for healthcare informatics practitioners aiding them in assessing the quality of assisted coding systems our findings highlight that selectively cherrypicking icd codes diminish retrieval performance without performance improvement over the selected subset we show that optimizing for metrics such as ndcg and auprc outperforms traditional f1based metrics in ranking performance we observe that neural network training on different icd levels simultaneously offers minor benefits for ranking and significant runtime gains however our models do not derive benefits from hierarchical or class imbalance correction techniques for icd code retrieval conclusion this study offers valuable insights for researchers and healthcare practitioners interested in developing and evaluating cac systems using a straightforward sequential neural network model we confirm that medical prescriptions are a rich data source for cac systems providing competitive retrieval capabilities for a fraction of the computational load compared to textbased models our study underscores the importance of metric selection and challenges existing practices related to icd code subsetting for model training and evaluation
early and accurate disease diagnosis is pivotal for effective phytosanitary management strategies in agriculture hyperspectral sensing has emerged as a promising tool for early disease detection yet challenges remain in effectively harnessing its potential this study compares parametric spectral vegetation indices vis and a nonparametric gaussian process classification based on an automated spectral band analysis tool gpcbat for diagnosing plant bacterial diseases using hyperspectral data the study conducted experiments on tomato plants in controlled conditions and kiwi plants in field settings to assess the performance of vis and gpcbat in the tomato experiment the modeling processes were applied to classify the spectral data measured on the healthy class of plants sprayed with water only and discriminate them from the data captured on plants inoculated with the two bacterial suspensions 108 cfu ml−1 in the kiwi experiment the standard modeling results of the spectral data collected on nonsymptomatic plants were compared to the ones obtained using symptomatic plants’ spectral data vis known for their simplicity in extracting biophysical information successfully distinguished healthy and diseased tissues in both plant species the overall accuracy achieved was 63 and 71 for tomato and kiwi respectively limitations were observed particularly in differentiating specific disease infections accurately on the other hand gpcbat after feature reduction showcased enhanced accuracy in identifying healthy and diseased tissues the overall accuracy ranged from 70 to 75 in the tomato and kiwi case studies despite its effectiveness the model faced challenges in accurately predicting certain disease infections especially in the early stages comparative analysis revealed commonalities and differences in the spectral bands identified by both approaches with overlaps in critical regions across plant species notably these spectral regions corresponded to the absorption regions of various photosynthetic pigments and structural components affected by bacterial infections in plant leaves the study underscores the potential of hyperspectral sensing in disease diagnosis and highlights the strengths and limitations of vis and gpcbat the identified spectral features hold biological significance suggesting correlations between bacterial infections and alterations in plant pigments and structural components future research avenues could focus on refining these approaches for improved accuracy in diagnosing diverse plant–pathogen interactions thereby aiding disease diagnosis specifically efforts could be directed towards adapting these methodologies for early detection even before symptom manifestation to better manage agricultural diseases
aiming at the problem of terminal position estimation under internet of vehicles environment the paper proposes a vibrio foraging optimization vfo positioning algorithm assisted by multiple reconfigurable intelligent surfaces riss the system model for multiple riss aided positioning under internet of vehicles environment is first constructed to achieve the goal of reducing system implementation complexity the method of exploiting a single transmitting end node to construct virtual anchor nodes for target positioning is adopted the proposed vfo algorithm is applied to solve the positioning objective function constructed by geometric relations to obtain the estimated position of the target vehicle additionally the direction of arrival doa parameter is introduced and matrix reconstructed estimation of signal parameters by rotational invariance technique mresprit algorithm is employed for reducing the computational complexity and further improving the performance of the positioning algorithm simulation results further demonstrate that the proposed vfo algorithm based on multiple riss can achieve better positioning performance with relatively low system complexity
despite a high rate of concurrent mathematical difficulties among children with dyslexia we still have limited information regarding the prevalence and severity of mathematical deficits in this population to address this gap we developed a comprehensive battery of cognitive tests known as the ucsf mathematical cognition battery mcb with the aim of identifying deficits in four distinct mathematical domains number processing arithmetical procedures arithmetic facts retrieval and geometrical abilities the mathematical abilities of a cohort of 75 children referred to the ucsf dyslexia center with a diagnosis of dyslexia along with 18 typically developing controls aged 7 to 16 were initially evaluated using a behavioral neurology approach a team of professional clinicians classified the 75 children with dyslexia into five groups based on parents’ and teachers’ reported symptoms and clinical history these groups included children with no mathematical deficits and children with mathematical deficits in number processing arithmetical procedures arithmetic facts retrieval or geometrical abilities subsequently the children underwent evaluation using the mcb to determine concordance with the clinicians’ impressions additionally neuropsychological and cognitive standardized tests were administered our study reveals that within a cohort of children with dyslexia 66 exhibit mathematical deficits and among those with mathematical deficits there is heterogeneity in the nature of these deficits if these findings are confirmed in larger samples they can potentially pave the way for new diagnostic approaches consistent subtype classification and ultimately personalized interventions
grocery inventory management system gims is an imaginative helper for minimarket stores that keeps track of what’s inactive in honestopportunity this guarantees that the store never fails of products or finishes up accompanying excessive stock the system processes orders without thinking making the process faster and smooth for both clients and store stick
aiming at the shortcomings of the traditional permanent magnet governor air gap speed regulation a sleevetype electromagnetic hybrid governor is proposed by controlling the current instead of adjusting the air gap the structural characteristics and working principle of the speed regulation model are introduced and the electromagnetic torque mathematical expression is derived based on the equivalent magnetic circuit method and the key parameters affecting the transmission performance of the governor are obtained and the simulation model of the sleeve electromagnetic hybrid governor is established by using the finite element analysis method to analyze the electromagnetic field change law in the governor under the transient field the change curve of air gap magnetic inductance intensity under different currents was obtained and the influence of speed difference under different currents on the output torque was revealed and finally an experimental platform was built for testing and the maximum error of torque simulation and the test was obtained by analyzing the results to verify the accuracy of the simulation method on this basis two structural schemes were proposed to improve the amplitude of the torque regulation of the governor by the current under the structure of scheme 1 the adjustment amplitude of the torque increased from 35 of the original structure to 96 and the improvement effect was ideal its laws and conclusions can provide a reference for the further design and optimization of sleeve electromagnetic hybrid governor
mobile operation belongs to the innovation of a business model at present the management form of mobile operation is still in manual management and there are many problems in manual management in order to solve the current situation an optimized scheduling model for power mobile jobs based on improved genetic algorithm is proposed the model’s objective is to enhance scheduling efficiency and accuracy of power mobile jobs while minimizing automation scheduling costs and fault impact the research conducts simulation experiments to validate the model’s efficacy in the model considering the total task completion time and the cost of idle hours the algorithm performance of the model is significantly proposed when the model has completed around 70 iterations it converges and maintains a fitness value in the range of 600 to 800 in the task assignment of the model the total task completion time is shortened by 3 hours the task assignment of each team is more uniform and the path planning of each team is more reasonable the research utilizes a genetic algorithm to intelligently schedule human resources automating the scheduling process and achieving the lowest cost for completing the work
the new generation video coding standard h266vvc brings performance gains while significantly increasing coding complexity most of the complexity optimization methods for intra coding focus on reducing the number of prediction modes entering the ratedistortion optimization but there are few concerning the newly introduced secondary transform this letter extends the scope of existing studies and provides valuable insights for intra coding complexity optimization based on early skipping of the secondary transform specifically considering the distribution of residuals assessed by coefficients after the primary transform we propose to skip the secondary transform when the residual variation is relatively flat or the distortion caused by zeroing highfrequency coefficients is significant based on this analysis we establish two skip conditions through statistical analysis and enable efficient decisionmaking by skipping the unnecessary secondary transform experimental results demonstrate the effectiveness of our algorithm achieving 384 and 1029 reductions in encoding time with only 039 and 103 increases in bjontegaard delta bitrate compared to fraunhofer versatile video encoder vvenc’s medium preset under randomaccess ra and allintra ai configurations respectively
closedloop bidirectional brainmachine interface clbbmi consisting of a neural signal recorder a neural signal processing unit npu and a modulation module plays an important role in the cuttingedge research in neuroscience 1–2 in addition to traditional electricalmode modulation optogenetic stimulation shows great potential in bbmi in recent years 3–4 among the three modules the npu is a key component for the closedloop but very limited solutions have been reported in literature 4 a neural network nn is typically a highaccuracy solution for npu but suffers unaffordable power consumption especially in the implantable scenarios of bmi table lookup based multiplierfree nn is a feasible lowpower approach however the current lookup table lut based table lookup solution still requires a huge quantity of randomaccess memory ram 5
federated learning collaboratively trains machine learning models among different clients while keeping data privacy and has become the mainstream for breaking data silos however the nonindependently and identically distribution ie noniid characteristic of different image domains among different clients reduces the benefits of federated learning and has become a bottleneck problem restricting the accuracy and generalization of federated models in this work we propose a novel federated image segmentation method based on style transfer fedst by using a denoising diffusion probabilistic model to achieve feature disentanglement and image synthesis of crossdomain image data between multiple clients thus it can share style features among clients while protecting structure features of image data which effectively alleviates the influence of the noniid phenomenon experiments prove that our method achieves superior segmentation performance compared to stateofart methods among four different noniid datasets in objective and subjective assessment the code is available at httpsgithubcomyoferchenfedst
smartphones have drastically transformed communication and information access becoming integral to various aspects of daily life the surge in mobile application adoption for diverse needs has further solidified their importance the study is motivated by the rising popularity of flutter in mobile application development particularly for interactive applications due to its crossplatform capabilities and ability to create visually appealing interfaces with customizable widgets however there is a notable gap in mobile programming education with a need for practical handson learning to address this a learning topic in the flutter programming learning assistance system fplas is proposed which aims to facilitate selflearning in android programming using flutter it incorporates testdriven development and automated testing making it easier for students to learn through a projectbased approach the systems effectiveness was validated through an evaluation involving 40 students resulting in a 100 success rate and positive feedback highlighting its utility in enhancing ui design and programming skills though some constructive suggestions were noted for improvement
—time is relative which makes the interaction so sensitive indeed contemplating the concept of realtime enterprises resembled envisioning an idealized notion that seemed unattainable and impracticable in reality consequently we give a new definition of the realtime concept according to our needs and targets for a successful business process according to this definition we can go towards a realtime business process validation algorithm which has the goal of ensuring quality in terms of time ie time latency ≃ 0 put simply it serves as a method to assess the consistency of a process this approach aids in comprehending the temporal patterns inherent in a process as it evolves empowering decisionmakers to glean insights and swiftly form initial judgments for effective problemsolving and the identification of appropriate solutions thus our main purpose is to deliver the right information and knowledge to the right person at the right time to achieve this we introduce a novel realtime component within the business process management notation bpmn encompassing various attributes that facilitate process monitoring this extension transforms the bpmn into a unified realtime business process metamodel to be more specific our contribution proposes a continuous temporal improvement assessment and knowledge management as temporal knowledge helps to evaluate the realtime situation of the business process
we consider a parabolicparabolic interface problem and construct a loosely coupled predictioncorrection scheme based on the robinrobin splitting method analyzed in j numer math 3115977 2023 we show that the errors of the correction step converge at mathcal odelta t2 under suitable convergence rate assumptions on the discrete time derivative of the prediction step where delta t stands for the timestep length numerical results are shown to support our analysis and the assumptions
to develop a novel deep learning‐based method inheriting the advantages of data distribution prior and end‐to‐end training for accelerating mri
urban expansion within greater irbid municipality gim witnessed an extraordinary rise expanding approximately ninefold between 1967 and 2020 recent trends revealed a shift in urban growth towards southern and eastern regions these dynamics carry critical implications for urban planners and environmental managers urging a comprehensive understanding of the driving factors behind this expansion to anticipate future challenges employing logistic regression lr and geographically weighted logistic regression gwlr analyses using remote sensing data and gis spatially variant coefficients for driving factors emerged illuminating the evolving landscape of urban development drivers within gim yarmouk university historically promoted urban expansion but recent proximity to yarmouk university and just university coupled with higher existing building percentages inhibited further urbanization the analysis also revealed that elevation and slope had negligible impacts on urban expansion these findings underline the evolving dynamics of urban development drivers within the study region the local perspective depicted significant spatial disparities in coefficients highlighting variations in magnitude and direction gwlr emerged as a more robust methodology effectively capturing regional variations and enhancing model reliability these findings hold immense value for informing current and future urban planning practices in greater irbid municipality proactively addressing identified challenges and understanding the intricate dynamics of urban expansion can assist irbid in shaping a sustainable and resilient future avoiding potential pitfalls in its urban development endeavors
a new class of quantum states is introduced by demanding that the computational measurement statistics approach the boltzmann distribution of higherorder strongly coupled ising models the states referred to as ncoupled states are superpositions of even or odd parity nqubit states generalize bell states and form an orthonormal basis for the nqubit hilbert space for any n the states are maximally connected and locally maximally entangled it is proven that the nqubit w and ghz multipartite entanglement classes have vanishing hyperdeterminant for all ngeq 3 and ngeq 4 respectively and that the ncoupled states fall in the latter still multiple novel protocols for multiparty secure dense coding and stabiliser code construction are presented which rely on the structure of ncoupled states as well as symmetrybreaking phase perturbations
multimodal sentiment analysis msa aims to regress or classify the overall sentiment of utterances through acoustic visual and textual cues however most of the existing efforts have focused on developing the expressive ability of neural networks to learn the representation of multimodal information within a single utterance without considering the global cooccurrence characteristics of the dataset to alleviate the above issue in this paper we propose a novel hierarchical graph contrastive learning framework for msa aiming to explore the local and global representations of a single utterance for multimodal sentiment extraction and the intricate relations between them specifically regarding to each modality we extract the discrete embedding representation of each modality which includes the global cooccurrence features of each modality based on it for each utterance we build two graphs local level graph and global level graph to account for the levelspecific sentiment implications then two graph contrastive learning strategies is adopted to explore the different potential presentations based on graph augmentations respectively furthermore we design a crosslevel comparative learning for learning local and global potential representations of complex relationships
current industry trends demand automation in every aspect where machines could replace humans recent advancements in conversational agents have grabbed a lot of attention from industries markets and businesses building conversational agents that exhibit human communication characteristics is a need in todays marketplace thus by accumulating emotions we can build emotionallyaware conversational agents emotion detection in textbased dialogues has turned into a pivotal component of conversational agents enhancing their ability to understand and respond to users emotional states this paper extensively compares various ai  techniques adapted to textbased emotion detection for conversational agents this study covers a wide range of methods ranging from machine learning models to cuttingedge pretrained models as well as deep learning models the authors evaluate the performance of these techniques on the benchmark unbalanced topical chat and empathetic dialogue balanced datasets this paper offers an overview of the practical implications of emotion detection techniques in conversational systems and their impact on user response the outcomes of this paper contribute to the ongoing development of empathetic conversational agents emphasizing natural humanmachine interactions
beamforming techniques are essential to compensate for severe path loss in millimeterwave mmwave communications these techniques adopt large antenna arrays and formulate narrow beams to obtain satisfactory received powers however performing accurate beam alignment over such narrow beams for efficient link configuration by traditional beam selection approaches mainly relied on channel state information and exhaustive search typically impose significant latency and computing overheads which is often infeasible in vehicletovehicle v2v communications like highly dynamic scenarios in contrast utilizing outofband contextual information such as vehicular position information is a potential alternative to reduce such overheads this paper proposes a solution that utilizes deep learning to predict the optimal beams for vehicular communication at 60 ghz by analyzing vehicular position information the solution can identify the beams that provide sufficient mmwave received powers ensuring the best lineofsight links for vehicletovehicle v2v communications the proposed solution was tested on realworld measured mmwave sensing and communication datasets and the results show that it can achieve an average of 8458 of received power of link status making it a promising solution for beamforming in mmwave enabled v2v communications
"objective the article studies the application of ai in the field of criminal justice since then the article evaluates the feasibility and offers solutions and recommendations to ensure the right to a fair trial when applying ai to criminal justice in vietnam
 
methods to conduct research on the application of ai in criminal justice in vietnam to ensure the right to a fair trial the authors used traditional research methods of social science and legal science methods of analysis synthesis and case study to achieve the objective of the research
 
results artificial intelligence ai and its application in general are a matter of concern in social life in general and law in particular the application of artificial intelligence in criminal justice to digitize the judicial field is being applied in many parts of the world such as the united states and european countries based on the theory of the order of justice before the law the authors analyzed and assessed the impacts and effects of ai and found that the application of ai in criminal justice can negatively affect the right to a fair trial
 
conclusion through this study we propose the following contents to effectively apply ai in vietnamese criminal justice as follows i the principle of the right to a fair trial must be respected when applying ai in decisionmaking ii enhancing the role of investigators prosecutors and judges in ai predictive decisionmaking iii building a database and ai system development agency in vietnam iv upgrading technology infrastructure and databases at criminal justice agencies v developing communication and training plan on technology and human rights content vi integrating the right to a fair trial systematically into every stage of the design development implementation and ongoing monitoring of products services and systems using ai vi establishing the department of ai development and use in the future the completion of the legal framework to ensure human rights under the influence of ai and legal issues on ai are issues that need to be further studied in vietnam"
"background
recent studies have identified plasma metabolites associated with cognitive decline and alzheimers disease however little research on this topic has been conducted in latinos especially puerto ricans


objective
this study aims to add to the growing body of metabolomics research in latinos to better understand and improve the health of this population


methods
we assessed the association between plasma metabolites and global cognition over 12 years of followup in 736 participants of the boston puerto rican health study bprhs metabolites were measured with untargeted metabolomic profiling metabolon inc at baseline we used covariable adjusted linear mixed models lmm with a metabolite  time interaction term to identify metabolites of 621 measured associated with ∼12 years cognitive trajectory


results
we observed strong inverse associations between mediumchain fatty acids caproic acid and the dicarboxylic acids azelaic and sebacic acid and global cognition nformylphenylalanine a tyrosine pathway metabolite was associated with improvement in cognitive trajectory


conclusions
the metabolites identified in this study are generally consistent with prior literature and highlight a role medium chain fatty acid and tyrosine metabolism in cognitive decline"
data and privacy laws such as the gdpr require mobile apps that collect and process the personal data of their citizens to have a legallycompliant policy since these mobile apps are hosted on app distribution platforms such as google play store and app store the app publishers also require the app developers who wish to submit a new app or make changes to an existing app to be transparent about their app privacy practices regarding handling sensitive user data that requires sensitive permissions such as calendar camera microphone to verify compliance with privacy regulators and app distribution platforms the app privacy policies and permissions are investigated for consistency however little has been done to investigate gdpr completeness checking within the android permission ecosystem in this paper we investigate the design and runtime approaches towards completeness checking of sensitive ‘dangerous’ android permission policy declarations against gdpr in this paper we investigate the design and runtime approaches towards completeness checking of dangerous android permission policy declarations against gdpr leveraging the mpp270 annotated corpus that describes permission declarations in application privacy policies six natural language processing and language modelling algorithms are developed to measure permission completeness during runtime while a proof of concept class unified modeling language diagram uml tool is developed to generate gdprcompliant permission policy declarations using uml diagrams during design time this paper makes a significant contribution to the identification of appropriate permission policy declaration methodologies that a developer can use to target particular gdpr laws increasing gdpr compliance by 12 in cases during runtime using bert word embedding measuring gdpr compliance in permission policy sentences and a umldriven tool to generate compliant permission declarations
in this work we address the problem of hessian inversion bias in distributed secondorder optimization algorithms we introduce a novel shrinkagebased estimator for the resolvent of gram matrices which is asymptotically unbiased and characterize its nonasymptotic convergence rate in the isotropic case we apply this estimator to bias correction of newton steps in distributed secondorder optimization algorithms as well as randomized sketching based methods we examine the bias present in the naive averagingbased distributed newtons method using analytical expressions and contrast it with our proposed biasfree approach our approach leads to significant improvements in convergence rate compared to standard baselines and recent proposals as shown through experiments on both real and synthetic datasets
measurements of harmonics and in general of power quality in distribution grids are becoming more and more an essential task due to the proliferation of switching power devices instrument transformers its which are necessary for these kinds of measurements in distribution grids in turn need to be frequency characterized although international standards about its do not suggest procedures for their frequency characterization there are some characterization procedures that are acknowledged in the scientific literature they are based on the application of realistic power system waveforms to the its in order to test their behavior in conditions very similar to the actual operating conditions a drawback of these procedures is the quite long duration of the tests in the order of 1–10 h therefore this article proposes a new waveform for the frequency characterization of inductive voltage transformers vts composed by the superposition of a fundamental component and a periodic transient phenomenon namely a damped sine wave with its use it is possible with a single waveform to evaluate the frequency behavior of an inductive vt thus with a test duration in the order of 1 s the waveform is theoretically analyzed and experimentally applied to two commercial vts for mediumvoltage distribution grids results are compared with those obtained with the it frequency characterization methods most acknowledged in scientific literature
thirtyfive women were included in a clinical study to characterize the volatile organic compounds vocs emitted by the skin during exposure to psychological stress an original siliconbased polymeric phase was used for voc sampling on the forehead before and after stress induction cognitive stress was induced using specialized software that included a chronometer for semantic and arithmetic tasks assessment of stress was monitored using a statetrait anxiety inventory questionnaire analysis of participants’ verbal expressions and clinical measurements identification and relative quantification of vocs were performed by gas chromatographymass spectrometry stress induction was validated by a significant increase in stateanxiety as indicated by the questionnaire modifications in electrodermal activity measurements and the expression of stress verbatims in parallel a sebum production increase and a skin ph decrease were observed a total of 198 vocs with different potential sources were identified they were categorized in 5 groups probable cosmetic composition vocs produced by the body or its microbiota environmental origin and dietary intake in our qualitative statistical approach three vocs were found to be correlated with stress induction and 14 compounds showed significance in the paired wilcoxon test fattyacyls derived from lipids were predominantly identified as well as ethylbenzenes
rosaceae plants are widely distributed worldwide these plants including wellknown ornamental plants and those producing temperate fruits have significant economic value the utilization of omics technologies has significantly advanced the research progress on rosaceae plants in various areas including germplasm resources molecular breeding and evolutionary domestication here an overview of the applications of omics technologies in rosaceae is provided it includes wholegenome sequencing and assembly of various rosaceae plants as well as the utilization of genomewide association studies transcriptome proteomics metabolomics and comparative genomics these techniques are used to understand the genetic regulatory mechanisms underlying important traits in rosaceae plants such as flower color fragrance stress tolerance and fruit quality we outline the prospects of genomics research in rosaceae plants aiming to establish a foundation for comprehending the genetic mechanisms of molecular breeding and significant ornamental traits this research could potentially provide theoretical support for rapidly cultivating new germplasm and varieties
background competencybased medical education cbme is a method of medical training that focuses on developing learners’ competencies rather than simply assessing their knowledge and skills attitude ethics and communication aetcom are important components of cbme and the use of artificial intelligence ai tools such as chatgpt for cbme has not been studied hence we aimed to assess the capability of chatgpt in solving aetcom case scenarios used for cbme in india materials and methods a total of 11 case scenarios were developed based on the aetcom competencies the scenarios were presented to chatgpt and the responses generated by chatgpt were evaluated by three independent experts by awarding score ranging from 0 to 5 the scores were compared with a predefined score of 25 50 accuracy and 4 80 accuracy of a onesample median test scores among the three raters were compared by the kruskal–wallis h test the interrater reliability of the evaluations was assessed using the intraclass correlation coefficient icc results the mean score of solution provided by chatgpt was 388 ± 047 out of 5 indicating an accuracy of approximately 78 the responses evaluated by three raters were similar kruskal–wallis h p value 051 and the icc value was 0796 which indicates a relatively high level of agreement among the raters conclusion chatgpt shows moderate capability in solving aetcom case scenarios used for cbme in india the interrater reliability of the evaluations suggests that chatgpt’s responses were consistent and reliable further studies are needed to explore the potential of chatgpt and other ai tools in cbme and to determine the optimal use of these tools in medical education
cette contribution s’attachera d‘abord à examiner la manière dont la notion de contexte joue un rôle central dans l’histoire du numérique et de l’ia d’un côté et dans celle du droit relatif à la protection des données personnelles de l’autre pour examiner ensuite la manière dont elle rejoue la conception de la subjectivité de la normativité et de l’éthique nous montrerons comment cette évolution est parallèle à la mise en place de la gouvernementalité néolibérale ou de celle qualifiée plus récemment d’algorithmique et comment il nous confronte à la portée non seulement éthique mais aussi politique de l’ia ubiquitaire notre démarche vise deux apports  une proposition complémentaire de classification des ia qui repose sur l’évolution du rôle que le contexte joue dans l’action et l’éthos de l’utilisateur  une heuristique renouvelée afin de saisir l’articulation entre l’opérationnalisation des systèmes et le maintien de l’autodétermination informationnelle ou en d’autres mots entre l’efficacité technique et la normativité sociale
significance despite the need to be able to predict the effects of climatic warming on animals we lack methods to assess actual thermal limits of flying insects such as pollinators we assessed the relative danger of overheating and desiccation for honey bees carrying loads due to the capacity of hot bees to reduce metabolic heat production during flight our data suggest that under dry and poor forage conditions desiccation may limit activity before overheating impairing critical pollination services provided by honey bees
in this article we present a new method called point spread function psf‐radon transform algorithm this algorithm consists on recovering the instrument psf from the radon transform in the line direction axis of the line spread function ie the image of a line we present the method and tested with synthetic images and real images from macro lens camera and microscopy a stand‐alone program along with a tutorial is available for any interested user in martinez psf‐radon transform algorithm standalone program
in this work we present a novel emotion assessment assistant based on respiration detection using a humidity sensor with high performance the humidity sensor consists of a nanoforestbased capacitor to enhance sensitivity a microheater to shorten response time and a thermistor for temperature compensation such a sensitive humidity sensor can distinguish respiration states in different emotions with an external stimulus for respiration regulation an emotion assessment assistant is established such an assistant can distinguish emotions of different people thus provides a visual way to monitor the emotional changes in real time the assistant may have applications in smart mental healthcare in the future
aiming at the demand for higher precision and lower complexity in compensating for scale factor temperature errors in interferometric fiber optic gyroscopes ifogs in this work a dualparameter joint compensation method based on the multiple linear regression model mlrm the relationship between the average wavelength and optical power of the light source is derived theoretically integration of optical power information and temperature information significantly improves compensation efficiency compared to the model without optical power information the amount of calculation is reduced by 438 while maintaining the same level of precision the implementation on a field programmable gate array fpga has been carried out resulting in a nearly 20fold improvement in scale factor repeatability this improvement confirms the effectiveness of the method the implementation was constructed without any additional hardware devices which aligns with the trend of miniaturization in spaceborne ifogs this is particularly beneficial for small satellites
it is important in production to achieve accurate counting and density estimation of highdensity culture fry under the environmental conditions of aquaculture scenarios in an efficient and accurate manner however none of the current methods for fry counting works well under the highdensity and highoverlap conditions of real aquaculture scenarios therefore in this paper we propose a highdensity farming fry monitoring network model superresolution gan density estimate attention network sgdan which incorporating an image enhancement algorithm and an attention mechanism and we create a highdensity farming fry dataset hdfrydataset based on the environmental conditions of real aquaculture scenarios the network model is designed to improve and optimize the targeted subnetworks for several key aspects of highdensity fish fry monitoring work four subnetworks are included for image optimization feature extraction attention and density map estimation the experimental results show that the sgdan network model achieved an average counting accuracy of 9757 on the highdensity culture fry dataset which was 823 and 206 higher than those of mcnn and csrnet respectively additionally the mae and rmse of the model were reduced by 719 and 673 and by 343 and 332 compared with those of mcnn and csrnet respectively the model proposed in this paper also has a better ability to generate predictive density maps the density maps generated by sgdan have values of the evaluation metrics psnr and ssim of 2033 and 0933 respectively which are 331 and 0037 and 263 and 0031 higher than those of mcnn and csrnet in general the network model proposed in this paper outperforms existing network models in two applications accurate counting of fry and generation of density maps for highdensity culture in aquaculture it also provides a good solution for digitizing the number of fry and visualizing the density of highdensity culture in intelligent aquaculture systems
the study here explores the link between transcranial direct current stimulation tdcs and brainbehavior relationships we propose that tdcs may indirectly influence the complex relationships between brain volume and behavior we focused on the dynamics between the hippocampus hpc and cerebellum cb in cognitive processes a relationship with significant implications for understanding memory and motor skills seventyfour young adults mean age 22±042 years mean education 147±025 years were randomly assigned to receive either anodal cathodal or sham stimulation following stimulation participants completed computerized tasks assessing working memory and sequence learning in a magnetic resonance imaging mri environment we investigated the statistical interaction between cb and hpc volumes our findings showed that individuals with larger cerebellar volumes had shorter reaction times rt on a highload working memory task in the sham stimulation group in contrast the anodal stimulation group exhibited faster rts during the lowload working memory condition these rt differences were associated with the cortical volumetric interaction between cbhpc literature suggests that anodal stimulation downregulates the cb and here those with larger volumes perform more quickly suggesting the potential need for additional cognitive resources to compensate for cerebellar downregulation this new insight suggests that tdcs can aid in revealing structurefunction relationships due to greater performance variability especially in young adults it may also reveal new targets of interest in the study of aging or in diseases where there is also greater behavioral variability
nested estimation involves estimating an expectation of a function of a conditional expectation and has many important applications in operations research and machine learning nested simulation is a classic approach to this estimation and the convergence rate of the mean squared error mse of nested simulation estimators is only of order formula see text where γ is the simulation budget to accelerate the convergence in this paper we establish a jackknifebased nested simulation fast method for nested estimation and a unified theoretical analysis for general functions in the nested estimation shows that the mse of the proposed method converges at the faster rate of formula see text or even formula see text we also provide an efficient algorithm that ensures the estimator’s mse decays at its optimal rate in practice in numerical experiments we apply the proposed estimator in portfolio risk measurement and bayesian experimental design in operations research and machine learning areas respectively and numerical results are consistent with the theory presented history accepted by bruno tuffin area editor for simulation funding this work was supported by the national natural science foundation of china grants 72031006 72101260 and 72394375 supplemental material the software that supports the findings of this study is available within the paper and its supplemental information  httpspubsonlineinformsorgdoisuppl101287ijoc20230118  as well as from the ijoc github software repository  httpsgithubcominformsjoc20230118  the complete ijoc software and data repository is available at httpsinformsjocgithubio 
 heart failure is one of the most urgent problems of medicine because the number of patients suffering from this pathology continues to grow steadily the arsenal of drugs for the treatment of chronic heart failure has been replenished with new samples that have demonstrated significant effectiveness in improving the course and prognosis of patients in this cohort however the search for other promising drugs continues the aim of our work is to summarize existing scientific data on the effect of sodiumglucose cotransporter2 inhibitors vericiguat omecamtiv mecarbil on the clinical course and prognosis in patients with chronic heart failure as well as the analysis of current clinical studies aimed at evaluating hemodynamic effects tolerability and safety of experimental drugs in patients with chronic heart failure the positive effects of sodiumglucose cotransporter2 inhibitors have led to their inclusion in european guidelines for treating patients with chronic heart failure regardless of left ventricular ejection fraction vericiguat is recommended for patients with nyha class iiiv chronic heart failure and reduced left ventricular ejection fraction who have decompensated heart failure within the past 3 months despite receiving optimal chronic heart failure therapy omecamtiv mecarbil has been considered a promising drug for the treatment of heart failure due to its ability to reduce the primary composite endpoint compared to placebo for many years however in 2023 the us food and drug administration refused to approve omecamtiv mecarbil for the treatment of adult heart failure patients with reduced left ventricular ejection fraction citing insufficient evidence of the drug’s effectiveness development of new drugs for patients with chronic heart failure opens up new opportunities to improve the quality of life and prognosis of patients ongoing clinical trials of experimental drugs give hope for breakthrough results
proton magnetic resonance spectroscopy 1h‐mrs is increasingly used for clinical brain tumour diagnosis but suffers from limited spectral quality this retrospective and comparative study aims at improving paediatric brain tumour classification by performing noise suppression on clinical 1h‐mrs eighty‐threeforty‐two children with either an ependymoma ages 46 ± 5393 ± 54 a medulloblastoma ages 69 ± 3565 ± 44 or a pilocytic astrocytoma 80 ± 3663 ± 50 recruited from four centres across england were scanned with 15t3t short‐echo‐time point‐resolved spectroscopy the acquired raw 1h‐mrs was quantified by using totally automatic robust quantitation in nmr tarquin assessed by experienced spectroscopists and processed with adaptive wavelet noise suppression awns metabolite concentrations were extracted as features selected based on multiclass receiver operating characteristics and finally used for identifying brain tumour types with supervised machine learning the minority class was oversampled through the synthetic minority oversampling technique for comparison purposes post‐noise‐suppression 1h‐mrs showed significantly elevated signal‐to‐noise ratios p  05 wilcoxon signed‐rank test stable full width at half‐maximum p  05 wilcoxon signed‐rank test and significantly higher classification accuracy p  05 wilcoxon signed‐rank test specifically the cross‐validated overall and balanced classification accuracies can be improved from 81 to 88 overall and 76 to 86 balanced for the 15t cohort whilst for the 3t cohort they can be improved from 62 to 76 overall and 46 to 56 by applying naïve bayes on the oversampled 1h‐mrs the study shows that fitting‐based signal‐to‐noise ratios of clinical 1h‐mrs can be significantly improved by using awns with insignificantly altered line width and the post‐noise‐suppression 1h‐mrs may have better diagnostic performance for paediatric brain tumours
radar systems operating aboard unmanned aerial systems uass or drones are gaining lot of interest in the remote sensing field uass have recently been tested as platforms for synthetic aperture radar sar systems there are many expectations regarding the interferometric capabilities of uasbased radar interferometers nevertheless its implementation introduces new problems which demand advanced processing techniques to date only few works presented interferometric data acquired aboard uas in this paper repeatpass interferometric tests are presented performed with an sband uasbased sar by using corner reflectors subject to known displacements the interferometric capabilities of the sensor are demonstrated results from two measurement campaigns are presented data acquired in controlled scenario and in a vegetated slope a focusing algorithm that considers the uas position and attitude information able to cope with the uas instabilities is presented and discussed at the knowledge of the authors the experimental results obtained are the first systematic demonstration of repeatpass uasbased sar interferometry validated by ground truth
this paper presents a highperformance mems accelerometer with a dcac electrostatic stiffness tuning capability based on doublesided parallel plates dspps dc and ac electrostatic tuning enable the adjustment of the effective stiffness and the calibration of the geometric offset of the proof mass respectively a dynamical model of the proposed accelerometer was developed considering both dcac electrostatic tuning and the temperature effect based on the dynamical model a selfcentering closed loop is proposed for pulling the reference position of the forcetorebalance ftr to the geometric center of dspp the selfcentering accelerometer operates at the optimal reference position by eliminating the temperature drift of the readout circuit and nulling the net electrostatic tuning forces the stiffness closedloop is also incorporated to prevent the pullin instability of the tuned lowstiffness accelerometer under a dramatic temperature variation realtime adjustments of the reference position and the dc tuning voltage are utilized to compensate for the residue temperature drift of the proposed accelerometer as a result a novel controlling approach composed of a selfcentering closed loop stiffnessclosed loop and temperature drift compensation is achieved for the accelerometer realizing a temperature drift coefficient tdc of approximately 7 μg°c and an allan bias instability of less than 1 μg
in the bounded storage model introduced by maurer the adversary is computationally unbounded and has a bounded storage capacity in this model informationtheoretic secrecy is guaranteed by using a publicly available random string whose length is larger than the adversary storage capacity the protocol proposed by maurer is simple from the perspective of implementation and efficient from the perspective of the initial secret key size and random string length however he provided the proof of the security for the case where the adversary can access a constant fraction of the random string and store only original bits of the random string in this paper we provide a new proof of the security of the protocol proposed by maurer for the general bounded storage model ie the adversary can access all bits of the random string and store the output of any boolean function on the string we reaffirm that the protocol is absolutely semantically secure in the general bounded storage model
background in recent years the use of conometric systems to connect dental implant abutments and prosthetic caps has been advocated because they seem to eliminate the side effects reported when using screw and cementconnected prosthetic restorations objectives the present case study is focused on conometric connection characterization and its performance in terms of the microarchitecture of periimplant soft tissues by using a crosslinked approach based on optical microscopy and threedimensional imaging methods two dental implants were characterized using microct and another identical one was implanted into a patient the latter was retrieved 45 days later due to changes in prosthetic needs afterward the periimplant soft tissues were investigated using synchrotronbased phase contrast imaging histology and polarized light microscopy results microct analysis showed perfect adhesion between the abutment and prosthetic cap histology and polarized light microscopy showed that connective tissue was richly present around the abutment retrieved from the patient moreover the quantitative evaluation of connective tissues using synchrotron imaging supported by artificial intelligence revealed that this tissue was rich in mature collagen with longitudinal and transverse collagen bundles intertwined the number and connectivity of transverse bundles were consistently greater than those of the longitudinal bundles conclusion it was found that the periimplant soft tissue was already mature and well organized after only 45 days of implantation supporting the hypothesis that conometric connections contribute to the significant stabilization of periimplant soft tissues
a recent line of work has proposed to quantify the contribution of database tuples to query answers using shapley values a game theoretic function that has been extensively used as means of attribution in other areas notably machine learning in this paper we analyze and evaluate learnshapley  a solution that employs machine learning to rank input facts based on their estimated shapleybased contribution to query answers learnshapley is trained on a corpus of spju queries their output and the shapley values of each input tuple with respect to each output tuple at inference time learnshapley is given a new spju query over the same database schema an output tuple of interest and its lineage ie the set of all facts that have contributed in some way to the generation of the tuple our experiments evaluate to what extent learnshapley is able to leverage similarity measures applied to the query in hand and the queries stored in the repository to compute a ranking of the facts in the lineage based on their contribution overall our experiments indicate that a log of past queries output tuples and their shapley values includes a reasonably relevant signal for predicting the ranking of facts contributions for a new spju query over the same database both dbshap and our code are publicly available and may serve for further investigation of machine learning approaches for explainability in databases
background the phyllosphere microbiome is crucial for plant health and ecosystem functioning while host species play a determining role in shaping the phyllosphere microbiome host trees of the same species that are subjected to different environmental conditions can still exhibit large degrees of variation in their microbiome diversity and composition whether these intraspecific variations in phyllosphere microbiome diversity and composition can be observed over the broader expanse of forest landscapes remains unclear in this study we aim to assess the variation in the top canopy phyllosphere bacterial communities between and within host tree species in the temperate european forests focusing on fagus sylvatica european beech and picea abies norway spruce results we profiled the bacterial diversity composition driving factors and discriminant taxa in the top canopy phyllosphere of 211 trees in two temperate forests veluwe national parks the netherlands and bavarian forest national park germany we found the bacterial communities were primarily shaped by host species and large variation existed within beech and spruce while we showed that there was a core microbiome in all tree species examined community composition varied with elevation tree diameter at breast height and leafspecific traits eg chlorophyll and p content these driving factors of bacterial community composition also correlated with the relative abundance of specific bacterial families conclusions while our results underscored the importance of host species we demonstrated a substantial range of variation in phyllosphere bacterial diversity and composition within a host species drivers of these variations have implications at both the individual host tree level where the bacterial communities differed based on tree traits and at the broader forest landscape level where drivers like certain highly plastic leaf traits can potentially link forest canopy bacterial community variations to forest ecosystem processes we eventually showed close associations between forest canopy phyllosphere bacterial communities and host trees exist and the consistent patterns emerging from these associations are critical for host plant functioning supplementary information the online version contains supplementary material available at 101186s40793024005656
introduction primary progressive aphasia ppa is a neurodegenerative disease characterized by linguistic impairment the two main clinical subtypes are semantic svppa and nonfluentagrammatic nfvppa variants diagnosing and classifying ppa patients represents a complex challenge that requires the integration of multimodal information including clinical biological and radiological features structural neuroimaging can play a crucial role in aiding the differential diagnosis of ppa and constructing diagnostic support systems methods in this study we conducted a white matter texture analysis on t1weighted images including 56 patients with ppa 31 svppa and 25 nfvppa and 53 age and sexmatched controls we trained a treebased algorithm over combined clinicalradiomics measures and used shapley additive explanations shap model to extract the greater impactful measures in distinguishing svppa and nfvppa patients from controls and each other results radiomicsintegrated classification models demonstrated an accuracy of 95 in distinguishing svppa patients from controls and of 937 in distinguishing svppa from nfvppa an accuracy of 937 was observed in differentiating nfvppa patients from controls moreover shapley values showed the strong involvement of the white matter near left entorhinal cortex in patients classification models discussion our study provides new evidence for the usefulness of radiomics features in classifying patients with svppa and nfvppa demonstrating the effectiveness of an explainable machine learning approach in extracting the most impactful features for assessing ppa
video text spotting vts is a fundamental visual task that aims to predict the trajectories and content of texts in a video previous works usually conduct local associations and apply ioubased distance and complex postprocessing procedures to boost performance ignoring the abundant temporal information and the morphological characteristics in vts in this paper we propose a novel global video text spotting transformer glotsformer to model the tracking problem as global associations and utilize the gaussian wasserstein distance to guide the morphological correlation between frames our main contributions can be summarized as three folds 1 we propose a transformerbased global tracking method glotsformer for vts and associate multiple frames simultaneously 2 we introduce a wasserstein distancebased method to conduct positional associations between frames 3 we conduct extensive experiments on public datasets on the icdar2015 video dataset glotsformer achieves 560 mota with 46 absolute improvement compared with the previous sota method and outperforms the previous transformerbased method by a significant 83 mota
this paper proposes an eventtriggered model predictivepreview control strategy for autonomous vehicle trajectory tracking first the dynamic equation is established based on the preview road curvature and the vehicle’s 2 degreeoffreedom relationship a model predictive tracking controller is designed by predicting the system dynamics in the future second to reduce the computational burden of the controller the triggering conditions are designed according to the system’s stability and feasibility the proposed eventtriggered model predictivepreview control strategy not only makes the autonomous vehicles maintain tracking accuracy but also reduces the number of online optimization then asymptotically stable of closedloop systems are proved by using lyapunov’s theorem finally simulation experiments show that the proposed strategy has some advantages over traditional model predictive control in terms of improving vehicle tracking accuracy and reducing algorithm complexity
sulphur dioxide so2 emissions from mt sinabung eruption were quantified in time series for 2019 both pyroclastic materials and gas or aerosol ejected during volcanic eruption contain sulphur as sulphate salt deposits coating volcanic ash grains or gasses sulphur dioxide from the eruption will directly impact the surrounding area spectral from satellite optical sensors can be used to monitor and measure so2 gas near realtime after an eruption the distribution of so2 column density in the atmosphere was tracked using the sentinel5p satellite regression kriging rk is applied to predict the spatial distribution of sulphur the area under study is located in a radius of 3 to 7 km from the eruptive center covering an area of about 4517 ha a total of 51 soil samples and volcanic ash were collected from 0 20 cm soil depth based on a 1x1 km grid interval all samples were air dried sieved and analyzed for ph sulphate and total so3 using xrf the google earth engine gee platform was also used to process sentinel5p satellite imagery to determine the number and distribution of so2 column density in the atmosphere during 2019 the ph of the ash is very acidic to neutral 356  655 while soils are considered acidic to neutral 467  652 the available sulphate content in soil ranges from 0 to 30339 ppm and 0 to 14247 ppm in volcanic ash samples so2 content in ash ranges from 0 to 1653 and 0 to 371 in soils sentinel5p satellite image spectral data shows that so2 is concentrated mainly in the southern region with the highest level occurring in august 2019 this study can serve as one of the volcanic mitigation programs and forecast the distribution of so2 in an active volcanic region of indonesia
with the expanding scope of human activities in marine environments the efficient detection and tracking of mobile targets on the oceans surface have become increasingly crucial synthetic aperture radar sar constellation can obtain ground observation data based on user requests and subject to visibility conditions now it is an indispensable tool in sea surface moving target search tasks satellite constellation resources are scarce and limited and user demands are diverse how to rationally dispatch satellite constellation resources to meet user needs to the maximum extent and improve the application efficiency of satellite resources is an urgent scientific problem that needs to be solved this article mainly expounds two respects of work first modeling sar constellation scheduling problem for sea surface moving target search tasks to establish the objective function second a novel multistrategy discrete constrained differential evolution algorithm denoted as msdcde is proposed in the article the proposed msdcde algorithm integrates cross strategy based on discrete variables constraint handling techniques population restart strategy and leftshift local strategy which can effectively avoid falling into local optimality thereby achieving global optimality and improving search and rescue performances six sets of experiments totaling 215 runs have been conducted to validate the effectiveness of the proposed resolution process framework and the msdcde algorithm the proposed method demonstrated an over 4898 performance improvement compared with some stateoftheart algorithms and significantly reduced task completion time
implementation of the persistent current mode pcm provides an effective solution for mitigating thermal load on superconducting magnets used in maglev trains and addressing external power supply issues however the hts magnet is affected by harmonics by the armature coil during running operation notably the dynamic resistance by the external timevarying magnetic field under the dc transport current generates additional ac loss this must be considered during the design of a quasipersistent current switch quasipcs as it causes additional field attenuation and thermal loss this paper describes the characteristics of field attenuation of hts magnet of labscale with the quasipcs due to the dynamic resistance for dc transport currents condition under various external ac field by experiments and numerical analysis
emojis have become very popular in recent years and they help analyze the meaning of texts in many natural language processingnlp tasks such as sentiment analysis and questionanswering systems the ability to process emojis appropriately can significantly enhance the performance of these task models however neural networks are fragile and inherently vulnerable to various attacks such as adversarial attacks and backdoor attacks which can seriously reduce the usability of the models there is no systematic study of the security issues that emojis may pose to different classification models this paper presents the first systematic study on the security of emojibased classification models we analyze the security issues that may be raised by emoji processing methods using adversarial and backdoor attack methods across different models and application scenarios our research shows emoji processing can increase model performance and raise security concerns our experiments show that most wordbased attack methods equally apply to emojis given the widespread use of emojis and flaws in their processing methods emojis could be a severe security threat to nlp tasks in addition we propose the characteristics of emojibased attacks finally we summarize the possible defense mechanisms based on existing research
"background
digital health technology is progressively transforming physiotherapy practice despite a maturing body of literature relating to physiotherapy digital health capability research examining digital health physiotherapy competency standards is both lacking and lagging


objective
examine international professional practice competency standards for physiotherapists to identify themes common to digital health practice competency published by international peak organizations governing physiotherapy practice


methods
systematic metasynthesis of international peak organization physiotherapy practice competency standards the study was undertaken over nine stages competency statements related to digital health were extracted and further coded into resultant themes


results
eleven documents were analyzed fiftytwo statements explicitly referenced digital health competency identified themes were as follows 1 digital health data governance 2 digital health data translation and 3 digital health technologies where digital healthrelated competency statements do exist they are skewed toward health information management activities


conclusions
digital health practice is currently underrepresented in competency standards for physiotherapists workforce advancement in light of the burgeoning impact of digital health will prompt further updates to professional competency standards set by our peak organizations this will have a flow on effect whereby education providers eg universities and other professional development providers should consider curriculum and training that prepares individuals for digitally enabled practice"
as a powerful forwardlooking risk management technique fmea has been widely used for reliability and security analysis in engineering management fields in practice the implementation of fmea involves the participation of multiple members with distinct knowledge experience and cultural background thus fmea is a typical multicriteria group decisionmaking problem however the existence of the aggregation discrepancy issue in fmea problems with group decisionmaking leads to collective evaluation conflicts that bring about decision bias for further risk management this article proposes an aadfmea method based on multipleobjective optimization to promote the accuracy and reliability of collective assessment information in fmea members in which the 10point qualitative scale is used for evaluation additionally the aadfmea method responds to the different individual cognitions to a specific evaluation value from multiple members by constraining the risk prioritization to derive more reasonable collective risk evaluation information finally two case studies are presented to show that the aadfmea method has a good application value and the simulation analysis was conducted to verify the effectiveness of the proposed aadfmea approach
this paper proposes an approach to incorporate reliability and maintainability rm tasks into the digital model of a complex system of systems leveraging modelbased systems engineering mbse and the unified architecture framework uaf the benefits and significance of this integration are analyzed and a realworld implementation as part of an ongoing effort with nasa is presented the effective handling of rm tasks such as risk analysis fault tree analysis fta or failure mode and effect analysis fmea is critical to large organizations that are faced with significant backlogs of deferred maintenance and large annual maintenance budgets a significant accomplishment highlighted in this article is the integration and unified modeling of questions within the fmea process the approach presented herein aims to model the infrastructure of a large organization to help address the backlog of deferred maintenance and minimize the risk that unreliable andor unavailable infrastructure may have on the ability of such organizations to successfully carry their missions the integration of risk and reliability information is essential to the development of a comprehensive digital representation of a system inevitably integrating rm activities into the digital engineering ecosystem dee has become a recognized need this is illustrated by recent efforts from the object management group omg to develop a beta version of a new modeling language called raaml risk analysis and assessment modeling language raaml 1 helps assess the performance of various analysis techniques including fmea fta systems theoretic process analysis stpa goal structuring notation gsn and iso 26262 road vehicles functional safety using mbse while mbse serves as a valuable foundation for the dee it alone does not offer a complete solution for modeling complex enterprise architectures one of the key challenges is the absence of a standardized modeling approach which can hinder effective collaboration and information sharing across organizations to address this gap enterprise architecture frameworks have been developed to provide a unified modeling approach among these frameworks the uaf stands out as a recent development that enables organizations to establish a cohesive and standardized architecture 2 by adopting uaf organizations can collaborate seamlessly share information effectively and achieve a unified architecture that spans various domains and stakeholders uaf serves as a valuable tool in enhancing the modeling capabilities within the dee and enables organizations to tackle the complexities of enterprise architecture more efficiently collaborating with nasa stakeholders global technology connection inc gtc and the aerospace systems design laboratory asdl at the georgia institute of technology are creating a digital representation of nasa goddard space flight center gsfcs enterprise enabling datadriven decisionmaking to help minimize the impact that unavailable facilitiescapabilities due to repair etc may have on mission success this effort aligns with nasas goal of modernizing digital systems for managing aging infrastructure 3 in particular the objective of this paper is to further emphasize and demonstrate through a realworld implementation of mbseuaf to an infrastructure problem the benefits that such modelbased approaches can bring to the rm community at large this paper also recommends two practical initiatives to facilitate the integration of rm activities into a dee • skill enhancement for reliability and safety engineers in mbse • collaboration between mbse and rm software vendors to share knowledge and expertise
this research addresses the pressing challenge of intrusion detection and prevention in wireless sensor networks wsns offering an innovative and comprehensive approach the research leverages support vector regression svr models to predict the number of barriers necessary for effective intrusion detection and prevention while optimising their strategic placement the paper employs the ant colony optimization aco algorithm to enhance the precision of barrier placement and resource allocation the integrated approach combines svr predictive modelling with acobased optimisation contributing to advancing adaptive security solutions for wsns feature ranking highlights the critical influence of barrier count attributes and regularisation techniques are applied to enhance model robustness importantly the results reveal substantial percentage improvements in model accuracy metrics a 483571 reduction in mean squared error mse for acosvr1 an 86208 improvement in mean absolute error mae for acosvr1 and an 8629 enhancement in rsquared r2 for acosvr1 acosvr2 has a 220285 reduction in mse a 73398 improvement in mae and a 5403 enhancement in rsquared these considerable improvements verify the method’s effectiveness in enhancing wsns ensuring reliability and resilience in critical infrastructure the paper concludes with a performance comparison and emphasises the remarkable efficacy of regularisation it also underscores the practicality of precise barrier count estimation and optimised barrier placement enhancing the security and resilience of wsns against potential threats
simultaneous conﬁdence intervals sci for multinomial proportions are a corner stone in count data analysis and a key component in many applications a variety of schemes were introduced over the years mostly focusing on an asymptotic regime where the sample is large or a small sample regime where the alphabet size is relatively small in this work we introduce a new sci framework which considers a large alphabet setup our proposed framework utilizes bootstrap sampling with the goodturing probability estimator as a plugin distribution we demonstrate the favorable performance of our proposed method in synthetic and realworld experiments importantly we provide an exact analytical expression for the bootstraped statistic which replaces the computationally costly sampling routine our proposed framework is publicly available at the ﬁrst author’s webpage
new technologies can help older persons age in place and support their caregivers however they need to be accepted by the endusers to do so technology acceptance models such as tam and utaut and their extensions use factors like performance expectancy and effort expectancy to explain acceptance furthermore they are based on quantitative methods our qualitative study investigates factors fostering and hindering acceptance among older persons and their caregivers for a variety of assistive technologies including wearables ambient sensors at home with and without cameras and social companion robots the goal of this paper is twofold on the one hand it investigates the factors of technology acceptance models in a qualitative setting on the other hand it informs these models with aspects currently overlooked by them the results reveal that performance expectancy and effort expectancy are relevant for acceptance we also find that reliability anxiety around technology and different social aspects have an influence on acceptance of assistive technology in aged care for all enduser groups our findings can be used to update current technology acceptance models and provide indepth knowledge about the currently used factors
although there is no doubt from an empirical viewpoint that reflex mechanisms can contribute to tongue motor control in humans there is limited neurophysiological evidence to support this idea previous results failing to observe any tonic stretch reflex in the tongue had reduced the likelihood of a reflex contribution in tongue motor control the current study presents experimental evidence of a human tongue reflex in response to a sudden stretch while holding a posture for speech the latency was relatively long 50 ms which is possibly mediated through corticalarc the activation peak in a speech task was greater than in a nonspeech task while background activation levels were similar in both tasks and the peak amplitude in a speech task was not modulated by the additional task to react voluntarily to the perturbation computer simulations with a simplified linear massspringdamper model showed that the recorded muscle activation response is suited for the generation of tongue movement responses that were observed in a previous study with the appropriate timing when taking into account a possible physiological delay between reflex muscle activation and the corresponding force our results evidenced clearly that reflex mechanisms contribute to tongue posture stabilization for speech production
source separation of a mixed signal in the timefrequency domain is critical for joint communication and radar jcr systems to achieve the required performance especially at a low signaltonoise ratio snr in this paper we propose the use of a generative model such as the unsupervised variational autoencoder vae to separate sensing and data communication signals we first analyse the vae system using different mask techniques then the best technique is selected for comparison with popular blind source separation bss algorithms we verify the performance of the proposed vae by adopting different metrics such as the signaltodistortion ratio sdr sourcetointerference ratio sir and sourcestoartifacts ratio sar simulation results show that the proposed vae outperforms the bss techniques at low snr for the case of a mixed signal in the timefrequency domain and at low and high snr for a mixed signal in the time domain it enables the jcr system in the challenging first scenario to obtain sdr gains of 111 db and 6 db at 0 db snr for recovering the sensing and data communication signals respectively finally we analyse the robustness of the jcr system in detecting an interference signal operating in the same frequency band where the simulation result indicates an accuracy of 91 based on the proposed steps
the bidirectional encoder representations from transformers bert model is used in this work to analyse sentiment on twitter data a kaggle dataset of manually annotated and anonymized covid19related tweets was used to refine the model location tweet date original tweet content and sentiment labels are all included in the dataset when compared to the multinomial naive bayes mnb baseline berts performance was assessed and it achieved an overall accuracy of 87 on the test set the results indicated that for negative feelings the accuracy was 093 the recall was 084 and the f1score was 088 for neutral sentiments the precision was 086 the recall was 078 and the f1score was 082 and for positive sentiments the precision was 082 the recall was 094 and the f1score was 088 the models proficiency with the linguistic nuances of twitter including slang and sarcasm was demonstrated this study also identifies the flaws of bert and makes recommendations for future research paths such as the integration of external knowledge and alternative designs
robotic operations such as opening doors in an unstructured environment pose a challenge for robot control due to unmodelled constraints in this article a multiple working mode approach is proposed to control a mobile serial robot manipulator to perform operations on mechanisms with unmodelled constraints the multiple working mode control switches the working mode of a number of active joints during the operation to relax the problem of overactuation of the constrained robot manipulator when modeling errors cause excessive internal and external reaction forces three practical criteria are derived for the assignment of active and passive control modes two based on the coordinate partitioning method for multibody dynamics and another based on minimizing task friction forces the proposed control method has been implemented on a mobile reconfigurable robot performing constrained motion along an elliptical trajectory similar to a dooropening task simulation and experimental results are presented to show the effectiveness of the proposed control approach
"
 the utilization of biohydrogen as a fuel source holds immense promise as a renewable energy option offering compelling economic and environmental advantages this study investigates the economic and environmental advantages of biohydrogen as a renewable energy source compared to fossil fuels focusing on the reduction of greenhouse gas emissions such as carbon dioxide and carbon monoxide the enhancement of anaerobic hydrogen production reactor capacity is explored through the application of a fuzzy controller system numerical simulations demonstrate that the fuzzy controller outperforms other methods in augmenting biological hydrogen production effectively addressing the inherent nonlinear characteristics of the system in contrast limitations in robustness against system uncertainty are observed with the nonlinear controller exceptional tracking of desired values by the fuzzy controller even in the presence of model uncertainty results in a lower integral of time multiplied by squared error itse performance index compared to nonlinear and proportional–integral controllers emphasizing the viability of the fuzzy method for regulating hydrogen production processes potential gains of up to 95 in biological hydrogen production are indicated compared to openloop configurations this cleanburning fuel holds promise for industrial applications contributing to the reduction of harmful gas emissions the findings underscore the transformative potential of the fuzzy controller system in advancing sustainable hydrogen production and its significant role in addressing environmental concerns"
if a person firmly believes in a nonfactual statement such as “the earth is flat” and argues in its favor there is no inherent intention to deceive as the argumentation stems from genuine belief it may be unlikely to exhibit the linguistic properties associated with deception or lying this interplay of factuality personal belief and intent to deceive remains an understudied area disentangling the influence of these variables in argumentation is crucial to gain a better understanding of the linguistic properties attributed to each of them to study the relation between deception and factuality based on belief we present the defabel corpus a crowdsourced resource of beliefbased deception to create this corpus we devise a study in which participants are instructed to write arguments supporting statements like “eating watermelon seeds can cause indigestion” regardless of its factual accuracy or their personal beliefs about the statement in addition to the generation task we ask them to disclose their belief about the statement the collected instances are labelled as deceptive if the arguments are in contradiction to the participants’ personal beliefs each instance in the corpus is thus annotated or implicitly labelled with personal beliefs of the author factuality of the statement and the intended deceptiveness the defabel corpus contains 1031 texts in german out of which 643 are deceptive and 388 are nondeceptive it is the first publicly available corpus for studying deception in german in our analysis we find that people are more confident in the persuasiveness of their arguments when the statement is aligned with their belief but surprisingly less confident when they are generating arguments in favor of facts the defabel corpus can be obtained from httpswwwimsunistuttgartdedatadefabel 
the objective of this study is mainly to analyze the drying kinetic parameters effective diffusivity and thermodynamic performance indicators energy exergy heat and mass transfer coefficients of tea under different drying conditions of different drying temperatures dt and thin layer thicknesses tt experimental drying was conducted at drying temperatures of 70°c 80°c and 90°c with thin layer thicknesses of 10 mm 15 mm and 20 mm the results show that a higher drying temperature and a lower thin layer thickness can increase evaporation moisture content and shorten drying time by evaluating and comparing the fitting of five drying models adopted by the coefficient of determination r2 and chisquare χ2 it can be found that the logarithmic model is the best to describe drying behavior the effective moisture diffusivity shows a positively correlated trend with the increase in dt and tt with the value of activation energy ranging from 14030 to 22344 kj mol−1 k−1 the specific moisture evaporation rate smer energy efficiency exergy efficiency and sustainability index si descend as the tt increases at all dt the specific energy consumption sec and improvement potential ip rate perform in an opposite manner but the tt remains unchange the conclusion drawn from the thermodynamic parameters is opposite to the aforementioned as the tt increases the heat and mass transfer coefficients show a significant decrease trend in addition the heat and mass transfer coefficients are given as functions of dt and tt and further knowledge shows that the mass transfer coefficient is positively correlated with dt but the heat transfer coefficient is negatively correlated in conclusion this article provides new insights into the effects of drying characteristics energy consumption characteristics and heat and mass transfer characteristics in the process of tea drying under different drying conditions and provides certain theoretical reference bases for promoting the optimization of industrialized tea drying production machinery design and drying process optimization
compared to block precoding bp interference exploitation symbollevel precoding ieslp can effectively reduce transmit power in multiantenna wireless communication systems however because most existing ieslp methods are symbollevel optimizations of zeroforcing zf bp which is usually not the best bp the performance and application range of ieslp are restricted in this article we investigate the symbollevel optimization of any given bp method to reduce transmit power we first define generalized constructive interference gci as a new constructive interference metric for any received bp signals where the interference is constructive as long as it will not impair the symbol error rate ser then convex gci regions gcirs of received bp signals for both phase shift keying psk and quadrature amplitude modulation qam constellations are designed with the aid of gcir we propose bpbased symbollevel precoding bpslp methods to reduce transmit power moreover due to the difficulty of obtaining perfect channel state information csi we also propose robust bpslp rbpslp methods to reduce transmit power for psk and qam constellations where outage probability op is constrained for robustness numerical results show that compared to existing precoding methods our proposed precoding methods effectively reduce transmit power it is worth noting that due to the bounded or even onepoint constructive interference region cir there are usually no feasible solutions for existing robust ieslp methods under qam constellations but our proposed rbpslp method for qam constellations is feasible as long as a feasible robust bp matrix exists
interturn short circuit fault of distribution transformer winding occurs frequently and is difficult to accurately realtime monitoring which seriously affects the reliability of the distribution network power supply therefore the identification method of interturn short circuit faults for distribution transformers based on power loss variation is proposed to realize the online monitoring of winding interturn short circuit faults and early warning of insulation deterioration first the “fieldcircuit” coupling threedimensional simulation model is established as consistent with the actual transformer on the premise of verifying the reliability and accuracy of the model the variation characteristics of each physical quantity are simulated and analyzed when a singleturn short circuit occurs in different positions in the secondary side inner and outer winding of the distribution transformer the winding power loss with the most significant change rate and easy to detect is obtained as the sensitive state variable then by changing the winding interturn insulation resistance and the number of shortcircuit turns the gradual altering process of interturn insulation deterioration is simulated and the variation law of winding current and power loss in the main circuit is explored moreover the relationship between the interturn insulation state and influence factors including the critical resistance value of interturn insulation deterioration and insulation collapse the power loss rate of change is analyzed finally a method of interturn short circuits fault identification of distribution transformer is proposed which can diagnose the interturn insulation state of transformer winding in realtime and predict the interturn shortcircuit fault
to understand how macromolecular content varies in the human brain with age in a large cohort of healthy subjects
this research article investigates the sustainable development of transport infrastructure within the national economy emphasizing the critical intersection of economic growth environmental responsibility and social equity the study explores theoretical frameworks and practical approaches employed in the planning implementation and management of sustainable transport systems the role of integrated transportation planning multimodal systems green infrastructure and smart technologies is analyzed to assess their impact on reducing congestion minimizing emissions and enhancing overall accessibility examining the effectiveness of publicprivate partnerships regulatory measures and community engagement the research delves into the challenges and opportunities associated with achieving sustainable transport infrastructure additionally the article explores the significance of smart traffic management systems fleet modernization and incentives for environmentally friendly modes of transportation the regulatory measures and standards implemented to guide sustainable practices are scrutinized focusing on their role in shaping a transportation landscape that aligns with broader sustainability objectives the findings aim to provide insights for policymakers urban planners and researchers offering a comprehensive perspective on the complex dynamics involved in fostering a sustainable transport infrastructure system within the national economy
transfer learning has become an essential part of medical imaging classification algorithms often leveraging imagenet weights the domain shift from natural to medical images has prompted alternatives such as radimagenet often showing comparable classification performance however it remains unclear whether the performance gains from transfer learning stem from improved generalization or shortcut learning to address this we conceptualize confounders by introducing the medical imaging contextualized confounder taxonomy miccat and investigate a range of confounders across it  whether synthetic or sampled from the data  using two public chest xray and ct datasets we show that imagenet and radimagenet achieve comparable classification performance yet imagenet is much more prone to overfitting to confounders we recommend that researchers using imagenetpretrained models reexamine their model robustness by conducting similar experiments our code and experiments are available at httpsgithubcomdoviledosourcematters
a sustainable future entails addressing net‐zero emissions which balances greenhouse gas emissions thereby mitigating climate change this study explores the relationship between the information quality system quality and service quality of blockchain‐enabled smart contracts bscs with collaboration quality to achieve net‐zero emissions the study further explores the role of collaboration quality in influencing the intention‐to‐use bsc and organizational satisfaction the study employed the revised delone and mclean model and validated it with data from 434 respondents using structural equation modeling the results validate all the hypotheses derived from the updated delone and mclean information system is success model there exist significant relationships between information quality system quality and service quality in the bscs leading to collaboration quality to achieve net‐zero emissions the study establishes the significant role of collaboration quality which positively impacts usage behavior and organizational satisfaction mediated by the intention to use the findings of this study offer insights for organizations and policymakers seeking to harness blockchain for environmental sustainability
in quantum computation optimizing depth and number of ancillary qubits in quantum circuits is crucial due to constraints imposed by current quantum devices this paper presents an innovative approach to implementing arbitrary symmetric boolean functions using polylogarithmic depth quantum circuits with logarithmic number of ancillary qubits symmetric functions are those whose outputs rely solely on the hamming weight of the inputs these functions find applications across diverse domains including quantum machine learning arithmetic circuit synthesis and quantum algorithm design eg grovers algorithm moreover by fully leveraging the potential of qutrits an additional energy level the ancilla count can be further reduced to 1 the key technique involves a novel polylogarithmic depth quantum circuit designed to compute hamming weight without the need for ancillary qubits the quantum circuit for hamming weight is of independent interest because of its broad applications such as quantum memory and quantum machine learning
existing research on reconfigurable intelligent surfaces ris has mainly focused on narrowband applications due to limitations in programmable metasurfaces and field prototype realization this letter introduces a wideband ris meta‐atom consisting of a single‐layer frequency selective surface supported by a dogbone‐shaped patch ensuring a stable linear phase interval around the resonant frequency a ris communication prototype is constructed to evaluate its phase bandwidth upper limit and fundamental bandwidth properties an equivalent circuit model is derived to predesign the wideband ris meta‐atom controlled by a varactor and experimental validation in an anechoic chamber confirms its large bandwidth additionally a wideband ris‐assisted self‐contained prototype is demonstrated to illustrate the bandwidth characteristics and link gain enhancement at a system level test results indicate a researched bandwidth of up to 1965 with the 1‐bit ris board achieving a 3‐db gain bandwidth of about 862
"
 unsupervised machine learning methods are gaining attention in the seismological community as more and larger datasets of continuous waveforms are collected recently contrastive learning for unsupervised feature learning has shown great success in the field of computer vision and other domains and we aim to transfer these methods to the domain of seismology contrastive learning algorithms use data augmentation to implement an instancelevel discrimination task the feature representations of two augmented versions of the same data example are trained to be similar when at the same time dissimilar to other data examples in particular we use the popular contrastive learning method simclr we test data augmentation strategies varying amplitude and frequency of seismological signals and apply contrastive learning methods to automatically learn features we use a dataset containing various mostly cryogenic waveforms detected by an stalta shortterm averagelongterm average algorithm on continuous waveform recordings from the geophysical observatory at neumayer station antarctica the quality of the features is evaluated on a handlabeled dataset that includes icequakes earthquakes and spikes and on a larger unlabeled dataset using a classical clustering method kmeans results show that the approach separates the different handlabeled groups with an accuracy of up to 88 and separates meaningful groups within the unlabeled data thus we provide an effective tool for the unsupervised exploration of large seismological datasets and the automated compilation of event catalogs"
many modern systems such as financial transportation and telecommunications systems are timesensitive in the sense that they demand lowlatency predictions for realtime decisionmaking such systems often have to contend with continuous unbounded data streams as well as concept drift which are challenging requirements that traditional regression techniques are unable to cater to there exists a need to create novel data stream regression methods that can handle these scenarios we present a databaseinspired datastream regression model that a uses inspiration from rtrees to create granules from incoming datastreams such that relevant information is retained b iteratively forgets granules whose information is deemed to be outdated thus maintaining a list of only recent relevant granules and c uses the recent data and granules to provide lowlatency predictions the rtreeinspired approach also makes the algorithm amenable to integration with database systems our experiments demonstrate that the ability of this method to discard data produces a significant orderofmagnitude improvement in latency and training time when evaluated against the most accurate stateoftheart algorithms while the rtreeinspired granulation technique provides competitively accurate predictions
recently demand for electric vehicles has been increasing as a countermeasure against global warming but they currently face many problems compared to gasolinepowered vehicles for example charging takes time and there are few places where electric vehicles can be charged if ac power supplies that can transfer energy to electric vehicles wirelessly exist under the lanes where electric vehicles drive the cruising range will be increased in this study assuming wireless power transfer to a moving electric vehicle an experiment was conducted to light up a lightemitting diode led on a moving electric model car to improve the efficiency of transfer the optimal frequency for the position of the electric model car was calculated and the value was fed back to the power supply to adjust the frequency in real time
soil moisture sm data can provide guidance for decisionmakers in fields such as drought monitoring and irrigation management soil moisture active passive smap satellite offers sufficient spatial resolution for globalscale applications but its utility is limited in regional areas due to its lower spatial resolution to address this issue this study proposed a downscaling framework based on the stacking strategy the framework integrated extreme gradient boosting xgboost light gradient boosting machine lightgbm and categorical boosting catboost to generate 1 km resolution sm data using 15 highresolution factors derived from multisource datasets in particular to test the influence of terrain partitioning on downscaling results anhui province which has diverse terrain features was selected as the study area the results indicated that the performance of the three base models varied and the developed stacking strategy maximized the potential of each model with encouraging downscaling results specifically we found that 1 the stacking model achieved the highest accuracy in all regions and the performance order of the base models was xgboost  catboost  lightgbm 2 compared with the measured sm at 87 sites the downscaled sm outperformed other 1 km sm products as well as the downscaled sm without partitioning with an average ubrmse of 0040 m3m3 3 the downscaled sm responded positively to rainfall events and mitigated the systematic bias of smap it also preserved the spatial trend of the original smap with higher levels in the humid region and relatively lower levels in the semihumid region overall this study provided a new strategy for soil moisture downscaling and revealed some interesting findings related to the effectiveness of the stacking model and the impact of terrain partitioning on downscaling accuracy
we propose an attentionweighted model for parallel extraction of spatialtemporal features to enhance the detection capabilities in the message queuing telemetry transport protocol widely used in the internet of things our approach involves constructing perception node collection graphs based on packet header information which capture transmissiondependent and contextsequencedependent relationships in the data streams we leverage a messagepassing mechanism to aggregate adjacent nodes and update the weight matrix accordingly additionally we employ a bidirectional long shortterm memory model to capture longdistance dependencies in the sequence the updated graph and the output of the timeseries model are fused and processed by a selfattention mechanism generating weights for classification the classification results are obtained using a fully connected network we evaluate our approach on four datasets toniot botiot unswnb15 and dohbrw2020 and compare it against nine different algorithms experimental results demonstrate the effectiveness of our method achieving high accuracy levels such as 08874 on toniot 09386 on botiot 09390 on dohbrw2020 and the best accuracy of 08659 on the unbalanced unswnb15 dataset
we present an accurate and costeffective method for investigating the accretion reactions between unsaturated hydrocarbons and oxidized organic radicals we use accretion between isoprene and primary secondary and tertiary alkyl peroxy radicals as model reactions we show that a systematic semiempirical transition state search can lead to better transition state structures than relaxed scanning with density functional theory with a significant gain in computational efficiency additionally we suggest accurate and effective quantum chemical methods to study accretion reactions between large unsaturated hydrocarbons and oxidized organic radicals furthermore we examine the atmospheric relevance of these types of reactions by calculating the bimolecular reaction rate coefficients and formation rates under atmospheric conditions from the quantum chemical reaction energy barriers
"objective this study aims to enhance early detection and prediction by exploiting drug molecular substructures overcoming challenges posed by limited authentic patient data in the medical domain
methods the study implemented a neural network approach to optimize molecular fingerprint algorithms and employed various machine learning algorithms for predictions additionally the study identified and extracted substructures associated with severe adverse drug reactions adrs validating their presence within drug structures through a comparison with a random set of drug structures predictions were made for specific molecular structures and results were validated using clinical evidence from the literature
results optimized molecular fingerprint algorithms and diverse machinelearning models yielded promising outcomes the area under curve auc value for the fingerprint dataset was obtained at approximately 65 and integrating it with patient data significantly improved the performance by about 30 substructure analysis pinpointed key components linked to severe adrs reinforcing the predictive prowess of the model predictions for specific molecular structures were corroborated using clinical evidence from the literature fortifying the credibility of the proposed approach
conclusion in conclusion this research effectively tackles challenges in the early detection and prediction of adrs by leveraging machine learning algorithms focusing on drug molecular substructures the optimized model incorporating both fingerprint and patient datasets demonstrated significant improvements in predictive performance identifying and validating substructures linked to severe adrs contribute to the models reliability the studys findings are vital for advancing drug safety and laying the groundwork for further strides in predictive modeling within the medical domain"
this paper investigates the geography of facebook use at an urbanregional scale focussing on placenamed groups meaning various interest groups with names relating to places such as towns neighbourhoods or points of interest conceptualising facebook as a digital infrastructure – that is the platform’s urban footprint in the form of its placenamed groups rather than what individuals share and create using the service – we explore the location theme and scale of 3016 groups relating to places in greater london firstly we address the quantitative and qualitative methodological challenges that we faced to identify the groups and ground them geographically secondly we analyse the scale of the toponyms in the group names which are predominantly linked to london’s suburbs thirdly we study the spatial distribution of groups both overall and by specific types in relation to the sociodemographic characteristics of residents at the borough level through correlation and robust regression analyses the presence and activity of groups are linked to a relatively older nondeprived and nonimmigrant population living in less dense areas with high variability across different group types these results portray placenamed facebook groups as communication infrastructure skewed towards more banal interactions and places in greater london’s outlying boroughs this research is among the first to explore and visualise the urban geographies of facebook groups at a metropolitan scale showing the extent nature and locational tendencies of largescale social media use as increasingly ordinary aspects of how people come to know experience live and work in cities
coalition logic is a central logic in logical studies of strategic reasoning whose models are concurrent game models in this paper first we systematically discuss three assumptions of concurrent game models and argue that they are too strong the first is seriality that is every coalition always has an available joint action the second is the independence of agents that is the merge of two available joint actions of two disjoint coalitions is always an available joint action of the union of the two coalitions the third is determinism that is all available joint actions of the grand coalition always have a unique outcome second we present a coalition logic based on general concurrent game models which do not have the three assumptions and show its completeness this logic seems minimal for reasoning about coalitional powers
zebrafish an essential vertebrate model has greatly expanded our understanding of hearing however one area that remains unexplored is the biomechanics of the weberian apparatus crucial for sound conduction and perception using microcomputed tomography μct bioimaging we created threedimensional finite element models of the zebrafish weberian ossicles these models ranged from the exact size to scaled isometric versions with constrained geometry 1 to 10 mm in ossicular chain length harmonic finite element analysis of all 11 models revealed that the resonance frequency of the zebrafishs weberian ossicular chain is approximately 900 hz matching their optimal hearing range interestingly resonance frequency negatively correlated with size while the ratio of peak displacement and difference of resonance frequency between tripus and scaphium remained constant this suggests the transmission efficiency of the ossicular chain and the homogeneity of resonance frequency at both ends of the chain are not sizedependent we conclude that the weberian apparatuss resonance frequency can explain zebrafishs best hearing frequency and their biomechanical characteristics are not influenced by isometric ontogeny as the first biomechanical modelling of atympanic ear and among the few nonhuman ear modelling this study provides a methodological framework for further investigations into hearing mechanisms and the hearing evolution of vertebrates
cellpenetrating peptides cpps enable the transport of nanoparticles through cell membranes using molecular simulations we conduct an indepth investigation into the thermodynamic forces governing the passive translocation of cppcoated nanoparticles across lipid bilayers contrasting their behavior with that of bare particles to dissect the contribution of the peptides our analysis unveils a distinctive twostage translocation mechanism where the adsorption energy of the particles overcomes the cost of forming a hydrophilic transmembrane pore proper evaluation of the translocation mechanisms is only possible when using two reaction coordinates in particular one that explicitly includes the density of the lipids on the binding site of the particle an analysis of adsorption and activation free energies in terms of a simple kinetic model provides a clearer understanding of the cpp effect experimental validation using nonendocytic cells confirms the superior membrane permeation of cppcoated particles our findings have implications for the rational design of more efficient cellpermeating particles
abstract a selfconsistent field method based on a density matrix functional theory scheme is presented to compute the potential energy curve of the hydrogen molecule a functional to recover the socalled cumulant correlation energy is introduced the form of such functional is very simple being dependent explicitly only on the square of the overlap matrix element between the unrestricted nonorthogonal occupied orbitals of the calculation the functional is a sum of two contributions referring to nondynamic and dynamic correlation the agreement with the fullci potential energy curve is within chemical accuracy
when dealing with engineering design problems designers often encounter nonlinear and nonconvex features multiple objectives coupled decision making and various levels of fidelity of subsystems to realize the design with limited computational resources problems with the features above need to be linearized and then solved using solution algorithms for linear programming the adaptive linear programming alp algorithm is an extension of the sequential linear programming algorithm where a nonlinear compromise decision support problem cdsp is iteratively linearized and the resulting linear programming problem is solved with satisficing solutions returned the reduced move coefficient rmc is used to define how far away from the boundary the next linearization is to be performed and currently it is determined based on a heuristic the choice of rmc significantly affects the efficacy of the linearization process and hence the rapidity of finding the solution in this paper we propose a rulebased parameterlearning procedure to vary the rmc at each iteration thereby significantly increasing the speed of determining the ultimate solution to demonstrate the efficacy of the alp algorithm with parameter learning alppl we use an industryinspired problem namely the integrated design of a hotrolling process chain for the production of a steel rod using the proposed alppl we can incorporate domain expertise to identify the most relevant criteria to evaluate the performance of the linearization algorithm quantify the criteria as evaluation indices and tune the rmc to return the solutions that fall into the most desired range of each evaluation index compared with the old alp algorithm using the golden section search to update the rmc the alppl improves the algorithm by identifying the rmc values with better linearization performance without adding computational complexity the insensitive region of the rmc is better explored using the alppl—the alp only explores the insensitive region twice whereas the alppl explores four times throughout the iterations with alppl we have a more comprehensive definition of linearization performance—given multiple design scenarios using evaluation indices eis including the statistics of deviations the numbers of binding active constraints and bounds the numbers of accumulated linear constraints and the number of iterations the desired range of evaluation indices dei is also learned during the iterations the rmc value that brings the most eis into the dei is returned as the best rmc which ensures a balance between the accuracy of the linearization and the robustness of the solutions for our test problem the hotrolling process chain the alp returns the best rmc in twelve iterations considering only the deviation as the linearization performance index whereas the alppl returns the best rmc in fourteen iterations considering multiple eis the complexity of both the alp and the alppl is on2 the parameterlearning steps can be customized to improve the parameter determination of other algorithms
introduction to date there has been little research on the general health literacy of trans and gender diverse individuals even though previous research undermines the importance of good health literacy in this sample the aim of the article is therefore to describe the general health literacy of trans and gender diverse individuals based on a german survey methods in september 2022 a survey study was conducted in which health literacy was recorded using hlseu16 data will be presented descriptively gender differences will be explored using a χ2 test and a univariate analysis of variance anova results out of n  223 participants n  129 individuals 578 identified as nonbinary n  49 220 identified themselves as male while n  45 202 identified as female mean age was 2803 years overall 264 of all the participants showed an inadequate health literacy as proposed by the hlseu16 in trend healthrelated task related to media use were more often perceived as easy compared to the german general population conclusion individuals who identify as trans and gender diverse may have a general health literacy below average compared to the german general population however tasks related to media use were perceived as easy which might be a good starting point for health literacy related interventions trial registration drks00026249 date of registration 15032022
in an electrical power distribution system harmonic distortion is the most prominent power quality problem that causes longterm adverse effects such as failure of distribution transformers considering that most transformer problems are caused by heat losses due to the presence of harmonics it was decided to use a numerical method with the highest accuracy finite element method fem to analyze the hot spot temperature hst of the thermal distribution transformer model through the use of comsol multiphysics software three phases of unbalanced harmonic loads are considered which contribute to three different total harmonic distortion current thdi levels and five different insulation temperature classes using the ieee c571102018 guidance the simulation outputs are then verified with hst results from the hst mathematical model the findings indicated that with the increased loadings the unbalanced harmonic currents have impacted the hst increment and distinguished the hst values between the phases
lightning is a rapidly evolving phenomenon exhibiting both mesoscale and microscale characteristics its prediction significantly relies on timely and accurate data observation with the implementation of new generation weather radar systems and lightning detection networks radar reflectivity image products and lightning observation data are becoming increasingly abundant research focus has shifted towards lightning nowcasting prediction of imminent events utilizing deep learning dl methods to extract lightning features from very large data sets in this paper we propose a novel spatiotemporal fusion deep learning lightning nowcasting network stflightnet for lightning nowcasting the network is based on a 3dimensional unet architecture with encoderdecoder blocks and adopts a structure of multiple branches as well as the main path for the encoder block to address the challenges of feature extraction and fusion of multisource data multiple branches are used to extract different data features independently and the main path fuses these features additionally a spatial attention sa module is added to each branch and the main path to automatically identify lightning areas and enhance their features the main path fusion is conducted in two steps the first step fuses features from the branches and the second fuses features from the previous and current levels of the main path using two different methodsthe weighted summation fusion method and the attention gate fusion method to overcome the sparsity of lightning observations we employ an inverse frequency weighted crossentropy loss function finally stflightnet is trained using observations from the previous half hour to predict lightning in the next hour the outcomes illustrate that the fusion of both the multibranch and main path structures enhances the network’s ability to effectively integrate features from diverse data sources attention mechanisms and fusion modules allow the network to capture more detailed features in the images
compared with our fossil ancestors and neandertal kin modern humans have evolved a distinctive skull shape with a rounder braincase and more delicate face competing explanations for this rounder skull have either linked it to changes in brain organisation or seen it as a byproduct of gracilization evolution of thinner and lighter skeletal anatomy here we combined palaeoanthropological data from hominin fossils and imaging genomics data from living humans to gain insight into evolutionary and developmental mechanisms shaping this uniquely modern human phenotype we analysed endocranial globularity from magnetic resonance imaging mri brain scans and genetic data of more than 33000 adults we discovered 28 genomic loci significantly associated with endocranial globularity there was genetic overlap with the brain’s ventricular system white matter microstructure and sulcal morphology and with multivariate genetic analyses of readinglanguage skills but not with general cognition the associated genes exhibited enriched expression in the brain during prenatal development and early childhood the connection to the ventricular system hints at a role for cerebrospinal fluid pressure in shaping the endocranium during development genes linked to endocranial globularity also showed enhanced expression in the cardiovascular and female reproductive systems this finding suggests coevolutionary pathways whereby changes impacting factors such as energy needs pregnancy or fertility concurrently shape the brain and its structure
identifying correspondences in noisy data is a critically important step in estimation processes when an informative initial estimation guess is available the data association challenge is less acute however the existence of a highquality initial guess is rare in most contexts we explore graphtheoretic formulations for data association which do not require an initial estimation guess existing graphtheoretic approaches optimize over unweighted graphs discarding important consistency information encoded in weighted edges and frequently attempt to solve nphard problems exactly in contrast we formulate a new optimization problem that fully leverages weighted graphs and seeks the densest edgeweighted clique we introduce two relaxations to this problem a convex semidefinite relaxation which we find to be empirically tight and a fast firstorder algorithm called clipper which frequently arrives at nearlyoptimal solutions in milliseconds when evaluated on point cloud registration problems our algorithms remain robust up to at least 95 outliers while existing algorithms begin breaking down at 80 outliers
ongoing efforts to turn machine learning ml into a design material have encountered limited success this paper examines the burgeoning area of ai art to understand how artists incorporate ml in their creative work drawing upon related hci theories we investigate how artists create ambiguity by analyzing nine ai artworks that use computer vision and image synthesis our analysis shows that in addition to the established types of ambiguity artists worked closely with the ml process dataset curation model training and application and developed various techniques to evoke the ambiguity of processes our finding indicates that the current conceptualization of ml as a design material needs to reframe the ml process as design elements instead of technical details finally this paper offers reflections on commonly held assumptions in hci about ml uncertainty dependability and explainability and advocates to supplement the artifactcentered design perspective of ml with a processcentered one
—authentication is one of the established practices to ensure user security personally identifiable information pii such as national identity card number id number and bank card number is used widely in china’s mobile apps as an additional secret to authenticate users ie piiasfactor authentication  pafa  in this paper we found a new threat that calls on the cautiousness of pafa  the simultaneous usages and businessrelated interactions of apps make the authentication strength of a target app weaker than designed an adversary who knows fewer authentication factors only sms otp than a pafa system required can break the authentication by gathering information or abusing crossapp authorization from other apps to systematically study the potential risks we proposed a semiautomatic system maggie  to evaluate the security of pafa in target apps by measuring 234 realworld apps in chinese app markets with the help of maggie  we found 754 of apps that deployed pafa can be bypassed including the popular and sensitive ones eg alipay wechat unionpay leading to severe consequences like hijack user accounts and making unauthorized purchases additionally we conducted a survey to demonstrate the practical implications of the new risk on users finally we reported our findings to the vendors and provided several mitigation measures
microwave ablation mwa represents one of the most powerful tools in cancer treatment this therapeutic modality process is governed by the temperature and absorbed dose of radiation of the cell tissue this study was performed to control the temperature effect using simulation during the mwa thermal damage of lung tumor for this reason a twodimensional 2d computational modeling generated for adaptive lung tissue was designed and analyzed using the finite element method fem different approaches such as firstorder arrhenius rate equations maxwell equations and the bioheat equation have been used to simulate necrosis in cells to control the heat a proportional–integral–derivative pid controller was used to moderate the input microwave power source and to maintain the temperature of the target tip at a lower level of the initial temperature data furthermore full cancer tissue necrosis was also evaluated by processing time and thermal damage fraction the obtained data proved that the target tip temperature was affected by the temperature distribution and specific absorption rate sar however a specific treatment period of tumor ablation is required to control and decrease the damage of surrounding healthy tissue to ensure a safe operation without any risk
autonomous mobile robots must maintain safety but should not sacrifice performance leading to the classical reachavoid problem find a trajectory that is guaranteed to reach a goal and avoid obstacles this paper addresses the near danger case also known as a narrow gap where the agent starts near the goal but must navigate through tight obstacles that block its path the proposed method builds off the common approach of using a simplified planning model to generate plans which are then tracked using a highfidelity tracking model and controller existing approaches use reachability analysis to overapproximate the error between these models and ensure safety but doing so introduces numerical approximation error conservativeness that prevents goalreaching the present work instead proposes a piecewise affine reachavoid computation parc method to tightly approximate the reachable set of the planning model parc significantly reduces conservativeness through a careful choice of the planning model and set representation along with an effective approach to handling timevarying tracking errors the utility of this method is demonstrated through extensive numerical experiments in which parc outperforms stateoftheart reach avoid methods in neardanger goal reaching furthermore in a simulated demonstration parc enables the generation of provablysafe extreme vehicle dynamics drift parking maneuvers a preliminary hardware demo on a turtlebot3 also validates the method
humans have good natural intuition to recognize when another person has something to say it would be interesting if an ai can also recognize intentions to speak especially in scenarios when an ai is guiding a group discussion this can be a useful skill this work studies the inference of successful and unsuccessful intentions to speak from accelerometer data this is chosen because it is privacypreserving and feasible for inthewild settings since it can be placed in a smart badge data from a reallife social networking event is used to train a machinelearning model that aims to infer intentions to speak a subset of unsuccessful intentiontospeak cases in the data is annotated the model is trained on the successful intentions to speak and evaluated on both the successful and unsuccessful cases in conclusion there is useful information in accelerometer data but not enough to reliably capture intentions to speak for example posture shifts are correlated with intentions to speak but people also often shift posture without having an intention to speak or have an intention to speak without shifting their posture more modalities are likely needed to reliably infer intentions to speak
the transformations of supersymmetric quantum mechanics are discussed within a formalism that employs a sixparameter function from which the superpotential and the supersymmetric partner potentials v−x and vx are constructed in a general form by specific choice of the parameters v−x and vx are matched with the general form of pi class potentials and their rationally extended versions the choice of the parameters also determines which of the four possible susy transformations ti i1…4 is in effect after this general discussion the formulae are specified to the three members of this potential class the scarf i scarf ii and generalized pöschl–teller potentials due to the different domains of definition and their consequences on the boundary conditions the results turn out to be rather diverse for the three potentials while the mathematical formalism and the network of the potentials interconnected by the susyqm transformations still remains common to a large extent the general framework allows a unified and consistent interpretation of earlier isolated findings it also helps to connect the results to further potential classes and to place them into a more general context within the zoo of exactly solvable potentials
the exponential recursive trees model several kinds of networks at each step of growing of these trees each node independently attracts a new node with probability p or fails to do with probability 1 − p here we investigate the number of protected nodes total path length of protected nodes and a mean study of the protected node profile of such trees
in this manuscript we will apply the regularized meshless method coupled with an error estimation technique to tackle the challenge of modeling oblique incident waves interacting with multiple cylinders given the impracticality of obtaining an exact solution in many real engineering problems we introduce an error estimation technique designed to achieve reliable solutions this technique excels in providing dependable solutions that closely approximate analytical solutions an additional advantage is its capacity to identify the optimal number of points for both source and collocating points thereby enhancing computational efficiency the validity of the proposed method will be demonstrated through three numerical cases presenting results that exhibit substantial agreement
power transformers are important equipment in power systems most medium and large transformers are oilimmersed transformers apart from some small and mediumcapacity transformers and gas transformers with special purpose because the reliability and safety of oilimmersed transformers can cause significant implications for power systems it is extremely critical to detect malfunction timely and accurately therefore the fault diagnosis model based on the evidential reasoning er rule with reference points in the gaussian distribution form optimized by a constrained genetic optimization algorithm ga named gerg model is proposed in this paper the gerg fault diagnosis model can monitor and identify the degree of transformer faults in real time first the concentration of dissolved gases in the oil of oilimmersed transformers varies at different fault degrees so the gases with high weight are selected using the maximum correlationminimum redundancy mrmr to constitute the indicator system second to solve the uncertainty existing in the fault information of transformers the reference points of the er rule are continuous probability distribution reference points described by gaussian distribution form third the constrained ga is proposed to adapt the gaussian distribution and the accuracy of fault diagnosis results can be enhanced by optimizing the grade parameters using constrained ga finally the internal and external influences in the environment are quantified using perturbation analysis the gerg fault diagnosis model is applied to the dataset of dissolved gases in the oil and the robustness and validity of the gerg fault diagnosis model are validated meanwhile the validation results also show that the gerg fault diagnosis model possesses higher accuracy compared with other fault diagnosis methods
in the contemporary sports science domain artificial intelligence ai has emerged as a pivotal tool driving breakthroughs in athlete training and rehabilitation personalized training schedules sculpted through ai algorithms promise optimized performance and reduced injury likelihood wearable technologies combined with aiboosted functionalities offer invaluable realtime physiological data facilitating immediate training modifications orand adjustments the utilization of ai in biomechanical feedback has been groundbreaking particularly in technique enhancement through meticulous gait analysis the cognitive training tools are honing athletes’ decisionmaking and reflexes attributes crucial in competitive scenarios the psychological wellbeing of athletes is also catered to with ai discerning early indicators of emotional strain furthermore postcompetition reviews have been enriched with aienabled video analytics delivering nuanced insights for strategic refinements the goal of this study was to provide an aiassisted solution to identify problems in recovery exercises or to boost the athletes’ performance
carbon monoxide can cause severe harm to humans even at low concentrations metal oxide semiconductor mos carbon monoxide gas sensors have excellent sensing performance regarding sensitivity selectivity response speed and stability making them very desirable candidates for carbon monoxide monitoring however mos gas sensors generally work at temperatures higher than room temperature and need a heating source that causes high power consumption high power consumption is a great problem for longterm portable monitoring devices for pointofcare or wireless sensor nodes for iot application roomtemperature mos carbon monoxide gas sensors can function well without a heater making them rather suitable for iot or portable applications this review first introduces the primary working mechanism of mos carbon monoxide sensors and then gives a detailed introduction to and analysis of roomtemperature mos carbon monoxide sensing materials such as zno sno2 and tio2 lastly several mechanisms for roomtemperature carbon monoxide sensors based on moss are discussed the review will be interesting to engineers and researchers working on mos gas sensors
biomolecular phase separation is a vital mechanism for orchestrating biomolecules within living cells this crucial role has spurred an intense pursuit to comprehend the molecular underpinnings governing and regulating these processes computational methodologies offer a unique perspective augmenting experimental techniques by providing detailed information that cannot be obtained otherwise in this review we briefly overview the theoretical and computational approaches to investigate biomolecular phase separation as a short primer we explain the factors driving and affecting phase separation of biomolecules and then we delve into analytical and simulation methods used to study phase separation we explain how analytical methods like the flory–huggins theory random phase approximation and graph‐based methods have been used to study phase behaviors of various proteins we also discuss principles and applications of all‐atom simulations coarse‐grained simulations and field‐theoretical approaches additionally we explore the recent advances in machine learning approach to predict phase separation of biomolecules
the amesquantitative structure–activity relationship qsar international challenge projects held during 2014–2017 and 2020–2022 evaluated the performance of various predictive models despite the significant insights gained the rules allowing participants to select prediction targets introduced ambiguity in model performance evaluation this reanalysis identified the highestperforming prediction model assuming a 100 coverage rate cov for all prediction target compounds and an estimated performance variation due to changes in cov all models from both projects were evaluated using balance accuracy ba the matthews correlation coefficient mcc the f1 score f1 and the first principal component pc1 after normalizing the cov a correlation analysis with these indicators was conducted and the evaluation index for all prediction models in terms of the cov was estimated in total using 109 models the model with the highest estimated ba 769 at 100 cov was mmivote1 as reported by meiji pharmaceutical university mpu the best models for mcc f1 and pc1 were all mmistk1 also reported by mpu all the models reported by mpu ranked in the top four mmistk1 was estimated to have f1 scores of 592 615 and 631 at cov levels of 90 60 and 30 respectively these findings highlight the current state and potential of the ames prediction technology
texttoimage diffusion models have shown great success in generating highquality textguided images yet these models may still fail to semantically align generated images with the provided text prompts leading to problems like incorrect attribute binding andor catastrophic object neglect given the pervasive objectoriented structure underlying text prompts we introduce a novel objectconditioned energybased attention map alignment ebama method to address the aforementioned problems we show that an objectcentric attribute binding loss naturally emerges by approximately maximizing the loglikelihood of a zparameterized energybased model with the help of the negative sampling technique we further propose an objectcentric intensity regularizer to prevent excessive shifts of objects attention towards their attributes extensive qualitative and quantitative experiments including human evaluation on several challenging benchmarks demonstrate the superior performance of our method over previous strong counterparts with better aligned attention maps our approach shows great promise in further enhancing the textcontrolled image editing ability of diffusion models
abstract this study reports the first timeresolved particle image velocimetry characterization of a planar twophase mixing layer flow whose velocity field is measured simultaneously in gas and liquid streams two parallel air and water flows meet downstream of a splitter plate giving rise to an initially spanwise invariant configuration the aim is to elucidate further the mechanisms leading to the flow breakup in gasassisted atomization the complete experimental characterization of the velocity field represents a database that could be used in datadriven reducedorder models to investigate the global behaviour of the flow system after the analysis of a selected reference case a parametric study of the flow behaviour is performed by varying the liquid rel and gas reg reynolds numbers and as a consequence also the gastoliquid dynamic pressure ratio m shedding light on both timeaveraged mean and unsteady velocity fields in the reference case it is shown that the mean flow exhibits a wake region just downstream of the splitter plate followed by the development of a mixing layer by increasing both rel and reg the streamwise extent of the wake decreases and eventually vanishes the flow resulting in a pure mixing layer regime the spectral analysis of the normaltoflow velocity fluctuations outlines different flow regimes by variation of the governing parameters giving more insights into the global characteristics of the flow field as a major result it is found that at high reg and m values the velocity fluctuations are characterized by lowfrequency temporal oscillations synchronized in several locations within the flow field which suggest the presence of a global mode of instability the proper orthogonal decomposition of velocity fluctuations performed in both gas and liquid phases reveals finally that the synchronized oscillations are associated with a lowfrequency dominant flapping mode of the gas–liquid interface higherorder modes correspond to interfacial wave structures travelling with the socalled dimotakis velocity for lower gas reynolds numbers the leading modes describe higher frequency fingers shedding at the interface
herein we report a highly efficient agicatalyzed indolyzation with friedelcrafts alkylation through a cascade cyclization strategy for accessing valuable hybrid heterocycles for the first time this general strategy consists of forming four cccnco bonds toward dual annulation reactions of 2alkynylanilines with methyl benzoate2carboxaldehydes and aromatic amines as well as with salicylaldehydes and malononitrile variably substituted new indolo4hphthalimidines and indolo4hchromenes were synthesized with excellent yields 8593 under mild reaction conditions
the personalization of machine learning ml models to address data drift is a significant challenge in the context of internet of things iot applications presently most approaches focus on finetuning either the full base model or its last few layers to adapt to new data while often neglecting energy costs however various types of data drift exist and finetuning the full base model or the last few layers may not result in optimal performance in certain scenarios we propose target block finetuning tbft a lowenergy adaptive personalization framework designed for resourceconstrained devices we categorize data drift and personalization into three types inputlevel featurelevel and outputlevel for each type we finetune different blocks of the model to achieve optimal performance with reduced energy costs specifically input feature and outputlevel correspond to finetuning the front middle and rear blocks of the model we evaluate tbft on a resnet model three datasets three different training sizes and a raspberry pi compared with the blockavg where each block is finetuned individually and their performance improvements are averaged tbft exhibits an improvement in model accuracy by an average of 1530 whilst saving 4157 energy consumption on average compared with full finetuning
abstract in the internet context traditional media have gone online and corporate new media urgently need to find a new communication direction establishing an innovative media communication path for enterprises in the mobile internet environment through innovative development the decisionmaking optimization of corporate new media communication is based on the core objectives of minimizing the promotion cost of implanted online ads and maximizing the communication efficiency of implanted online ads the multiobjective particle swarm optimization algorithm is used to construct the optimal decisionmaking model of media selection for corporate implantable online advertising obtain the optimal solution of the model according to the objective function and analyze the example of a corporate new media communication path the results show that guidance support x4 basic positioning x5 future layout x6 and driving force x1 have a significant positive influence on media functionality y2 and their explanatory power for implantable advertising functionality is 131 42 37 and 29 respectively and implantable advertising functionality will become the key path of enterprise business expansion and operation mode and functional differentiation also will become one of their competitiveness the goal of this study is to combine new media marketing and traditional marketing to achieve complementary advantages and maximize the communication effectiveness of enterprises
cryptoaltruism refers to the ways in which distributed ledger technologies especially blockchains are changing the nature of the nonprofit sector this study specifically investigates how the blockchain technology has been used by ukrainian nonprofits during the current russiaukraine war to link this to the more general literature on blockchains we consider whether blockchains are used primarily as a generalpurpose technology or as an institutional technology which redefines how nonprofits coordinate activities our analysis of ukrainian nonprofits provides evidence supporting both perspectives widespread acceptance of cryptocurrency suggests blockchains are an efficiencyenhancing new technology we also show that novel applications on nonfungible tokens to preserve art and culture and to raise funds as well as uses of blockchains to address challenges with trust lend support to the idea of blockchains as an innovative institutional technology that is transforming the nature of the nonprofit sector this study intends to motivate further development of the emergent agenda on cryptoaltruism and its role in the nonprofit sector
students who began their undergraduate university studies in the midst of the covid19 pandemic the ‘covid cohort’ may have been particularly at risk for experiencing increased loneliness this study employed an exploratory egocentric network and mixedmethods approach to investigate the links between social networks and loneliness in the covid cohort of sixtyone respondents meeting inclusion criteria for the study fiftyeight firstyear undergraduate students from the september 2020 intake at a large scottish university provided egocentric network data via an online survey as well as responses to three openended questions which were aimed at generating qualitative data about participants’ experiences of starting university in the context of the covid19 pandemic bivariate analyses suggest that having a larger social network and higher satisfaction with that network was associated with reduced loneliness we additionally explored these associations in subsamples of students living oncampus and living offcampus our qualitative data adds valuable insight into the impact that pandemicrelated socialdistancing restrictions had on limiting students’ opportunities for meeting their peers and forging meaningful social connections at university limitations of this study include a small sample size and an exploratory approach requiring further investigation and replication however in the context of universities continuing to use hybrid teaching models this study provides useful initial insights highlighting potential avenues for institutions to support students in developing social connections in the transition to higher education
hepatocellular carcinoma hcc is the most prevalent type of liver cancer since the tricarboxylic acid cycle is widely involved in tumor metabolic reprogramming and cuproptosis investigating related genes may help to identify prognostic signature of patients with hcc data on patients with hcc were sourced from public datasets and were divided into train test and singlecell cohorts a variety of machine learning algorithms were used to identify different molecular subtypes and determine the prognostic risk model our findings revealed that the risk score trscore based on the genes ogdhl cfhr4 and spp1 showed excellent predictive performance in different datasets pathways related to cell cycle and immune inflammation were enriched in the highrisk group whereas metabolismrelated pathways were significantly enriched in the lowrisk group the highrisk group was associated with a greater number of mutations of detrimental biological behavior and higher levels of immune infiltration immune checkpoint expression and anticancer immunotherapy response lowrisk patients demonstrated greater sensitivity to erlotinib and phenformin spp1 was mainly involved in the interaction among tumorassociated macrophages t cells and malignant cells via spp1–cd44 and spp1–itga5  itgb1 ligandreceptor pairs in summary our study established a prognostic model which may contribute to individualized treatment and clinical management of patients with hcc
simple summary this research focuses on the perception of residents and tourists from the tourist resorts in prahova valley romania on human–bear interactions as a foundation for an integrative analysis of the presence of bears in this space we used questionnaires interviews mass media and the local toponymy including bearrelated names and souvenirs that embody the bear our results have highlighted the beginning of the coexistence between humans and bears and the seeds of a posthumanist vision abstract our research focuses on a complex and integrative analysis of bear presence in four tourist resorts in prahova valley romania sinaia bușteni azuga and predeal employing innovative mixed methods including questionnaires interviews newspaper analysis and consideration of the local toponymy including bearrelated names and souvenirs we aim to highlight the extent to which a posthumanist attitude is evident in the region the sustained appearance of bears is attributed to habitat invasion through deforestation road construction residential neighborhoods and tourist infrastructure ambiguity arises from the presence of food sources and voluntary feeding both by locals and tourists the mass media initially heightened fear and panic during the onset of human–bear interactions but later adopted a more tolerant tone regarding the bear’s presence in tourist resorts reflecting an openness to the posthumanist approach in prahova valley that is why locals express fear and concern about bear encounters advocating for a clear separation between animal and human spaces tourists exhibit attitudes ranging from unconscious appreciation to ambivalence often contributing to the problem through practices such as feeding bears for fun the use of bearrelated names for tourist establishments is identified as anthropocentric despite their appeal for attracting tourists souvenir sales through increasing socioeconomic value and contributing to tourist experiences are also recognized as anthropocentric however souvenirs can provide elements of support for bear conservation efforts and the equal consideration of human and nonhuman entities this study concludes that a successful adaptive coexistence requires a posthumanist vision overcoming anthropocentrism in a landscape altered by human activities supported by bear management programs in bucegi natural park and conservation efforts in prahova valley in a landscape altered by people
micromobility choices are emerging as prominent options in transportation but limited research has been conducted on their use in nonurban settings within a unified shared mobility system this study seeks to address this gap using realworld data from the more sharing system in a nonurban setting between march and august 2023 key findings show that escooters and ebikes were the most popular often used in conjunction with public transport highlighting their role in addressing the lastmile commute challenge while escooters became popular in spring emopeds saw increased use in summer usage trends varied through the week with some vehicles particularly emopeds and ecargo bikes seeing heightened demand on weekends ecargo bikes and emopeds primarily replaced cars user satisfaction was generally high across the board these findings can inform mobility planning transportation policy and stakeholder decisions emphasizing shared transports growing role in urban and suburban contexts
inertial pedestrian navigation systems ipns face challenges in practical applications due to inherent noise and bias instability of inertial sensor to construct a highprecision prior informationindependent low computational complexity inertialbased pedestrian navigation system this article proposes the inertial magnetic field hybrid map simultaneous localization and mapping imhmslam scheme first to improve the system’s precision imhmslam scheme utilizes inertialbased maps to represent the passable areas within enclosed environments and correct most localization errors caused by inertial sensors additionally magnetic field maps are employed to describe the environmental characteristics of the passable areas thereby resolving the ambiguity associated with the association between different regions second to enable the system to operate independently of prior information referred to as pedestrian activity ranges in this study a scalable magnetic field map model is introduced this model is based on recursive gaussian process regression rgpr and map basis vector update methods third in order to reduce the computational complexity of magnetic field map updates when pedestrians move in larger areas a local update strategy is proposed this strategy ensures a fixed upper limit on computational complexity finally simulations and field trial evaluations demonstrate that the imhmslam scheme utilizing the lowcost inertial sensor and magnetometer in smartphones achieves an average positioning accuracy of 141 m in a smallarea 500 textm2  field trials and 256 m in a largearea 2600 textm2  field trials
upper limb functional impairments persisting after stroke significantly affect patients’ quality of life precise adjustment of robotic assistance levels based on patients’ motion intentions using semg signals is crucial for active rehabilitation this paper systematically reviews studies on continuous prediction of upper limb single joints and multijoint combinations motion intention using modelbased mb and modelfree mf approaches over the past decade based on 186 relevant studies screened from six major electronic databases the findings indicate ongoing challenges in terms of subject composition algorithm robustness and generalization and algorithm feasibility for practical applications moreover it suggests integrating the strengths of both mb and mf approaches to improve existing algorithms therefore future research should further explore personalized mbmf combination methods incorporating deep learning attention mechanisms muscle synergy features motor unit features and closedloop feedback to achieve precise realtime and longduration prediction of multijoint complex movements while further refining the transfer learning strategy for rapid algorithm deployment across days and subjects overall this review summarizes the current research status significant findings and challenges aiming to inspire future research on predicting upper limb motion intentions based on semg
we consider the planar dynamic convex hull problem in the literature solutions exist supporting the insertion and deletion of points in polylogarithmic time and various queries on the convex hull of the current set of points in logarithmic time if arbitrary insertion and deletion of points are allowed constant time updates and fast queries are known to be impossible this paper considers two restricted cases where worstcase constant time updates and logarithmic time queries are possible we assume all updates are performed on a deque doubleended queue of points the first case considers the monotonic path case where all points are sorted in a given direction say horizontally lefttoright and only the leftmost and rightmost points can be inserted and deleted the second case assumes that the points in the deque constitute a simple path note that the monotone case is a special case of the simple path case for both cases we present solutions supporting deque insertions and deletions in worstcase constant time and standard queries on the convex hull of the points in olog n time where n is the number of points in the current point set the convex hull of the current point set can be reported in ohlog n time where h is the number of edges of the convex hull for the 1sided monotone path case where updates are only allowed on one side the reporting time can be reduced to oh and queries on the convex hull are supported in olog h time all our time bounds are worst case in addition we prove lower bounds that match these time bounds and thus our results are optimal for a quick comparison the previous best update bounds for the simple path problem were amortized olog n time by friedman hershberger and snoeyink socg 1989
a new pixel circuit consisting of eleven ntype igzo transistors tfts and four capacitors is proposed and applied for micro light emitting diode mled displays the circuit is driven by pulse hybrid modulation phm combining pulse amplitude modulation pam and pulse width modulation pwm in purpose high grayscales are achieved by pam mode and low grayscales are achieved by pwm mode which keep the driving current of led out of the small value range so as to avoid unstable luminance performance it is conducted that the mled display driven by phm demonstrate a stable luminance at low grayscale with well suppressed luminance errors of over 30 decrease in comparison with the display driven by pam moreover both threshold voltage vth shifts of driving tfts for pam mode and pwm mode could be well compensated in this circuit as revealed by simulations the driving current variations of led could be retained within 10 when vth shifts in the ranges from −1 to 2 v for pam driving tft and from −1 to 15 v for pwm driving tft respectively in practice a good brightness uniformity over 90 after 240hours aging of the mled display could be realized
the transition from 5g to 6g will result in an explosion of internet of things iot devices that provide pervasive and constant connection across all spheres of human activities in this regard nonterrestrial networks ntns will play a crucial role in supporting and supplementing terrestrial systems in order to cope with such a vast number of iot devices and to fulfill the massive capacity needs of the most sophisticated of them this study investigates the implication of several multiple access techniques for 6g enabled ntnassisted iot technologies first general architecture of ntn ntnassisted iot technologies and key features and challenges of ntns are presented then different types of multiple access schemes are discussed and compared in the context of ntnassisted iot systems simulation results are presented and important performance parameters such as energy efficiency and spectrum efficiency are examined for the discussed multiple access schemes furthermore challenges and future research directions are discussed
infographics are visual representations of data that utilize various graphic elements including pie charts bar graphs line graphs and histograms educators and designers can maximize the potential of infographics as powerful educational tools by carefully addressing challenges and capitalizing on emerging technologies however current education systems showcase the need for development guidelines and the best practices targeted at designing and developing infographics while exploring the major economic and social impacts of infographics on education this study examines the concept and role of infographics in education methodologies trends and obstacles it evaluates potential economic implications and gives insights to design and development experts the study is based on a scoping literature review methodology uncovering the conceptual background and the role of infographics the study emphasizes the unique functions of infographics in data visualization for educational purposes and investigates the current trends and practices in infographics creation the key challenges associated with the use of infographics are also discussed furthermore the study attempts to identify the cuttingedge frameworks for infographic creation and development while evaluating their economic implications for the role of global education finally the potential recommendations for creating successful infographics while focusing on professional design and development are also covered the guided literature review will be vital for understanding and using infographics in education
this paper summarises the design of the coolchic candidate for the challenge on learned image compression this candidate attempts to demonstrate that neural coding methods can lead to low complexity and lightweight image decoders while still offering competitive performance the approach is based on the already published overfitted lightweight neural networks coolchic further adapted to the human subjective viewing targeted in this challenge
conflicts in construction projects have received considerable attention because of their significant impact on cooperation between participants and project performance existing literature typically categorizes interorganizational conflict into three types task relationship and process conflict these conflicts have been studied from a separation perspective leading to inconsistent views in the existing literature in this article we aim to fill this gap by examining interorganizational conflict profiles in construction projects providing a more accurate understanding of the influence of conflict on cooperative behavior and project performance based on the survey data from the chinese construction industry this study reveals that interorganizational conflict exists in the form of four profiles high level balanced taskprocess dominant and low level among these profiles the levels of cooperative behavior were highest in the taskprocessdominant and lowlevel conflict profiles furthermore conflict affects project performance through the mediating role of cooperative behavior with the highest project performance observed in the lowlevel conflict profiles by uncovering the existence of these interorganizational conflict profiles and their subsequent effects this study offers a more comprehensive view of the impact of conflict on organizational cooperation and project performance under conditions of bounded rationality it helps explain the contradictions in the interorganizational conflict literature provides supplementary insights into conflict profile research and provides guidance for construction project participants in conflict management
the commercial production of multifunctional biocompatible and biodegradable biopolymers such as poly‐γ‐glutamic acid via microbial fermentation requires the development of simple and cheap methods for mass production this study optimized the poly‐γ‐glutamic acid production of bacillus licheniformis atcc 9945a in several steps at first the most critical components of the culture medium including l‐glutamic acid citric acid and glycerol were selected by screening nine factors through the plackett–burman experimental design and then were optimized using the response surface method and the central composite design algorithm under optimal conditions the production of poly‐γ‐glutamic acid increased by more than 42 times from 112 to 472 gl this is one of the highest production rates of this strain in submerged batch fermentation reported so far using the optimized medium compared to the conventional base medium a novel and efficient sudden pulse feeding strategy achieved by a novel one‐factorial statistical technique of l‐glutamic acid to the optimized medium increased biopolymer production from 472 to 661 gl the highest value reported in published literature with this strain this simple reproducible and cheap fermentation process can considerably enhance the commercial applications of the poly‐γ‐glutamic acid synthesized by b licheniformis atcc 9945a
considering that the human brain is the most powerful generalizable and energyefficient computer we know of it makes the most sense to look to neuroscience for ideas regarding deep learning model improvements i propose one such idea augmenting a traditional advantageactorcritic a2c model with additional learning signals akin to those in the brain pursuing this direction of research should hopefully result in a new reinforcement learning rl control paradigm that can learn from fewer examples train with greater stability and possibly consume less energy
landraces are important resources for breeding programs however the diversity harbored at the genomic level by landraces is yet to be analyzed here we sequenced more than 200 individual plants from the longlasting and durably resistant rice landrace acuce oryza sativa subsp indica we found that acuce exhibits diversity levels as high as 38 of the entire indica subspecies we pinpoint signatures of selection in favor of polymorphism associated with resistance but not other agronomic trait genes furthermore we provide evidences that emerging properties upon mixing genomic diversity also increase acuce’s performances
given the ongoing development of the global economy the demand for hazardous materials which serve as essential components for numerous industrial products is steadily increasing consequently it becomes imperative to devise a methodology for mitigating the risks associated with the road transportation of hazardous materials the objective of this study is to establish an integrated quality function deployment and multicriteria decisionmaking qfdmcdm framework and identify the pivotal factors that propel industry 50 i50 thus fortifying supply chain resilience scr and ameliorating the hazardous material transportation risks hmtr these measures encompass various strategic areas including establish a safe and inclusive work environment customized products and services enhance production flexibility and strengthen control redundancy and realtime data collection and analysis by adopting these measures enterprises can lead to sustainable and stable business operations the findings of this study demonstrate the synergistic potential of integrating i50 and scr in effectively mitigating hmtr additionally these findings offer valuable insights and practical implications for enterprises across diverse industries
in an experimental investigation the development of sonic crystal noise barriers scnbs is undertaken to address the issue of train brake noise tbn focusing on the use of local resonances in scatterers of sonic crystals recent research has shown that the inclusion of cavity resonators in the crystal scatterers allows for the modification of their insulating properties in those works it has been demonstrated that this interaction can be used to build highly insulating structures the study proposes an scnb design that includes a resonant cavity specifically to mitigate tbn and validates this design through experimental measures the experiments confirm the enhanced sound insulation capabilities of scnbs compare them to the conventional noise barriers ones and demonstrate the applicability and effectiveness of the proposed design in realworld scenarios
in light of the challenging conditions of exploration environments coupled with escalating exploration expenses seismic data acquisition frequently entails the capturing of signals entangled amidst diverse noise interferences and instances of data loss the unprocessed state of these seismic signals significantly jeopardizes the interpretative phase evidently the integration of attention mechanisms and the utilization of generative adversarial networks gans have emerged as prominent techniques within signal processing owing to their adeptness in discerning intricate global dependencies our research introduces a pioneering approach for reconstructing and denoising seismic signals amalgamating the principles of selfattention and generative adversarial networks—hereafter referred to as sagan notably the incorporation of the selfattention mechanism into the gan framework facilitates an enhanced capacity for both the generator and discriminator to emulate meaningful spatial interactions subsequently leveraging the feature map generated by the selfattention mechanism within the gan structure enables the interpolation and denoising of seismic signals rigorous experimentation substantiates the efficacy of sagan in simultaneous signal interpolation and denoising initially we benchmarked sagan against prominent methods such as unet cnn and wavelet for the concurrent interpolation and denoising of twodimensional seismic signals manifesting varying levels of damage subsequently this methodology was extended to encompass threedimensional seismic data notably performance metrics reveal sagan’s superiority over comparative methods specifically the quantitative tables exhibit sagan’s pronounced advantage with a 346 increase in psnr value over unet and an impressive 1190 surge compared to wavelet moreover the rmse values affirm sagan’s robust performance showcasing an 1154 reduction in comparison to unet and an impressive 2927 decrement relative to wavelet hence unequivocally establishing the sagan method as a preeminent choice for seismic signal recovery
microservice architecture has become a dominant architectural style in the serviceoriented software industry poor practices in the design and development of microservices are called microservice bad smells in microservice bad smells research the detection of these bad smells relies on feature data from microservices however there is a lack of an appropriate opensource microservice feature dataset the availability of such datasets may contribute to the detection of microservice bad smells unexpectedly to address this research gap this paper collects a number of opensource microservice systems utilizing spring cloud additionally feature metrics are established based on the architecture and interactions of spring boot style microservices and an extraction program is developed the program is then applied to the collected opensource microservice systems extracting the necessary information and undergoing manual verification to create an opensource feature dataset specific to microservice systems using spring cloud the dataset is made available through a csv file we believe that both the extraction program and the dataset have the potential to contribute to the study of microservice bad smells
"  
 
 
 
 
a globalização possibilitou o emprego de sistemas de qualidade dentro das organizações passou por um processo de constantes mudanças que foram motivadas por fatores econômicos e sociais de abrangência mundial e em função disso tornouse um dos temas mais trabalhados na empresa com base nos padrões internacionais o estudo se refere a uma revisão sistemática da literatura baseada na pergunta norteadora quais as soluções inovadoras que vêm sendo desenvolvidas para gerenciar o controle da qualidade dos alimentos a seleção dos documentos científicos foi feita em abril de 2022 por meio do levantamento nas bases de dados scielo scopus e web of science no qual utilizouse as palavraschaves “gestão de qualidade” “controle de qualidade de alimentos” “inovações tecnológicas” “quality management” “food quality control” e “technology innovations” referente aos últimos vinte anos como um dos resultados podemos destacar a utilização das redes neurais para caracterização de diferentes tipos de leite  como uma opção bastante viável apresentando índice  de 95 de amostras classificadas corretamente assim percebeuse a importância do desenvolvimento de métodos eficazes e de baixo custo para garantir a qualidade  e segurança dos produtos alimentícios para o consumo humano a gestão da qualidade é uma ferramenta muito importante dentro da indústria agroalimentar pois está diretamente relacionada à saúde e satisfação dos clientes o que remete também a uma maior rentabilidade e competitividade dos empreendimentos 
 
 
 
 
 "
this paper investigates the problem of cyberphysical systems in the presence of false data injection attacks when the communication network connects the controller and the actuator malicious adversaries can execute false data injection attacks to modify the control commands deteriorating the performance of the system a more general attack model is established to describe false data injection attacks including multiplicative and additive attacks to effectively mitigate such attacks an outputbased secure controller is designed sufficient criteria are derived to guarantee that the cyberphysical system under such attacks is stochastically stable furthermore a condition is proposed to design the secure control gain using singular value decomposition providing a more specific form of the controller finally simulation results are provided to illustrate the effectiveness of the proposed secure control scheme
generally multiple choice questions are an effective and extensive form used in standard tests in order to evaluate the learner’s skills and knowledge nonetheless the multiplechoice question composition particularly the distractor construction is quite difficult the distracters are needed to be both plausible and inappropriate and adequate to mystify the learners who did not master the information thus the distractor generation emergence is important that can help several standard tests in an extensive range of domain in this research questionanswer generation system is developed with a distractor model by developing an optimized t5 model at first bert tokenization is used to preprocess the passagecontext and question which are given as the input to train the approach then the question and answer generation is performed by utilizing the t5 approach that is trained by proposed serial exponentialslime mould approach sexpsma exponential weighted moving average is extended to serial exponential weighted moving average and incorporated in slime mould algorithm sma to propose sexpsma in addition the proposed sexpsmabased t5 model is employed to generate distractors for the questions eventually experimentation analysis exhibits that proposed sexpsmabased t5 model achieves better outcomes regarding the metrics like rouge bleu and meteor with the values of 0919 0918 0488 respectively
in collaborative robotics to improve human–robot interaction hri it is necessary to avoid accidental impacts in this direction several works reported how to modify the trajectories of collaborative robots cobots monitoring the operator’s position in the cobot workspace by industrial safety devices cameras or wearable tracking devices the detection of the emotional state of the operator could further prevent possible dangerous situations this work aimed to increase the predictability of anomalous behavior on the part of human operators by the implementation of emotional intelligence ei that allows a cobot to detect the operator’s level of attention loa implicitly associated with the emotional state and to decide the safest trajectory to complete a task consequently the operator is induced to pay due attention the safety rate of the hri is improved and the cobot downtime is reduced the approach was based on a vision transformer vit architecture trained and validated by the level of attention dataset load the ad hoc dataset created and developed on facial expressions and hand gestures vit was integrated into a digital twin of the omron tm5700 cobot suitably developed within this project and the effectiveness of the ei was tested on a pickandplace task then the proposed approach was experimentally validated with the physical cobot the results of the simulation and experimentation showed that the goal of the work was achieved and the decisionmaking process can be successfully integrated into existing robot control strategies
with the acceleration of economic globalization a large amount of research studies have been conducted for the exploration of industrial economics in the visualization community common visualization tools present potential features of industrial economics they hardly meet the various and complex user requirements for insightful analysis and decisionmaking in this article we design viea a webbased visualization system that integrates a rich set of views and tailored interactions enabling users to easily perceive economic features such as geographical distributions trade relationships and pattern comparisons case studies and user studies based on realworld datasets have been conducted to demonstrate the effectiveness of our system in the exploration of industrial economics
this paper provides an overview of current approaches for solving inverse problems in imaging using variational methods and machine learning a special focus lies on point estimators and their robustness against adversarial perturbations in this context results of numerical experiments for a one‐dimensional toy problem are provided showing the robustness of different approaches and empirically verifying theoretical guarantees another focus of this review is the exploration of the subspace of data‐consistent solutions through explicit guidance to satisfy specific semantic or textural properties
chatgpt has significantly impacted software development practices providing substantial assistance to developers in a variety of tasks including coding testing and debugging despite its widespread adoption the impact of chatgpt as an assistant in collaborative coding remains largely unexplored in this paper we analyze a dataset of 210 and 370 developers shared conversations with chatgpt in github pull requests prs and issues we manually examined the content of the conversations and characterized the dynamics of the sharing behavior ie understanding the rationale behind the sharing identifying the locations where the conversations were shared and determining the roles of the developers who shared them our main observations are 1 developers seek chatgpt assistance across 16 types of software engineering inquiries in both conversations shared in prs and issues the most frequently encountered inquiry categories include code generation conceptual questions howto guides issue resolution and code review 2 developers frequently engage with chatgpt via multiturn conversations where each prompt can fulfill various roles such as unveiling initial or new tasks iterative followup and prompt refinement multiturn conversations account for 332 of the conversations shared in prs and 369 in issues 3 in collaborative coding developers leverage shared conversations with chatgpt to facilitate their rolespecific contributions whether as authors of prs or issues code reviewers or collaborators on issues our work serves as the first step towards understanding the dynamics between developers and chatgpt in collaborative software development and opens up new directions for future research on the topic
the lion optimizer has been a promising competitor with the adamw for training large ai models with advantages on memory computation and sample efficiency in this paper we introduce distributed lion an innovative adaptation of lion for distributed training environments leveraging the sign operator in lion our distributed lion only requires communicating binary or lowerprecision vectors between workers to the center server significantly reducing the communication cost our theoretical analysis confirms distributed lions convergence properties empirical results demonstrate its robustness across a range of tasks worker counts and batch sizes on both vision and language problems notably distributed lion attains comparable performance to standard lion or adamw optimizers applied on aggregated gradients but with significantly reduced communication bandwidth this feature is particularly advantageous for training large models in addition we also demonstrate that distributed lion presents a more favorable performancebandwidth balance compared to existing efficient distributed methods such as deep gradient compression and ternary gradients
this study investigates the evaluation of multivariate time series data using a generative adversarial network gan calculating the value at risk var for the euro overnight index average eonia over different time periods and evaluating the financial risk consequences of the eonia to euro shortterm rate ester transition are the main objectives through the use of a particular gan called timegan which focuses on macrofinance temporal and latent representation the study aims to predict shortrate risk for eonia when estimating lower var and the 1day higher var for eonia the timegan model performs poorly however it performs well when estimating upper var for 10day and 20day periods the variation of timegan with plsfm which uses positive label smoothing and feature matching shows the upper and lower var for eonia over 10 and 20day periods are excellently estimated by this enhanced model simulations for the 20day eonia show less variation between timegan variations than a onefactor vasicek model even with the proper var estimations this study evaluates the proposed transition mapping from ester to eonia by the european central bank ecb calculating an ester85bps shift with the timegan with plsfm the results do not refute the validity of the ecbs proposed eoniaester mapping additionally the timegan with plsfm accurately predicts var for 10 and 20day periods for ester using the eoniaester mapping whereas the onefactor vasicek model finds it difficult to estimate higher var for ester over the same time frames
with the continuous prosperity of maritime transportation on a global scale and the resulting escalation in port trade volume tugboats assume a pivotal role as essential auxiliary tools influencing the ingress and egress of vessels into and out of ports as a result the optimization of port tug scheduling becomes of paramount importance as it contributes to the heightened efficiency of ship movements cost savings in port operations and the promotion of sustainable development within the realm of maritime transportation however a majority of current tugboat scheduling models tend to focus solely on the maximum operational time alternatively the formulated objective functions often deviate from realworld scenarios furthermore prevailing scheduling methods exhibit shortcomings including inadequate solution accuracy and incompatibility with integer programming consequently this paper introduces a novel multiobjective tugboat scheduling model to align more effectively with practical considerations we propose a novel optimization algorithm the improved grey wolf optimization igwo for solving the tugboat scheduling model the algorithm enhances convergence performance by optimizing convergence parameters and individual updates making it particularly suited for solving integer programming problems the experimental session designs several scale instances according to the reality of the port carries out simulation experiments comparing several groups of intelligent algorithms verifies the effectiveness of igwo and verifies it in the comprehensive port area of huanghua port to get the optimal scheduling scheme of this port area and finally gives management suggestions to reduce the cost of tugboat operation through sensitivity analysis
pornographic content occurring in humanmachine interaction dialogues can cause severe side effects for users in opendomain dialogue systems however research on detecting pornographic language within humanmachine interaction dialogues is an important subject that is rarely studied to advance in this direction we introduce censorchat a dialogue monitoring dataset aimed at detecting whether the dialogue session contains pornographic content to this end we collect reallife humanmachine interaction dialogues in the wild and break them down into single utterances and singleturn dialogues with the last utterance spoken by the chatbot we propose utilizing knowledge distillation of large language models to annotate the dataset specifically first the raw dataset is annotated by four opensource large language models with the majority vote determining the label second we use chatgpt to update the empty label from the first step third to ensure the quality of the validation and test sets we utilize gpt4 for label calibration if the current label does not match the one generated by gpt4 we employ a selfcriticism strategy to verify its correctness finally to facilitate the detection of pornographic text we develop a series of text classifiers using a pseudolabeled dataset detailed data analysis demonstrates that leveraging knowledge distillation techniques with large language models provides a practical and costefficient method for developing pornographic text detectors
for the incipient fault weakness irregular pulse disturbance and pseudocyclic periodicity of localized damage impact in the fault diagnosis of axlebox bearing abb under complex service environments the cyclic impulse spectrum cis based on timefrequency spectrogram slicing is proposed to discriminate repetitive transients under noise contamination in this approach a new indicator called the cyclic impulse degree cid is designed based on the coefficient of variation of the impulse peak moment within a specified timeshifted periodic window which allows for a generalization of impulsiveness and pseudocyclic periodicity to quantify the characteristics of bearing fault impacts first the normalized s transform nst highlights the energy distribution of fault impacts in the time–frequency plane from which the cid is constructed to distinguish the cyclic impulses from random transients secondly the nst slice at the frequency associated with the maximum integrated cid is used as a spare representation of fault impact response finally the cis derived from the nst slice presents the fault characteristic frequencies clearly and accurately the numerical simulation and data processing results of the abb faults demonstrated that the cis can identify the multiple faults of abb but also exhibits good performance in resisting random transient interference and discriminating noisecontaminated periodic impulses caused by the bearing localized defects
making decisions and building strategies to enhance the supply chain are currently essential if companies want to keep up with modern advances and compete in both the local and global markets however the coronavirus pandemic heightened the necessity for decision making and the adoption of strategies for supply chain improvement and transformation to effectively deal with crises and ongoing change this paper introduces a framework for recovery strategies to characterize the correlation at various stages among the key concerns of the supply chain because of covid19 recovery decisions the areas of development the strategic recovery plan and the objectives of the recovery strategies to restore the supply chain sc this study used a combination of quantitative and qualitative techniques to identify possible areas for supply chain improvement and rank viable solutions a combined fuzzy anp–fuzzy topsis method is proposed to prioritize and rank the areas of improvement the results indicate that digitization and the utilization of emerging technology play critical roles in supplychain recovery from disruptions and different current and future sc changes
this article presents a postprocessingbased spectral–spatial classification ssc approach for hyperspectral hs images the approach effectively overcomes the limitations of traditional pixelbased classifiers by integrating spectral and spatial information to achieve improved classification results specifically the proposed method uses principal component analysis to transform the hs images and the naive bayes nb classifier to quickly derive spectralposterior probabilities spatialposterior probabilities are then computed using an adaptive fast fourier transform afft and a probabilistic closeness function these probabilities are then combined to generate a precise ssc map the proposed approach is available in two distinct styles the conventional nb–afft–ssc method and the proposed iterationwise variable sequencing based nb–afft–ssc ivs–nb–afft–ssc method which classifies one designated class in each iteration in addition two wrapperbased feature selection methods are proposed to obtain a set of principal components pcs for each class of the hs image significantly improving classification accuracy the approachs efficacy is demonstrated through extensive experimentation on three real hs datasets including washington dc mall salinasa and botswana the generality of the approach has been proven through the use of other wellknown machinelearning algorithms such as support vector machine and knearest neighbor as wrappers in the approach the results confirm that the proposed approach is highly effective with the ivs approach helping users concentrate on a particular set of pcs for the class of interest
motisi mozrb and mohfb are promising alloy systems for hightemperature applications as they show higher toughness and higher creep resistance than other mobased alloys regarding ductility and toughness the chemical composition of the mo solidsolution phase is the main parameter with which to tweak these properties of multiphase mobased alloys besides the common solidsolution hardening one goal is to minimize embrittlement by decreasing the detrimental effects of interstitials like oxygen atoms in mo alloys which might be present in the bulk material due to trapping for a better understanding of the trapping mechanisms and behavior of mo solid solutions the bonding situation and interaction of mo atoms with the atoms of the alloying partners as well as oxygen atoms is worthwhile to investigate for this an indepth analysis of the chemical bonding situation with calculations based on density functional theory in selected motmo tm  ti zr hf solid solutions is conducted in this work it is shown that ti atoms in a mo solid solution are strong traps for oxygen atoms while hf and even more clearly zr atoms are not it is pointed out that the ionic and covalent interactions are the primary influence on the trapping behavior as the change in ionic and covalent interactions between trapping and nontrapping models follows the trend mo1ti  mo1hf  mo1zr which resembles the trend of the trapping energy
arousal during sleep can result in sleep fragmentation and various physiological effects impairing cognitive function and raising blood pressure and heart rate however the current definition of arousal has limitations in assessing both amplitude and duration making it challenging to measure sleep fragmentation accurately moreover there is inconsistency among interraters in arousal scoring which renders it susceptible to subjective variability therefore this study aims to identify a highly accurate classifier for each sleep stage by employing optimized feature selection and machine learning models according to electroencephalography eeg signals during the arousal phase the intensity level was categorized into four levels for control the nonarousal cases were used as level 0 and referred as sham arousal resulting in five arousal intensity levels wavelet transform was applied to analyze sleep arousal to extract features from eeg based on these features we classified arousal intensity levels through machine learning algorithms due to the different characteristics of eeg in each sleep stage the classification model was optimized for the four sleep stages excluding sham arousals a total of 13532 arousal events were used the lowest intensity in the entire data level 1 was computed to be 3107 level 2 was 3384 level 3 was 3472 and the highest intensity of level 4 was 3569 the optimized classification model for each sleep stage achieved an average sensitivity of 8268 specificity of 9568 and auroc of 9630 the sensitivity of the control arousal intensity level 0 was 8307 a 125 increase over the unoptimized model and a 1422 increase over previous research this study used machine learning techniques to develop classifiers for each sleep stage improving the accuracy of arousal intensity classification the classifiers showed high sensitivity and specificity and revealed the unique characteristics of arousal intensity during different sleep stages these findings represent a novel approach to arousal research and have implications for developing more accurate predictive models in sleep research
total organic carbon toc content is a key indicator for determining the hydrocarbon content of shale the current model for calculating the toc content of shale is relatively simplistic the modeling process is cumbersome and the parameters involved are influenced by subjective factors which have certain shortcomings to address this problem a timedomain convolutional neural network tcn model for predicting total organic carbon content based on logging sequence information was established by starting from logging sequence information conducting logging parameter sensitivity analysis experiments prioritizing loggingsensitive parameters as model feature vectors and constructing a tcn network meanwhile to overcome the problem of an insufficient sample size a fivefold crossvalidation method was used to train the tcn model and obtain the weight matrix with the minimum error and then a shale reservoir toc content prediction model based on the tcn model was established the model was applied to evaluate the toc logging of the lianggaoshan formation in the sichuan basin china and the predicted results were compared with the traditional δlogr model the results indicate that the tcn model predicts the toc content more accurately than the traditional model as demonstrated by laboratory tests this leads to a better application effect additionally the model fully explores the relationship between the logging curve and the total organic carbon content resulting in improved accuracy of the shale toc logging evaluation
the high voltage direct current hvdc transmission lines represent the prospective way for longdistance transmission between countries remote areas and offshore wind farms to decrease power loss however the hvdc protection systems have many challenges against any system issue such as maintenance and short circuits thus the vital role played by high voltage direct current circuit breaker cb has made it the center of attention in hvdc protection systems the main challenge in hvdc cb is the lack of naturally exiting current zero that allows the breaker to extinguish the arc during the opening process thus a commutation lc circuit is required to inject an oscillating current and enforce a zerocrossing nevertheless the lc branch is affected directly by the arcing time the transient recovery voltage trv and the rate of rise of recovery voltage rrrv this paper investigates the parametric uncertainties of lc and sf6 mechanical interrupters including cooling power and arc time constant upon trv and rrrv based on mayr’s blackbox model furthermore a part of 3000 mva 500 kv hvdc transmission line between egypt and the kingdom of saudi arabia is used as a testing system employing atpemtp software package to demonstrate the effect of cb’s parameters variations the results indicate that the capacitance represents the major parameter having a direct impact on trv and rrrv in essence the proper parameterization of the cb is highly required in the design process of hvdc cb to enhance the decisionmaking and ensure that the lc is capable of reducing arcing time and trv simultaneously
large pretrained protein language models plms have improved protein property and structure prediction from sequences via transfer learning in which weights and representations from plms are repurposed for downstream tasks although plms have shown great promise currently there is little understanding of how the features learned by pretraining relate to and are useful for downstream tasks we perform a systematic analysis of transfer learning using plms conducting 370 experiments across a comprehensive suite of factors including different downstream tasks architectures model sizes model depths and pretraining time we observe that while almost all downstream tasks do benefit from pretrained models compared to naive sequence representations for the majority of tasks performance does not scale with pretraining and instead relies on lowlevel features learned early in pretraining our results point to a mismatch between current plm pretraining paradigms and most applications of these models indicating a need for better pretraining methods
recent advancements in large language models llms have significantly enhanced their coding capabilities however existing benchmarks predominantly focused on simplified or isolated aspects of programming such as singlefile code generation or repository issue debugging falling short of measuring the full spectrum of challenges raised by realworld programming activities to this end we propose devbench a comprehensive benchmark that evaluates llms across various stages of the software development lifecycle including software design environment setup implementation acceptance testing and unit testing devbench features a wide range of programming languages and domains highquality data collection and carefully designed and verified metrics for each task empirical studies show that current llms including gpt4turbo fail to solve the challenges presented within devbench analyses reveal that models struggle with understanding the complex structures in the repository managing the compilation process and grasping advanced programming concepts our findings offer actionable insights for the future development of llms toward realworld programming applications our benchmark is available at httpsgithub comopencompassdevbench 
due to its excellent material performance the algangan highelectronmobility transistor hemt provides a wide platform for biosensing the high density and mobility of twodimensional electron gas 2deg at the algangan interface induced by the polarization effect and the short distance between the 2deg channel and the surface can improve the sensitivity of the biosensors the high thermal and chemical stability can also benefit hemtbased biosensors’ operation under for example high temperatures and chemically harsh environments this makes creating biosensors with excellent sensitivity selectivity reliability and repeatability achievable using commercialized semiconductor materials to synthesize the recent developments and advantages in this research field we review the various algangan hemtbased biosensors’ structures operations mechanisms and applications this review will help new researchers to learn the basic information about the topic and aid in the development of nextgeneration of algangan hemtbased biosensors
in this paper we propose a dense multiscale adaptive graph convolutional network dmagcn method for automatic segmentation of the knee joint cartilage from mr images under the multiatlas setting the suggested approach exhibits several novelties as described in the following first our models integrate both locallevel and globallevel learning simultaneously the local learning task aggregates spatial contextual information from aligned spatial neighborhoods of nodes at multiple scales while global learning explores pairwise affinities between nodes located globally at different positions in the image we propose two different structures of building models whereby the local and global convolutional units are combined by following an alternating or a sequential manner secondly based on the previous models we develop the dmagcn network by utilizing a densely connected architecture with residual skip connections this is a deeper gcn structure expanded over different block layers thus being capable of providing more expressive node feature representations third all units pertaining to the overall network are equipped with their individual adaptive graph learning mechanism which allows the graph structures to be automatically learned during training the proposed cartilage segmentation method is evaluated on the entire publicly available osteoarthritis initiative oai cohort to this end we have devised a thorough experimental setup with the goal of investigating the effect of several factors of our approach on the classification rates furthermore we present exhaustive comparative results considering traditional existing methods six deep learning segmentation methods and seven graphbased convolution methods including the currently most representative models from this field the obtained results demonstrate that the dmagcn outperforms all competing methods across all evaluation measures providing dsc9571 and dsc9402 for the segmentation of femoral and tibial cartilage respectively
detecting vulnerable road users is a major challenge for autonomous vehicles due to their small size various sensor modalities have been investigated including mono or stereo cameras and 3d lidar sensors which are limited by environmental conditions and hardware costs radar sensors are a lowcost and robust option with highresolution 4d radar sensors being suitable for advanced detection tasks however they involve challenges such as few and irregularly distributed measurement points and disturbing artifacts learningbased approaches utilizing pillarbased networks show potential in overcoming these challenges however the severe sparsity of radar data makes detecting small objects with only a few points difficult we extend a pillar network with our novel sparsityrobust feature fusion srff neck which combines high and lowlevel multiresolution features through a lightweight attention mechanism while lowlevel features aid in better localization highlevel features allow for better classification as sparse input data are propagated through a network the increasing effective receptive field leads to feature maps of different sparsities the combination of features with different sparsities improves the robustness of the network for classes with few points
this study aims to determine the effect of the internet of things integrated discovery learning model on students critical thinking skills this type of research is metaanalysis research this research data comes from 12 national and international journals published in 20202024 data tracing through goole scholar sage journal researchgate and sciencedirect the inclusion criteria in this metaanalysis are research derived from national and international journals indexed by sinta and scopus research must be experimental methods and quasiexperiments research related to the internet of things iot integrated discovery learning model on students critical thinking skills and research reports complete data to calculate effect size values data analysis using microsoft excel the results concluded that there is an influence of the iot integrated discovery learning model on critical thinking skills with a high mean effect size  es  0915 effect category
"
purpose
this study aims to analyze the university environment’s role in the intention–action gap iagof highly successful startup founders in an emerging market


designmethodologyapproach
using multiple regression analysis this study analyzed data collected from 314 founders representing 99 successful startups 289 valid observations renowned for their high funding and value operating in an emerging market brazil


findings
the results demonstrate that extracurricular activities and exchange programs lead to a reduced iag while living in a significant economic center extends it computer science and industrial engineering students show reduced iags studying together with future cofounders also leads to reduced gaps


research limitationsimplications
the study contributes to the microfoundations theory by presenting new interactions between students and the university environment that influence entrepreneurial action limitations are related to the sample limited to brazilian founders and selected only through venture capital firms’ filters


practical implications
this study also provides practical insights to the universities’ leaders on how they can create programs that improve the rate of startup creation potentially leading to successful companies


originalityvalue
this study investigates the association between the university role and the entrepreneur’s iag in emerging markets the entrepreneur’s iag is still a relatively new phenomenon explored in entrepreneurship even less understanding and limited empirical data exist on successful startups from emerging markets this study drew on the microfoundations literature to answer how universities in emerging markets could address specific resources and entrepreneurship programs to reduce the iag among students and alumni
"
digital streaming platforms including twitch spotify netflix disney and kindle have emerged as one of the main sources of entertainment with significant growth potential many of these platforms distribute royalties among streamers artists producers or writers based on their impact in this paper we measure the relevance of each of these contributors to the overall success of the platform which is information that can play a key role in revenue allocation we perform an axiomatic analysis to provide normative foundations for three relevance metrics the uniform the proportional and the subscriberproportional indicators the last two indicators implement the socalled prorata and usercentric models which are extensively applied to distribute revenues in the music streaming market the axioms we propose formalize different principles of fairness stability and nonmanipulability and are tailormade for the streaming context we complete our analysis with a case study that measures the influence of the 19 mostfollowed streamers worldwide on the twitch platform
security concerns have been raised about cascading failure risks in evolving power grids this article reveals for the first time that the risk of cascading failures can be increased at low network demand levels when considering securityconstrained generation dispatch this occurs because critical transmission corridors become very highly loaded due to the presence of centralized generation dispatch eg large thermal plants far from demand centers this increased cascading risk is revealed in this work by incorporating securityconstrained generation dispatch into the risk assessment and mitigation of cascading failures a securityconstrained ac optimal power flow which considers economic functions and security constraints eg network constraints n  1 security and generation margin is used to provide a representative dayahead operational plan cascading failures are simulated using two simulators a quasisteady state dc power flow model and a dynamic model incorporating all frequencyrelated dynamics to allow for result comparison and verification the risk assessment procedure is illustrated using synthetic networks of 200 and 2000 buses further a novel preventive mitigation measure is proposed to first identify critical lines whose failures are likely to trigger cascading failures and then to limit power flow through these critical lines during dispatch results show that shifting power equivalent to 1 of total demand from critical lines to other lines can reduce cascading risk by up to 80
the great majority of spinal cord injury sci patients have debilitating chronic pain despite decades of research these pain pathways of neuropathic pain np are unknown sci patients have been shown to have abnormal brain pain pathways we hypothesize that sci np patients pain matrix is altered compared to sci patients without np this study examines the functional connectivity fc in sci patients with moderatesevere chronic np compared to sci patients with mildno np these groups were compared to control subjects the neuropathic pain questionnaire and neurological evaluation based on the international standard neurological classification of sci were utilized to define the severity and level of injury of the 10 sci patients 7 486 ± 1702 years old 6 male and 1 female indicated that they had np and 3 did not have np 3933 ± 808 years old 2 male and 1 female ten uninjured neurologically intact participants were used as controls 248 ± 461 years old 5 male and 5 female fc metrics were obtained from the comparisons of restingstate functional magnetic resonance imaging among our various groups controls sci with np and sci without np for each comparison a regionofinterest roitoroi connectivity analysis was pursued encompassing a total of 175 rois based on a customized atlas derived from the aal3 atlas the analysis accounted for covariates such as age and sex to correct for multiple comparisons a strict bonferroni correction was applied with a significance level of p  005nrois when comparing sci patients with moderatetosevere pain to those with mildtono pain specific thalamic nuclei had altered connections these nuclei included medial pulvinar lateral pulvinar medial geniculate nucleus lateral geniculate nucleus and mediodorsal magnocellular nucleus there was increased fc between the lateral geniculate nucleus and the anteroventral nucleus in np postsci our analysis additionally highlights the relationships between the frontal lobe and temporal lobe with pain this study successfully identifies thalamic neuroplastic changes that occur in patients with sci who develop np it additionally underscores the pain matrix and involvement of the frontal and temporal lobes as well our findings complement that the development of np postsci involves cognitive emotional and behavioral influences
roughly every decade the acm and ieee professional organizations have produced recommendations for the education of undergraduate computer science students these guidelines are used worldwide by research universities liberal arts colleges and community colleges for the latest 2023 revision of the curriculum aaai has collaborated with acm and ieee to integrate artificial intelligence more broadly into this new curriculum and to address the issues it raises for students instructors practitioners policy makers and the general public this paper describes the development process and rationale that underlie the artificial intelligence components of the cs2023 curriculum discusses the challenges in curriculum design for such a rapidly advancing field and examines lessons learned during this threeyear process
due to the strong demand of massive storage capacity the density of flash memory has been improved in terms of technology node scaling multibit per cell technique and 3d stacking however these techniques also degrade read performance and reliability the long read latency comes from increased data sensing time and timeconsuming ecc decoding time storing multiple bits per cell results in more read reference voltages and increased latency of identifying appropriate threshold voltages to deal with error correction ldpc is widely used in flash memory to provide stronger ecc capability however ldpc incurs a long decoding latency when bit errors are numerous in this work we propose coupled data storage cds to improve the read performance of 3d nand flashmemory storage devices cds supports two modes to improve read latency the high readspeed mode is designed to improve data sensing time with reduced voltage states while the data correction mode is designed to mitigate bit errors and ldpc overhead experiment results showed that cds could reduce 50inlineformulatexmath notationlatexsimtexmathalternativesmmlmathmmlmo∼mmlmommlmathinlinegraphic xlinkhrefhsiehieq13338474gifalternativesinlineformula666 read latency and 257inlineformulatexmath notationlatexsimtexmathalternativesmmlmathmmlmo∼mmlmommlmathinlinegraphic xlinkhrefhsiehieq23338474gifalternativesinlineformula275 write latency under the high readspeed mode for the data correction mode rber could be decreased by 37inlineformulatexmath notationlatexsimtexmathalternativesmmlmathmmlmo∼mmlmommlmathinlinegraphic xlinkhrefhsiehieq33338474gifalternativesinlineformula52 and the lifetime could be prolonged to 16 to 3 times
cyber resilience has become a major concern for both academia and industry due to the increasing number of data breaches caused by the expanding attack surface of existing it infrastructure cyber resilience refers to an organisation’s ability to prepare for absorb recover from and adapt to adverse effects typically caused by cyberattacks that affect business operations in this survey we aim to identify the significant domains of cyber resilience and measure their effectiveness we have selected these domains based on a literature review of frameworks strategies applications tools and technologies we have outlined the cyber resilience requirements for each domain and explored solutions related to each requirement in detail we have also compared and analysed different studies in each domain to find other ways of enhancing cyber resilience furthermore we have compared cyber resilience frameworks and strategies based on technical requirements for various applications we have also elaborated on techniques for improving cyber resilience in the supplementary section we have presented applications that have implemented cyber resilience this survey comprehensively compares various popular cyber resilience tools to help researchers practitioners and organisations choose the best practices for enhancing cyber resilience finally we have shared key findings limitations problems and future directions
the asynchronous transfer mode atm network is crucial due to its ability to efficiently transmit data provide reliable connections and support various service classes with specific quality of service qos requirements in this paper we utilize the opnet network simulation software to model an atm network and analyze the impact of qos classification on network performance we investigate the effects of constant bit rate cbr variable bit rate vbr available bit rate abr and unspecified bit rate ubr models on various network traffic types such as voice video and data for voice traffic we examine key qos parameters including jitter packet delay variation and endtoend delay for video traffic we evaluate packet delay variation and endtoend delay additionally we analyze download response time for data traffic to assess the influence of qos on the atm network our results demonstrate that cbr and vbr are preferred for realtime traffic like voice and video providing low delay and jitter the simulation approach enables us to test various configurations and gain insights not possible in hardware tests our findings can help network operators determine the optimal qos settings and tradeoffs when deploying atm for modern multiservice networks
deep learning models produce impressive results in any natural language processing applications when given a better learning strategy and trained with large labeled datasets however the annotation of massive training data is far too expensive especially in the legal domain due to the need for trained legal professionals data augmentation solves the problem of learning without labeled big data in this paper we employ pretrained language models and prompt engineering to generate largescale pseudolabeled data for the legal overruling task using 100 data samples we train small recurrent and convolutional deeplearning models using this data and finetune a few other transformer models we then evaluate the effectiveness of the models both with and without data augmentation using the benchmark dataset and analyze the results we also test the performance of these models with the stateoftheart gpt3 model under fewshot setting our experimental findings demonstrate that data augmentation results in better model performance in the legal overruling task than models trained without augmentation furthermore our bestperforming deep learning model trained on augmented data outperforms the fewshot gpt3 by 18 in the f1score additionally our results highlight that the small neural networks trained with augmented data achieve outcomes comparable to those of other large language models
community detection in multilayer networks has emerged as a crucial area of modern network analysis however conventional approaches often assume that nodes belong exclusively to a single community which fails to capture the complex structure of realworld networks where nodes may belong to multiple communities simultaneously to address this limitation we propose novel spectral methods to estimate the common mixed memberships in the multilayer mixed membership stochastic block model the proposed methods leverage the eigendecomposition of three aggregate matrices the sum of adjacency matrices the debiased sum of squared adjacency matrices and the sum of squared adjacency matrices we establish rigorous theoretical guarantees for the consistency of our methods specifically we derive pernode error rates under mild conditions on network sparsity demonstrating their consistency as the number of nodes andor layers increases under the multilayer mixed membership stochastic block model our theoretical results reveal that the method leveraging the sum of adjacency matrices generally performs poorer than the other two methods for mixed membership estimation in multilayer networks we conduct extensive numerical experiments to empirically validate our theoretical findings for realworld multilayer networks with unknown community information we introduce two novel modularity metrics to quantify the quality of mixed membership community detection finally we demonstrate the practical applications of our algorithms and modularity metrics by applying them to realworld multilayer networks demonstrating their effectiveness in extracting meaningful community structures
in this editorial for our special topic forum stf on global supply chain research we present a novel approach that empowers firms to evaluate their supply chain complexitys current state and potentially reveal hidden patterns of complexity our focus lies in developing a configurational understanding of global supply chain complexity leveraging the diverse perspectives and insights provided by the existing literature on this subject furthermore we shed light on exciting opportunities for future research and introduce the accepted research papers featured in this stf
world’s mangroves are decreasing and the remaining are continuously at risk so restoration seen as one of key strategies in the mangrove management mangrove ecosystems are ecologically important for coastal life as well as play a key role for the livelihood and food security that put coastal community as important actor in the mangrove restoration efforts including in indonesia this study aims to understand the current practices and lesson learned from the implementation of communitybased mangrove restoration cbmr we used a combination of systematic review and colearning workshops to identify relevant case studies interventions and outcomes from the implementation cbmr we selected 71 relevant case studies from nine countries and ran a principal component analysis pca we identified four group of intervention implemented in the case studies those are active restoration include replanting passive restoration focus on protection and depend on natural regeneration model business development and strengthening the community institution case studies analysis suggested that combination of four type of interventions helps to achieve both aims of cbmr which are restored mangrove and improved livelihood
background interstitial brachytherapy is a form of intensive local irradiation that facilitates the effective protection of surrounding structures and the preservation of organ functions resulting in a favourable therapeutic response as surgical robots can perform needle placement with a high level of accuracy our team developed a fully automatic radioactive seed placement robot and this study aimed to evaluate the accuracy and feasibility of fully automatic radioactive seed placement for the treatment of tumours in the skull base methods a fully automatic radioactive seed placement robot was established and 4 phantoms of skull base tumours were built for experimental validation all the phantoms were subjected to computed tomography ct scans then the ct data were imported into the remebot software to design the preoperative seed placement plan after the phantoms were fixed in place navigation registration of the remebot was carried out and the automatic seed placement device was controlled to complete the needle insertion and particle placement operations after all of the seeds were implanted in the 4 phantoms postoperative image scanning was performed and the results were verified via image fusion results a total of 120 seeds were implanted in 4 phantoms the average error of seed placement was 251 ± 144 mm conclusion this study presents an innovative fully automated radioactive particle implantation system utilizing the remebot device which can successfully complete automated localization needle insertion and radioactive particle implantation procedures for skull base tumours the phantom experiments showed the robotic system to be reliable stable efficient and safe however further research on the needlesoft tissue interaction and deformation mechanism of needle puncture is still needed
the cerebral cortex is vital for the processing and perception of sensory stimuli in the somatosensory axis information is received primarily by two distinct regions the primary s1 and secondary s2 somatosensory cortices topdown circuits stemming from s1 can modulate mechanical and cooling but not heat stimuli such that circuit inhibition causes blunted perception this suggests that responsiveness to particular somatosensory stimuli occurs in a modality specific fashion and we sought to determine additional cortical substrates in this work we identify in a mouse model that inhibition of s2 output increases mechanical and heat but not cooling sensitivity in contrast to s1 combining 2photon anatomical reconstruction with chemogenetic inhibition of specific s2 circuits we discover that s2 projections to the secondary motor cortex m2 govern mechanical and heat sensitivity without affecting motor performance or anxiety taken together we show that s2 is an essential cortical structure that governs mechanical and heat sensitivity
background objective prognostic information is essential for good clinical decision making in case of unknown diseases scarcity of evidence and limited tacit knowledge prevent obtaining this information prediction models can be useful but need to be not only evaluated on how well they predict but also how stable these models are under fast changing circumstances with respect to development of the disease and the corresponding clinical response this study aims to provide interpretable and actionable insights particularly for clinicians we developed and evaluated two regression tree predictive models for inhospital mortality of covid19 patient at admission and 24 hours 24 h after admission using a national registry we performed a retrospective analysis of observational routinely collected data methods two regression tree models were developed for admission and 24 h after admission the complexity of the trees was managed via cross validation to prevent overfitting the predictive ability of the model was assessed via bootstrapping using the area under the receiveroperatingcharacteristic curve brier score and calibration curves the tree models were assessed on the stability of their probabilities and predictive ability on the selected variables and compared to a fullfledged logistic regression model that uses variable selection and variable transformations using splines participants included covid19 patients from all icus participating in the dutch national intensive care evaluation nice registry who were admitted at the icu between february 27 2020 and november 23 2021 from the nice registry we included concerned demographic data minimum and maximum values of physiological data in the first 24 h of icu admission and diagnoses reason for admission as well as comorbidities for model development the main outcome measure was inhospital mortality we additionally analysed the lengthofstay los per patient subgroup per survival status results a total of 13369 confirmed covid19 patients from 70 icus were included with mortality rate of 28 the optimismcorrected auroc of the admission tree with seven paths was 072 95 ci 071–074 and of the 24 h tree with 11 paths was 074 074–077 both regression trees yielded good calibration and variable selection for both trees was stable patient subgroups comprising the tree paths had comparable survival probabilities as the fullfledged logistic regression model survival probabilities were stable over six covid19 surges and subgroups were shown to have added predictive value over the individual patient variables conclusions we developed and evaluated regression trees which operate at par with a carefully crafted logistic regression model the trees consist of homogenous subgroups of patients that are described by simple interpretable constraints on patient characteristics thereby facilitating shared decisionmaking
data from four lightning networks collected during three quasilinear convective systems qlcs are used to understand the differences in detection for optimizing their combined use additionally using unique aspects from each network provides a more complete picture of lightning in a thunderstorm the four lightning networks examined include a lightning mapping array lma the earth networks total lightning network entln the geostationary lightning mapper glm and the national lightning detection network nldn the data from each network are intermatched and locations where each network uniquely detected a flash versus all are analyzed in reference to three qlcss including two qlcss that occurred in the southeast 22 march 2022 and 30 march 2022 during the propagation evolution and rotation in linear systems perils field campaign and one case from oklahoma 26 february 2023 unique aspects of the lightning provided by each network are examined including flash initiation altitude size type and energy lightning flash trends and characteristics for each qlcs are similar between networks in general but deviate in certain conditions and locations times of decreased matching between networks were associated with localized increases in lightning rates smaller flash sizes and lowerenergy flashes the differences in each network’s performance across the qlcss demonstrates the importance of understanding the limitations in each and the advantage of using multiple networks
opioidergic mechanisms of repetitive transcranial magnetic stimulation rtms analgesia are shaped by rtms dose and different endogenous opioids a randomised controlled trial
in this paper we initiate a concept of graphproximal functions furthermore we give a notion of being generalized geraghty dominating for a pair of mappings this permits us to establish the existence of and unique results for a common best proximity point of complete metric space additionally we give a concrete example and corollaries related to the main theorem in particular we apply our main results to the case of metric spaces equipped with a reflexive binary relation finally we demonstrate the existence of a solution to boundary value problems of particular secondorder differential equations
the studys primary purpose was to review studies on the role of artificial intelligence in market performance artificial intelligence significantly impacts market performance by providing data analysis personalization demand forecasting pricing optimization customer support automation risk assessment and enhanced decisionmaking capabilities by leveraging artificial intelligence ai effectively businesses can improve their competitiveness improve customer satisfaction increase revenue and achieve sustainable growth in the market a thorough assessment of the literature was done and screening standards were applied all to improve the study based on the inclusion and exclusion criteria for the articles data extraction was done by preferred reporting items for systematic reviews and metaanalyses 45 published articles were analyzed and significant data was extracted the review’s findings collectively emphasize the crucial role of ai in enhancing market performance by improving sales customer satisfaction demand forecasting pricing optimization risk mitigation and decisionmaking processes as ai continues to advance further research and practical implementations will likely uncover additional benefits and insights into its impact on market performance to help more scholars understand and advance the numerous theories and models related to the topic this concept overview provides guidance
the advent of artificial intelligence ai technologies has taken the world of science by storm in 2023 the opportunities of this easy to access technology for clinical pharmacy research are yet to be fully understood the development of a custommade large language model llm delstar trained on a wide range of internationally recognised scientific publication databases pharmacovigilance sites and international product characteristics to help identify and summarise medication related information on delirium as a proofofconcept model identified new facilitators and barriers for robust clinical pharmacy practice research this technology holds great promise for the development of much more comprehensive prescribing guidelines practice support applications for clinical pharmacy increased patient and prescribing safety and resultant implications for healthcare costs the challenge will be to ensure its methodologically robust use and the detailed and transparent verification of its information accuracy
due to the complexity of the oil drilling environment the parameters of the wireless energy transmission system drift causing the output voltage to drop which affects the instability of the backstage circuit in view of the above problems this paper designs an energy and telecommunication cotransmission system based on the frequency tracking technology of output voltage stabilization simulates the parameter changes of the coupling mechanism due to the environmental changes establishes the model of the coupling mechanism by maxwell and builds the simulation model of the system in matlab the experimental results show that the output voltage of the system has been stabilized at the set value unchanged under the change of system parameters which proves the feasibility of this design
understanding and monitoring wildlife behavior is crucial in ecology and biomechanics yet challenging due to the limitations of current methods to address this issue we introduce wildpose a novel longrange motion capture system specifically tailored for freeranging wildlife observation this system combines an electronically controllable zoomlens camera with a lidar to capture both 2d videos and 3d point cloud data thereby allowing researchers to observe highfidelity animal morphometrics behavior and interactions in a completely remote manner field trials conducted in kgalagadi transfrontier park have successfully demonstrated wildpose’s ability to quantify morphological features of different species accurately track the 3d movements of a springbok herd over time and observe the respiratory patterns of a distant lion by facilitating nonintrusive longrange 3d data collection wildpose marks a significant complementary technique in ecological and biomechanical studies offering new possibilities for conservation efforts and animal welfare and enriching the prospects for interdisciplinary research
beyond diagonal reconfigurable intelligent surface bdris is a new advance and generalization of the ris technique bdris breaks through the isolation between ris elements by creatively introducing interelement connections thereby enabling smarter wave manipulation and enlarging coverage however exploring proper channel estimation schemes suitable for bdris aided communication systems still remains an open problem in this paper we study channel estimation and beamforming design for bdris aided multiantenna systems we first describe the channel estimation strategy based on the least square ls method derive the mean square error mse of the ls estimation and formulate the joint pilot sequence and bdris design problem with unique constraints induced by bdris architectures specifically we propose an efficient pilot sequence and bdris design which theoretically guarantees to achieve the minimum mse with the estimated channel we then consider two bdris scenarios and propose beamforming design algorithms finally we provide simulation results to verify the effectiveness of the proposed channel estimation scheme and beamforming design algorithms we also show that more interelement connections in bdris improve the performance while increasing the training overhead for channel estimation
this study explores the learning dynamics of neural networks by analyzing the singular value decomposition svd of their weights throughout training our investigation reveals that an orthogonal basis within each multidimensional weights svd representation stabilizes during training building upon this we introduce orthogonalityinformed adaptive lowrank oialr training a novel training method exploiting the intrinsic orthogonality of neural networks oialr seamlessly integrates into existing training workflows with minimal accuracy loss as demonstrated by benchmarking on various datasets and wellestablished network architectures with appropriate hyperparameter tuning oialr can surpass conventional training setups including those of stateoftheart models
since late 2019 covid19 has significantly impacted the world understanding the evolution of sarscov2 is crucial for protecting against future infectious pathogens in this study we conducted a comprehensive chronological analysis of sarscov2 evolution by examining mutation prevalence from the source countries of vocs united kingdom india brazil south africa plus two countries united states russia utilizing genomic sequences from gisaid our methodological approach involved largescale genomic sequence alignment using mafft pythonbased data processing on a highperformance computing platform and advanced statistical methods the maximal information coefficient mic and also long shortterm memory lstm models for correlation analysis our findings elucidate the dynamics of sarscov2 evolution highlighting the viruss changing behaviour over various pandemic stages key results include the discovery of three temporal mutation patternslineage distinct longspan and competitive mutationswith varying levels of impact on the virus notably we observed a convergence of advantageous mutations in the spike protein especially in the later stages of the pandemic indicating a substantial evolutionary pressure on the virus one of the most significant revelations is the predominant role of natural immunity over vaccinationinduced immunity in driving these evolutionary changes this emphasizes the critical need for regular vaccine updates to maintain efficacy against evolving strains in conclusion our study not only sheds light on the evolutionary trajectory of sarscov2 but also underscores the urgency for robust continuous global data collection and sharing it highlights the necessity for rapid adaptations in medical countermeasures including vaccine development to stay ahead of pathogen evolution this research provides valuable insights for future pandemic preparedness and response strategies
in april 2022 the california independent system operator caiso power grid reached momentary peaks of 100 renewable energy for the first time after a year momentary 100 supply from renewables isnt a news any more and caiso reported record wind and solar renewable curtailment of 606 gwh march 2023 and 686 gwh april 2023 in addition peak renewable coincided with curtailment rates of 8 gw in april 2023 our prior studies documented monthlypeak renewable curtailment growth of 40 annually 23 gwh in march 2015 growing to 82 gwh in march 2017 updating for 20182023 shows that 40 annual growth has continued through april 2023 and with it caiso has reached an average sunlight hours curtailment rate of 2 gw if this 9year trend continues unchecked for the next 5 years monthly curtailment is projected to reach 33 twh in both march and april of 2028 an average sunlight curtailment rate of nearly 11 gw we analyze causes of increased curtailment and discuss its likely future trajectory growth we also discuss the challenges its growth represents for grid decarbonization finally we outline the difficulties in reducing curtailment growth and highlight several new opportunities for power grids and computing systems
laser diode absorption spectroscopy is used to experimentally measure ar1s5 metastable density and translational gas temperature within a 94 ghz microplasma a square twodimensional photonic crystal phc at this resonance frequency serves to ignite and sustain the plasma from 20 to 200 torr 27 × 103–27 × 104 pa by using millimeter wave power from 300 to 1000 mw metastable density within the plasma is estimated from the absorption line shape of the laser traversing the phc the metastable density reaches an order of 1019 m−3 at lower pressure and decreases as pressure increases from the lorentzian line shape of the absorption profile at 81153 nm the gas temperature is extracted and found to increase from 500 k at 20 torr to 1300 k at 200 torr these data are analyzed and compared with a zerodimensional plasma model and with previous experimental plasma results at 43 ghz
purposethe rapid development of the internet has led to an increasingly significant role for ecommerce business this study examines how the green supply chain gsc operates on the ecommerce online channel resell mode and agency mode and the traditional offline channel with information sharing under demand uncertaintydesignmethodologyapproachthis study builds a multistage game model that considers the manufacturer selling green products through different channels on the traditional offline channel the competing retailers decide whether to share demand signals regarding the resale mode of ecommerce online channel just etailer 1 determines whether to share information and decides the retail price in the agency mode the manufacturer decides the retail price directly and etailer 2 sets the platform ratefindingsthis study reveals that information accuracy is conducive to information value and profits on both channels interestingly the platform fee rate in agency mode will inhibit the effect of a positive demand signal information sharing will cause double marginal effects and price competition behavior will mitigate such effects additionally when the platform fee rate is low the manufacturer will select the ecommerce online channel for operation but the retailers profit is the highest in the traditional channeloriginalityvaluethis research explores the interplay between different channel structures and information sharing in a gsc considering price competition and demand uncertainty besides we also considered what behaviors and factors will amplify or transfer the effect of double marginalization
children undergoing airway management during general anesthesia may experience airway complications resulting in a rare but life‐threatening situation known as “cant intubate cant oxygenate” this situation requires immediate recognition advanced airway management and ultimately emergency front‐of‐neck access the absence of standardized procedures lack of readily available equipment inadequate knowledge and training often lead to failed emergency front‐of‐neck access resulting in catastrophic outcomes in this narrative review we examined the latest evidence on emergency front‐of‐neck access in children
faculty members confront a variety of obstacles over time the most recent of which is the coronavirus disease 2019 pandemic which may increase their vulnerability to burnout bo this study aims to examine bo in medical school faculties as well as the factors that lead to bo and well‐being in them
this study explores the correlation between technology utilization and language acquisition while analyzing the impact of moderating variables on this relation our metaanalysis approach analyzes data from 43 extracts out of 19 primary studies published between 2012 and 2021 our data analysis employs a randomeffect model utilizing a significance level of α  005 additionally the authors examine four moderating variables level of education location of research proficiency in language and year of publication technologybased language acquisition outperforms traditional methods indicating a significant and moderate impact on the learning process this study enhances comprehension of the efficacy of technology in language acquisition by identifying various factors such as the geographical location of research methods of assessing language proficiency and technology type employed however there is insufficient evidence to support the notion that educational level or sample size significantly impact technologybased language acquisition this metaanalysis highlights the importance of considering nuanced factors when integrating technology into language learning the findings emphasize the possibility of technology to transform methods of acquiring language and urge additional investigation into customized strategies that optimize its advantages
the literature indicated that deceivers in facetoface communication experience psychological strains derived from guilt or distress associated with violating conversational rules we proposed that this also applies to telephonemediated deception drawing insights from the theoretical and empirical literature we surmised that strategic trickery utilized by outsourced call center agents would elicit adverse psychological reactions that have unfavorable impacts on their wellbeing cognition and work motivation we used structural equation modeling to test our hypotheses using data from a sample of 554 outsourced filipino call service agents who worked graveyard shifts to cater to mainly american customers the results suggested that strategic deception increases the experience of cognitive dissonance while negatively impacting psychological wellbeing and intrinsic work motivation the results also showed that dissonance negatively influences wellbeing and intrinsic motivation and partially mediates the deceptionmotivation relationship unlike previous findings however our multivariate analyses revealed that wellbeing and motivation were not correlated our original findings have theoretical and practical implications
while large language models llms excel at the winograd schema challenge wsc a coreference resolution task testing commonsense reasoning through pronoun disambiguation they struggle with instances that feature minor alterations or rewording to address this we introduce evograd an opensource platform that harnesses a humanintheloop approach to create a dynamic dataset tailored to such altered wsc instances leveraging chatgpt’s capabilities we expand our task instances from 182 to 3691 setting a new benchmark for diverse commonsense reasoning datasets additionally we introduce the error depth metric assessing model stability in dynamic tasks our results emphasize the challenge posed by evograd even the best performing llm gpt35 achieves an accuracy of 650 with an average error depth of 72 a stark contrast to human performance of 928 accuracy without perturbation errors this highlights ongoing model limitations and the value of dynamic datasets in uncovering them
artificial intelligence ai the uprising technology of computer science aiming to create digital systems with human behavior and intelligence seems to have invaded almost every field of modern life launched in november 2022 chatgpt chat generative pretrained transformer is a textual ai application capable of creating humanlike responses characterized by original language and high coherence although aibased language models have demonstrated impressive capabilities in healthcare chatgpt has received controversial annotations from the scientific and academic communities this chatbot already appears to have a massive impact as an educational tool for healthcare professionals and transformative potential for clinical practice and could lead to dramatic changes in scientific research nevertheless rational concerns were raised regarding whether the pretrained aigenerated text would be a menace not only for original thinking and new scientific ideas but also for academic and research integrity as it gets more and more difficult to distinguish its ai origin due to the coherence and fluency of the produced text this short review aims to summarize the potential applications and the consequential implications of chatgpt in the three critical pillars of medicine education research and clinical practice in addition this paper discusses whether the current use of this chatbot is in compliance with the ethical principles for the safe use of ai in healthcare as determined by the world health organization finally this review highlights the need for an updated ethical framework and the increased vigilance of healthcare stakeholders to harvest the potential benefits and limit the imminent dangers of this new innovative technology
this study presents a new approach for characterizing the 3d printing electromagnetic properties in polylactic acid pla using a novel crossfractal sensor the cst studio suite simulation employing the finite element method fem and the ads simulation software based on the equivalent circuit of this novel sensor are utilized to enhance confidence in the convergence of simulation and measurement results the analysis confirms that the real sensor based on a crosssectional fractal resonator exhibits excellent sensitivity selectivity and linearity sensor performance is evaluated through two methods the frequencybased approach and the s11 parameter analysis in the frequencybased method the sensor demonstrates a typical sensitivity of 00302 ghzcm3 while in the s11 parameter analysis it exhibits a typical sensitivity of 03065 dbcm3 these results underscore the high sensitivity and linearity of this innovative sensor the using of this novel cross fractal sensor it promises to show how to properly adjust 3d printer parameters potentially setting the “infill percentage” of the 3d print this is because the proportion of air present changes when the printing percentage is changed which can lead to  varepsilon textr  and losstangent tan  delta  with varying the infill percentage by the resonance frequency fr approach demonstrates a characteristic sensitivity of 01822 ghzriu the experimental study confirmed that the fabricated sensor exhibits miniaturization with an electrical size of lambda 0 8 high sensitivity and excellent linearity in the frequency approach with a typical sensitivity of 0004615 ghz the suggested biosensor demonstrated its capability to detect low concentrations of ethanol these aqueous solutions containing known alcohols like ethanol in low concentrations enables identification of halal and alcoholfree products in food sensing applications
in this paper we raise a new issue on unidentified foreground object ufo detection in 3d point clouds which is a crucial technology in autonomous driving in the wild ufo detection is challenging in that existing 3d object detectors encounter extremely hard challenges in both 3d localization and outofdistribution ood detection to tackle these challenges we suggest a new ufo detection framework including three tasks evaluation protocol methodology and benchmark the evaluation includes a new approach to measure the performance on our goal ie both localization and ood detection of ufos the methodology includes practical techniques to enhance the performance of our goal the benchmark is composed of the kitti misc benchmark and our additional synthetic benchmark for modeling a more diverse range of ufos the proposed framework consistently enhances performance by a large margin across all four baseline detectors second pointpillars pvrcnn and parta2 giving insight for future work on ufo detection in the wild
abstract as the application of authentic assessment aa in early childhood education ece programs is emerging the demand for professionals who understand the fundamental elements of aa and can incorporate them into their practices is increasing however teaching professional competencies and providing opportunities to put theory into practice specifically in online courses is challenging while technologybased learning is becoming increasingly common in education the utilization of simulationbased technology in higher education is still emerging this reflection on practice describes the development and implementation of a specially designed simulationbased learning widget called the playbased assessment widget pbaw the pbaw was used to assist undergraduate students in practicing professionalism including communication and collaboration skills as they simulate interaction with a family in a webbased instructional tool called widget instructors’ findings indicate that the pbaw is a proper tool for imitating reallife scenarios connecting theory with practice and evaluating students’ content knowledge
deep learning dl is gaining prominence in various wireless communications applications including modulation classification channel estimation optimal power allocation etc nevertheless dl is susceptible to data poisoningbased attacks trojanbackdoor attacks where carefully designed imperceptible triggers are introduced to the training dataset causing models to make erroneous predictions we propose a novel trojan attack technique for wireless modulation classification where the adversary finds linearly independent and orthonormal triggers using the gramschmidt orthogonalization method the triggers are also input agnostic making them generalizable across different signals during the training phase a small amount of clean samples are poisoned by adding these triggers and changing the labels to a desired target label we show that in the deployment stage the dlbased classifier poisoned at the receiver correctly classifies the clean triggerfree signals with high accuracy however it misclassifies the wireless signals with the trigger to the desired target label which differs from the true label with a high attack success rate finally we propose an activation clusteringbased detection technique to determine the presence of poisoned samples in the dataset and found to be effective against the proposed trojan attack method
this study aims to examine the process of developing electronic textbooks for accounting learning evaluation courses to improve the quality of learning in accounting education study programs the study followed research and development r  d with the addie model analysis design development implementation and evaluation the students of the accounting education program at universitas negeri surabaya were involved in this research the findings indicated that electronic textbooks for accounting learning obtained very good responses with very appropriate interpretation criteria according to students’ needs for this matter this product is expected to be an alternative teaching material for students to enhance material understanding learning independence and learning motivation in the online course process of accounting learning evaluation
in order to improve the user experience in intricate interior settings this research uses datadriven insights to investigate the dynamics of internet of thingsenabled indoor navigation systems a link between prior navigation experience and contentment was found via analysis of user profiles users who reported having a high degree of prior experience also showed a 25 increase in happiness an review of sensor data revealed that environmental conditions are critical in determining user happiness with users reporting 12 greater levels of satisfaction in locations with higher temperatures 240°c furthermore customer preferences for customized routes were revealed by navigation data analysis highlighting the need of configurable navigation systems lastly an examination of user input revealed that resolving issues raised satisfaction levels by 18 the aforementioned results highlight the complex aspects of indoor navigation and highlight the significance of factors such as user profiles ambient comfort route customisation and responsive feedback systems in enhancing the overall experience
smart city project is today a domain of interest to community research which play wellknown role in road traffic management data exchange became complicated in terms of capacity in the intelligent transport system its and without the raise of big data the treatment is very difficult to manage vehicular adhoc network vanets faces many challenges mainly the voluminous data generated by different actors of vanet environment  we propose a real time anomalies detection system in an instantaneous way with parallel data treatment the system method intends to compute precisely vehicle density at each section on each road which help to handle the traffic and forward to vehicles information about the road and the best safe path to reach their destination also we build anomalies prediction system based on machine learning framework it is a good solution for avoiding traffic congestion and limiting the risk of accidents the simulation results demonstrate that the proposed system method reduces congestion greatly by taking into account the load balancing and therefore avoids saturation and reduces accidents it should also be noted that the results obtained show that the system is characterized by low latency and high accuracy
this paper introduces a roadmap for a comprehensive education system that leverages new technologies to empower lifelong learning it proposes a robust and efficient smart management information system smis framework for various applications especially for higher education institutions by exploring and integrating new trends as blockchain bc internet of things iot enterprise resource planning erp context awareness ca and cloud computing cc designed to meet the demands of modern education the proposed framework ensures a cohesive and secure system addressing challenges faced by traditional systems by combining erp for unified data management blockchain for security iot for realtime student data contextawareness for sensing and interpreting environmental changes and cloud computing for scalability the paper envisions an efficient educational ecosystem the framework envisions a wellintegrated mis that leverages the strengths of those new trends to enhance security efficiency collaborative learning and decisionmaking capabilities offering personalized paths for learners realtime insights for educators operational streamlining through automation and trustbuilding with tamperproof records the specific implementation details would depend on the unique requirements of the organization and the use case there is a research gap concerning the influence of technologies on the implementation process and its outcomes the findings indicate that sensors and internetconnected devices can autonomously handle the processed data stored in the cloud via erp eliminating the need for human intervention
autosuggestion is a cognitive process where the inner repetition of a thought actively influences one’s own perceptual state in spite of its potential benefits for medical interventions this technique has gained little scientific attention so far here we took advantage of the known link between intensity and frequency perception in touch ‘békésy effect’ in three separate experiments participants were asked to modulate the perceived intensity of vibrotactile stimuli at the fingertip through the inner reiteration of the thought that this perception feels very strong experiment 1 n  19 or very weak experiments 2 n  38 and 3 n  20 while they were asked to report the perceived frequency we show that the task to change the perceived intensity of a tactile stimulus via the inner reiteration of a thought modulates tactile frequency perception this constitutes the first experimental demonstration that an experimental design that triggers autosuggestion alters participants’ tactile perception using a response orthogonal to the suggested variable we discuss whether this cognitive process could be used to influence the perception of pain in a clinical context
to ensure global food security crop breeders conduct extensive trials across various locations to discover new crop varieties that grow more robustly have higher yields and are resilient to local stress factors these trials consist of thousands of plots each containing a unique crop variety monitored at intervals during the growing season requiring considerable manual effort in this study we combined satellite imagery and deep learning techniques to automatically collect plotlevel phenotypes from plant breeding trials in south australia and sonora mexico we implemented two novel methods utilising stateoftheart computer vision architectures to predict plotlevel phenotypes flowering canopy cover greenness height biomass and normalised difference vegetation index ndvi the first approach uses a classification model to predict for just the centred plot the second approach predicts perpixel and then aggregates predictions to determine a value perplot using a modified resnet18 model to predict the centred plot was found to be the most effective method these results highlight the exciting potential for improving crop trials with remote sensing and machine learning
smartphone consumers use various applications apps including online gaming chat streaming video calling and social networking the smartphone relies on the network backhaul such as a wifi access point to provide the required quality of service qos in its sendandreceive queues the smartphone processes the packets in a firstinfirstout fifo fashion many people worldwide started using video calling apps daily during the pandemic and postpandemic periods on the other hand online gaming apps skyrocketed and continue to engage people when realtime rt video calling and gaming apps race with nonrealtime nrt traffic we found a severe degradation in the quality of experience qoe in this work we propose an application prioritization engine ape framework that will improve user experience by dynamically allocating bandwidth to the different apps in the smartphone ape helps improve the enduser experience by detecting and prioritizing realtime traffic over concurrent besteffort traffic we introduced an ebpf extended berkeley packet filter that can control the nrt traffic to the extent that it does not affect the rt traffic we evaluate the performance of ape with the topchart video calling and gaming apps in a liveair scenario ape enhances video calling performance in poor network conditions by improving the bit rate to 110 furthermore it provides a fourfold gaming latency reduction despite nrt traffic ape is a techtransferred appagnostic serverindependent solution enabled in the latest samsung flagship smartphones with android 13 os
this work focuses on the main challenges and problems in developing a virtual oceanic environment reproducing real experiments using unmanned surface vehicles usv digital twins we introduce the key features for building virtual worlds considering using reinforcement learning rl agents for autonomous navigation and control with this in mind the main problems concern the definition of the simulation equations physics and mathematics their effective implementation and how to include strategies for simulated control and perception sensors to be used with rl we present the modeling implementation steps and challenges required to create a functional digital twin based on a real robotic sailing vessel the application is immediate for developing navigation algorithms based on rl to be applied on real boats
this study proposes the control of a threelevel npc converter applied in a pvfc hybrid generation system based on discretetime integral sliding mode control dismc combined with the particle swarm optimization pso technique first the comprehensive depiction and modeling of the systems main components are initially presented then the controllers detailed design procedure is given  the sliding manifold is designed to have a fast dynamic response and its stability analysis is verified using the lyapunov direct method next the optimization procedure is introduced to calculate the optimal values of the dismc gains furthermore a power management strategy is examined within the proposed control system to maximize the utility of the power produced by the hybrid system the control is done through the designed psodismc to allow decoupled control of the active and reactive powers in two distinct modes of operation the feederflow control ffc and the unitpower control upc modes the simulation of the approach is conducted in matlabsimulink and the findings demonstrate the effectiveness and robustness of the proposed control strategy
the increasing interest in filter pruning of convolutional neural networks stems from its inherent ability to effectively compress and accelerate these networks currently filter pruning is mainly divided into two schools normbased and relationbased these methods aim to selectively remove the least important filters according to predefined rules however the limitations of these methods lie in the inadequate consideration of filter diversity and the impact of batch normalization bn layers on the input of the next layer which may lead to performance degradation to address the above limitations of normbased and similaritybased methods this study conducts empirical analyses to reveal their drawbacks and subsequently introduces a groundbreaking complex hybrid weighted pruning method by evaluating the correlations and norms between individual filters as well as the parameters of the bn layer our method effectively identifies and prunes the most redundant filters in a robust manner thereby avoiding significant decreases in network performance we conducted comprehensive and direct pruning experiments on different depths of resnet using publicly available image classification datasets imagenet and cifar10 the results demonstrate the significant efficacy of our approach in particular when applied to the resnet50 on the imagenet dataset achieves a significant reduction of 535 in floatingpoint operations with a performance loss of only 06
as artificial intelligence aienabled wireless communication systems continue their evolution distributed learning has gained widespread attention for its ability to offer enhanced data privacy protection improved resource utilization and enhanced fault tolerance within wireless communication applications federated learning further enhances the ability of resource coordination and model generalization across nodes based on the above foundation enabling the realization of an aidriven communication and computing integrated wireless network this paper proposes a novel wireless communication system to cater to a personalized service needs of both privacysensitive and privacyinsensitive users we design the system based on based on multiagent federated weighting deep reinforcement learning mafwdrl the system while fulfilling service requirements for users facilitates realtime optimization of local communication resources allocation and concurrent decisionmaking concerning computing resources additionally exploration noise is incorporated to enhance the exploration process of offpolicy deep reinforcement learning drl for wireless channels federated weighting fedwgt effectively compensates for heterogeneous differences in channel status between communication nodes extensive simulation experiments demonstrate that the proposed scheme outperforms baseline methods significantly in terms of throughput calculation latency and energy consumption improvement
sensory processing disorder spd is a clinical condition characterized by difficulties in the neurological processes of registering discriminating organizing and responding to various sensory sensations this study aimed to review the association between impaired white matter wm tract structure and neurofunctional deficits in children with spd using diffusion tensor imaging dti a comprehensive literature search was conducted using the online databases google scholar and pubmed from 2010 to july 2023 resulting in the selection of nine relevant studies findings revealed that the splenium of the corpus callosum scc superior longitudinal fasciculus slf posterior corona radiata pcr and posterior thalamic radiation ptr exhibited reduced microstructural integrity strongly associated with spd specifically auditory over‐responsivity a subtype of spd was linked to impaired integrity of the pcr ptr anterior corona radiata and slf tactile over‐responsivity tor was correlated with markers of decreased integrity in the scc superior corona radiata and left ptr among the dti parameters decreased fractional anisotropy fa emerged as the most reliable factor for identifying spd followed by increased radial diffusivity rd and mean diffusivity md notably significant correlations were observed between with auditory over‐responsivity and tor with the dti parameters positive for fa and negative for rd and md overall this review confirms the impaired integrity of specific wm tracts in children with spd and establishes correlations between dti parameters and neurobehavioral deficits associated with the disorder the insights gained from this review contribute to a better understanding of spd and hold clinical implications for its diagnosis and treatment
the limited modulation bandwidth of the light emitting diodes leds presents a challenge in the development of practical highdatarate visible light communication vlc systems in this paper a novel adaptive coded probabilistic shaping psbased nonorthogonal multiple access noma scheme is proposed to improve spectral efficiency se of vlc systems in multiuser uplink communication scenarios the proposed scheme adapts its rate to the optical signaltonoise ratio osnr by utilizing nonuniformly distributed discrete constellation symbols and low complexity channel encoder furthermore an alternate optimization algorithm is proposed to determine the optimal channel coding rate constellation spacing and probability mass function pmf of each user the extensive numerical results show that the proposed psbased noma scheme closely approaches the capacity of noma with fine granularity presented results demonstrate the effectiveness of our scheme in improving the se of vlc systems in multiuser scenarios for instance our scheme exhibits substantial se gains over existing schemes namely the pairwise coded modulation pcm geometric shaping gs and uniformdistribution schemes these findings highlight the potential of our approach to significantly enhance vlc systems
purposein recent years there has been a growing interest in the potential of data analytics to enhance project delivery yet many argue that its application in projects is still lagging behind other disciplines this paper aims to provide a review of the current use of data analytics in project delivery encompassing both academic research and practice to accelerate current understanding and use this to formulate questions and goals for future researchdesignmethodologyapproachwe propose to achieve the research aim through the creation of a systematic review of the status of data analytics in project delivery fusing the methodology of integrative literature review with a recently established practice to include both white and grey literature amounts to an approach tailored to the state of the domain it serves to delineate a research agenda informed by current developments in both academic research and industrial practicefindingsthe literature review reveals a dearth of work in both academic research and practice relating to data analytics in project delivery and characterises this situation as having “more gap than knowledge” some work does exist in the application of machine learning to predicting project delivery though this is restricted to disparate single context studies that do not reach extendible findings on algorithm selection or key predictive characteristics grey literature addresses the potential benefits of data analytics in project delivery but in a manner reliant on “thoughtexperiments” and devoid of empirical examplesoriginalityvaluebased on the review we articulate a research agenda to create knowledge fundamental to the effective use of data analytics in project delivery this is structured around the functional framework devised by this investigation and highlights both organisational and data analytic challenges specifically we express this structure in the form of an “onionskin” model for conceptual structuring of data analytics in projects we conclude with a discussion about if and how today’s project studies research community can respond to the totality of these challenges this paper provides a blueprint for a bridge connecting data analytics and project management
the performance state evaluation method of circuit breaker energy storage spring mainly judges its performance state indirectly by measuring the pretightening force or prepressure of the spring however there may be some errors in this indirect measurement method which will affect the accuracy of the evaluation results therefore the performance state evaluation based on intelligent algorithm is proposed select the evaluation characteristic quantity of performance state calculate the energy storage spring impulse according to the momentum theorem and obtain the pressure value of the closing energy storage spring through the pressure sensor as the evaluation quantity reflecting the energy storage spring performance state the bp neural network is established and the fireworks algorithm is applied to the bp neural network to optimize the initial weight and threshold so as to realize the performance state evaluation of energy storage spring based on bp neural network the experimental results show that the spring energy release speed of the proposed method is in the range of 0  10ms and the estimated spring pressure value is basically consistent with the actual value
promoting green innovative development is a key path to promote the transformation and upgrading of highemission and highenergy
this paper examines the quantization methods used in largescale data analysis models and their hyperparameter choices the recent surge in data analysis scale has significantly increased computational resource requirements to address this quantizing model weights has become a prevalent practice in data analysis applications such as deep learning quantization is particularly vital for deploying large models on devices with limited computational resources however the selection of quantization hyperparameters like the number of bits and value range for weight quantization remains an underexplored area in this study we employ the typical case analysis from statistical physics specifically the replica method to explore the impact of hyperparameters on the quantization of simple learning models our analysis yields three key findings i an unstable hyperparameter phase known as replica symmetry breaking occurs with a small number of bits and a large quantization width ii there is an optimal quantization width that minimizes error and iii quantization delays the onset of overparameterization which mitigate overfitting as indicated by the double descent phenomenon we also discover that nonuniform quantization can enhance stability additionally we develop an approximate messagepassing algorithm to validate our theoretical results
in contemporary wireless communication systems multicarrier modulation schemes have become widely adopted over singlecarrier techniques due to their improved capacity to address challenges posed by multipath fading channels leading to enhanced spectral efficiency orthogonal frequency division multiplexing ofdm a prevalent multicarrier scheme in 4g is favored for its ease of implementation interference resilience and high data rate provision but it falls short of meeting the requirements for 5g and beyond due to limitations such as outofband oob emissions and cyclic prefixes this paper introduces the filter bank multicarrier fbmc and universal filtered multicarrier ufmc with quadrature amplitude modulation qam and phase shift keying psk waveforms through additive white gaussian noise channel awgn rayleigh fading channel and rician channel the objective of this paper is to enhance the performance of ufmc with reduced complexity through the new filtering approach for achieving optimal outcomes the proposed scheme incorporating tukey filtering technique demonstrates superior performance in reducing peaktoaverage power ratio papr and improving bit error ratio ber compared to the original ufmc signal without necessitating additional power increments specifically the ufmc system with tukey filtering achieves a notable net gain of 5 db simulation results demonstrate that utilizing various filter types in fbmc and ufmc systems combined with qam modulation significantly reduces oob emissions compared to conventional systems in aspect to ber tukey window showed almost 10−6 at 15 db snr in ufmc which is better than fbmc
currently china is using the sf6n2 gas mixture to gradually replace sf6 in gis and other equipment however gasinsulated equipment will inevitably have local overheating faults the sf6n2 gas mixture will decompose to some extent at high temperatures impurities such as moisture in the equipment can affect this decomposition process at present there are fewer studies on the moisture content of the superheated decomposition of the sf6n2 gas mixture therefore this paper carries out the sf6n2 mixed gas superheat decomposition experiment on the constructed superheat decomposition simulation experiment platform by changing the content of trace water the influence of trace water on the decomposition of the sf6n2 mixed gas insulation medium with superheat failure was initially investigated it is found that trace h2o will promote the generation of sf6 characteristic decomposition products especially for the generation of so2f2 the generation of so2 in the product is the largest and its generation process needs the full participation of h2o in addition the addition of trace water will also improve the yield of nitrogencontaining products no and no2 and under the experimental conditions of this paper by adding the microwater the yield reaches 100–300 µll the reaction mechanism of h2o decomposition and combining with n atoms to generate no and no2 at high temperatures was analyzed in this paper the thermodynamic properties of the main reaction paths and the equilibrium constants were calculated based on the density functional theory which provided theoretical references for the further study of the mechanism of sf6n2 superheated decomposition
starting from the perspective of adolescents this study selects the maritime silk road art park in quanzhou fujian province as the focal point for investigating environmental education requirements for urban theme parks it comprehensively reviews existing literature and research findings establishes evaluation indices for environmental education requirements and systematically analyzes collected data through questionnaires and interviews the study employs the kano model to initiate a survey focusing on requirement types and importance ranking at the maritime silk road art park it aims to identify improvement factors and key factors subsequently conducting a detailed analysis summary and explanation of the environmental education requirements for the youth the results indicate that 11 out of the 25 requirement factors categorized into five groups significantly impact youth satisfaction based on the sensitivity ranking of improvement factors these include the following environmental education game landscape facility leisure and recreation facility plant landscape planning “five senses experience” activity trail route design guided signage facility public sanitation facility facility maintenance management park functional zoning and consultancy services platform through an indepth analysis of the five prevalent factors influencing environmental education requirements in urban theme parks for adolescents this study establishes a scientific evaluation system centered on the construction of urban theme parks it integrates with the development and construction of the parks proposing innovative and constructive suggestions based on a summary of the analysis results the aim is to provide references and insights for similar requirements in other theme parks
highrise building machines hbms play a critical role in the successful construction of superhigh skyscrapers providing essential support and ensuring safety the hbm’s climbing system relies on a jacking mechanism consisting of several independent jacking cylinders a reliable control system is imperative to maintain the smooth posture of the construction steel platform sp under the action of the jacking mechanism long shortterm memory lstm gated recurrent unit gru and temporal convolutional network tcn are three multivariate time series mts neural network models that are used in this study to predict the posture of hbms the models take pressure and stroke measurements from the jacking cylinders as inputs and their outputs determine the levelness of the sp and the posture of the hbm at various climbing stages the development and training of these neural networks are based on historical onsite data with the predictions subjected to thorough comparative analysis the proposed lstm and gru prediction models have similar performances in the prediction process of hbm posture with medians r2 of 0903 and 0871 respectively however the median mae of the gru prediction model is more petite at 04 which exhibits stronger robustness additionally sensitivity analysis showed that the change in the levelness of the position of the sp portion of the hbm exhibited high sensitivity to the stroke and pressure of the jacking cylinder which clarified the position of the cylinder for adjusting the posture of the hbm the results show that the mts neural networkbased prediction model can change the hbm posture and improve work stability by adjusting the jacking cylinder pressure value of the hbm
wireless power transfer wpt technology has become increasingly popular due to its ability to transmit energy without mechanical contact making it suitable for various industrial fields this article deals with an inductive power transfer system which is driven with a singleswitch inverter such as class e in several megahertz or higher frequencies one of main issues with such systems is that the output and efficiency are significantly affected by the relative position of transmitter and receiver coils many existing solutions require additional tunable components and driving circuits leading to increased cost and complexity this article provides a systematic optimization procedure of twoport impedance matching networks for wireless power transfer systems to achieve constant output power characteristics even with misalignment or airgap variation since the system itself has a characteristic that is robust against misalignment the need for any control and tunable component can be significantly reduced the effectiveness of the proposed method is verified through a prototype of 678 mhz60 w and experimental results
achieving the un sustainable development goals is impossible without sustainable oil and gas production both in terms of ensuring equal access to cheap energy and preserving the environment as well as caring for nonrenewable fossil energy sources this actualizes the need for the digital 3d modeling of wells which allows one to assess their current condition and predict their future condition as well as determine the feasibility of investing in their reconstruction and the expansion of the well network this is possible due to the fact that the visualization of a well reflecting its physical and technical parameters gives engineers designers and ecologists the opportunity to recognize the defects of the well cracks cavities behind casing and interlayer leaks etc and predict their appearance and proliferation on which the service life hydrocarbon flow rate pollution operating and capital costs directly depend the software required for this must be versatile enough to cover different types of logging and fit different operating systems the goal of the study is to develop a software for creating visual 3d models of wells based on acoustic and various types of radioactive logging data capable of integrating drilling rig parameters and geophysical survey data with modern 3d modeling and programming methods the developed software meets the requirements of various operating systems and the specifics of different types of logging which is designed to help in increasing the productivity of oil and gas wells save energy consumption and reduce groundwater pollution from chemicals used in hydrocarbon production this is achievable by ensuring troublefree execution and the operation of well systems minimizing the risks of collapses and the destruction of well walls through accurate monitoring and forecasting their dynamic condition in real time using 3d models which is not available for static 2d models
dynamic wireless charging dwc has emerged as a viable approach to mitigate range anxiety by ensuring continuous and uninterrupted charging for electric vehicles in motion dwc systems rely on the length of the transmitter which can be categorized into longtrack transmitters and segmented coil arrays the segmented coil array favored for its heightened efficiency and reduced electromagnetic interference stands out as the preferred option however in such dwc systems the need arises to detect the vehicle’s position specifically to activate the transmitter coils aligned with the receiver pad and deenergize uncoupled transmitter coils this paper introduces various machine learning algorithms for precise vehicle position determination accommodating diverse ground clearances of electric vehicles and various speeds through testing eight different machine learning algorithms and comparing the results the random forest algorithm emerged as superior displaying the lowest error in predicting the actual position
studies have repeatedly shown sex differences in some areas of language development typically with an advantage for female over male children however the tested samples are typically small and the effects do not always replicate here we used a metaanalytic approach to address this issue in a larger sample combining seven fnirs studies on the neural correlates of repetition and nonrepetitionbased rule learning in newborns and 6monthold infants the ability to extract structural regularities from the speech input is fundamental for language development it is therefore highly relevant to understand whether this ability shows sex differences the metaanalysis tested the effect of sex as well as of other moderators on infants’ hemodynamic responses to repetitionbased eg abb “mubaba” and nonrepetitionbased eg abc “mubage” sequences in both anatomically and functionally defined regions of interests our analyses did not reveal any sex differences at birth or at 6 months suggesting that the ability to encode these regularities is robust across sexes interestingly the metaanalysis revealed other moderator effects thus in newborns we found a greater involvement of the bilateral temporal areas compared to the frontal areas for both repetition and nonrepetition sequences further nonrepetition sequences elicited greater responses in 6montholds than in newborns especially in the bilateral frontal areas when analyzing functional clusters of hbr timetraces we found that a larger rightleft asymmetry for newborn boys in brain responses compared to girls which may be interpreted in terms of a larger rightleft asymmetry in cerebral blood flow in boys than in girls early in life we conclude that extracting repetitionbased regularities from speech is a robust ability with a welldefined neural substrate present from birth and it does not exhibit sex differences
epinephelus awoara as known as yellow grouper is a significant economic marine fish that has been bred artificially in china however the genetic structure and evolutionary history of yellow grouper remains largely unknown here this work presents the highquality chromosomelevel genome assembly of yellow grouper using pacbio single molecule sequencing technique smrt and highthrough chromosome conformation capture hic technologies the 98448 mb chromosomelevel genome of yellow grouper was assembled with a contig n50 length of 3977 mb and scaffold n50 length of 4139 mb approximately 9976 of assembled sequences were anchored into 24 pseudochromosomes with the assistance of hic reads furthermore approximately 4117 of the genome was composed of repetitive elements in total 24541 proteincoding genes were predicted of which 22509 9172 genes were functionally annotated the highly accurate chromosomelevel reference genome assembly and annotation are crucial to the understanding of population genetic structure adaptive evolution and speciation of the yellow grouper
this study introduces a direct power control dpc strategy for voltagesource converters vscs connected to the grid specifically when the network voltage is unbalanced the dpc strategy is compared to a voltageoriented control vocbased pi controller strategy the proposed method comprises a positive and negative sequence controller which adjusts the output active and reactive power and removes the ripple of currents respectively the objective of the control is to eliminate negative current sequence components by regulating the instantaneous power flow while also reducing output current harmonics and power ripples the mathematical model of the threephase acdc vsc under unbalanced working principles and control strategies is discussed indepth a simulation is conducted using the matlabsimulink platform to demonstrate the effectiveness of the proposed method
systematic reviews are cumbersome yet essential to the epistemic process of medical science finding significant reports however is a daunting task because the sheer volume of published literature makes the manual screening of databases timeconsuming the use of artificial intelligence could make literature processing faster and more efficient sentence transformers are groundbreaking algorithms that can generate rich semantic representations of text documents and allow for semantic queries in the present report we compared four freely available sentence transformer pretrained models allminilml6v2 allminilml12v2 allmpnetbasev2 and alldistilrobertav1 on a convenience sample of 6110 articles from a published systematic review the authors of this review manually screened the dataset and identified 24 target articles that addressed the focused questions fq of the review we applied the four sentence transformers to the dataset and using the fq as a query performed a semantic similarity search on the dataset the models identified similarities between the fq and the target articles to a varying degree and sorting the dataset by semantic similarities using the bestperforming model allmpnetbasev2 the target articles could be found in the top 700 papers out of the 6110 dataset our data indicate that the choice of an appropriate pretrained model could remarkably reduce the number of articles to screen and the time to completion for systematic reviews
introduction cannabis consumption is known to immediately affect ocular and oculomotor function however cannabis consumption is also known to affect it for a prolonged period of time the purpose of this study is to identify an eye tracking or pupillometry metric which is affected after recent cannabis consumption but is not confounded by cannabis consumption history or demographic variables methods quasiexperimental design participants who would consume inhalable cannabis n  159 mean age 310 years 54 male performed baseline neurobehavioral testing and eyefunction assessments when they were sober eye function assessments included eyetracking gaze point of visual focus saccades smooth movement and pupillometry participants then inhaled cannabis until they selfreported to be high and performed the same assessment again controls who were cannabis naïve or infrequent users n  30 mean age 326 years 57 male performed the same assessments without consuming cannabis in between results cannabis significantly affected several metrics of pupil dynamics and gaze pupil size variability was the most discriminant variable after cannabis consumption this variable did not change in controls on repeat assessment ie no learning effect did not correlate with age gender raceethnicity or selfreported level of euphoria but did correlate with thc concentration of cannabis inhaled discussion a novel eyetracking metric was identified that is affected by recent cannabis consumption and is not different from nonusers at baseline a future study that assesses pupil size variability at multiple intervals over several hours and quantifies cannabis metabolites in biofluids should be performed to identify when this variable normalizes after consumption and if it correlates with blood thc levels
the embedding of a dnn model on the fpga provides users with significant computing acceleration however the intellectual property ip of these models is at risk of being stolen since they become publicly accessible once the fpga is delivered traditional cryptographic methods are unsuitable for protecting dnn models due to computational cost additional hardware cost or the risk of secret key and parameter leakage in this brief we propose an intellectual property protection scheme for dnn models based on physical unclonable function puf which permits only authorized fpga boards to recover the original model and produce accurate results we developed a prototype to protect the lenet model using arbiter puf apuf compared with the original model the accuracy of the obfuscated model remains unchanged and the additional latency accounts for 16 of the inference latency for the 3rd and 5th convolutional layers in alexnet the luts required to be obfuscated account for 033 and 034 of the original convolutional layers respectively the experimental results demonstrate that the proposed scheme effectively protects dnn models even if an adversary can guess 106 bits of the 128bit puf response the recovered dnn models including alexnet densenet and resnet fail to operate correctly our project is opensourced on httpsgithubcomrenaturationdnnpuffpga
"in this singleblind randomized controlled trial we tested the hypotheses that in comparison with control participants receiving only selfstudy materials ss group caregivers of manual wheelchair users who additionally receive remote training rt group have greater total wheelchair skills test questionnaire wstq performance and confidence scores posttraining and at followup and that selfstudy and remote training each individually lead to such gains we studied 23 dyads of wheelchair users and their caregivers caregivers in the ss group received a handbook and videorecording those in the rt group also received up to four realtime synchronous sessions remotely the wstq 51 was administered pretraining t1 posttraining t2 and after a 3month followup t3 the mean total wstq scores of both groups rose slightly at each new assessment for the t2t1 and t3t1 gains there were no statistically significant differences between the groups for either wstq performance or wstq confidence for performance the t2t1 gain was statistically significant for the rt group and the t3t2 gain was statistically significant for the ss group for both groups the t3t1 gains in performance were statistically significant with gains of 129 and 185 relative to baseline for the ss and rt groups for confidence only the t3t1 gain for the ss group was statistically significant with a gain of 45 relative to baseline although less than the gains previously reported for inperson training modest but important gains in total wstq performance scores can be achieved by selfstudy with or without remote training


registration number
nct03856749"
agriculture can be defined as the systematic and intentional practice of cultivating and managing plants and animals to produce food fiber and other agricultural products agricultural practices in india hold the second position globally and encompass approximately 611 of the total land area in the country the indian economy primarily relies on agriculture and agroindustrial products various factors such as soil composition including elements like nitrogen phosphorus and potassium crop rotation practices soil moisture content ambient temperatures precipitation patterns and other relevant variables can significantly influence crop productivity smart agriculture sa implementation has recently yielded significant practical benefits establishing it as a highly significant and valuable system using environmental information including wind velocity temperature and moisture in outdoor plantations facilitates farming operations’ strategic management and regulation enhancing crop yield and quality accurately predicting crop yield trends poses a challenge due to the intricate nature of sensing data characterized by complexity nonlinearity and multiple variables this study proposes a hybrid deep learning model for predicting crop yields hdlpcy using the internet of things iot the hdlpcy system utilizes the empirical mode decomposition emd technique to break down the crop yield information into distinct element groups with varying frequency attributes subsequently a long shortterm memory lstm network is trained for each group to serve as a subpredictor finally the predictions generated by the lstm networks are combined to produce the overall prediction result the obtained results demonstrate that the proposed hdlpcy can achieve higher levels of accuracy of 9732 9803 9874 and 9592 for precipitation temperature ph and moisture content respectively thereby catering to the requirements of sa
sequential optimization methods are often confronted with the curse of dimensionality in highdimensional spaces current approaches under the gaussian process framework are still burdened by the computational complexity of tracking gaussian process posteriors and need to partition the optimization problem into small regions to ensure exploration or assume an underlying lowdimensional structure with the idea of transiting the candidate points towards more promising positions we propose a new method based on markov chain monte carlo to efficiently sample from an approximated posterior we provide theoretical guarantees of its convergence in the gaussian process thompson sampling setting we also show experimentally that both the metropolishastings and the langevin dynamics version of our algorithm outperform stateoftheart methods in highdimensional sequential optimization and reinforcement learning benchmarks
this study aims to examine the intention of youth in indonesia to adopt central bank digital currency cbdc a sample of 285 respondents was analyzed using sempls and indepth interviews the results of the study found that involvement facilitation institutional trust and technology trust had a significant effect on youths intention to use cbdc these findings emphasize the importance of building trust in technology and institutions as well as increasing public confidence in the ability of central banks to launch and regulate cbdc systems empirical evidence also shows that facilitation and literacy provided by the authorities need to be a significant focus in addressing potential barriers to technology use among the public furthermore there are maximum literacy facilities and consultation given can increase public confidence in cbdc provider authorities to continue to develop technology to the fullest
thousands of the worlds languages are in danger of extinctiona tremendous threat to cultural identities and human language diversity interlinear glossed text igt is a form of linguistic annotation that can support documentation and resource creation for these languages communities igt typically consists of 1 transcriptions 2 morphological segmentation 3 glosses and 4 free translations to a majority language we propose wav2gloss a task in which these four annotation components are extracted automatically from speech and introduce the first dataset to this end fieldwork a corpus of speech with all these annotations derived from the work of field linguists covering 37 languages with standard formatting and traindevtest splits we provide various baselines to lay the groundwork for future research on igt generation from speech such as endtoend versus cascaded monolingual versus multilingual and singletask versus multitask approaches
hybrid teaching synchronous online and onsite teaching offers many advantages eg increased flexibility however previous research has suggested that students who join classes online suffer higher levels of distractibility which might translate into students engaging in more offtask activities this in turn can impair students’ learning performance the following quasiexperimental field study investigated this specific link between teaching mode engagement in offtask activities and learning performance we collected survey data from n  690 students in six hybrid classes n  254 online n  436 onsite participants reported the amount of time they spent engaging in digital and nondigital offtask activities and responded to a quiz on the course material results revealed that online students spent more time engaging in offtask activities than onsite students further results were consistent with our hypothesis that joining the class online is associated with lower learning performance via time spent on digital offtask activities
in the wake of the surging tide of deep learning over the past decade automatic speech recognition asr has garnered substantial attention leading to the emergence of numerous publicly accessible asr systems that are actively being integrated into our daily lives nonetheless the impartial and replicable evaluation of these asr systems encounters challenges due to various crucial subtleties in this paper we introduce the speechcolab leaderboard a generalpurpose opensource platform designed for asr evaluation with this platform i we report a comprehensive benchmark unveiling the current stateoftheart panorama for asr systems covering both opensource models and industrial commercial services ii we quantize how distinct nuances in the scoring pipeline influence the final benchmark outcomes these include nuances related to capitalization punctuation interjection contraction synonym usage compound words etc these issues have gained prominence in the context of the transition towards an endtoend future iii we propose a practical modification to the conventional tokenerrorrate ter evaluation metric with inspirations from kolmogorov complexity and normalized information distance nid this adaptation called modifiedter mter achieves proper normalization and symmetrical treatment of reference and hypothesis by leveraging this platform as a largescale testing ground this study demonstrates the robustness and backward compatibility of mter when compared to ter the speechcolab leaderboard is accessible at httpsgithubcomspeechcolableaderboard
in microenterprises financial decisionmaking is typically concentrated in the hands of one person therefore the competences of the leading financial decisionmaker significantly determine the financial success of the enterprise in our pilot study we examined the impact of three competences financial literacy digital and entrepreneurial competences on financial success among microenterprises in the southern great plain region in a 92item sample using pls path analysis the results showed that the most important factors in financial decisionmaking are the behaviour and acquired practices of the decisionmakers and we proved that the combination of the three competences has a positive effect on the financial success of enterprises
the structural design is a prerequisite for the creation of mechanical equipment this study focuses on the structural synthesis of epicyclic chains ecs and proposes a new topological model and synthesis method of ecs to address the existing research problems ie synthesis dependence on multiple graphic transformations and the generation of pseudoisomorphism first the structural characteristics of ecs are analyzed to explore the relation between their axles gears and planet carriers and the corresponding topological model and the adjacency matrix expression of ecs are provided then the topological elements corresponding to the axles gears and gear pairs are added sequentially to the topological graph corresponding to the planet carriers and the constraints are established based on similarity to inhibit the generation of isomorphism finally all the databases of feasible ecs are obtained through gear loop discrimination all processes are carried out with computer assistance the synthesis results of the ecs with 1–3 degrees of freedom and up to 10 components are presented the logic of this paper is to simplify or optimize the synthesis process to improve efficiency of structural synthesis which will break through the threshold of computing power and provide more possibilities for the design of mechanical equipment
models of software systems are used throughout the software development lifecycle dataflow diagrams dfds in particular are wellestablished resources for security analysis many techniques such as threat modelling are based on dfds of the analysed application however their impact on the performance of analysts in a security analysis setting has not been explored before in this paper we present the findings of an empirical experiment conducted to investigate this effect following a withingroups design participants were asked to solve securityrelevant tasks for a given microservice application in the control condition the participants had to examine the source code manually in the modelsupported condition they were additionally provided a dfd of the analysed application and traceability information linking model items to artefacts in source code we found that the participants n  24 performed significantly better in answering the analysis tasks correctly in the modelsupported condition 41  increase in analysis correctness further participants who reported using the provided traceability information performed better in giving evidence for their answers 315 increase in correctness of evidence finally we identified three open challenges of using dfds for security analysis based on the insights gained in the experiment
communication infrastructure is often severely disrupted in postdisaster areas which interrupts communications and impedes rescue recently the technology of reconfigurable intelligent surface risequippeduav has been investigated as a feasible approach to assist communication under such conditions however the channel characteristics in the postdisaster area rapidly change due to the topographical changes caused by secondary disasters and the high mobility of uavs in this paper we develop a new fadingshadowing model to fit the path loss caused by the debris following this we derive the exact distribution of the new channel statistics for a small number of ris elements and the approximate distribution for a large number of ris elements respectively then we derive the closedform expressions for performance analysis including average capacity ac energy efficiency ee and outage probability op based on the above analytical derivations we maximize the energy efficiency by optimizing the number of ris elements and the coverage area by optimizing the altitude of the risequipped uav respectively finally simulation results validate the accuracy of derived expressions and show insights related to the optimal number of ris elements and the optimal uav altitude for emergency wireless communication ewc
arguably geodesics are the most important geometric objects on a differentiable manifold they describe candidates for shortest paths and are guaranteed to be unique shortest paths when the starting velocity stays within the socalled injectivity radius of the manifold in this work we investigate the injectivity radius of the stiefel manifold under the canonical metric the stiefel manifold stnp is the set of rectangular matrices of dimension nbyp with orthogonal columns sometimes also called the space of orthogonal pframes in mathbbrn using a standard curvature argument rentmeesters has shown in 2013 that the injectivity radius of the stiefel manifold is bounded by sqrtfrac45pi it is an open question whether this bound is sharp with the definition of the injectivity radius via cut points of geodesics we gain access to the information of the injectivity radius by investigating geodesics more precisely we consider the behavior of special variations of geodesics called jacobi fields by doing so we are able to present an explicit example of a cut point in addition since the theoretical analysis of geodesics for cut points and especially conjugate points as a type of cut points is difficult we investigate the question of the sharpness of the bound by means of numerical experiments
out of all natural catastrophes floods are one of the most destructive and common occurrences as it affects human lives economy and environment pakistan is prone to flooding because it is greatly affected by climate change mianwali region pakistan was selected as study area because this region is vulnerable to floods and have been affected by major floods in pakistan in 2010 and no proper study on vulnerability assessment and flood extent mapping has been done in this area the current study was carried out to evaluate flood extent mapping of the mianwali region by using modis and ndwi techniques and to develop a cumulative flood risk map for 10 years 2010–2020 for the flood extent mapping modis product “modis combined 16day ndwi” was used this product is generated from the modismcd43a4 surface reflectance composites and provides a single ndwi value for 16 daily composite images earth data catalogue the flood extent maps of years 2010 2011 2012 2013 2014 2015 2016 2017 2018 2019 and 2020 were developed these flood extent maps demonstrated mianwali villages of high flood risk zones and shallow flood risk zones results revealed that the people living in villages named dhup sari sheikhan wala arsala wala moza zareef wala allah khel wala and janay khel wala are at a greater risk of experiencing natural disasters to their geographical location as they are located alongside river indus due to the cloud coverage increase and being on the bank of river indus mianwali is hit by floods almost every year lastly a cumulative flood risk map was produced by combining the all extents identified during the flooding events 2010–20 the cumulative flood risk zones are classified as high risk zones and mixed zones based on the ndwi values results clearly suggest that 7 out of 20 the communities in the study area are situated in the high risk flood zone and whereas the remaining 13 communities lies in the mixed zone which could be an area with very high soil moisture due to shallow flooding the apparent land cover of the study area is dominated by agriculture upon which lively hood of the communities depend it can be concluded that mianwali is at high danger of flooding due to its location and lack of predisaster preparedness the data obtained from this study is critical for emergency response and rehabilitation efforts the findings can assist local disaster organizations planners researchers line agencies and local government in managing flood hazards in the area
this paper studies an extremely largescale reconfigurable intelligent surface xlris empowered covert communication system in the nearfield region alice covertly transmits messages to bob with the assistance of the xlris while evading detection by willie to enhance the covert communication performance we maximize the achievable covert rate by jointly optimizing the hybrid analog and digital beamformers at alice as well as the reflection coefficient matrix at the xlris an alternating optimization algorithm is proposed to solve the joint beamforming design problem for the hybrid beamformer design a semiclosedform solution for fully digital beamformer is first obtained by a weighted minimum meansquare error based algorithm then the baseband digital and analog beamformers at alice are designed by approximating the fully digital beamformer via manifold optimization for the xlris’s reflection coefficient matrix design a lowcomplexity alternating direction method of multipliers based algorithm is proposed to address the challenge of largescale variables and unitmodulus constraints numerical results unveil that i the nearfield communications can achieve a higher covert rate than the farfield covert communications in general and still realize covert transmission even if willie is located at the same direction as bob and closer to the xlris ii the proposed algorithm can enhance the covert rate significantly compared to the benchmark schemes iii the proposed algorithm leads to a beam diffraction pattern that can bypass willie and achieve highrate covert transmission to bob
built‐up land changes along slope gradients bcsg are widespread globally driven by natural environmental and socio‐economic variations this process has profound effects on vegetation cover and is closely related to the united nations sustainable development goals however the mechanisms involved in the bcsg remain to be elucidated and the impacts of bcsg on vegetation cover need to be quantified the bcsg from 1990 to 2020 was described through slope spectrum analysis by taking the three gorges reservoir area tgra as an example then the normalized difference vegetation index ndvi was used as an indicator of vegetation cover and its influence by bcsg was explored through a multiple linear regression model the built‐up land in the tgra from 1990 to 2020 was first dominated by slope climbing sc and then by horizontal expansion and the intensity of built‐up land sc showed a general trend of weakening over time during the 30 years the intensity of built‐up land sc weakened and then strengthened in townships with mainly intermountain plains while gradually decreasing in townships with mainly hills and townships with mainly mountains socio‐economic factors natural factors and regional special events drive the bcsg in tgra built‐up land sc in the tgra will have a negative impact on ndvi but to a progressively lesser degree from 1990 through 2020 the built‐up land sc in townships with mainly hills and mountains has a negative effect on ndvi the built‐up land sc in townships with mainly intermountain plains has a positive impact on ndvi these results can provide a scientific basis for formulating reasonable vertical management policies for built‐up land and help reduce the ecological risks caused by land development
this paper presents a layered architecture qcstack for the quantum computing domain it is a comprehensive framework explaining quantum computing systems by identifying three fundamental layers the physical system and application layers each plays a crucial role in implementing quantum computing systems in practice the physical layer evaluates quantum hardware technologies based on crucial performance parameters such as qubit quality connectivity and scalability the system layer which includes quantum firmware compilers operating systems and software development tools bridges the gap between the physical and application layers the application layer incorporates quantum programming languages and algorithms emphasizing their significance in leveraging quantum computing potential across various domains the paper provides a comparative analysis at a higher level of existing quantum algorithms protocols and hardware and software technologies within each sublayer it also highlights the strengths and weaknesses of different technologies at each layer this holistic model can guide researchers and practitioners offering insights into the everevolving landscape of quantum technology
different from traditional sentiment analysis tasks aspectbased sentiment analysis absa aims to automatically identify the emotional polarity of different aspects in the same sentence which helps to mine users more delicate emotional expressions for different targets and has become a research hotspot in the field of natural language processing in recent years thanks to the rapid development of attentionbased deep neural network models the accuracy of aspect sentiment analysis has continuously made breakthroughs however existing works pay little attention to the performance bounds of different application scenarios such as different text topics or text lengths in response to the above issues this paper conducts a comparative analysis of several classic endtoend neural network models to explore topiclevel sentiment polarity analysis by quantitatively evaluating the accuracy and f1 score of each model at different fixed text lengths this paper summarizes the strengths weaknesses and different aspects of each models efficiency in use in addition this paper also discusses the existing problems with these models and puts forward suggestions for improvement which can provide some new insights into the research of absa
in this study welldefined tungsten oxide wo3 nanowall nw thin films were synthesized via a controlled hot filament chemical vapor deposition hfcvd technique and applied for electrochemical detection of methylamine toxic substances herein for the thinfilm growth by hfcvd the temperature of tungsten w wire was held constant at 1450 °c and gasification was performed by heating of w wire using varied substrate temperatures ranging from 350 °c to 450 °c at an optimized growth temperature of 400 °c welldefined and extremely dense wo3 nanowalllike structures were developed on a si substrate structural crystallographic and compositional characterizations confirmed that the deposited wo3 thin films possessed monoclinic crystal structures of high crystal quality for electrochemical sensing applications wo3 nw thin film was used as an electrode and cyclic voltammetry cv and linear sweep voltammetry lsv were measured with a wide concentration range of 20 μm1 mm of methylamine the fabricated electrochemical sensor achieved a sensitivity of 18365 μa mm−1 cm−2 a limit of detection lod of 20 μm and a quick response time of 10 s thus the fabricated electrochemical sensor exhibited promising detection of methylamine with considerable stability and reproducibility
in 2020 one news that shook the world was the murder of an afroamerican named george floyd by minneapolis police officers this study focuses on the media coverage of george floyd’s murder case by two news outlets cnn and aljazeera we used a corpusassisted critical discourse analysis cacda technique to compare how each news outlet portrayed the social actors involved in the incident and to identify if any social actors were excluded from the reporting eventually revealing which side each media was inclined to the analysis was based on theo van leeuwen’s theory and utilized the antconc application the data used in this study consists of ten news articles five from cnn and aljazeera published between may and july 2020 a literature study was conducted to collect the data the antconc corpusprocessing application was then used to process the large amount of data collected enabling quick extensive and comprehensive analysis we uploaded all ten articles to the antconc application and used the frequency feature to identify the five most used words in the ten news articles the top five words were further analyzed using van leeuwen’s critical discourse analysis excluding irrelevant particles the result shows that cnn aligned with the police and aljazeera aligned with floyd by the ideological stances of cnn and aljazeera readers can become more aware of potential biases in news articles concerning george floyd’s murder
many dna storage codes take into account homopolymer and gccontent constraints still these codes often need to meet additional practical database requirements such as error correction and data queries necessitating considerable financial and time investment in their training or design as dna storage technologies including sequencing and synthesis continue to evolve rapidly these codes may need to be retrained or redesigned to adapt to new constraints in this study we aim to design a method for adapting an existing dna storage code to satisfy a new constraint specifically concerning homopolymer variations we present a simple and effective framework known as transfer coding which directly maps dna sequences from an original homopolymer constraint inlineformula texmath notationlatexh1 texmathinlineformula to a new constraint inlineformula texmath notationlatexh2 texmathinlineformula this approach essentially combines the existing coding scheme with a transfer encoder the proposed method uses strategic base replacements to ensure compliance with constraints achieving results close to the theoretical limit while keeping alterations to the original sequence minimal
point cloud registration is a fundamental task in various intelligence applications including simultaneous localization and mapping as well as scene reconstruction however in largescale scenes the majority of point clouds exhibit partial overlap posing a significant challenge to the registration process this study introduces a registration network named okrnet specifically designed to efficiently align partially overlapping point clouds the okrnet comprises two innovative modules a joint estimation module adept at identifying the keypoints within the overlapping region and a coarsetofine registration module designed to aggregate the overlap and descriptor information thereby reducing the outliers and yielding robust corresponding point pairs in addition an overlap labeling method for generated keypoints is introduced the efficiency of the proposed registration network is validated utilizing two largescale outdoor datasets kitti and nuscenes the results demonstrate that the proposed method outperforms existing global registration methods encompassing both classical and learningbased methods in realworld scenarios
the current study examines the impact of integrating artificial intelligence ai in higher education in saudi arabia specifically focusing on the roles of university teachers and students’ learning outcomes it investigates the transformation of teaching practices and the challenges faced by university teachers during ai integration and identifies opportunities for collaboration and innovation a quantitative research method using a survey questionnaire was employed to gather data and inform practical recommendations the findings indicate that university teachers recognize the potential benefits of ai in education but also highlight the need to address challenges for effective implementation participants perceive ai as a tool that enables more creative and engaging teaching methods automates tasks and brings about changes in teaching approaches difficulties in utilizing ai effectively and adapting to new teaching methods were identified ai also offers opportunities for innovation collaboration and creating engaging learning environments there were slight variations in perspectives based on gender and current role with female university teachers and teaching assistantsprofessors perceiving higher impact and opportunities for collaboration and innovation participants with higher ai expertise perceived a greater impact of ai and fewer challenges those who aligned with the educational purposes of ai saw greater impact and opportunities for collaboration while acknowledging usage difficulties
because of the processing of continuous unstructured large streams of data mining realtime streaming data is a more challenging research issue than mining static data the privacy issue persists when sensitive data is included in streaming data in recent years there has been significant progress in research on the anonymization of static data for the anonymization of quasiidentifiers two typical strategies are generalization and suppression however the high dynamicity and potential infinite properties of the streaming data make it a challenging task to end this we propose a novel efficient approximation and privacy preservation algorithms eappa framework in this paper to achieve efficient data preprocessing from the live streaming and its privacy preservation with minimum information loss il and computational requirements as the existing privacy preservation solutions for streaming data suffer from the challenges of redundant data we first propose the efficient technique of data approximation with data preprocessing we design the flajolet martin fm algorithm for robust and efficient approximation of unique elements in the data stream with a data cleaning mechanism we fed the periodically approximated and preprocessed streaming data to the anonymization algorithm using adaptive clustering we propose innovative kanonymization and ldiversity privacy principles for data streams the proposed approach scans a stream to detect and reuse clusters that fulfill the kanonymity and ldiversity criteria for reducing anonymization time and il the experimental results reveal the efficiency of the eappa framework compared to stateofart methods
with the rising prominence of wifi in common spaces efforts have been made in the robotics community to take advantage of this fact by incorporating wifi signal measurements in indoor slam simultaneous localization and mapping systems slam is essential in a wide range of applications especially in the control of autonomous robots this paper describes recent work in the development of wifibased localization and addresses the challenges currently faced in achieving wifibased geometric mapping inspired by the field of research into kvisibility this paper presents the concept of inverse kvisibility and proposes a novel algorithm that allows robots to build a map of the free space of an unknown environment essential for planning navigation and avoiding obstacles experiments performed in simulated and realworld environments demonstrate the effectiveness of the proposed algorithm
in this article we deal with the task of hostile post detection in hindi the objective is to predict whether a social media post is hostile or not furthermore if the post is hostile we identify one or more finegrained hostile dimensions out of the following four—fake hate offensive and defamation we propose hostilenet a novel deeplearning framework that leverages hindibertbased contextual representations and handcrafted features like lexicon emoticon and hashtag embeddings for hostile post classification moreover we also propose a novel mechanism to finetune hindibert’s attention vectors with respect to each hostile dimension we evaluate hostilenet on the constraint2021 shared task dataset on hostile post detection in hindi for both coarsegrained hostile versus nonhostile and finegrained fake versus hate versus offensive versus defamation setups hostilenet outperforms the bestperforming system as reported in the constraint2021 shared task for both the setups furthermore we provide a thorough analysis of the obtained results in the form of an ablation study error analysis attention heatmap analysis lexicon feature analysis and so on we also perform inthewild evaluation and conduct a user survey to assess the robustness of our proposed model we make the code and the curated multilabel hostile lexicon available for research use at httpsgithubcomlcs2iiitdhostilenet
"objective
this paper discusses the sterilization efficiency of three low temperature sterilization methods used in thermosensitive medical devices and makes a preliminary analysis of sterilization costs so as to provide the basis for reasonable selection of low temperature sterilizer in central sterile supply department


methods
medical devices compatible with the three sterilization methods were selected for sterilization and two packaging materials were selected for the three lowtemperature sterilization equipment according to the compatibility of the packaging materials the equipment packed with the same packaging materials were sterilized for five times and each lowtemperature sterilizer was sterilized for a total of ten times the sterilization effect sterilization cycle time energy consumption and cost of the three sterilizers were compared


results
the cycle time of ethylene oxide sterilizer was 3936 min and the cycle time of hydrogen peroxide low temperature plasma sterilizer was 561 min the cycle time of low temperature steam and formaldehyde sterilizer was 1057 min the hydrogen peroxide low temperature plasma sterilizes single cycle power consumption at a maximum of 5 kwh the single cycle energy consumption of compressed air ethylene oxide sterilizer is up to 12 l in terms of sterilization application cost hydrogen peroxide low temperature plasma sterilization has the highest cost followed by ethylene oxide sterilization and low temperature steam and formaldehyde sterilization is the lowest


conclusion
the sterilization efficiency of hydrogen peroxide low temperature plasma sterilization is the highest followed by low temperature steam and formaldehyde sterilization and the lowest is ethylene oxide sterilization the three low temperature sterilization methods can achieve effective sterilization of devices each hospital can choose an appropriate low temperature sterilization method according to the characteristics of thermosensitive instruments turnover efficiency requirements and financial status"
in water monitoring environmental analysis cell culture stability and biomedical applications precise ph control is demanded traditional methods like ph strips and meters have limitations ph strips lack precision while electrochemical meters though more accurate are fragile prone to drift and unsuitable for small volumes in this paper we propose a method for optical detection of the ph value based on the multiplexed sensor with 4d microcavities fabricated with twophoton polymerization this approach employs phtriggered reversible variations in microresonator geometry and integrates hundreds of dual optically coupled 4d microcavities to achieve the detection limit of 0003 ph units the proposed solution is a clear instance for the usecase oriented application of the twophoton polymerized structures of high optical quality with benefits of the multiplexed imaging platform dual 4d microresonators can be integrated alongside other microresonator types for phcorrected biochemical studies
authors summary this study compared a novel telemonitoring system using a singlelead electrocardiogram ecg patch with a conventional telemonitoring system in a realtime inpatient setting the novel telemonitoring system offers performance comparable to a conventional system while significantly reducing signal loss owing to the seamless wireless network through the dualconnection configuration between an ecg patch and neighboring gateways efficiently communicated by the bluetooth low energy protocol the signal noise was also significantly reduced by preventing motion artifacts or poor contact between the electrodes and skin using a lightweight singlelead ecg patch
class imbalance in graphstructured data where minor classes are significantly underrepresented poses a critical challenge for graph neural networks gnns to address this challenge existing studies generally generate new minority nodes and edges connecting new nodes to the original graph to make classes balanced however they do not solve the problem that majority classes still propagate information to minority nodes by edges in the original graph which introduces bias towards majority classes to address this we introduce buffgraph which inserts buffer nodes into the graph modulating the impact of majority classes to improve minor class representation our extensive experiments across diverse realworld datasets empirically demonstrate that buffgraph outperforms existing baseline methods in classimbalanced node classification in both natural settings and imbalanced settings code is available at httpsanonymous4opensciencerbuffgraph730a
large language models llms have demonstrated remarkable capabilities for reinforcement learning rl models such as planning and reasoning capabilities however the problems of llms and rl model collaboration still need to be solved in this study we employ a teacherstudent learning framework to tackle these problems specifically by offering feedback for llms using rl models and providing highlevel information for rl models with llms in a cooperative multiagent setting within this framework the llm acts as a teacher while the rl model acts as a student the two agents cooperatively assist each other through a process of recursive help such asi help you help i helpthe llm agent supplies abstract information to the rl agent enabling efficient exploration and policy improvement in turn the rl agent offers feedback to the llm agent providing valuable realtime information that helps generate more useful tokens this bidirectional feedback loop promotes optimization exploration and mutual improvement for both agents enabling them to accomplish increasingly challenging tasks remarkably we propose a practical algorithm to address the problem and conduct empirical experiments to evaluate the effectiveness of our method
background the increasing incidence of gestational diabetes mellitus gdm is a global health problem that is more likely to occur in pregnant women with overweight or obesity adhering to a healthy lifestyle is associated with a reduced risk of gdm with the development of it mobile health mhealth interventions have become widely available in health care however there are no definitive conclusions on the effectiveness of mhealthbased lifestyle interventions in preventing gdm objective this study aims to evaluate the impact of mhealthbased lifestyle interventions on gdm and other pregnancy outcomes in pregnant women with overweight or obesity methods a systematic literature search was conducted in 5 english databases medline embase web of science central and cinahl and 4 chinese databases cbm cnki vip and wanfang to identify randomized controlled trials rcts on the effectiveness of mhealthbased interventions for gdm from inception to january 10 2023 in total 2 authors independently screened the studies and extracted the data the quality of the included studies was examined using the cochrane riskofbias tool data synthesis was conducted using review manager version 54 the cochrane collaboration results a total of 16 rcts with 7351 participants were included in this study the included studies were published between 2014 and 2021 and were conducted in china the united states australia new zealand the united kingdom ireland and norway the sample sizes of the studies ranged from 75 to 2202 and the duration of the mhealthbased lifestyle interventions ranged from 4 to 28 weeks compared with usual care mhealthbased lifestyle interventions significantly reduced the incidence of gdm odds ratio or 074 95 ci 056096 p03 i265 preterm birth or 065 95 ci 048087 p004 i225 macrosomia or 059 95 ci 040087 p008 i259 and gestational weight gain mean difference−112 kg 95 ci −144 to −080 p001 i243 the subgroup analysis showed that interventions delivered via apps or 055 95 ci 037083 p004 i244 provided by obstetricians or 069 95 ci 051093 p02 i260 and targeted at asian populations or 044 95 ci 034058 p001 i20 and that used the international association of diabetes and pregnancy study groups diagnostic criteria or 058 95 ci 039086 p007 i269 showed a statistically significant reduction in the risk of gdm conclusions mhealthbased lifestyle interventions had a favorable impact on the prevention of gdm in pregnant women with overweight and obesity future studies need to further explore the potential of mhealthbased interventions for gdm through better design and more rigorous largescale rcts trial registration prospero international prospective register of systematic reviews crd42021286995 httpswwwcrdyorkacukprosperodisplayrecordphprecordid286995
this paper presents an original approach to predicting oil slick movement and dispersion at the water surface special emphasis is placed on the impact of evolving hydrometeorological conditions and the thickness of the oil spill layer the main gap addressed by this study lies in the need for a comprehensive understanding of how changing environmental conditions and oil thickness interact to influence the movement and dispersion of oil slicks by focusing on this aspect this study aims to provide valuable insights into the complex dynamics of oil spill behaviour enhancing the ability to predict and mitigate the environmental impacts of such incidents selfdesigned software was applied to develop and modify previously established mathematical probabilistic models for predicting changes in the shape of the oil trajectory first a semimarkov model of the process is constructed and the oil thickness is analysed at the sea surface over time next a stochasticbased procedure to forecast the horizontal movement and dispersion of an oil slick in diverse hydrometeorological conditions considering a varying oil layer thickness is presented this involves determining the trajectory and movement of a slick domain which consists of an elliptical combination of domains undergoing temporal changes by applying the procedure and program a shortterm forecast of the horizontal movement and dispersion of an oil slick provided its trajectory at the bornholm basin of the baltic sea within two days the research results obtained are preliminary prediction results although the approach considered in this paper can help responders understand the scope of the problem and mitigate the effects of environmental damage if the oil discharge reaches sensitive ecosystems finally further perspectives of this research are given
buddha statues as a symbol of many religions have significant cultural implications that are crucial for understanding the culture and history of different regions and the recognition of buddha statues is therefore the pivotal link in the field of buddha study however the buddha statue recognition requires extensive time and effort from knowledgeable professionals making it a costly task to perform convolution neural networks cnns are inherently efficient at processing visual information but cnns alone are likely to make inaccurate classification decisions when subjected to the class imbalance problem therefore this paper proposes an endtoend automatic buddha statue recognition model based on saliency map sampling the proposed gridwise local selfattention module glsa provides extra salient features which can serve to enrich the dataset and allow cnns to observe in a much more comprehensive way eventually our model is evaluated on a buddha dataset collected with the aid of buddha experts and outperforms stateoftheart networks in terms of top1 accuracy by 463 on average while only marginally increasing muladd
disentangled representation learning seeks to align individual dimensions or separate groups of coordinates of latent factors with attributes of observed data such that perturbing certain latent factors uniquely changes particular attributes a main challenge in unsupervised disentanglement using autoencoders is that strong regularisation while necessary for consistent disentanglement comes at the expense of accurate data reconstruction to address this we introduce a teacherstudent framework that incorporates a variational sequential autoencoder and a jacobian constraint that regularises the variation of observations relative to latent factors in realworld audio recordings of musical instruments our approach outperforms a stateoftheart method in both sampling quality and unsupervised pitchtimbre disentanglement
this paper proposes a new distributed nonconvex stochastic optimization algorithm that can achieve privacy protection communication efficiency and convergence simultaneously specifically each node adds general privacy noises to its local state to avoid information leakage and then quantizes its noiseperturbed state before transmitting to improve communication efficiency by using a subsampling method controlled through the samplesize parameter the proposed algorithm reduces cumulative differential privacy parameters epsilon delta and thus enhances the differential privacy level which is significantly different from the existing works by using a twotimescale stepsizes method the mean square convergence for nonconvex cost functions is given furthermore when the global cost function satisfies the polyaklojasiewicz condition the convergence rate and the oracle complexity of the proposed algorithm are given in addition the proposed algorithm achieves both the mean square convergence and finite cumulative differential privacy parameters epsilon delta over infinite iterations as the samplesize goes to infinity a numerical example of the distributed training on the mnist dataset is given to show the effectiveness of the algorithm
the use of quantum satellites can be significant paving the way for novel telecommunications network paradigms making it possible to connect remote regions given the rapid development of quantum communication technologies many studies are focusing on quantum satellites interconnections via free space optic fso especially considering that the adoption of wired alternatives did not achieve significant results mainly in terms of covering long distances in this context the global interconnection of quantum computers qcs through the socalled quantum internet qi compels for new communications and computing architectures such as quantum cloud qcloud to this purpose the deployment of satellite quantum networks sqns could make easily available innovative services spanning from security to advanced computing especially by using softwaredefined networking sdn technology which is considered a significant enabler for the management of sqns moreover considering the rapid variability of the topology in terms of nodes and links features the paper proposes a specific temporal networks tns optimization strategy embedded in an sdn controller ecosystem dynamically optimizing the path durations to achieve the highest entanglement rate while limiting the disconnections performance evaluation in a worst case scenario shows that the proposed framework is able to support distributed applications by exploiting all the connection opportunities
we study the classic correlation clustering in the dynamic setting given n objects and a complete labeling of the objectpairs as either similar or dissimilar the goal is to partition the objects into arbitrarily many clusters while minimizing disagreements with the labels in the dynamic setting an update consists of a flip of a label of an edge in a breakthrough result bdhss focs19 showed how to maintain a 3approximation with polylogarithmic update time by providing a dynamic implementation of the pivot algorithm of acn stoc05 since then it has been a major open problem to determine whether the 3approximation barrier can be broken in the fully dynamic setting in this paper we resolve this problem our algorithm modified pivot locally improves the output of pivot by moving some vertices to other existing clusters or new singleton clusters we present an analysis showing that this modification does indeed improve the approximation to below 3 we also show that its output can be maintained in polylogarithmic time per update
the internet of things iot is a paradigm that has been widely deployed in various areas connecting commonly used devices from home to industry and health among others iot devices generally have a small amount of memory low processing resources and it is often required that they have low energy consumption therefore one objective of this research is to try to take advantage of a classic cable communications standard rs485 so as to allow the connection of simple devices to the tcpip network through an intermediary node thus the iot device would only require an asynchronous serial port and an interface to the electrical network to that end data transmission tests have been carried out over interfaces based on the thvd8000 integrated circuit the achieved results encourage to continue working on this issue
"
purpose
the purpose of this study is to analyse and compile the literature on various option pricing models opm or methodologies the report highlights the gaps in the existing literature review and builds recommendations for potential scholars interested in the subject area


designmethodologyapproach
in this study the researchers used a systematic literature review procedure to collect data from scopus bibliometric and structured network analyses were used to examine the bibliometric properties of 864 research documents


findings
as per the findings of the study publication in the field has been increasing at a rate of 6 on average this study also includes a list of the most influential and productive researchers frequently used keywords and primary publications in this subject area in particular thematic map and sankey’s diagram for conceptual structure and for intellectual structure cocitation analysis and bibliographic coupling were used


research limitationsimplications
based on the conclusion presented in this paper there are several potential implications for research practice and society


practical implications
this study provides useful insights for future research in the area of opm in financial derivatives researchers can focus on impactful authors significant work and productive countries and identify potential collaborators the study also highlights the commonly used opms and emerging themes like machine learning and deep neural network models which can inform practitioners about new developments in the field and guide the development of new models to address existing limitations


social implications
the accurate pricing of financial derivatives has significant implications for society as it can impact the stability of financial markets and the wider economy the findings of this study which identify the most commonly used opms and emerging themes can help improve the accuracy of pricing and risk management in the financial derivatives sector which can ultimately benefit society as a whole


originalityvalue
it is possibly the initial effort to consolidate the literature on calibration on option price by evaluating and analysing alternative opm applied by researchers to guide future research in the right direction
"
this paper proposes new bivariate distributions based on the poisson generalized lindley distribution as marginal these models include the basic bivariate poisson generalized lindley bpgl and the sarmanovbased bivariate poisson generalized lindley spgl distributions subsequently we introduce the bpgl and spgl distributions as joint innovation distributions in a novel bivariate firstorder integervalued autoregressive process binar1 based on binomial thinning the model parameters in the bpgl and spgl distributions are estimated using the method of maximum likelihood ml while we apply the conditional maximum likelihood cml for the binar1 process we conduct some simulation experiments to assess the small and large sample performances further we implement the new binar1s to the pittsburgh crime series data and they show better fitting criteria than other competing binar1 models in the literature
this paper reports a highperformance broadband flexible photodetector pd based on a novel covalant organic polymeric framework material tpatztpa one pd on sisubstrate and another pd on itocoated petbased flexible substrate were fabricated both pds exhibited a similar type of photodetection capability ranging between 350 to 950 nm in the presence of a broadband white light source the flexible pd exhibited a peak phototodark current ratio of inlineformulatexmath notationlatex1823 times 103texmathinlineformula with a bias of 2 v experimental results indicate that the fabricated flexible pd offers a peak responsivity external quantum efficiency and specific detectivity of 569 aw at 570 nm 1306 at 565 nm and inlineformulatexmath notationlatex127 times 1011texmathinlineformula jones at 570 nm respectively the transient study reveals that the fabricated tpatztpabased flexible pd offers a rise and fall time of 029 s and 028 s respectively a simple 50 cm long visible lightwave communication vlc system utilizing the fabricated flexible pd exhibited its efficacy by successfully demodulating intensitymodulated pn9 prbs sequence
the development of energysaving technology is important to the promotion of electric vehicles evs in the design process of evs the drive configuration is generally determined first and then the energy management strategy is studied however this process does not design the drive configuration with higher energy saving potential from the economic perspective of vehicle driving conditions but directly determines the drive configuration so in this article the characteristics of the vehicle’s driving conditions are analyzed first and then based on the analysis results a new dualmotor fourwheeldrive 4wd ev configuration with higher energy saving potential is proposed which only has a twospeed transmission in the rear axle then the parameters of the proposed driving configuration are calculated to meet the vehicle dynamic or economic needs of various driving conditions then an energy management strategy is developed for the judgment of optimal torque distribution of the front and rear axle motors and gear shifting logic finally the comparison simulations are conducted and the results show that in worldwide lightduty test procedure wltc dualmotor 4wd configuration with a twospeed transmission dm2st can reduce vehicle energy consumption by 913 and 484 respectively compared to dm with the fixed proportional torque distribution strategy dmftd and dm with the efficiency optimal torque distribution strategy dmotd in udds dm2st can reduce vehicle energy consumption by 1104 and 577 respectively compared to dmftd and dmotd
the restricted edgeconnectivity of a connected graph g denoted by λ′g if it exists is the minimum cardinality of a set of edges whose deletion makes g disconnected and each component has at least two vertices it was proved that λ′g exists if and only if g has at least four vertices and g is not a star in this case a graph g is called maximally restricted edgeconnected if λ′gξg and a graph g is called super restricted edgeconnected if each minimum restricted edgecut isolates an edge of g the strong product of graphs g and h denoted by g⊠h is the graph with the vertex set vg×vh and the edge set x1y1x2y2x1x2 and y1y2∈eh or y1y2 and x1x2∈eg or x1x2∈eg and y1y2∈eh in this paper we determine for any nontrivial connected graph g the restricted edgeconnectivity of g⊠pn g⊠cn and g⊠kn where pn cn and kn are the path cycle and complete graph of order n respectively as corollaries we give sufficient conditions for these strong product graphs g⊠pn g⊠cn and g⊠kn to be maximally restricted edgeconnected and super restricted edgeconnected
existing reconfigurable intelligent surfaceaided receive spatial modulation risrsm suffers from severe performance degradation in rician fading channels to solve this problem we propose a novel risrsm scheme in this letter for multiinputmultioutput communications in contrast to the existing risrsm that uses passive beamforming only to maximize the received power at a targeted receive antenna our advanced risrsm employs joint active and passive beamforming japb to minimize the error probability of antenna index detection the active and passive beamforming vectors are optimized alternately it is shown that the active beamforming vector is the eigenvector associated with the largest eigenvalue of a matrix and the passive beamforming vector can be obtained in closedform in each iteration via a majorizationminimization method simulation results reveal the effectiveness of risrsmjapb by comparing with the existing rsm and risrsm schemes in terms of bit error rate and detection complexity
purpose close mri surveillance of patients with brain metastases following stereotactic radiosurgery srs treatment is essential for assessing treatment response and the current disease status in the brain this followup necessitates the comparison of target lesion sizes in pre prior and postsrs treatment current t1wgad mri scans our aim was to evaluate simunet a novel deeplearning model for the detection and volumetric analysis of brain metastases and their temporal changes in paired prior and current scans methods simunet is a simultaneous multichannel 3d unet model trained on pairs of registered prior and current scans of a patient we evaluated its performance on 271 pairs of t1wgad mri scans from 226 patients who underwent srs an expert oncological neurosurgeon manually delineated 1889 brain metastases in all the mri scans 1368 with diameters  5 mm 834  10 mm the simunet model was trainedvalidated on 205 pairs from 169 patients 1360 metastases and tested on 66 pairs from 57 patients 529 metastases the results were then compared to the ground truth delineations results simunet yielded a mean std detection precision and recall of 100±000 and 099±006 for metastases  10 mm 090±022 and 097±012 for metastases  5 mm of and 076±027 and 094±016 for metastases of all sizes it improves lesion detection precision by 8 for all metastases sizes and by 125 for metastases  10 mm with respect to standalone 3d unet the segmentation dice scores were 090±010 089±010 and 089±010 for the above metastases sizes all above the observer variability of 080±013 conclusion automated detection and volumetric quantification of brain metastases following srs have the potential to enhance the assessment of treatment response and alleviate the clinician workload
the efficient management and remediation of contaminated fractured aquifers necessitate an accurate prediction of the spatial distribution of contaminant concentration within the system related existing analytical solutions are only applicable to single fractures and have not yet been extrapolated to the aquifer scale where a network of connected fractures exists the random walk particle tracking rwpt method has been extensively adopted for concentration mapping in discrete fracture networks dfns albeit at exorbitant computational costs and without efficiently accommodating complex physical processes eg twosite kinetics this study introduces an analytically enhanced spatiotemporal random walk strw approach that facilitates the efficient timedependent mapping of contaminant concentration in dfns the strw approach employs a distribution function to simultaneously estimate the displacement of particles released through the system either instantaneously or over time the strw approach efficiently reproduced the contaminant concentration calculated using available analytical solutions under a range of fate and transport mechanisms the efficacy of the strw approach is also confirmed in a synthetic impermeable dfn through replicating the concentration maps produced using the rwpt method the developed approach represents an accurate and computationally efficient dynamic concentration mapping technique that can support the effective operation management and remediation of fractured aquifers under contamination events
accurate transcription of bengali text to the international phonetic alphabet ipa is a challenging task due to the complex phonology of the language and contextdependent sound changes this challenge is even more for regional bengali dialects due to unavailability of standardized spelling conventions for these dialects presence of local and foreign words popular in those regions and phonological diversity across different regions this paper presents an approach to this sequencetosequence problem by introducing the district guided tokens dgt technique on a new dataset spanning six districts of bangladesh the key idea is to provide the model with explicit information about the regional dialect ordistrictof the input text before generating the ipa transcription this is achieved by prepending a district token to the input sequence effectively guiding the model to understand the unique phonetic patterns associated with each district the dgt technique is applied to finetune several transformerbased models on this new dataset experimental results demonstrate the effectiveness of dgt with the byt5 model achieving superior performance over wordbased models like mt5 banglat5 and umt5 this is attributed to byt5s ability to handle a high percentage of outofvocabulary words in the test set the proposed approach highlights the importance of incorporating regional dialect information into ubiquitous natural language processing systems for languages with diverse phonological variations the following work was a result of thebhashamulchallenge which is dedicated to solving the problem of bengali text with regional dialects to ipa transcription httpswwwkagglecomcompetitionsregipa the training and inference notebooks are available through the competition link
plan deordering removes unnecessary ordering constraints between actions in a plan facilitating plan execution flexibility and several other tasks such as plan reuse modification and decomposition block deordering is a variant of plan deordering that encapsulates coherent actions into blocks to eliminate further ordering constraints from a partialorder plan pop and is useful in many applications eg generating macroactions and improving the overall plan quality the existing block deordering strategy is formulated in propositional encodings finitedomain state variable encodings eg sas representation in contrast with propositional encodings can capture the internal structure and the behavior of state variables of a planning instance through concise constructs such as causal graphs cgs and domain transition graphs dtgs this work redefines the semantics of block deordering terminologies and related plan deordering concepts in finite domain representation fdr our proposed semantics also resolves some limitations of the existing block semantics and further enhance plan flexibility in addition this work exploits block deordering to eliminate redundant actions from a pop a comparative analysis is also performed on block deordering with various deorderingreordering techniques using explanationbased order generalization eog and maxsat our experiments on the benchmark problems from international planning competitions ipc show that our fdr formalism of block deordering significantly improves the plan execution flexibility while maintaining good coverage and execution time
recent advancements in implicit 3d representations and generative models have markedly propelled the field of 3d object generation forward however it remains a significant challenge to accurately model geometries with defined sharp features under parametric controls which is crucial in fields like industrial design and manufacturing to bridge this gap we introduce a framework that employs large language models llms to generate textdriven 3d shapes manipulating 3d software via program synthesis we present 3dpremise a dataset specifically tailored for 3d parametric modeling of industrial shapes designed to explore stateoftheart llms within our proposed pipeline our work reveals effective generation strategies and delves into the selfcorrection capabilities of llms using a visual interface our work highlights both the potential and limitations of llms in 3d parametric modeling for industrial applications
the involved object of systematic innovation and management is neither a single goal nor a single method firstly the research object is a systematic and organic whole composed of multiple node elements with a certain degree of correlation secondly the research method is multidimensional comprehensive all round and highly efficient innovation should break through the existing thinking framework change the existing structure add new components give new energy and produce new functions the 5w1h is a questioning approach and a problemsolving method that answers all the basic elements within a problem which are what who when where why and how it aims to view ideas from various perspectives and gain an indepth understanding of a specific situation on the basis of 5w1h analysis method this paper puts forward the 6whi systematic innovation management method which increases the query words weather and if of the innovation expansion solution increases the number of questions from 6 to 8 and expands innovative forwardlooking and innovative solutions diversity with smoother pronunciation more exciting storm furthermore a systematic innovation method based on 6whi and total factor analysis is established and a more general and normative model wime method is presented this method can be utilized as a continuous processimprovement technique in an organization taking the 17 letters of “if question patents” as the prefix they are just 17 common key elements in system management and innovation for each element mentioned above 8 questions and answers of 6whi are given to realize systematic management and innovation the above two types of work are arranged according to the matrix format and then the weighted evaluation method is introduced to evaluate the above management and innovation work so as to give a general and standardized systematic management and innovation wime method
rayleigh brillouin optical time domain analysis botda uses the backscattered rayleigh light generated in the fiber as the probe light which has a lower detection light intensity compared to the botda technique as a result its temperaturesensing technology suffers from a low signaltonoise ratio snr and severe sensing unreliability due to the influence of the low probe signal and high noise level the pulse coding and lmd denoising method are applied to enhance the performance of the brillouin frequency shift detection and temperature measurement in this study the mechanism of rayleigh botda based on a fewmode fiber fmf is investigated the principles of the golay code and local mean decomposition lmd algorithm are analyzed and the experimental setup of the rayleigh botda system using an fmf is constructed to analyze the performance of the sensing system compared with a single pulse of 50 ns the 32bit golay coding with a pulse width of 10 ns improves the spatial resolution to 1 m further enhanced by the lmd algorithm the snr and temperature measurement accuracy are increased by 55 db and 105 °c respectively finally a spatial resolution of 112 m and a temperature measurement accuracy of 285 °c are achieved using a twomode fiber with a length of 1 km
blurred regions in images can hinder visual analysis and have a notable impact on applications such as navigation systems and virtual tours many existing approaches in the literature assume the presence of blurred regions in an image and process the entire image even when no blurred regions are actually present this approach leads to unnecessary computational overhead resulting in inefficiency and resource consumption in this paper we introduce a streetview images blur detection network sbdnet consisting of two interconnected subnetworks the classifier network and the identifier network the classifier network is responsible for categorizing streetview images as either blurred or not blurred once the classifier network determines that an image is blurred the identifier network is then activated to estimate the specific areas that are blurred within the image highlevel semantic features from the classifier network are used to construct the blur map estimation in the identifier network when necessary the algorithm was trained and evaluated using the streetview blur images dataset svbi and three publicly available blur detection datasets cuhk dut szubd our quantitative and qualitative results demonstrate that sbdnet competes with state of the arts in blur map estimation
selenopeptide identification relies on databases to interpret the selenopeptide spectra a common database search strategy is to set selenium as a variable modification instead of sulfur on peptides however this approach generally detects only a fraction of selenopeptides an alternative approach termed selenium decipher is proposed in the present study it involves identifying collisioninduced dissociationcleavable selenomethioninecontaining peptides by iteratively matching the masses of selenoamino acids in selenopeptide spectra this approach uses variabledataindependent acquisition vdia for peptide detection providing a flexible and customizable window for secondary mass spectral fragmentation the attention mechanism was used to capture global information on peptides and determine selenomethioninecontaining peptide backbones the core structure of selenium on selenomethioninecontaining peptides generates a series of fragment ions namely c3h7se c4h10nse c5h7ose c5h8nose and c7h11n2o2se with known mass gaps during higherenergy collisional dissociation hcd fragmentation deselenium spectra are generated by removing selenium originating from selenium replacement and then reassigning the precursors to peptides seleniumenriched milk is obtained by feeding seleniumrich forage fed to cattle which leads to the formation of native selenium through biotransformation a novel antihypertensive selenopeptide thraspaspilesemetcysvallys tddisemcvk was identified from seleniumenriched milk the selenopeptide ic50  6071 μm is bound to four active residues of the angiotensinconverting enzyme ace active pocket ala354 tyr523 his353 and his513 and two active residues of zinc ligand his387 and glu411 and exerted a competitive inhibitory effect on the spatial blocking of active sites the integration of vdia and the iteratively matched selenoamino acids was applied for selenium decipher which provides high validity for selenomethioninecontaining peptide identification
healthrelated social control refers to intentional attempts to influence people’s health behaviors often seen in personal relationships social robots hold promise in influencing people’s health by exerting healthrelated social control but it is unclear which social control strategies used by robots are appropriate and potentially effective this study investigates the effects of positive versus negative and relationshiporiented versus targetoriented social control strategies from a social robot on people’s psychological reactions in an online video prototype study participants viewed scenarios of a social robot attempting to change their sedentary behaviors by using different strategies we found that positive versus negative strategies elicited stronger positive affect enjoyment and perceived social appropriateness reduced perceived threats to freedom and strengthened behavioral intention meanwhile the relationshiporiented versus targetoriented strategies elevated people’s negative affect reduced enjoyment and perceived appropriateness elevated perceived threats to freedom and weakened behavioral intentions given these findings we give recommendations for designing health influence strategies in social robotsccs concepts• humancentered computing → human computer interaction hci → empirical studies in hci
reversible phosphorylation is the basis for signal transduction in eukaryotic cells and this is tightly controlled by the complex interplay of kinases and phosphatases many malignancies are characterized by dysregulation of the delicate protein phosphorylation balance the targeting of protein phosphatases has been gaining attention as their role in cancer development and progression has been elucidated the protein phosphatase2a pp2a family of phosphatases are the primary cellular serinethreonine phosphatases pp2a heterotrimers containing the b55α pr55α regulatory subunit have been associated with oncogenic signaling and b55 subunits are found exclusively in forms of pp2a in which the carboxyl terminus of the catalytic subunit pp2ac is methylated methylation of pp2ac is primarily mediated by leucine carboxyl methyltransferase1 lcmt1 demethylation is controlled by an esterase pp2a methylesterase pme1 we tested two potential lcmt1 small molecule inhibitors and found that methyl 4methyl22methylbenzoylamino53methylphenylaminocarbonyl3thiophenecarboxylate henceforth referred to as compound 2 significantly inhibited pp2ac methylation in vitro p  00024 and in the mdamb231 breast carcinoma p  00431 and rosi melanoma p  00335 cell lines compound 2 significantly reduced survival in hek293 hs5 mdamb231 and rosi cells and constrained clonogenic colony formation in mcf7 mdamb231 and rosi cells the lcmt1inhibitor induced g0g1 cell cycle arrest in rosi cells p  00193 and induced apoptosis in mdamb231 cells p  00001 increased phosphorylation of the receptorinteracting serinethreonine protein kinase1 ripk1 was detected in mdamb231 p  00273 and rosi cells p  00179 in response to treatment with compound 2 these data add to the body of evidence pointing to lcmt1 as an oncogenic target
the enhancement of seismic migration and inversion processes critically depends on the precise interpolation of seismic data in recent years the rapid advancements in deep learning dl have led to the widespread adoption of convolutional neural networks cnns in seismic interpolation applications nonetheless the inherent limitations of traditional cnns due to the fixed structure of their convolution kernels impede the extraction of highlevel features of seismic data the accuracy of cnnbased seismic data characterization and interpolation is open to improvement therefore we introduce deformable convolution kernels and design a novel deformable convolution residualunet drunet for a more nuanced characterization and interpolation of seismic data the proposed drunet allows a deformable convolution kernel with adaptive shape adjustments for the receptive field by using learnable offsets to effectively extract advanced features from the training data in conjunction with a residual learning approach the drunet significantly refines the network’s learning process and boosts interpolation precision through numerical experiments on synthetic and field data with irregular traces missing the proposed drunet achieves superior precision in seismic data interpolation compared with traditional unet and unet with the squeeze and excitation block seblock
for highcapacity flywheel energy storage system fess applied in the field of wind power frequency regulation highpower wellperformance machine and magnetic bearings are developed however due to the existence of axial magnetic force in this machine structure along with the uncontrollability of the magnetic bearing the axial stability of the flywheel needs to be focused on firstly a fess with an axial flux permanent magnet synchronous machine afpmsm based on soft magnetic composite smc material and halbach axial passive magnetic bearing pmb structure is proposed and its principle and structural superiority are introduced secondly a threedimensional 3d finite element method fem simulation model of the machine and bearing is established the effects of current air gap and bearing parameters on the rotor axial force are investigated using the 3d fem in addition the relationship between current and displacement on axial force is fitted by the response surface method rsm the startup process and the effect of current change on displacement of the flywheel under different operating conditions are investigated by the rungekutta rk method after that the rotor displacement under various air gaps and bearing forces is studied to ensure that the rotor displacement is smaller than the air gap thereby ensuring the flywheel rotor stays within a controllable range finally the fess prototype is manufactured and tested which finally enables it to operate safely and stably
in realworld industries production line assets may be affected by several factors both known and unknown which dynamically and unpredictably evolve so that past data are of little value for present ones in addition data is collected without assigned labels how can someone use runtofailure data to develop a suitable solution toward achieving predictive maintenance pdm in this case these issues arise in our case which refers to a coldforming press such a setting calls for an unsupervised solution that can predict upcoming failures investigating a wide spectrum of approaches namely similaritybased forecastingbased and deeplearning ones but before we decide on the best solution we first need to understand which key performance indicators are appropriate to evaluate the impact of each such solution a comprehensive study of available evaluation methods is presented highlighting misconceptions and limitations of broadly used evaluation metrics concerning runtofailure data while proposing an extension of stateoftheart rangebased anomaly detection evaluation metrics to serve pdm purposes finally an investigation of preprocessing distance metrics incorporation of domain expertise and the role of deep learning shows how to engineer an unsupervised solution for predictive maintenance providing insightful answers to all these problems our experimental evaluation showed that judicious design choices can improve efficiency of solutions up to two times
background smartphone usage is an essential everyday tool in iran however problematic use has escalated and become a concern for the iranian health policy system particularly during and following the covid19 pandemic this study’s aim was investigation of the prevalence of smartphone addiction patterns of use and the relationship to specific demographic characteristics and associated musculoskeletal disorders during the covid19 pandemic methods a descriptiveanalytical correlational study recruited participants from a population of convenience n  2344 who were smartphone owners with  1 year of use for demographic information an electronic selfreport questionnaire collected age sex marital status usage for daily hours and patterns to assess addiction levels the ‘smartphone addiction scaleshort version’ sassv patientreported outcome measure was used cutoff  31 for experienced discomfort the extended nordic musculoskeletal questionnaire enmq was used results the participants female  666 n  1561 mean age  2907 ± 1234 years range 6–60 years smartphone use averaged 575 ± 344 hday the general prevalence of smartphone addiction was 4616 females  4606 males  4636 married  445 single  4763 school students had the greatest addiction 532 and those with a higher education to or above a master’s degree were the lowest 3938 the highest pattern of use was for social networks at 891 of participants female  8834 male  9054 the areas of highest reported discomfort were the eyes 435 and neck 433 a significant correlation was found between smartphone addiction and hours of daily usage and the amount of usage increased during the covid19 pandemic period conclusion a high level of smartphone addiction in the iranian population was found to have occurred during the covid19 pandemic those most affected were unmarried individuals and school students with the predominant areas being the eyes and neck health decisionmakers should consider these findings when developing recommendations and plans for public health particularly those focused on students
cyber–physical systems cpss driven by a local controller and a remote controller have been gaining significant research interest in recent years due to its application scenarios in practice such as unmanned aerial vehicles uavs in this article we consider the security issue in the remote control system with a local controller under stealthy attacks under this framework one controller is designed locally based on the limited measurements collected by a local sensor and the other controller is designed remotely and is transmitted to the actuator through a wireless communication channel which may suffer malicious attacks due to its openness to defend attacks on remote control signal the k–l divergencebased detector or chi 2 detector is often adopted however there may be attackers adopting stealthy attacks which can bypass such detectors therefore we analyze the existence of such attacks and analytically characterize the worstestimation performance degradation induced by the remote control signal attack further we construct the optimal attack signal to achieve the upper bound on the estimation performance degradation in addition we also give countermeasures against such stealthy attacks simulations are provided to illustrate the proposed results
the ability to learn continuously in dynamic environments is a crucial requirement for reinforcement learning rl agents applying in the real world despite the progress in continual reinforcement learning crl existing methods often suffer from insufficient knowledge transfer particularly when the tasks are diverse to address this challenge we propose a new framework hierarchical continual reinforcement learning via large language model hicore designed to facilitate the transfer of highlevel knowledge hicore orchestrates a twolayer structure highlevel policy formulation by a large language model llm which represents agenerates a sequence of goals and lowlevel policy learning that closely aligns with goaloriented rl practices producing the agents actions in response to the goals set forth the framework employs feedback to iteratively adjust and verify highlevel policies storing them along with lowlevel policies within a skill library when encountering a new task hicore retrieves relevant experience from this library to help to learning through experiments on minigrid hicore has demonstrated its effectiveness in handling diverse crl tasks which outperforms popular baselines
this article proposes a novel fast electromagnetic em inversion solver for heterogeneous scatterers with high contrast in order to ensure inversion accuracy the conventional solvers for the em inverse scattering emis problems usually require em scattered field information originating from multiple incident em waves resulting in tedious measurement operation and considerable measurement data for inversion computation thus the conventional solvers are difficult to suit for realtime quantitative em problems which require relatively simple measurement operations and small measurement data dimensions to overcome these challenges a novel em inversion solver is proposed based on a conditional deep convolutional generative adversarial network cdcgan the proposed cdcgan includes the generator with an em scattering simulator and the corresponding discriminator both consisting of complexvalued deep convolutional neural networks dconvnets the trained cdcgan can be successfully applied to inhomogeneous and highcontrast scatterers only by employing one single incident em wave in singlefrequency and farfield measurement which greatly simplifies measurement and reduces the computation complexity for its further inversion computation numerical examples have indicated the accuracy and feasibility of the proposed dlbased inversion solver which acts as the potential candidate for solving realtime quantitative em problems
martian terrain segmentation aims to assign all pixels of an input image with various terrain labels which provides a firm support for the downstream research on rover traversing and geologic analysis tasks however existing studies in this field suffer from limitations in two aspects one is the lack of largescale and highquality martian terrain datasets and the other is the overreliance on purely supervised learning that is very datahungry and sensitive to domain shifts among different datasets in this article we overcome these from the perspective of both data and methodology first we publish marsscapes a panorama dataset with appreciable data volume and finegrained annotations for martian terrain understanding the dataset contains 195 terrain panoramas composed of 3779 subimages and all pixels in the panoramas are split into nine semantic categories then we propose the first transformerbased unsupervised domain adaptation uda framework udaformer for the crossdomain terrain segmentation on mars which consists of a teacher–student model and an outputguided biased sampling ogbs module the teacher–student model performs knowledge distillation to explore robust crossdomain features where a modified augmentation regularization mar is designed to alleviate the interference of undesirable augmentations to domain adaption the ogbs helps the teacher–student network to emphasize the categories that tend to be ambiguous or submerged during the training elevating the overall accuracy for the uda segmentation of martian terrains extensive experiments on the marsscapes and another dataset called marsseg demonstrate the superiority of udaformer over the stateoftheart methods in uda martian terrain segmentation
abstract the yellow river basin is a complex spatial system integrating ecology economy culture and society the research in this paper takes the innovation of cultural heritage in the basin as the fundamental orientation and analyzes the influence mechanism of digitization on the innovative development of yellow river culture through gis the interregional heterogeneity of the digital transformation of the yellow river culture is examined through the use of the kernel density function to examine the spatial correlation between the yellow river basin and the development of cultural digitalization innovations the gravity model is utilized the yellow river cultural industry’s innovation adaption capacity is measured through two indicators innovation input capacity and innovation absorption capacity based on the analysis of the spatial distribution pattern and the trend of spatiotemporal change it is tentative to observe that there is a possible spatial correlation between the two the results show that the field energy variation of the yellow river cultural memory space ranges from 00928 to 05384 and the distribution has significant spatial dependence the logarithm of the digitization level of the yellow river culture in 2019 is mainly concentrated around 22 and the overall digital development of cities along the yellow river has not been polarized a new framework and reference for protecting cultural heritage and promoting civilization innovation in the yellow river basin is provided by this paper
breeding programs require exhaustive phenotyping of germplasms which is timedemanding and expensive genomic prediction helps breeders harness the diversity of any collection to bypass phenotyping here we examined the genomic prediction’s potential for seed yield and nine agronomic traits using 26171 single nucleotide polymorphism snp markers in a set of 337 flax linum usitatissimum l germplasm phenotyped in five environments we evaluated 14 prediction models and several factors affecting predictive ability based on crossvalidation schemes models yielded significant variation among predictive ability values across traits for the whole marker set the ridge regression rr model covering additive gene action yielded better predictive ability for most of the traits whereas it was higher for low heritable traits by models capturing epistatic gene action marker subsets based on linkage disequilibrium decay distance gave significantly higher predictive abilities to the whole marker set but for randomly selected markers it reached a plateau above 3000 markers markers having significant association with traits improved predictive abilities compared to the whole marker set when marker selection was made on the whole population instead of the training set indicating a clear overfitting the correction for population structure did not increase predictive abilities compared to the whole collection however stratified sampling by picking representative genotypes from each cluster improved predictive abilities the indirect predictive ability for a trait was proportionate to its correlation with other traits these results will help breeders to select the best models optimum marker set and suitable genotype set to perform an indirect selection for quantitative traits in this diverse flax germplasm collection
the majority of business processes described in literature are discrete ie they result in an identifiable and distinct outcome such as a settled customer claim or a produced part however there also exists a plethora of processes in process and control engineering that are continuous ie processes that require realtime control systems with constant inlet and outlet flows as well as temporally stable conditions examples comprise chemical synthesis and combustion processes despite their prevalence and relevance a standard method for modeling continuous processes with bpmn is missing hence the paper provides bpmn modeling extensions for continuous processes enabling an exact definition of the parameters and loop conditions as well as a mapping to executable processes the bpmn modeling extensions are evaluated based on selected use cases from process and control engineering and interviews with experts from three groups ie process engineers and two groups of process modelers one with experience in industrial processes and one without the results from the expert interviews are intended to identify i the key characteristics for the representation of continuous processes ii how experts evaluate the current usability and comprehensibility of bpmn for continuous processes and iii potential improvements can be identified regarding the introduced bpmn modeling extensions
a lowcost highsensitivity large offset salinity sensor with a long sensing area is proposed and demonstrated in this article the sensor is composed of three misaligned singlemode fibers smfs in which the middle longdistance smf is used for sensing which is the smf with long sensing area and is misaligned with a pair of smfs to form smflong sensing area smfsmf structure that is smflssmfsmf structure according to the simulation results the smf large offset structure with a sensing area length of 1000mu textm was prepared and used to measure seawater salinity the experimental results reveal that the salinity sensitivity of the structure can reach −122 nm‰ −626311 nmriu and it has more spectral periods in the same spectral range than the short sensing area probe smflssmfsmf has the advantages of compact structure high sensitivity simple production method and outstanding repeatability which can increase spectral information provide richer period information for subsequent demodulation processing eliminate the problem that the period cannot be judged after spectral aliasing under high sensitivity and has great potential in the field of highprecision seawater salinity measurement
recently multitask instruction tuning has been applied into sentence representation learning which endows the capability of generating specific representations with the guidance of task instruction exhibiting strong generalization ability on new tasks however these methods mostly neglect the potential interference problems across different tasks and instances which may affect the training and convergence of the model to address it we propose a data curriculum method namely datacube that arranges the orders of all the multitask data for training to minimize the interference risks from the two views in the task level we aim to find the optimal task order to minimize the total crosstask interference risk which is exactly the traveling salesman problem hence we utilize a simulated annealing algorithm to find its solution in the instance level we measure the difficulty of all instances per task then divide them into the easytodifficult minibatches for training experiments on mteb sentence representation evaluation tasks show that our approach can boost the performance of stateoftheart methods our code and data are publicly available at the link urlhttpsgithubcomrucaiboxdatacube
in engineering applications the effectiveness of rotating machinery fault diagnosis is often disturbed by both vibration noise and labeling noise vibration noise makes it more difficult for the model to extract a potential fault impact feature while labeling noise makes the feature extraction go in the wrong direction through error backpropagation both of these factors lead to a decrease in fault diagnosis accuracy hence an adaptive symmetric loss in dynamic widekernel residual network asdwresnet is proposed to solve the rotating machinery fault diagnosis under noisy labels first wider convolution kernels are designed for branch convolution to enhance the extraction of vibration features second a parallel network structure is constructed to fully and accurately extract the fault features by dynamically weighting different branches to improve the basic diagnostic performance of the network in the background of vibration noise third an improved adaptive symmetric crossentropy asce loss function is adapted to reduce the effect of label noise in training datasets and hyperparameters in loss functions the experimental results show that in the background of vibration noise and severe noisy labels the proposed method still keeps more than 9908 accuracy and has stronger noise robustness and stability than other networks and loss functions
person reidentification reid networks are often affected by factors such as pose variations changes in viewpoint and occlusion leading to the extraction of features that encompass a considerable amount of irrelevant information however most research has struggled to address the challenge of simultaneously endowing features with both attentive and diversified information to concurrently extract attentive yet diverse pedestrian features we amalgamated the strengths of convolutional neural network cnn attention and selfattention by integrating the extracted latent features we introduced a hybrid attentiondiversity network mixnet which adeptly captures attentive but diverse information from personal images via a fusion of attention branches and attention suppression branches additionally to extract latent information from secondary important regions to enrich the diversity of features we designed a novel discriminative part mask dpm experimental results establish the robust competitiveness of our approach particularly in effectively distinguishing individuals with similar attributes
in the research on fanets flying adhoc networks and distributed coordination of uavs unmanned aerial vehicles also known as drones there are many studies that validate their proposals through simulations simulations are important but beyond them there is also a need for realworld tests to validate the proposals and enhance results however field experiments involving drones and fanets are not trivial and this work aims to share experiences and results obtained during the construction of a testbed actively used in comparing simulations and field tests
with the rapid development of smart manufacturing datadriven deep learning dl methods are widely used for bearing fault diagnosis aiming at the problem of model training crashes when data are imbalanced and the difficulty of traditional signal analysis methods in effectively extracting fault features this paper proposes an intelligent fault diagnosis method of rolling bearings based on gramian angular difference field gadf and improved dual attention residual network idarn the original vibration signals are encoded as 2dgadf feature images for network input the residual structures will incorporate dual attention mechanism to enhance the integration ability of the features while the group normalization gn method is introduced to overcome the bias caused by data discrepancies and then the model is trained to complete the classification of faults in order to verify the superiority of the proposed method the data obtained from case western reserve university cwru bearing data and bearing fault experimental equipment were compared with other popular dl methods and the proposed model performed optimally the method eventually achieved an average identification accuracy of 992 and 979 on two different types of datasets respectively
robust instability analysis is intimately related to minimumnorm strong stabilization and arises in the study of oscillatory behavior in nonlinear systems this paper analyzes the robust instability of linear discretetime systems against stable perturbations in a direct manner without the use of bilinear transformations and notes several important differences from its continuoustime counterpart the results in this paper are particularly useful in the context of sampleddata control in which the plant is often discretized for control synthesis purposes and minimumnorm strong stabilization in discretetime is of interest
miniaturization trends in electronic packaging led to reduced spacing of individual packages on the printed circuit boards and thus increased the overall electromagnetic interference emi levels within a device stateoftheart conformal coatings as emi shielding layers are typically applied via sputter technology and offer design limitations with respect to coating selectivity on individual positions of the overall package our new technology of additively ag layer application via inkjet printing overcomes today’s design limitations and is key enabler for next generations packaging designs in this study we compare the shielding effectiveness se on specifically designed transmission line emitter samples coated with stateoftheart sputtered conformal layers against silver coatings applied via inkjet printing of particle free metal organic decomposition inks to evaluate the new technology for its potential on package level shielding performance mechanical and electrical characterization of the silver coatings indicate appropriate tools to specify layer characteristics and ensure se of 40 db or higher at frequencies of 800 mhz and higher the suitability of our welltailored silver coatings to match typical se requirements is shown with even much thinner layer thickness as compared to stateoftheart coatings
in this study we address multirobot localization issues with a specific focus on cooperative localization and observability analysis of relative pose estimation cooperative localization involves enhancing each robots information through a communication network and message passing if odometry data from a target robot can be transmitted to the ego robot observability of their relative pose estimation can be achieved through rangeonly or bearingonly measurements provided both robots have nonzero linear velocities in cases where odometry data from a target robot are not directly transmitted but estimated by the ego robot both range and bearing measurements are necessary to ensure observability of relative pose estimation for rosgazebo simulations we explore four sensing and communication structures we compare extended kalman filtering ekf and pose graph optimization pgo estimation using different robust loss functions filtering and smoothing with varying batch sizes of sliding windows in terms of estimation accuracy in hardware experiments two turtlebot3 equipped with uwb modules are used for realworld interrobot relative pose estimation applying both ekf and pgo and comparing their performance
the primary goal of safety films for glass in buildings is to retrofit existing monolithic elements and prevent in the postfracture stage any fallout of shards their added value is that—as far as the fragments are kept bonded—a cracked filmglass element can ensure a minimum residual mechanical and loadbearing capacity which is strictly related to the shards interlocking and debond to prevent critical issues such a mechanical characterization is both important and uncertain and requires specific methodologies in this regard a dynamic investigation is carried out on fractured filmbonded glass samples to assess their postfracture stiffness trends and its sensitivity to repeated vibrations the adopted laboratory layout is chosen to assess the effects of random vibrations 220 repetitions on a total of 12 cracked specimens in a cantilever setup with 05–5 ms2 the range of randomly imposed acceleration peaks by monitoring the cracked vibration frequency the film efficiency and corresponding residual bending stiffness of cracked glass samples are quantified as a function of damage severity with a focus on fragments interlock quantitative experimental estimates are comparatively analyzed and validated with the support of finite element fe numerical models and analytical calculations as shown—at least at the smallscale level—a progressive postfracture stiffness reduction takes place under repeated random vibrations and this implicitly affects the residual loadbearing capacity of glass members most importantly for the tested configurations it is shown that the cracked vibration frequency is minimally affected by crack geometry and follows a rather linear decrease with the number of imposed random impacts up to an average of ≈20 for each sample thus confirming the retrofit potential and efficiency in providing some mechanical capacity through fragments interlock
accurate temperature prediction is vital for the canned permanent magnet synchronous motor cpmsm used in the vacuum pump as it experiences severe heating in this paper a novel motor temperature calculation method is proposed which takes into account the temperature impact on the heat transfer capacity in contrast to existing electromagneticthermal coupled calculation methods which solely address the temperature effect on the motor electromagnetic field the proposed method comprehensively considers its impact on motor losses permanent magnet magnetic properties thermal conductivity and heat dissipation ability of motor components resulting in a motor temperature simulation that closely resembles the actual physical process to verify the reliability of the proposed temperature calculation method a 15 kw cpmsm was chosen as the research subject the method was used to analyze the temperature distribution characteristics of the motor and assess the impact of ambient temperature on motor temperature rise furthermore a prototype was fabricated and an experimental platform was established to test the motor temperature the results demonstrate good agreement between the calculated results obtained using the proposed method and the experimental data this research not only provides a theoretical foundation for optimizing the design of the cpmsm but also provides valuable insights into its operational safety and reliability
we consider the problem of multiobjective optimization moo of expensive blackbox functions with the goal of discovering highquality and diverse pareto fronts where we are allowed to evaluate a batch of inputs this problem arises in many realworld applications including penicillin production where diversity of solutions is critical we solve this problem in the framework of bayesian optimization bo and propose a novel approach referred to as pareto frontdiverse batch multiobjective bo pdbo pdbo tackles two important challenges 1 how to automatically select the best acquisition function in each bo iteration and 2 how to select a diverse batch of inputs by considering multiple objectives we propose principled solutions to address these two challenges first pdbo employs a multiarmed bandit approach to select one acquisition function from a given library we solve a cheap moo problem by assigning the selected acquisition function for each expensive objective function to obtain a candidate set of inputs for evaluation second it utilizes determinantal point processes dpps to choose a paretofrontdiverse batch of inputs for evaluation from the candidate set obtained from the first step the key parameters for the methods behind these two steps are updated after each round of function evaluations experiments on multiple moo benchmarks demonstrate that pdbo outperforms prior methods in terms of both the quality and diversity of pareto solutions
recent works introduce a semiclairvoyant model in which the system mode transition is revealed on the arrival of highcriticality jobs to solve the problem of inconsistency between the correctness criterion for mixedcriticality systems mcss with a semiclairvoyant and the actual situation we study the problem of schedulability and energy in mcs with the semiclairvoyant model in this article first we propose a new correctness criterion for mcs with semiclairvoyant and graceful degradation and develop the schedulability test based on demand bound function methods denoted as scsgd second we propose an energyefficient semiclairvoyant scheduling algorithm based on scsgd denoted as eescsgd finally we conduct an experimental evaluation of scsgd and eescsgd by synthetically generated task sets the experimental results show that scsgd can improve the schedulability ratio by 598 compared to existing algorithms while eescsgd can save 5617 energy compared to scsgd
with the recent advancements in machine learning technology the accuracy of autonomous driving object detection models has significantly improved however due to the complexity and variability of realworld traffic scenarios such as extreme weather conditions unconventional lighting and unknown traffic participants there is inherent uncertainty in autonomous driving object detection models which may affect the planning and control in autonomous driving thus the rapid and accurate quantification of this uncertainty is crucial it contributes to a better understanding of the intentions of autonomous vehicles and strengthens trust in autonomous driving technology this research pioneers in quantifying uncertainty in the yolov5 object detection model thereby improving the accuracy and speed of probabilistic object detection and addressing the realtime operational constraints of current models in autonomous driving contexts specifically a novel probabilistic object detection model named myolov5 is proposed which employs the mcdrop method to capture discrepancies between detection results and the real world these discrepancies are then converted into gaussian parameters for class scores and predicted bounding box coordinates to quantify uncertainty moreover due to the limitations of the mean average precision map evaluation metric we introduce a new measure probabilitybased detection quality pdq which is incorporated as a component of the loss function this metric simultaneously assesses the quality of label uncertainty and positional uncertainty experiments demonstrate that compared to the original yolov5 algorithm the myolov5 algorithm shows a 747 improvement in pdq when compared with the most advanced probabilistic object detection models targeting the ms coco dataset myolov5 achieves a 14 increase in map a 17 increase in pdq and a 65 improvement in fps furthermore against the stateoftheart probabilistic object detection models for the bdd100k dataset myolov5 exhibits a 3167 enhancement in map and a 1256 increase in fps
musculoskeletal disorders significantly affect the general population especially those involving lower back discomfort this is the most important reason for the decline in work efficiencies in industrial and defense applications under these conditions an active exoskeleton will be quite helpful selecting an appropriate control strategy for an active exoskeleton requires the prediction of human motion profiles however the complex and dynamic nature of the exoskeleton model limits the modelbased approach thus datadriven parameter estimation models are preferred in this scenario human activity recognition har research has recently focused on deep neural networks there is a noticeable delay between the observed motion profile and detected human activity due to the intrinsic latency in the exoskeleton control architecture as a result har is not enough to achieve the objective on its own on the contrary there is not much prior study on estimating and predicting human motion profiles this study presented a brief comparison of the widely used predictive models the authors emphasized human activity profile prediction by employing deep learning techniques to compensate for the lag and facilitate the realtime control strategy according to their performance comparison the long shortterm memory lstm model performed better than other neural networks and provided the highest prediction accuracy
recently leveraging large language models llms or multimodal large language models mllms for document understanding has been proven very promising however previous works that employ llmsmllms for document understanding have not fully explored and utilized the document layout information which is vital for precise document understanding in this paper we propose layoutllm an llmmllm based method for document understanding the core of layoutllm is a layout instruction tuning strategy which is specially designed to enhance the comprehension and utilization of document layouts the proposed layout instruction tuning strategy consists of two components layoutaware pretraining and layoutaware supervised finetuning to capture the characteristics of document layout in layoutaware pretraining three groups of pretraining tasks corresponding to documentlevel regionlevel and segmentlevel information are introduced furthermore a novel module called layout chainofthought layoutcot is devised to enable layoutllm to focus on regions relevant to the question and generate accurate answers layoutcot is effective for boosting the performance of document understanding meanwhile it brings a certain degree of interpretability which could facilitate manual inspection and correction experiments on standard benchmarks show that the proposed layoutllm significantly outperforms existing methods that adopt opensource 7b llmsmllms for document understanding
testing is one of the most important steps in software development–it ensures the quality of software continuous integration ci is a widely used testing standard that can report software quality to the developer in a timely manner during development progress performance especially scalability is another key factor for high performance computing hpc applications there are many existing profiling and performance tools for hpc applications but none of these are integrated into ci tools in this work we propose beeswarm an hpc container based parallel scaling performance system that can be easily applied to the current ci test environments beeswarm is mainly designed for hpc application developers who need to monitor how their applications can scale on different compute resources we demonstrate beeswarm using three different hpc applications comd lulesh and nwchem we utilize github actions and provision resources from google compute engine our results show that beeswarm can be used for scalability and performance testing of a variety of hpc applications allowing developers to monitor application performance over time
prevention of clostridium difficile infection is challenging worldwide owing to its high morbidity and mortality rates c difficile is currently being classified as an urgent threat by the cdc devising a new therapeutic strategy become indispensable against c difficile infection due to its high rates of reinfection and increasing antimicrobial resistance the current study is based on core proteome data of c difficile to identify promising vaccine and drug candidates immunoinformatics and vaccinomics approaches were employed to construct multiepitopebased chimeric vaccine constructs from topranked t and bcell epitopes the efficacy of the designed vaccine was assessed by immunological analysis immune receptor binding potential and immune simulation analyses additionally subtractive proteomics and druggability analyses prioritized several promising and alternative drug targets against c difficile these include fmndependent nitroreductase which was prioritized for pharmacophorebased virtual screening of druggable molecule databases to predict potent inhibitors a molport001785965 druggable molecule was found to exhibit significant binding affinity with the conserved residues of fmndependent nitroreductase the experimental validation of the therapeutic targets prioritized in the current study may worthy to identify new strategies to combat the drugresistant c difficile infection
matrix factorization mf is a widely used collaborative filtering cf algorithm for recommendation systems rss due to its high prediction accuracy great flexibility and high efficiency in big data processing however with the dramatically increased number of usersitems in current rss the computational complexity for training a mf model largely increases many existing works have accelerated mf by either putting in additional computational resources or utilizing parallel systems introducing a large cost in this paper we propose algorithmic methods to accelerate mf without inducing any additional computational resources in specific we observe finegrained structured sparsity in the decomposed feature matrices when considering a certain threshold the finegrained structured sparsity causes a large amount of unnecessary operations during both matrix multiplication and latent factor update increasing the computational time of the mf training process based on the observation we firstly propose to rearrange the feature matrices based on joint sparsity which potentially makes a latent vector with a smaller index more dense than that with a larger index the feature matrix rearrangement is given to limit the error caused by the later performed pruning process we then propose to prune the insignificant latent factors by an early stopping process during both matrix multiplication and latent factor update the pruning process is dynamically performed according to the sparsity of the latent factors for different usersitems to accelerate the process the experiments show that our method can achieve 12165 speedups with up to 2008 error increase compared with the conventional mf training process we also prove the proposed methods are applicable considering different hyperparameters including optimizer optimization strategy and initialization method
in today’s dynamic business landscape retailers are progressively turning to omnichannel strategies to offer integrated shopping experiences across various touchpoints this study endeavors to elucidate the determinants driving loyalty among omnichannel consumers a comprehensive research framework encompassing technological elements instore attributes online reviews and behavioral variables was formulated a sample of 252 customers familiar with omnichannel shopping was evaluated structural equation modeling was utilized for data analysis results indicate that personal interactions and merchandise variety significantly influence crowd perception perceived ease of use was found to significantly affect perceived usefulness attitude and satisfaction furthermore both crowd perception and attitudes toward online reviews markedly influence the overall attitude toward omnichannel the data underscores the pivotal role of attitude in enhancing satisfaction and fostering loyalty among consumers in conclusion this research not only sheds light on the intricate dynamics of omnichannel customer loyalty but also offers valuable insights for both academia and industry underscoring the importance of integrated multichannel strategies in contemporary retailing
conventional oct retinal disease classification methods primarily rely on fully supervised learning which requires a large number of labeled images however sometimes the number of labeled images in a private domain is small but there exists a large annotated open dataset in the public domain in response to this scenario a new transfer learning method based on subdomain adaptation tlsda which involves a first subdomain adaptation and then finetuning was proposed in this study firstly a modified deep subdomain adaptation network with pseudolabel dsanpl was proposed to align the feature spaces of a public domain labeled and a private domain unlabeled the dsanpl model was then finetuned using a small amount of labeled oct data from the private domain we tested our method on three open oct datasets using one as the public domain and the other two as the private domains remarkably with only 10 labeled oct images 100 images per category tlsda achieved classification accuracies of 9363 and 9659 on the two private datasets significantly outperforming conventional transfer learning approaches with the gradientweighted class activation map gradcam technique it was observed that the proposed method could more precisely localize the subtle lesion regions for oct image classification tlsda could be a potential technique for applications where only a small number of images is labeled in a private domain and there exists a public database having a large number of labeled images with domain difference
our paper presents a novel defence against black box attacks where attackers use the victim model as an oracle to craft their adversarial examples unlike traditional preprocessing defences that rely on sanitizing input samples our stateless strategy counters the attack process itself for every query we evaluate a countersample instead where the countersample is the original sample optimized against the attackers objective by countering every black box query with a targeted white box optimization our strategy effectively introduces an asymmetry to the game to the defenders advantage this defence not only effectively misleads the attackers search for an adversarial example it also preserves the models accuracy on legitimate inputs and is generic to multiple types of attacks we demonstrate that our approach is remarkably effective against stateoftheart black box attacks and outperforms existing defences for both the cifar10 and imagenet datasets additionally we also show that the proposed defence is robust against strong adversaries as well
distributed optimization in resource constrained devices demands both communication efficiency and fast convergence rates newtontype methods are getting preferable due to their superior convergence rates compared to the firstorder methods in this paper we study a new problem in regard to the secondorder distributed optimization over unreliable networks the working devices are powerlimited or operate in unfavorable wireless channels experiencing packet losses during their uplink transmission to the server our scenario is very common in realworld and leads to instability of classical distributed optimization methods especially the secondorder methods because of their sensitivity to the imprecision of local hessian matrices to achieve robustness to high packet loss communication efficiency and fast convergence rates we propose a novel distributed secondorder method called rednew packet loss resilient distributed approximate newton each iteration of rednew comprises two rounds of lightweight and lossy transmissions in which the server aggregates the local information with a new developed scaling strategy we prove the linearquadratic convergence rate of rednew experimental results demonstrate its advantage over firstorder and secondorder baselines and its tolerance to packet loss rate ranging from 5 to 40
flexible electrode films with good filmforming properties large deformation ability high conductivity and strong charge and discharge capabilities are crucial for ionic electroactive polymer soft actuators however there are still challenges in preparing highquality electrode films that can combine well with the intermediate polyelectrolyte to form highperformance soft actuators herein we propose an advanced sandwich ionic electroactive actuator utilizing selfstanding bacterial cellulose bc reinforced poly34ethylenedioxythiophenepoly4styrenesulfonate pp doped with graphene oxide go conductive composite electrodes and a nafion ionexchange membrane via a hotpressing method the prepared bc–pp–go electrodes have good filmforming properties with a youngs modulus of 1360 mpa and a high conductivity of 150 s cm−1 the hotpressed bc–pp–gonafion ionic actuator exhibited a large bending displacement of 62 mm 1 v 01 hz with a longterm actuation stability up to 95 over 360 cycles without degradation furthermore we introduced the actuators potential applications including bionic grippers flies and fish providing more opportunities for the development of nextgeneration micromanipulators and biomimetic microrobots in cmscale space
heterogeneity is omnipresent across all living systems diversity enriches the dynamical repertoire of these systems but remains challenging to reconcile with their manifest robustness and dynamical persistence over time a fundamental feature called resilience to better understand the mechanism underlying resilience in neural circuits we considered a nonlinear network model extracting the relationship between excitability heterogeneity and resilience to measure resilience we quantified the number of stationary states of this network and how they are affected by various control parameters we analyzed both analytically and numerically gradient and nongradient systems modeled as nonlinear sparse neural networks evolving over long time scales our analysis shows that neuronal heterogeneity quenches the number of stationary states while decreasing the susceptibility to bifurcations a phenomenon known as trivialization heterogeneity was found to implement a homeostatic control mechanism enhancing network resilience to changes in network size and connection probability by quenching the systems dynamic volatility
the popularity of commercial unmanned aerial vehicles has drawn great attention from the ecommerce industry due to their suitability for lastmile delivery however the organization of multiple aerial vehicles efficiently for delivery within limitations and uncertainties is still a problem the main challenge of planning is scalability since the planning space grows exponentially to the number of agents and it is not efficient to let humanlevel supervisors structure the problem for largescale settings algorithms based on deep qnetworks had unprecedented success in solving decisionmaking problems extension of these algorithms to multiagent problems is limited due to scalability issues this work proposes an approach that improves the performance of deep qnetworks on multiagent delivery by drone problems by utilizing state decompositions for lowering the problem complexity curriculum learning for handling the exploration complexity and genetic algorithms for searching efficient packetdrone matching across the combinatorial solution space the performance of the proposed method is shown in a multiagent delivery by drone problem that has 10 agents and ≈1077 state–action pairs comparative simulation results are provided to demonstrate the merit of the proposed method the proposed geneticalgorithmaided multiagent drl outperformed the rest in terms of scalability and convergent behavior
climate change is escalating the threat of heat stress to global public health with the majority of humans today facing increasingly severe and prolonged heat waves accurate weather data reflecting the complexity of measuring heat stress is crucial for reducing the impact of extreme heat on health worldwide previous studies have employed heat index hi and wet bulb globe temperature wbgt metrics to understand extreme heat exposure forming the basis for heat stress guidelines however systematic comparisons of meteorological and climate data sets used for these metrics and the related parameters like air temperature humidity wind speed and solar radiation crucial for human thermoregulation are lacking we compared three heat measures himax wbgtbernard and wbgtliljegren approximated from gridded weather data sets era5‐land prism daymet with ground‐based data revealing strong agreement from hi and wbgtbernard r2 076–095 rmse 169–664°c discrepancies varied by köppen‐geiger climates eg adjusted r2 himax 088–095 wbgtbernard 079–097 and wbgtliljegren 080–096 and metrological input variables adjusted r2 tmax 086–094 tmin 091–094 wind 033 solarmax 038 solaravg 038 relative humidity 051–074 gridded data sets can offer reliable heat exposure assessment but further research and local networks are vital to reduce measurement errors to fully enhance our understanding of how heat stress measures link to health outcomes
abstract the escalating complexity and sophistication of software vulnerabilities demand innovative approaches in cybersecurity this study introduces a groundbreaking framework named “codesentry” employing a transformerbased model for vulnerability detection in software code “codesentry” leverages a finelytuned version of the generative pretrained transformer gpt optimized for pinpointing vulnerable code patterns across various benchmark datasets this approach stands apart by its remarkable computational efficiency making it suitable for realtime applications − a significant advancement over traditional resourceintensive deep learning models like cnns and lstms empirical results showcase “codesentry” achieving an impressive 9265 accuracy in vulnerability detection surpassing existing stateoftheart methods such as sysevr and vuldebert this novel methodology marks a paradigm shift in vulnerability detection blending advanced ai with practical application efficiency
the cornea is a viscoelastic biological tissue with nonlinear anisotropic
abstract this article describes the rationale aims and methodology of the accelerating medicines partnership® schizophrenia amp® scz this is the largest international collaboration to date that will develop algorithms to predict trajectories and outcomes of individuals at clinical high risk chr for psychosis and to advance the development and use of novel pharmacological interventions for chr individuals we present a description of the participating research networks and the data processing analysis and coordination center their processes for data harmonization across 43 sites from 13 participating countries recruitment across north america australia europe asia and south america data flow and quality assessment processes data analyses and the transfer of data to the national institute of mental health nimh data archive nda for use by the research community in an expected sample of approximately 2000 chr individuals and 640 matched healthy controls amp scz will collect clinical environmental and cognitive data along with multimodal biomarkers including neuroimaging electrophysiology fluid biospecimens speech and facial expression samples novel measures derived from digital health technologies including smartphonebased daily surveys and passive sensing as well as actigraphy the study will investigate a range of clinical outcomes over a 2year period including transition to psychosis remission or persistence of chr status attenuated positive symptoms persistent negative symptoms mood and anxiety symptoms and psychosocial functioning the global reach of amp scz and its harmonized innovative methods promise to catalyze the development of new treatments to address critical unmet clinical and public health needs in chr individuals
extracting portable performance in an application requires structuring that program into a dataflow graph of coarsegrained tasks cgts structuring applications that interconnect multiple external libraries and custom code ie “code from the wild” cftw is challenging when experts manually restructure a program they trivialize the extraction of structure however this expertise is not broadly available automatic structuring approaches focus on the intersection of hot code and static loops ignoring the data dependencies between tasks and significantly reducing the scope of analyzeable programs this work addresses the problem of extracting the dataflow graph of cgts from cftw to that end we present cyclebite our approach extracts cgts from unstructured computeprograms by detecting cgt candidates in the simplified markov control graph mcg and localizing cgts in an epoch profile additionally the epoch profile extracts the data dependence between cgts required to build the dataflow graph of cgts cyclebite demonstrates a robust selectivity for critical cgts relative to the stateoftheart soa leading to a potential speedup of 12x on average and threadscaling of 24x on average compared to modern compiler optimizers we validate the results of cyclebite and compare them to two soa techniques using an input corpus of 25 opensource cc libraries with 2019 unique execution profiles
geometric matching is an important topic in computational geometry and has been extensively studied over decades in this paper we study a geometricmatching problem known as geometric manytomany matching in this problem the input is a set s of n colored points in mathbbrd which implicitly defines a graph g  ses where es  pq pq in s text have different colors and the goal is to compute a minimumcost subset e subseteq es of edges that cover all points in s here the cost of e is the sum of the costs of all edges in e where the cost of a single edge e is the euclidean distance or more generally the lpdistance between the two endpoints of e our main result is a 1varepsilonapproximation algorithm with an optimal running time ovarepsilonn log n for geometric manytomany matching in any fixed dimension which works under any lpnorm this is the first nearlinear approximation scheme for the problem in any d geq 2 prior to this work only the bipartite case of geometric manytomany matching was considered in mathbbr1 and mathbbr2 and the best known approximation scheme in mathbbr2 takes ovarepsilonn15 cdot mathsfpolylog n time
background the phenotypic traits of leaves are the direct reflection of the agronomic traits in the growth process of leafy vegetables which plays a vital role in the selection of highquality leafy vegetable varieties the current imagebased phenotypic traits extraction research mainly focuses on the morphological and structural traits of plants or leaves and there are few studies on the phenotypes of physiological traits of leaves the current research has developed a deep learning model aimed at predicting the total chlorophyll of greenhouse lettuce directly from the full spectrum of hyperspectral images results a cnnbased onedimensional deep learning model with spectral attention module was utilized for the estimate of the total chlorophyll of greenhouse lettuce from the full spectrum of hyperspectral images experimental results demonstrate that the deep neural network with spectral attention module outperformed the existing standard approaches including partial least squares regression plsr and random forest rf with an average r2 of 0746 and an average rmse of 2018 conclusions this study unveils the capability of leveraging deep attention networks and hyperspectral imaging for estimating lettuce chlorophyll levels this approach offers a convenient nondestructive and effective estimation method for the automatic monitoring and production management of leafy vegetables
background p value is the most common statistic reported in scientific research articles choosing the conventional threshold of 005 commonly used for the p value in research articles is unfounded many researchers have tried to provide a reasonable threshold for the p value some proposed a lower threshold eg  0005 however none of the proposals has gained universal acceptance using the analogy between the diagnostic tests with continuous results and statistical inference tests of hypothesis i wish to present a method to calculate the most appropriate p value significance threshold using the receiver operating characteristic curve roc analysis results as with diagnostic tests where the most appropriate cutoff values are different depending on the situation there is no unique cutoff for the p significance threshold unlike the previous proposals which mostly suggest lowering the threshold to a fixed value  eg  from 005 to 0005 the most appropriate p significance threshold proposed here in most instances is much less than the conventional cutoff of 005 and varies from study to study and from statistical test to test even within a single study the proposed method provides the minimum weighted sum of type i and type ii errors conclusions given the perplexity involved in using the frequentist statistics in a correct way dealing with different p significance thresholds even in a single study it seems that the p value is no longer a proper statistic to be used in our research it should be replaced by alternative methods eg  bayesian methods
text mining is a valuable technique that empowers users to gain a deeper understanding of existing textual data ultimately allowing them to make more informed decisions one important application of text mining is in the field of sentiment analysis which has gained significant traction among companies aiming to understand how customers perceive their products and services in response to this growing need various research efforts have been made to improve the accuracy of sentiment analysis classification models the purpose of this article is to discuss a specific approach using the support vector machine svm algorithm which is often used in machine learning for text classification tasks and then combined with the application of particle swarm optimization pso which optimizes the svm model parameters to achieve the best classification results this dynamic combination not only improves accuracy but also enhances the models ability to efficiently handle large amounts of text data to achieve better results the research findings highlight the effectiveness of this approach the application of the svm algorithm with pso resulted in an outstanding accuracy performance of 9492 the substantial increase in accuracy compared to previous studies shows the promising potential of this methodology this proves that the svm algorithm model approach with particle swarm optimization provides good performance
compact antenna test range catr can generate a region of uniform pseudoplane wave called quiet zone which can provide a test zone for obtaining farfield measurement data of highperformance antenna in the near field the most important thing to be concerned in catr is the evaluation of quiet zone however most of catrs are mainly composed of reflectors which are electric large size antennas especially in millimeter mm and terahertz thz waveband thus it is timeconsuming and inefficient to use fullwave simulation method such as the method of moment mom or finitedifference time domain fdtd to analyze their performance and highfrequency solving techniques cannot calculate the exact radiated field in some cases in this article an accurate analytical method to fast determine the quiet zone of catr is proposed the aperture electric field distribution on the surface of main reflector can be obtained by the modified nearfield physical optics po algorithm according to the known aperture distribution the nearfield calculation formula is derived to calculate near fields of it and the correctness of our proposed expression is verified via the comparison with the reflector antenna working in the mm waveband with fullwave simulated method our proposed expression can be applied to the fast estimation of quiet zone different typical kinds of antennas applied in the compact range are employed for verification and the deviations between quiet zone amplitudes calculated by the proposed expression and amplitudes obtained from simulation or published paper are both less than 05 db our analytical method can ensure the calculation accuracy and improve the speed of quiet zone determination at the same time
the gaofen6 gf6 satellite equipped with a wide fullswath wfv sensor offers high spatial resolution and extensive coverage making it widely utilized in agricultural and forestry classification land resource monitoring and other fields accurate onorbit radiometric calibration of gf6wfv is crucial for these quantitative applications currently the absolute radiometric calibration of gf6wfv relies primarily on vicarious calibration conducted by the china center for resources satellite data and application cresda however annual vicarious calibration may not adequately capture the radiometric performance of gf6wfv due to performance degradation therefore increasing the frequency of onorbit radiometric calibration throughout the lifetime of gf6wfv is essential this study proposes a method for conducting longterm crossradiometric calibrations of gf6wfv by taking the multispectral imager msi onboard the sentinel2 satellite as a reliable reference sensor and the sites from radcalnet as reference ground targets firstly we conducted 62 onorbit crossradiometric calibrations of gf6wfv since its launch by tracking with the sentinel2msi sensor after correcting the discrepancy spectrum and solar zenith angle then validation of crossradiometric calibration results against radcalnet products indicated an average absolute relative error between 355 and 464 crossvalidation with additional reference sensors including landsat8oli and modis confirmed the reliability of calibration demonstrating relative differences from gf6wfv of less than 5 furthermore the overall uncertainty of the crossradiometric calibration was estimated to be from 408 to 489 finally trend analysis of the timeseries radiometric performance was also conducted and revealed an annual degradation rate ranging from 057 to 231 this degradation affects surface reflectance retrieval introducing a bias of approximately 00073 to 00084 our findings highlight the operational effectiveness of the proposed method in achieving longtimeseries onorbit radiometric calibration and degradation monitoring of gf6wfv the study also demonstrates that the radiometric performance of gf6wfv is relatively stable and suitable for further quantitative applications especially for longterm monitoring applications
federated studying is a disbursed system learning method that lets a set of dispensed users or clients collaboratively learn a shared predictive model without sharing any statistics through taking part in the model education method every purchaser contributes to the development of a worldwide predictive model without sacrificing its very own information privacy this privateness renovation may be achieved through various techniques which include differential privateness cozy multicelebration computation and information obfuscation in this way model schooling isnt always a task exclusively for centralized facts creditors but as an alternative a largescale collaboration amongst multiple entities with disparate hobbies and facts silos in the technology of privacy protection federated studying represents a powerful tool that enables the construction of predictive fashions that might be knowledgeable by means of various information resources at the same time as retaining the integrity and privateness of the individual statistics units
stochastic computing is a relatively new approach to computing that has gained interest in recent years due to its potential for lowpower and highnoise environments it is a method of computing that uses probability to represent and manipulate data therefore it has applications in areas such as signal processing machine learning and cryptography stochastic steganography involves hiding a message within a cover image using a statistical model unlike traditional steganography techniques that use deterministic algorithms to embed the message stochastic steganography uses a probabilistic approach to hide the message in a way that makes it difficult for an adversary to detect due to this error robustness and large bit streams stochastic computing they are well suited for high capacity and secure image steganography in this paper as per the authors’ best knowledge image steganography using stochastic computing based on linear feedback shift register lfsr is proposed for the first time in the proposed technique the cover image is converted to stochastic representation instead of the binary one and then a secret image is embedded in it the resulting stego image has a high psnr value transmitted with no visual trace of the hidden image the final results are stego image with psnr starting from 30 db and a maximum payload up to 40 bits per pixel bpp with an effective payload up to 28 bpp the proposed method achieves high security and high capability of the number of stored bits in each pixel thus the proposed method can prove a vital solution for high capacity and secure image steganography which can then be extended to other types of steganography
low earth orbit leo satellites have emerged as crucial enablers of direct connections with remote terrestrial terminals however energy limitations and insufficient antenna capabilities at the terminals often hamper these connections resulting in inefficient communications and frequent pingpong handovers this paper proposes a distributed collaborative beamforming dcbbased uplink communication paradigm for enabling groundspace direct communications specifically dcb treats the terminals that are unable to establish efficient direct connections with the leo satellites as distributed antennas forming a virtual antenna array to enhance the terminaltosatellite uplink achievable rates and durations however such systems need multiple tradeoff policies that jointly balance the terminalsatellite uplink achievable rate energy consumption of terminals and satellite switching frequency to satisfy the scenario requirement changes thus we formulate a longterm multiobjective optimization problem to optimize these goals simultaneously to address availability in different terminal cluster scales we reformulate this problem into an action spacereduced and universal multiobjective markov decision process momdp then we propose an evolutionary multiobjective deep reinforcement learning emodrl algorithm to obtain multiple policies in which the lowvalue actions are masked to speed up the training process simulation results show that dcb enables terminals that cannot reach the uplink achievable rate threshold to achieve efficient direct uplink transmission moreover the proposed algorithm outmatches various baselines and saves 30 handover frequency with a similar uplink achievable rate compared with the rate greedy method which thus reveals that the proposed method is an effective solution for enabling direct groundspace communications
cancer treatments are known to introduce cardiotoxicity negatively impacting outcomes and survivorship identifying cancer patients at risk of heart failure hf is critical to improving cancer treatment outcomes and safety this study examined machine learning ml models to identify cancer patients at risk of hf using electronic health records ehrs including traditional ml timeaware long shortterm memory tlstm and large language models llms using novel narrative features derived from the structured medical codes we identified a cancer cohort of 12806 patients from the university of florida health diagnosed with lung breast and colorectal cancers among which 1602 individuals developed hf after cancer the llm gatortron39b achieved the best f1 scores outperforming the traditional support vector machines by 39 the tlstm deep learning model by 7 and a widely used transformer model bert by 56 the analysis shows that the proposed narrative features remarkably increased feature density and improved performance
background and objectives many research studies show that selfstigma related to weight can exacerbate mental health issues there is also evidence suggesting that depression anxiety and stress could be predictors of weight stigma however these connections have not been thoroughly investigated among young people in saudi arabia where there is a high prevalence of obesity this study aimed to explore the relationships between depression anxiety stress and weight selfstigma in saudi arabian adolescents and young adults materials and methods this crosssectional study was conducted between january and march 2022 and utilized online surveys including the weight selfstigma questionnaire wssq and depression anxiety and stress scale 21 dass21 we conducted descriptive analysis independent samples ttests analysis of variance anova and linear regression for the statistical analysis using spss version 25 ibm corp armonk ny results a total of 1624 participants were enrolled in this survey most participants 889 547 were females the mean age was 2073 years sd 263 males were more likely to report selfstigma compared to females p  0018 weight selfstigma demonstrated significant positive associations with body mass index bmi p  00001 and depression anxiety and stress p  00001 scores further analysis revealed weight selfstigma was positively correlated with psychological distress levels in the study population p  00001 the results demonstrated a statistically significant difference between different regions of saudi arabia with weight selfstigma being more prevalent in the albaha region and least prevalent in the almadinah region conclusions the results of this study indicate robust positive correlations between weight selfstigma scores and depression anxiety and stress scale scores among adolescents and young adults in saudi arabia further epidemiological and clinical studies on the national level are warranted
active contour model acm is considered as one of the most frequently employed models in image segmentation due to its effectiveness and efficiency however the segmentation results of images with intensity nonuniformity processed by the majority of existing acms are possibly inaccurate or even wrong in the forms of edge leakage long convergence time and poor robustness in addition they usually become unstable with the existence of different initial contours and unevenly distributed intensity to better solve these problems and improve segmentation results this paper puts forward an acm approach using adaptive local prefitting energy alpf for image segmentation with intensity nonuniformity firstly the prefitting functions generate fitted images inside and outside contour line ahead of iteration which significantly reduces convergence time of level set function next an adaptive regularization function is designed to normalize the energy range of datadriven term which improves robustness and stability to different initial contours and intensity nonuniformity lastly an improved length constraint term is utilized to continuously smooth and shorten zero level set which reduces the chance of edge leakage and filters out irrelevant background noise in contrast with newly constructed acms alpf model not only improves segmentation accuracy intersection over unioniou but also significantly reduces computation cost cpu operating time t while handling three types of images experiments also indicate that it is not only more robust to different initial contours as well as different noise but also more competent to process images with intensity nonuniformity
background small airways disease sad is a major cause of airflow obstruction in copd patients and has been identified as a precursor to emphysema although the amount of sad in the lungs can be quantified using our parametric response mapping prm approach the full breadth of this readout as a measure of emphysema and copd progression has yet to be explored we evaluated topological features of prmderived normal parenchyma and sad as surrogates of emphysema and predictors of spirometric decline methods prm metrics of normal lung prmnorm and functional sad prmfsad were generated from ct scans collected as part of the copdgene study n  8956 volume density v and eulerpoincaré characteristic χ image maps measures of the extent and coalescence of pocket formations ie topologies respectively were determined for both prmnorm and prmfsad association with copd severity emphysema and spirometric measures were assessed via multivariable regression models readouts were evaluated as inputs for predicting fev1 decline using a machine learning model results multivariable crosssectional analysis of copd subjects showed that v and χ measures for prmfsad and prmnorm were independently associated with the amount of emphysema readouts χfsad β of 0106 p  0001 and vfsad β of 0065 p  0004 were also independently associated with fev1 predicted the machine learning model using prm topologies as inputs predicted fev1 decline over five years with an auc of 069 conclusions we demonstrated that v and χ of fsad and norm have independent value when associated with lung function and emphysema in addition we demonstrated that these readouts are predictive of spirometric decline when used as inputs in a ml model our topological prm approach using prmfsad and prmnorm may show promise as an early indicator of emphysema onset and copd progression
while videostroboscopy is recognized as the most popular approach for investigating vocal fold function evaluating the numerical values such as the membranous glottal gap area remains too time consuming for clinical applications
we propose a new fast algorithm for simultaneous recovery of the coil sensitivities and of the magnetization image from incomplete fourier measurements in parallel mri our approach is based on a parameter model for the coil sensitivities using bivariate trigonometric polynomials of small degree the derived mocca algorithm has low computational complexity of onc n2 log n for n times n images and nc coils and achieves very good performance for incomplete mri data we present a complete mathematical analysis of the proposed reconstruction method further we show that mocca achieves similarly good reconstruction results as espirit with a considerably smaller numerical effort which is due to the employed parameter model our numerical examples show that mocca can outperform several other reconstruction methods
in a pvdominant dc microgrid the traditional energy distribution method based on the droop control method has problems such as output voltage drop insufficient power distribution accuracy etc meanwhile different battery energy storage units usually have different parameters when the system is running therefore this paper proposes an improved control method that introduces a reference current correction factor and a weighted calculation method for load power distribution based on the parameters of battery energy storage units is proposed to achieve weighted allocation of load power in addition considering the variation of bus voltage at the time of load mutation voltage secondary control is added to realize dynamic adjustment of dc bus voltage fluctuation the proposed method can achieve balance and stable operation of energy storage units the simulation results verified the effectiveness and stability of the proposed control strategy
effective interaction modeling and behavior prediction of dynamic agents play a significant role in interactive motion planning for autonomous robots although existing methods have improved prediction accuracy few research efforts have been devoted to enhancing prediction model interpretability and outofdistribution ood generalizability this work addresses these two challenging aspects by designing a variational autoencoder framework that integrates graphbased representations and timesequence models to efficiently capture spatiotemporal relations between interactive agents and predict their dynamics our model infers dynamic interaction graphs in a latent space augmented with interpretable edge features that characterize the interactions moreover we aim to enhance model interpretability and performance in ood scenarios by disentangling the latent space of edge features thereby strengthening model versatility and robustness we validate our approach through extensive experiments on both simulated and realworld datasets the results show superior performance compared to existing methods in modeling spatiotemporal relations motion prediction and identifying timeinvariant latent features
in complex and dynamic environments traditional pursuit–evasion studies may face challenges in offering effective solutions to sudden environmental changes in this paper a bioinspired neural network binn is proposed that approximates a pursuit–evasion game from a neurodynamic perspective instead of formulating the problem as a differential game the binn is topologically organized to represent the environment with only local connections the dynamics of neural activity characterized by the neurodynamic shunting model enable the generation of realtime evasive trajectories with moving or suddenchange obstacles several simulation and experimental results indicate that the proposed approach is effective and efficient in complex and dynamic environments
the covid19 pandemic has generated an unprecedented amount of epidemiological data yet concerns regarding the validity and reliability of the information reported by health surveillance systems have emerged worldwide in this paper we develop a novel approach to evaluating data integrity by combining the newcombbenford law with outlier methods we demonstrate the advantages of our framework using a case study from china to ensure more robust findings we employ multiple diagnostic procedures including three conformity estimates four goodnessoffit tests and two distance measures cook and mahalanobis to promote transparency we have made all computational scripts publicly available our findings indicates a significant deviation in the distribution of new deaths from the theoretical expectations of benfords law importantly these results remain accurate even when considering alternative model specifications and conducting various statistical tests furthermore the procedures developed here are easily applicable in other areas of knowledge and can be scaled to assess data quality in both the public and private sectors
we present a generalpurpose algorithm to extrapolate a low rank function of two variables from a small domain to a larger one it is based on the crossinterpolation formula we apply it to reconstruct physical quantities in some quantum manybody perturbative expansions in the real time keldysh formalism considered as a function of time t and interaction u these functions are of remarkably low rank this property combined with the convergence of the perturbative expansion in u both at finite t for any u and small u for any t is sufficient for our algorithm to reconstruct the physical quantity at long time strong coupling regime our method constitutes an alternative to standard resummation techniques in perturbative methods such as diagrammatic quantum monte carlo we benchmark it on the single impurity anderson model and show that it is successful even in some regime where standard conformal mapping resummation techniques fail
this study addresses the pervasive issue of abusive language in online video game communication channels focusing on dota 2 chat messages the aim was to employ diverse traditional machine learning algorithms and advanced deep learning architectures to identify and classify toxic and abusive language effectively leveraging tfidf glove word embeddings and selftrained embeddings the research compared various classical machine learning models such as naïve bayes logistic regression and support vector machine with convolutional and recurrent neural network models the results revealed a consistent trend where deep learning models particularly those employing grus and lstms outperformed classical machine learning models experiments also demonstrated that selftrained embeddings generally outperformed glove embeddings in the domain of online video game chat messages
decision making and learning under uncertainty especially with adversarial attacks is crucial for reliable operations this paper introduces a novel adversarial training method for robust linear and nonlinear classifiers inspired by support vector machine svm margins we derive finite sample complexity bounds for binary and multiclass classifiers which align with those of natural classifiers our algorithm uses linear programming lp and second order cone programming socp for linear and nonlinear models respectively experiments on mnist and cifar10 datasets demonstrate performance comparable to stateoftheart methods without requiring adversarial examples during training our approach provides a robust framework for enhancing classifier resilience against adversaries
the article introduces the stochastic nk interdiction problem for power grid operations and planning that aims to identify a subset of k components out of n components that maximizes the expected damage measured in terms of load shed uncertainty is modeled through a fixed set of outage scenarios where each scenario represents a subset of components removed from the grid we formulate the stochastic nk interdiction problem as a bilevel optimization problem and propose two algorithmic solutions the first approach reformulates the bilevel stochastic optimization problem to a singlelevel mixedinteger linear program milp by dualizing the inner problem and solving the resulting problem directly using a milp solver to global optimality the second is a heuristic cuttingplane approach which is exact under certain assumptions we compare these approaches in terms of computation time and solution quality using the ieeereliability test system and present avenues for future research
a polarizationinsensitive optical filter is proposed and demonstrated with 340nmthick silicon photonic waveguides it consists of a polarizationinsensitive mode demultiplexer and a polarizationinsensitive multimode waveguide grating mwg the mwg is designed optimally according to the polarizationinsensitive phasematching condition meanwhile longitudinal apodization is introduced in the mwg to suppress the sidelobes for the fabricated silicon photonic filter based on a single stage of mwg the 1db bandwidth as large as ∼13 nm the excess loss is ∼1 db and the sidelobe suppression ratio is about 10 db for both polarizations in particular the performance is improved greatly by introducing more than one mwgs in cascade and the fabricated device has a sidelobe suppression ratio as high as 30 db
this study delves into the exploration of hybrid nanofluids within the context of a stretching cylinder a domain that has captivated numerous researchers owing to its pivotal applications in industrial manufacturing processes particularly in metal forming and stretch dies the authors recognizing the significance of these applications have introduced a novel heat transfer fluid termed hybrid nanofluid comprising molybdenum disulfide and carbon nanotubes suspended in the base liquid water the investigation focuses on the flow of the hybrid nanofluid within a stretching cylinder considering various influential factors such as joule heating thermal radiation porous medium and magnetic field effects to model this complex problem we employed modified navier–stokes equations employing similarity transformations assumptions and nondimensional parameters the problem was effectively simplified the matlab bvp4c technique a wellestablished numerical approach was then employed to solve the resulting mathematical formulation graphical representations are presented to illustrate various aspects of the flow facilitating a comprehensive understanding of the system comparisons are drawn among the flow characteristics of mono nanofluid and the developed hybrid nanofluid it is noted from the current analysis that the temperature strength increases with higher values of the magnetic field parameter curvature parameter radiation parameter and eckert number in both fluid cases the nusselt number increases with higher values of prandtl number and thermal relaxation parameter the identified patterns in velocity distribution temperature strength and fluid behavior provide a valuable foundation for optimizing thermal efficiency in diverse industrial applications by leveraging the insights gained from this research manufacturers can make informed decisions to enhance heat transfer processes particularly in areas such as metal forming and stretch dies
a fundamental question in neurodevelopmental biology is how flexibly the nervous system changes during development to address this we reconstructed the chemical connectome of dauer an alternative developmental stage of nematodes with distinct behavioral characteristics by volumetric reconstruction and automated synapse detection using deep learning with the basic architecture of the nervous system preserved structural changes in neurons large or small were closely associated with connectivity changes which in turn evoked dauerspecific behaviors such as nictation graph theoretical analyses revealed significant dauerspecific rewiring of sensory neuron connectivity and increased clustering within motor neurons in the dauer connectome we suggest that the nervous system in the nematode has evolved to respond to harsh environments by developing a quantitatively and qualitatively differentiated connectome
despite the recent progress on 6d object pose estimation methods for robotic grasping a substantial performance gap persists between the capabilities of these methods on existing datasets and their efficacy in realworld grasping and mobile manipulation tasks particularly when robots rely solely on their monocular egocentric field of view fov existing realworld datasets primarily focus on tabletop grasping scenarios where a robot arm is placed in a fixed position and the objects are centralized within the fov of fixed external cameras assessing performance on such datasets may not accurately reflect the challenges encountered in everyday grasping and mobile manipulation tasks within kitchen environments such as retrieving objects from higher shelves sinks dishwashers ovens refrigerators or microwaves to address this gap we present kitchen a novel benchmark designed specifically for estimating the 6d poses of objects located in diverse positions within kitchen settings for this purpose we recorded a comprehensive dataset comprising around 205k realworld rgbd images for 111 kitchen objects captured in two distinct kitchens utilizing a humanoid robot with its egocentric perspectives subsequently we developed a semiautomated annotation pipeline to streamline the labeling process of such datasets resulting in the generation of 2d object labels 2d object segmentation masks and 6d object poses with minimal human effort the benchmark the dataset and the annotation pipeline are publicly available at httpskitchendatasetgithubiokitchen
accurate prediction of remaining useful life rul serves as the foundation for predictive maintenance of industrial equipment in recent years the fusion of multisource information has achieved remarkable advancements in the development and application of rul prediction however under timevarying operating conditions the distribution of monitoring data exhibits timevarying characteristics posing two challenges for rul prediction in this scenario one is the adaptive decoupling of operating condition data and monitoring data and the other is an adaptive weighting of multisource information to address these challenges a novel method for rul prediction is proposed in this article driven by the fusion of multisource information under timevarying operating conditions the proposed approach is designed to track the degradation process of equipment in scenarios involving cyclic variation and multiple levels in operating conditions an optimization function is constructed to comprehensively characterize the frequency domain distribution of current signals and the continuity of the health index over time then a timevarying observation matrix for the degradation state space model is derived which aims to eliminate the influence of operating condition data on degradation information two kalman filter models are developed based on the linear degradation model and double exponential degradation model focused on different stages of equipment degradation which can calculate timevarying weights for vibration and sound information at different time coordinates in this way a multidimensional data mapping from multisource information to the degradation curve is established under timevarying operating conditions in order to verify the superiority of the proposed method in rul prediction two sets of runtofailure experimental datasets are studied and analyzed the result demonstrates that the proposed method achieves superior performance compared with singlesource information methods
background expenditure of healthcare services has been growing over the past decades lean and agile are two popular paradigms that could potentially contain cost and improve proficiency of the healthcare system however no systematic review was found on leagilty in the healthcare research this study aims at synthesizing the extant literature of leagility in the healthcare area to consolidate its potential and identify research gaps for future study in the field methods a systematic literature review is conducted following the prisma checklist approach studies were searched in multiple databases the selection of articles was executed by dualscanning of two researchers to ensure quality of data and relevance to the topic scientific articles published between january 1999 and november 2023 concerning leagile healthcare are analysed using microsoft excel and vosviewer version 1618 results out of 270 articles identified from the inclusion and exclusion criteria 24 were included in the review a total of 11 target areas were identified in leagility applications in healthcare success and limiting factors of leagile healthcare were classified into macro and micro aspects and further categorized into six dimensions policy organization human resources marketing operation management and technology moreover four research gaps were revealed and suggestions were provided for future study conclusion leagility in the healthcare context is still being in its infancy few empirical validation was found in leagile healthcare literature further exploration into the application of theory in various sectors under the scope of healthcare is appealed for standardization and modularization leadership support skillfulness of professionals and staff training are the factors most frequently mentioned for a successful implementation of leagility in the healthcare sector supplementary information the online version contains supplementary material available at 101186s12913024107710
the pursuit of efficiency or legitimacy is an important choice facing corporate sustainability especially in uncertain environments digital transformation contributes to corporate efficiency while fulfilling corporate social responsibility csr is a nonmarketbased strategy for companies seeking legitimacy however sustainability research remains unclear about the link between digital transformation and csr and the mechanisms behind it to fill this research gap we incorporate economic policy uncertainty epu into our analytical framework and elucidate the theoretical mechanism of the simultaneous bidirectional relationship between digital transformation and csr from the perspective of organizational legitimacy we construct a simultaneous equation model and analyze a sample of 468 chinese listed companies collected from 2011 to 2018 using the threestage least squares method our findings reveal a mutually reinforcing bidirectional relationship between digital transformation and csr furthermore we discover that epu weakens the facilitating influence of digital transformation on csr but strengthens the positive impact of csr on digital transformation our conclusions suggest that companies taking on more csr in digital transformation contexts can help achieve the organic unity of efficiency and legitimacy for sustainable development but uncertainty can undermine this positive interaction
solidstate additive friction stir deposition afsd is a thermomechanicalbased additive manufacturing technique for this study afsd was utilized to produce aluminum alloy 6061 aa6061 blocks with varying layer thicknesses 1 mm 2 mm and 3 mm the mechanical properties were assessed through uniaxial tensile tests and vickers microhardness measurement and statistical analysis was employed to investigate differences among data groups the results revealed that the deposition layer thickness influences tensile properties in the building z direction while the properties in the x and y directions showed minor differences across the three afsd blocks furthermore variations in tensile properties were observed depending on the sample orientation in the afsd blocks and its depthwise position in the part in the building direction the microhardness values decreased nonlinearly along the building direction spread across the width of the part’s crosssection and highlighted that the deposition layer thickness significantly affects this property the 1 mm block exhibited lower average microhardness values than the 2 mm and 3 mm blocks the temperature histories and dynamic heat treatment are influenced by the deposition layer thickness and depend on the location of the point being studied in the part resulting in variations in the microstructure and mechanical properties along the building direction and across the part’s width
in the field of optoelectronic tracking precisely modeling the motion equations of the tracked target is often challenging and in some cases they may even be entirely unknown this necessitates the use of a robust state estimator for accurate state estimation additionally atmospheric turbulence variations in illumination and intricate observation backgrounds may introduce a significant increase in observation noise for the tracked target to address these challenges one approach is to introduce adaptive factors such as the mahalanobis method into the robust state estimator to enhance estimation accuracy however further exploration has revealed that adaptive factors designed using different methods offer unique advantages in scenarios with varying levels of noise amplification in this paper different adaptive factors are further combined using an interacting multiple model approach allowing the designed state estimator to exhibit stronger adaptability to noise amplification the stability and effectiveness of this algorithm are validated through program simulations double reflection mirror experiment and drone trace prediction demonstrating its applicability and reliability in diverse scenarios
this article provides a systematic literature review on applying different algorithms to municipal data processing aiming to understand how the data were collected stored preprocessed and analyzed to compare various methods and to select feasible solutions for further research several algorithms and data types are considered finding that clustering classification correlation anomaly detection and prediction algorithms are frequently used as expected the data is of several types ranging from sensor data to images it is a considerable challenge although several algorithms work very well such as long shortterm memory lstm for timeseries prediction and classification
due to the low cost and agile service provision mobile charging stations mcss have been deployed to complement fixed charging stations fcss in the internet of electric vehicles ioev with mcss a major concern is to enhance the charging efficiency of mcss the charging efficiency of mcss can be improved by prolonging the charging durations of mcss ie mcss should undertake the charging tasks as more as possible which can increase the charging profits of mcss and reduce the charging expenses of ievs evs with insufficient electricity besides evs and mcss are selfish in terms of charging expenses and charging profits respectively in this article we propose a bayesian game based bidding scheme for mobile charging enabled electric vehicles bbsmcev in bbsmcev each iev first calculates the maximum charging price mcp according to the potential expense if charged by nearby fcss and then the optimal charging price ocp is determined by the bayesian game model each mcs accepts the charging request with the largest charging profit extensive simulations and comparisons demonstrate the superior performance of our proposed bbsmcev ie with the bayesian game model ievs can rationally bid for the mobile charging services from mcss and thus bbsmcev can increase the charging profits of mcss and reduce the charging expenses of ievs effectively besides a proper tradeoff between the charging profits of mcss and the charging expenses of ievs can be achieved
nuclear power plays a vital role in providing reliable and clean energy to fulfill increasing demands in electricity worldwide it continues to be an essential source of national power supply as growing concerns about fossil fuel depletion global warming and emissions require utilizing sustainable energy sources one area contributing to the growth of nuclear power is the development of reactors that have enhanced protection and security thermal efficiency and design reactor efficiency can be studied by the burnup that occurs when a trisofueled pebble is inserted into the nuclear core and subsequently removed the levels of burnup are measured based on the length of time the pebble spends within the core in our design each pebble is numbered by multiple digits printed in six locations using ultrahigh temperature ceramic paint naturally computer vision techniques can be used to identify and time each pebble based on its digits as it enters and exits the core we present a deep learning approach that successfully tags each pebble by identifying its digits from a video stream of the entrance and exit of the core in a multistep method we extract only the clearest and most useful views of the pebble’s digits to classify as it rolls by this algorithm is robust against issues that occur for objects in movement such as motion blur rotations and glare we outperform other stateoftheart optical character recognition ocr models that fail to identify digits that are in motion our approach creates a safer and more efficient way to measure burnup within a core while contributing to the improvement of nuclear power produced by reactors1
automated speaking assessment asa typically involves automatic speech recognition asr and handcrafted feature extraction from the asr transcript of a learners speech recently selfsupervised learning ssl has shown stellar performance compared to traditional methods however sslbased asa systems are faced with at least three datarelated challenges limited annotated data uneven distribution of learner proficiency levels and nonuniform score intervals between different cefr proficiency levels to address these challenges we explore the use of two novel modeling strategies metricbased classification and loss reweighting leveraging distinct sslbased embedding features extensive experimental results on the icnale benchmark dataset suggest that our approach can outperform existing strong baselines by a sizable margin achieving a significant improvement of more than 10 in cefr prediction accuracy
heterogeneous autonomous robot teams consisting of multirotor and uncrewed surface vessels usvs have the potential to enable various maritime applications including advanced searchandrescue operations a critical requirement of these applications is the ability to land a multirotor on a usv for tasks such as recharging this paper addresses the challenge of safely landing a multirotor on a cooperative usv in harsh open waters to tackle this problem we propose a novel sequential distributed model predictive control mpc scheme for cooperative multirotorusv landing our approach combines standard tracking mpcs for the multirotor and usv with additional artificial intermediate goal locations these artificial goals enable the robots to coordinate their cooperation without prior guidance each vehicle solves an individual optimization problem for both the artificial goal and an input that tracks it but only communicates the former to the other vehicle the artificial goals are penalized by a suitable coupling cost furthermore our proposed distributed mpc scheme utilizes a spatialtemporal wave model to coordinate in realtime a safer landing location and time the multirotors landing to limit severe tilt of the usv
the effects of climate change on vegetation composition and distribution are evident in different ecosystems around the world although some climate‐derived alterations on vegetation are expected to result in changes in lifeform fractional cover disentangling the direct effects of climate change from different non‐climate factors such as land‐use change is challenging by applying “liebigs law of the minimum” in a geospatial context we determined the climate‐limited potential for tree shrub herbaceous and non‐vegetation fractional cover change for the conterminous united states and compared these potential rates to observed change rates for the period 1986 to 2018 we found that 10 of the land area of the conterminous united states appears to have climate limitations on the change in fractional cover with a high proportion of these sites located in arid and semiarid ecosystems in the southwest part of the country the rates of change in lifeform fractional cover for the remaining area of the country are likely limited by non‐climate factors such as the disturbance regime land management land‐use history soil conditions and species interactions and adaptations
students and educators spend significant time in learning spaces on university campuses energy efficiency has become a concern among facility managers given the need to maintain acceptable indoor air quality iaq levels during and after the covid19 pandemic this paper investigates the relationship between control and extraneous variables in a university classroom’s total mechanical ventilation kwh the model is built using grasshopper software on rhino version 7 our methodology encompasses 1 an extensive review of recent trends for studying iaq and energy 2 selecting parameters for simulation 3 model configuration on grasshopper and finally 4 a formulation of a pertinent equation to consolidate the relationship between the studied factors and the total mechanical ventilation energy kwh central to this study are two key research questions 1 what correlations exist between various parameters related to occupancy and iaq in educational spaces and 2 how can we optimize energy efficiency in university classrooms the main contribution of this research is a generated equation representing the annual mechanical ventilation energy consumption based on selected parameters of classroom height area occupancy window location and ventilation rate of hvac systems we find that occupancy and class volume are the two most influential factors directly affecting mechanical ventilation energy consumption the equation serves as a valuable estimation tool for facility managers designers and campus operations to investigate how fluctuations in occupancy can influence ventilation energy consumption in the physical attributes of a university classroom this enables proactive decisionmaking optimizing energy efficiency and resource allocation in realtime to promote sustainable and costeffective campus operations
the success of online retail platforms with hybrid reselling and agency selling has been well documented recently manufacturers are expanding channels in competitive retail platforms such as jingdong and tmall on the basis of reselling considering two asymmetric competitive retail platforms we investigate the incentive for channel expansion under various strategies agency selling in the incumbent platform agency selling or reselling in the asymmetric competing platform our analysis shows that the manufacturers channel expansion strategies depend on the market potential ratio competition intensity and agency channel costs manufacturers prefer channel expansion when agency channel costs are small or the market potential ratio is above a threshold interestingly even the competing platform has a smaller market potential manufacturers may still prefer to expand the agency selling channel in the competing platform rather than the incumbent platform when the market potential ratio is large under low agency channel costs the impact of competition on channel expansion strategies is nonmonotonic and moderate competition motivates manufacturers to expand the channel by reselling rather than agency selling in the competing platform furthermore we find moderate slotting fee or commission rate could make manufacturers and the incumbent platform better off when the market potential ratio is small
efficiently navigating through mobile applications to accomplish specific tasks can be timeconsuming and challenging particularly for users who are unfamiliar with the app or faced with intricate menu structures simplifying access to a particular screen is a shared user priority especially for individuals with diverse needs including those with specific accessibility requirements this underscores the exploration of innovative solutions to streamline the navigation process this work addresses the challenge of mapping natural language intents to user interfaces with a specific focus on the context of mobile applications the primary objective of this work is to provide users with a more intuitive and efficient method for accessing desired screens in mobile applications by expressing their intentions in natural language existing approaches to this task have relied heavily on qualitative human studies for evaluating the performance moreover widely used pretrained visionlanguage models such as contrastive languageimage pretraining clip struggle to generalize effectively to the unique visual characteristics of user interfaces acknowledging the limitations we introduce a novel approach that harnesses the power of the pretrained visionlanguage models specifically we investigate whether finetuning pretrained visionlanguage models on mobile screens can address the challenges posed by the intricate nature of mobile application interfaces our approach involves the utilization of stateoftheart pretrained text and image encoders and employing a supervised finetuning process where pretrained models are adapted to the specific needs of mobile screen interactions moreover a shared embedding space facilitates the alignment of embeddings of both text and image modalities fostering a cohesive understanding between the natural language intents and visual features of user interface elements we conduct extensive experimental evaluations using the screen2word dataset through systematic analysis and established metrics we examine the models’ ability to accurately map diverse linguistic intents to specific user interfaces our analysis demonstrates that finetuning yields substantial improvements over the zeroshot performance of the pretrained visionlanguage models
"
 desi aims to provide one of the tightest constraints on cosmological parameters by analysing the clustering of more than thirty million galaxies however obtaining such constraints requires special care in validating the methodology and efforts to reduce the computational time required through data compression and emulation techniques in this work we perform a rigorous validation of the pybird power spectrum modelling code with both a traditional emulated fullmodelling approach and the modelindependent shapefit compression approach by using cubic box simulations that accurately reproduce the clustering and precision of the desi survey we find that the cosmological constraints from shapefit and fullmodelling are consistent with each other at the ∼ 05σ level for the λcdm model both shapefit and fullmodelling are also consistent with the true λcdm simulation cosmology down to a scale of k
 max  020 hmpc1 even after including the hexadecapole for extended models such as the wcdm and the ocdm models we find that including the hexadecapole can significantly improve the constraints and reduce the modelling errors with the same k
 max while their discrepancies between the constraints from shapefit and fullmodelling are more significant than λcdm they remain consistent within 07σ lastly we also show that the constraints on cosmological parameters with the correlation function evaluated from pybird down to s
 min  30h
 1mpc are unbiased and consistent with the constraints from the power spectrum"
purposethis study aims to investigate the relationship between geographic diversification gd and export performance ep by analysing a sample of small exporters in an emerging marketdesignmethodologyapproachthe study sample comprised 96 small and mediumsized exporting enterprises smes in vietnam the data is analysed using multiple regression analysis mra hayes process model and fuzzyset qualitative comparative analysis fsqcafindingsthe results indicate that gd significantly negatively affects ep in this dilemma the export market orientation emo and digital transformation positively moderated the relationship between gd and ep such that the negative effect of gd on ep was weaker when emo and digital were strongeroriginalityvaluethis initial study contributes significantly to international business theories and practices which reveal the role of gd via firm digital capacity and emo in thriving smes’ ep this study might grant new insight into international business and a critical approach to addressing the new insights small firms may face in a fragile but technologically advanced world
abstract in this article we deal with the following p p fractional schrödingerkirchhoff equations with electromagnetic fields and the hardylittlewoodsobolev nonlinearity m   u  s  a p   − δ  p  a s u  v  x  ∣ u ∣ p − 2 u  λ ∫ r n ∣ u ∣ p μ  s  ∣ x − y ∣ μ d y ∣ u ∣ p μ  s  − 2 u  k ∣ u ∣ q − 2 u  x ∈ r n  mleftusapleftdelta pasuvleftx u p2ulambda leftmathopint limitsmathbbrnfrac u pmu s  xy mu rmdyright u pmu s 2uk u q2uhspace1emxin mathbbrn where 0  s  1  p 0lt slt 1lt p  p s  n pslt n  p  q  2 p s  μ  plt qlt 2psmu    0  μ  n 0lt mu lt n  λ lambda  and k k are some positive parameters p s  μ   p n − p μ 2 n − p s psmu  fracpnpfracmu 2nps is the critical exponent with respect to the hardylittlewoodsobolev inequality and functions v v and m m satisfy the suitable conditions by proving the compactness results using the fractional version of concentration compactness principle we establish the existence of nontrivial solutions to this problem
this paper studies the ratedistortionperception rdp tradeoff for a memoryless source model in the asymptotic limit of large blocklengths the perception measure is based on a divergence between the distributions of the source and reconstruction sequences conditioned on the encoder output first proposed by mentzer et al we consider the case when there is no shared randomness between the encoder and the decoder and derive a singleletter characterization of the rdp function for the case of discrete memoryless sources this is in contrast to the marginaldistribution metric case introduced by blau and michaeli whose rdp characterization remains open when there is no shared randomness the achievability scheme is based on lossy source coding with a posterior reference map for the case of continuous valued sources under the squared error distortion measure and the squared quadratic wasserstein perception measure we also derive a singleletter characterization and show that the decoder can be restricted to a noiseadding mechanism interestingly the rdp function characterized for the case of zero perception loss coincides with that of the marginal metric and further zero perception loss can be achieved with a 3db penalty in minimum distortion finally we specialize to the case of gaussian sources and derive the rdp function for gaussian vector case and propose a reverse waterfilling type solution we also partially characterize the rdp function for a mixture of gaussian vector sources
the emergence of neural radiance fields nerf has greatly impacted 3d scene modeling and novelview synthesis as a kind of visual media for 3d scene representation compression with high ratedistortion performance is an eternal target motivated by advances in neural compression and neural field representation we propose nerfcodec an endtoend nerf compression framework that integrates nonlinear transform quantization and entropy coding for memoryefficient scene representation since training a nonlinear transform directly on a large scale of nerf feature planes is impractical we discover that pretrained neural 2d image codec can be utilized for compressing the features when adding contentspecific parameters specifically we reuse neural 2d image codec but modify its encoder and decoder heads while keeping the other parts of the pretrained decoder frozen this allows us to train the full pipeline via supervision of rendering loss and entropy loss yielding the ratedistortion balance by updating the contentspecific parameters at test time the bitstreams containing latent code feature decoder head and other side information are transmitted for communication experimental results demonstrate our method outperforms existing nerf compression methods enabling highquality novel view synthesis with a memory budget of 05 mb
here we examine whether the personality dimension of openness to experience can be predicted from the individual google search history by web scraping individual text corpora ics were generated from 214 participants with a mean number of 5 million word tokens we trained word2vec models and used the similarities of each ic to label words which were derived from a lexical approach of personality these iclabelword similarities were utilized as predictive features in neural models for training and validation we relied on 179 participants and held out a test sample of 35 participants a grid search with varying number of predictive features hidden units and boost factor was performed as model selection criterion we used r2 in the validation samples penalized by the absolute r2 difference between training and validation the selected neural model explained 35 of the openness variance in the test sample while an ensemble model with the same architecture often provided slightly more stable predictions for intellectual interests knowledge in humanities and level of education finally a learning curve analysis suggested that around 500 training participants are required for generalizable predictions we discuss ics as a complement or replacement of surveybased psychodiagnostics
one of the fundamental arguments in quantum information theory is the uncertainty principle according to this principle two incompatible observables cannot be measured with high precision at the same time in this work we will use the entropic uncertainty relation in the presence of quantum memory considering a dissipative environment the effects of the detuning between the transition frequency of a quantum memory and the center frequency of a cavity on entropic uncertainty bound and the quantum correlation between quantum memory and measured particle will be studied it is shown that by increasing the detuning quantum correlation is maintained as a result due to the inverse relationship between the uncertainty bound and quantum correlations the measurement results are guessed more accurately
existing poi recommendation methods often fail to capture the finegrained preferences of users and face the challenge of modeling multiple relationships moreover knowledge graphbased recommendation methods are limited in storing dynamic user trajectories making them unsuitable for poi recommendation scenarios in this paper we propose a multiview heterogeneous knowledge learning model that utilizes techniques for heterogeneous knowledge representation learning and multiview context modeling our model comprehensively models user preferences and the relationships between users and pois by utilizing information from users’ visiting sequences and poi attributes knowledge graph specifically we design a heterogeneous knowledge embedding method to learn the representation of users and pois using poi attribute knowledge and users’ visiting sequences additionally we constructed a user trajectory similarity graph and a poi attribute similarity graph to explore potential relations between users and between pois the former measures the similarity of user behaviors based on user visit sequences and the latter quantifies the similarity between different pois through a novel feature mapping method finally we propose a multiview hybrid learning method that combines unsupervised and supervised learning paradigms to model complex relationships improving the overall recommendation performance extensive experiments on realworld datasets validate the effectiveness of our method
this paper describes a novel class of biscuit conditional the perspectival biscuit which arises when an ifclause containing a generic pronoun eg generic you is used to shift perspective for the interpretation of a perspectivesensitive item in the consequent eg fixing the directionality of behind in if youre at the door the cat is behind the desk this sentence is like a biscuit conditional in that it entails a fullyspecified propositionally stable consequent describing the spatial configuration of cat and desk but this reading vanishes in favor of a conditional dependence reading when the antecedent contains any nongeneric dp a prediction that is not straightforwardly accounted for by existing theories of biscuit conditionals an analysis is given demonstrating that biscuithood for perspectival biscuits arises due to generic quantification exclusively over individuals not worlds
bacic et al demonstrate that parp1hpf1 preferentially modify histone tails closest to the dna break directing alc1catalyzed nucleosome sliding these findings suggest a mechanism for rendering dna breaks more accessible to repair factors the chromatin remodeler alc1 is activated by dna damageinduced polyadpribose deposited by parp1parp2 and their cofactor hpf1 alc1 has emerged as a cancer drug target but how it is recruited to adpribosylated nucleosomes to affect their positioning near dna breaks is unknown here we find that parp1hpf1 preferentially initiates adpribosylation on the histone h2b tail closest to the dna break to dissect the consequences of such asymmetry we generate nucleosomes with a defined adpribosylated h2b tail on one side only the cryoelectron microscopy structure of alc1 bound to such an asymmetric nucleosome indicates preferential engagement on one side using singlemolecule fret we demonstrate that this asymmetric recruitment gives rise to directed sliding away from the dna linker closest to the adpribosylation site our data suggest a mechanism by which alc1 slides nucleosomes away from a dna break to render it more accessible to repair factors
in order to realize the automatic transfer task of aluminum ingot samples in an electrolytic aluminum plant an electrolytic aluminum sample transfer robot was designed based on the mechanical structure of the robot we focus on the fusion navigation method of the robot using multisensor fusion laser slam simultaneous localization and mapping algorithm for fusion mapping and scan context algorithm for global coarse alignment combining it with laser inertial guidance tightlycoupled odometry to get the global relocalization position and pure pursuit pure pursuit algorithm for trajectory tracking to realize the autonomous navigation through onsite testing the navigation stability positioning accuracy and operational error all meet the operational requirements and each unit operates normally
in this paper we introduce a notion of morsi fuzzy hemimetrics a common generalization of hemimetrics and morsi fuzzy metrics as the basic structure to define and study fuzzy rough sets we define a pair of fuzzy upper and lower approximation operators and investigate their properties it is shown that upper definable sets lower definable sets and definable sets are equivalent definable sets form an alexandrov fuzzy topology such that the upper and lower approximation operators are the closure and the interior operators respectively
phytoplankton size structure is crucial for the functionality of the ocean food web and biogeochemical cycling serving as a key indicator for assessing the state of marine ecosystems quantitatively despite the development of various algorithms to quantify phytoplankton size classes pscs using satellite ocean color data their reliability in optically complex coastal regions remains uncertain and requires regional optimization in this study we optimized and compared typical abundancebased threecomponent psc models for the bohai and yellow seas bys of china using an extensive in situ psc dataset derived from pigment samples analyzed via highperformance liquid chromatography hplc the optimized sea surface temperature sstdependent abundancebased psc model of the bys sapscbys generated the most accurate remotely sensed psc datasets for the bys and effectively reproduced the psc patterns observed in previous in situ studies these patterns highlight the prevalence of micro and picophytoplankton at higher and lower total biomass levels respectively and the dominance of nanophytoplankton at the midrange of total biomass this reliable psc model will be invaluable for future remote sensing studies aiming to understand the detailed spatiotemporal variability of pscs in the bys additionally it has the potential to enhance our understanding of the trophic connections between phytoplankton size structure and fisheries thereby facilitating the development of effective marine management strategies
robot training often takes place in simulated environments particularly with reinforcement learning therefore multiple training environments are generated using domain randomization to ensure transferability to realworld applications and compensate for unknown realworld states we propose improving domain randomization by involving human application experts in various stages of the training process experts can provide valuable judgments on simulation realism identify missing properties and verify robot execution our humanintheloop workflow describes how they can enhance the process in five stages validating and improving realworld scans correcting virtual representations specifying applicationspecific object properties verifying and influencing simulation environment generation and verifying robot training we outline examples and highlight research opportunities furthermore we present a case study in which we implemented different prototypes demonstrating the potential of human experts in the given stages our early insights indicate that human input can benefit robot training at different stages
often applications of selfsupervised learning to 3d medical data opt to use 3d variants of successful 2d network architectures although promising approaches they are significantly more computationally demanding to train and thus reduce the widespread applicability of these methods away from those with modest computational resources thus in this paper we aim to improve standard 2d ssl algorithms by modelling the inherent 3d nature of these datasets implicitly we propose two variants that build upon a strong baseline model and show that both of these variants often outperform the baseline in a variety of downstream tasks importantly in contrast to previous works in both 2d and 3d approaches for 3d medical data both of our proposals introduce negligible additional overhead over the baseline improving the democratisation of these approaches for medical applications
introduction deterioration of cognitive functions is commonly associated with aging although there is wide variation in the onset and manifestation albeit heterogeneity in agerelated cognitive decline has been studied at the cellular and molecular level there is poor evidence for electrophysiological correlates the aim of the current study was to address the electrophysiological basis of heterogeneity of cognitive functions in cognitively inferior and superior old 1920 months rats in the ventral tegmental area vta and the hippocampus having young 12 weeks rats as a control the midbrain vta operates as a hub amidst affective and cognitive facets processing sensory inputs related to motivated behaviours and hippocampal memory increasing evidence shows direct dopaminergic and nondopaminergic input from the vta to the hippocampus methods aged superior and inferior male rats were selected from a cohort of 88 animals based on their performance in a spatial learning and memory task using in vivo singlecell recording in the vta we examined the electrical activity of different neuronal populations putative dopaminergic glutamatergic and gabaergic neurons in the same animals basal synaptic transmission and synaptic plasticity were examined in hippocampal slices results electrophysiological recordings from the vta and hippocampus showed alterations associated with aging per se together with differences specifically linked to the cognitive status of aged animals in particular the bursting activity of dopamine neurons was lower while the firing frequency of glutamatergic neurons was higher in vta of inferior old rats the response to highfrequency stimulation in hippocampal slices also discriminated between superior and inferior aged animals discussion this study provides new insight into electrophysiological information underlying compromised cerebral ageing further understanding of brain senescence possibly related to neurocognitive decline will help develop new strategies towards the preservation of a high quality of life
in this study we apply a novel format of maxwell’s equations in si units for analyzing electromagnetic fields in conical cavities with a special focus on timedomain analysis this unique approach aligns the dimensions of electric e and magnetic h fields as inverse meters thereby facilitating theoretical investigations into complex geometrical field behaviors our primary focus is on deriving evolutionary equations for electromagnetic fields within conical structures additionally this study serves as a stepping stone for future indepth research into the mechanical properties of electromagnetic fields particularly due to the unified dimensional approach of e and h fields this method is expected to provide more insightful perspectives in understanding the dynamics of electromagnetic fields in conical cavities the implications of this research extend to practical applications notably in the design and analysis of microwave resonant cavities and conical antennas enhancing our comprehension of electromagnetic phenomena in specialized structures within the broader scope of electrodynamics
this contribution presents first numerical tests of some recently published alternative models for solution of viscous compressible and nearly incompressible models all models are solved by high resolution compact finite difference scheme with strong stability preserving rungekutta time stepping the two simple but challenging computational test cases are presented based on the doubleperiodic shear layer and the kelvinhelmholtz instability the obtained timedependent flow fields are showing pronounced shear and vorticity layers being resolved by the standard as well as by the new massdiffusive modified models the preliminary results show that the new models are viable alternative to the well established classical models
introduction leptodactylus latinasus and physalaemus cuqui are sympatric anuran species with similar environmental requirements and contrasting reproductive modes climatic configuration determines distribution patterns and promotes sympatry of environmental niches but specificityselectivity determines the success of reproductive modes species distribution models sdm are a valuable tool to predict spatiotemporal distributions based on the extrapolation of environmental predictors objectives to determine the spatiotemporal distribution of environmental niches and assess whether the protected areas of the world database of protected areas wdpa allow the conservation of these species in the current scenario and future methods we applied different algorithms to predict the distribution and spatiotemporal overlap of environmental niches of l latinasus and p cuqui within south america in the last glacial maximum lgm middleholocene current and future scenarios we assess the conservation status of both species with the wdpa conservation units results all applied algorithms showed high performance for both species x̅tss  087 x̅auc  095 the l latinasus predictions showed wide environmental niches from lgm to the current scenario 49  stable niches 37  gained niches and 13  lost niches suggesting historical fidelity to stable climaticenvironmental regions in the currentfuture transition l latinasus would increase the number of stable 70  and lost 20  niches suggesting fidelity to lowland regions and a possible trend toward microendemism p cuqui loses environmental niches from the lgm to the current scenario 25  and in the currentfuture transition 63  increasing the environmental sympathy between both species 31  spatial overlap in the current scenario and 70  in the future conclusion extreme drought events and rainfall variations derived from climate change suggest the loss of environmental niches for these species that are not currently threatened but are not adequately protected by conservation units the loss of environmental niches increases spatial sympatry which represents a new challenge for anurans and the conservation of their populations
selfsupervised learning ssl is an effective way of learning rich and transferable speech representations from unlabeled data to benefit downstream tasks however effectively incorporating a pretrained ssl model into an automatic speech recognition asr system remains challenging in this paper we propose a network architecture with lightweight adapters to adapt a pretrained ssl model for an endtoend e2e asr an adapter is introduced in each ssl network layer and trained on the downstream asr task while the parameters of the pretrained ssl network layers remain unchanged by carrying over all pretrained parameters we avoid the catastrophic forgetting problem at the same time we allow the network to quickly adapt to asr task with lightweight adapters the experiments using librispeech and wall street journal wsj datasets show that 1 the proposed adapterbased finetuning consistently outperforms fullfledged training in lowresource scenarios with up to 175122 relative word error rate wer reduction on the 10 min librispeech split 2 the adapterbased adaptation also shows competitive performance in highresource scenarios which further validates the effectiveness of the adapters
abstract inferring past demographic history of natural populations from genomic data is of central concern in many studies across research fields previously our group had developed dadi a widely used demographic history inference method based on the allele frequency spectrum afs and maximum compositelikelihood optimization however dadi’s optimization procedure can be computationally expensive here we present donni demography optimization via neural network inference a new inference method based on dadi that is more efficient while maintaining comparable inference accuracy for each dadisupported demographic model donni simulates the expected afs for a range of model parameters then trains a set of mean variance estimation neural networks using the simulated afs trained networks can then be used to instantaneously infer the model parameters from future genomic data summarized by an afs we demonstrate that for many demographic models donni can infer some parameters such as population size changes very well and other parameters such as migration rates and times of demographic events fairly well importantly donni provides both parameter and confidence interval estimates from input afs with accuracy comparable to parameters inferred by dadi’s likelihood optimization while bypassing its long and computationally intensive evaluation process donni’s performance demonstrates that supervised machine learning algorithms may be a promising avenue for developing more sustainable and computationally efficient demographic history inference methods
the dispersion coefficients are crucial in understanding the spreading of pollutant clouds in river flows particularly in the context of the depthaveraged twodimensional 2d advection–dispersion equation ade traditionally the 2d streamtube routing procedure 2d strp has been the predominant method for determining both the longitudinal and transverse dispersion coefficients of the 2d ade under transient concentration conditions this study aims to quantitatively analyze and address the limitations of the 2d strp using hypothetically generated data the findings of these evaluations revealed that the existing 2d strp failed to accurately reproduce reliable results when the tracer clouds reached wall boundaries this limitation prompted the development of the 2d strpi which effectively resolves this drawback the newly developed routingbased observation method 2d strpi enables the reliable estimation of dispersion coefficients considering the effect of the wall boundary the results indicated that the existing 2d strp yielded 2d dispersion coefficients with relative errors ranging from 40 to 200 while 2d strpi consistently yielded relative errors of 3 to 5 on average when applied to tracer test data obtained through remote sensing the 2d strpi demonstrated its ability to accurately observe temporal concentration distributions even when wall boundaries have a significant impact on contaminant transport
maintaining stability in bipedal walking remains a significant challenge in humanoid robotics largely due to the numerous involved hyperparameters traditional methods for determining these hyperparameters such as heuristic approaches can be both timeconsuming and potentially suboptimal in this paper we present an approach aimed at enhancing the stability of bipedal gait particularly when faced with floor perturbations and speed variations our main contribution is the integration of intrinsically stable model predictive control ismpc and wholebody admittance control within a closedloop reinforcement learning system we devised a reinforcement learning plugin implemented in the mcrtc framework that allows the control system to continuously monitor the robots current states maintain recursive feasibility and optimize parameters in realtime furthermore we propose a reward function derived from a combination of changes in single and double support time postural recovery divergent control of motion and action generation grounded in training optimization in the course of this research we conducted experiments on a real humanoid robot to validate initial aspects of our work the integrated modules effectiveness was further assessed through comprehensive simulations
body temperature variations including the generation transfer and dissipation of heat play an important role throughout life and participate in all biological events cellular temperature information is an indispensable link in the comprehensive understanding of life science processes but traditional testing strategies cannot provide sufficient information due to their low precision and inefficient cellular‐entrance in recent years with the help of luminescent nanomaterials a variety of new thermometers have been developed to achieve real‐time temperature measurement at the micronano scale in this review we summarized the latest advances in several nanoparticles for cellular temperature detection and their related applications in revealing cell metabolism and disease diagnosis furthermore this review proposed a few challenges for the nano‐thermometry expecting to spark novel thought to push forward its preclinical and translational uses
optical clearing agents ocas are substances that temporarily modify tissues optical properties enabling better imaging and light penetration this study aimed to assess the impact of ocas on the nail bed and blood using in vivo and in vitro optical methods in the in vivo part ocas were applied to the nail bed and optical coherence tomography and optical digital capillaroscopy were used to evaluate their effects on optical clearing and capillary blood flow respectively in the in vitro part the collected blood samples were incubated with the oca and blood aggregation properties were estimated using diffuse light scattering techniques the results indicate that ocas significantly influence the optical properties of the nail bed and blood microrheology these findings suggest that ocas hold promise for improving optical imaging and diagnostics particularly for nail bed applications and can modify blood microrheology
at present airspace congestion and flight delays have become widespread concerns this study aims to optimize the sequencing of arrival flights in the terminal area of multirunway airports considering the constraints of multiple runways slant intervals and moving flight positions this article establishes an optimization model for arrival flight sequencing in a multirunway airport terminal area accordingly an improved sparrow search algorithm issa is proposed based on chebyshev chaotic mapping the golden sine strategy and the variable neighborhood strategy through six basic test functions the issa is compared with particle swarm optimization the whale optimization algorithm the genetic algorithm and other algorithms to verify its superiority finally two sets of instance data from kunming changshui airport were used for experiments the results show that the total delay times tdts of smallscale flights number of aircraft 29 and largescale flights number of aircraft 147 are 553 and 205 lower respectively than those of the firstcomefirstserved algorithm the superiority of the issa designed in this article is verified and it can significantly reduce the tdts of arrival flights it is suitable for optimizing arrival flights during peak hours at most airports this approach provides theoretical support for optimizing the sorting of flights in terminal areas
predicting customer turnover is important for firms in all industries since keeping current customers is less expensive than finding new ones the goal of this research study is to offer insights into the most recent strategies and approaches by providing a thorough review of customer churn prediction tools the first section of the essay defines customer churn and discusses its importance to business operations then it examines different data sources and attributes that are frequently utilized for churn prediction it then explores various machine learning algorithms and methods used for churn prediction highlighting their advantages and disadvantages the paper also provides validation methods and evaluation metrics for churn prediction model performance evaluation it concludes by presenting a case study that illustrates the use of churn prediction in a practical situation
the current investigation focuses on detailed analysis of the anchor based optimization approach aboa its comparison with alternative global fitting protocols and on the global analysis of the truncation of basis effects in the calculation of binding energies it is shown that aboa provides a solution which is close to that obtained in alternative approaches but at small portion of their computational time the application of softer correction function after few initial iterations of aboa stabilizes and speeds up its convergence for the first time the numerical errors in the calculation of binding energies related to the truncation of bosonic and fermionic bases have been globally investigated with respect of asymptotic values corresponding to the infinite basis in the framework of covariant density functional theory cdft these errors typically grow up with the increase of the mass and deformation of the nuclei to reduce such errors in bosonic sector below 10 kev for almost all nuclei with proton number z120 one should truncate the bosonic basis at nb28 instead of presently used nb20 the reduction of the errors in binding energies due to the truncation of the fermionic basis in cdft is significantly more numerically costly for the first time it is shown that the pattern and the speed of the convergence of binding energies as a function of the size of fermionic basis given by nf depend on the type of covariant energy density functional the use of explicit density dependence of the mesonnucleon coupling constants or point couplings slows down substantially the speed of convergence of binding energies as a function of nf a new procedure for finding the asymptotic values of binding energies is suggested in the present paper it allows better control of numerical errors
nowadays optical fiber sensors are extensively being explored for the detection of magnetic fields against this backdrop a highly sensitive magnetic field sensor based on the combination of magnetostrictive material and vernier effect is proposed and experimentally demonstrated the proposed sensor consists of two fabryperot interferometers fpis each comprising two sections of hollowcore fiber as fp cavities and a singlemode fiber smf as a separator one of the fpis which is bonded to the magnetostrictive material undergoes further modulation by a highfrequency co2 laser to enhance its sensitivity both simulated and experimental results demonstrate that the proposed sensor exhibits significant spectral response to the extension effect of the material under magnetic field variation within the range of 0–1134 mt the highest sensitivity achieved is 110 nmmt while demonstrating excellent temperature stability and repeatability with its lowcost simple structure easy fabrication process and high sensitivity this proposed sensor exhibits great potential for magnetic field measurement
the paper aims to reveal the relationship between the geometrical features and linear and nonlinear optical properties of inas quantum dots qds this problem is justified by the extreme variety offered by the recent advances in growth techniques tailored to attainment of qds and nanostructures with virtually any shape to that end the finite element method in conjunction with the effective mass approximation and envelope function approximation was employed allowing the solution of the one particle eigenproblems in domains with any complex geometries the paper explores nanoplatelets spherical qds nanocones nanorods nanotadpoles and nanostars it has been found that there is a clear correlation between the complexity and symmetry of the qds and their linear and nonlinear absorption spectra for transitions between the electronic ground state and first three excited states
when diverse decision makers are involved in the decisionmaking process taking average of decision values might not reflect an accurate point of view to overcome such a scenario the circular fermatean fuzzy cff set an advancement of the fermatean fuzzy ff set and the intervalvalued fermatean fuzzy set ivffs are introduced in this current study the proposed cff set is a circle with a centre as association value av and nonassociation value nav with a radius at most equal to 2 it is built in such a way that it covers all the decision makers’ opinion value through a circle due to its geometric structure the cff set resolves ambiguity and risk more accurately and effectively than ff and ivff ff tnorm and tconorm are used to investigate the properties of cff sets subsequent to which the algebraic operations between them are defined a couple of cff distance measures between cff numbers are introduced and used in the selection of an electric autorickshaw along with the cff weighted averaging and geometric aggregation operators the overview and comparison analysis of the generated reports exemplifies the viability and compatibility of the cff set strategy for selecting the best choices
d2d coded caching originally introduced by ji caire and molisch significantly improves communication efficiency by applying the multicast technology proposed by maddahali and niesen to the d2d network most prior works on d2d coded caching are based on the assumption that all users will request content at the beginning of the delivery phase however in practice this is often not the case motivated by this consideration this paper formulates a new problem called requestrobust d2d coded caching the considered problem includes k users and a content server with access to n files only r users known as requesters request a file each at the beginning of the delivery phase the objective is to minimize the average and worstcase delivery rate ie the average and worstcase number of broadcast bits from all users among all possible demands for this novel d2d coded caching problem we propose a scheme based on uncoded cache placement and exploiting common demands and oneshot delivery we also propose informationtheoretic converse results under the assumption of uncoded cache placement furthermore we adapt the scheme proposed by yapar et al for uncoded cache placement and oneshot delivery to the requestrobust d2d coded caching problem and prove that the performance of the adapted scheme is order optimal within a factor of two under uncoded cache placement and within a factor of four in general finally through numerical evaluations we show that the proposed scheme outperforms known d2d coded caching schemes applied to the requestrobust scenario for most cache size ranges
one of the obstacles in the evolution from the finshaped fieldeffect transistor finfet to the gateallaround fieldeffect transistor gaafet is the etchingdepth variation in the process step for etching the sourcedrain sd regions the variation makes the overetching inevitable to avoid the fatal issue that underetched region may induces eg two adjacent gates can be connected in the following processes however the overetching goes with the degradation of device performance especially offstate performance to ease the degradation a counterdoped pocket was suggested in this work the process parameters for the pocket such as the overetching depth inlineformula texmath notationlatexttext ov texmathinlineformula the pocket epitaxy thickness inlineformula texmath notationlatexttext epi texmathinlineformula and the pocket doping concentration inlineformula texmath notationlatexntext pocket texmathinlineformula were explored subsequently the device performance was evaluated with respect to offstate leakage current inlineformula texmath notationlatexi mathrmscriptscriptstyle off texmathinlineformula and threshold voltage inlineformula texmath notationlatexvtext th texmathinlineformula variation it was confirmed that the inlineformula texmath notationlatexi mathrmscriptscriptstyle off texmathinlineformula decreased and the inlineformula texmath notationlatexvtext th texmathinlineformula variation was suppressed with optimized process parameters by analyzing the total doping concentration at substrate it also turned out that the gaafet with the pocket can give advantageous impact on the feasibility
this paper deals with the identifiability of varma models with var order greater than or equal to the ma order in the context of mixedfrequency data mfd using extended yule–walker equations the main contribution is that necessary and sufficient conditions for identifiability in the singlefrequency data case are expressed in an original way and yield new results in the mfd case we also provide two counterexamples that answer an open question in this topic about whether certain sufficient conditions are necessary for identifiability therefore this paper expands the set of models that can be identified with mfd using extended yule–walker equations the main idea is that with mfd some autocovariance blocks are not available from observed variables and in some cases the new conditions in this paper can be used to reconstruct all the nonavailable covariance blocks from available covariance blocks
agricultural environments are usually characterized by height differences and tree shading which pose challenges for communication in smart agriculture this study focuses on optimizing the packet loss rate and power consumption of lora’s practical communication quality the research includes the investigation of the phy antiframe loss mechanism encompassing phy frame loss detection and the response mechanism between gateways and nodes by implementing a closed loop for transmission and reception the study enhances the communication network’s resistance to interference and security theoretical performance calculations for the sx1278 radio frequency chip were conducted under different parameters to determine the optimal energy efficiency reducing unnecessary energy waste an experimental assessment of the packet loss rate was conducted to validate the practical efficacy of the research findings the results show that the lora communication with the antiframe loss mechanism and the optimal energy ratio parameter exhibits an adequate performance in the presence of strong and weak interferences the reception rates are maximally improved by 378 and 534 with effective distances of 250 m and 600 m corresponding to enhancements of 100 m and 400 m respectively this research effectively reduces lora energy consumption mitigates packet loss and extends communication distances providing insights for wireless transmission in agricultural contexts
we present human to humanoid h2o a reinforcement learning rl based framework that enables realtime wholebody teleoperation of a fullsized humanoid robot with only an rgb camera to create a largescale retargeted motion dataset of human movements for humanoid robots we propose a scalable simtodata process to filter and pick feasible motions using a privileged motion imitator afterwards we train a robust realtime humanoid motion imitator in simulation using these refined motions and transfer it to the real humanoid robot in a zeroshot manner we successfully achieve teleoperation of dynamic wholebody motions in realworld scenarios including walking back jumping kicking turning waving pushing boxing etc to the best of our knowledge this is the first demonstration to achieve learningbased realtime wholebody humanoid teleoperation
financial sentiment analysis refers to classifying financial text contents into sentiment categories eg positive negative and neutral in this paper we focus on the classification of financial news title which is a challenging task due to a lack of large amount of training samples to overcome this difficulty we propose to adapt the pretrained large language models llms 1 2 3 to solve this problem the llms which are trained from huge amount of text corporahave an advantage in text understanding and can be effectively adapted to domainspecific task while requiring very few amount of training samples in particular we adapt the opensource llama27b model 2023 with the supervised finetuning sft technique 4 experimental evaluation shows that even with the 7b model which is relatively small for llms our approach significantly outperforms the previous stateoftheart algorithms
"objective
although breastfeeding in the first 6 months postpartum benefits both infants and mothers breastfeeding rates remain low this study examined whether group prenatal care was associated with an increased breastfeeding initiation and duration compared with those receiving usual individual prenatal care a secondary aim was to investigate whether sociodemographic and motivational factors were associated with breastfeeding initiation and duration across prenatal care groups


methods
pregnant women in their third trimester n  211 from an innercity university medical center participated prenatal care type was identified from the medical chart and data on breastfeeding duration at 1 3 and 6 months postpartum were collected breastfeeding motivational factors were assessed with a survey logistic regressions and independentsamples t tests were used for data analyses


results
after controlling for demographic factors group prenatal care was associated with increased breastfeeding at 6 months postpartum odds ratio  266 p  045 compared with individual care breastfeeding intention p  001 competence p  003 and autonomous motivation p  001 were significantly higher while amotivation p  034 was significantly lower in group compared with individual prenatal care


conclusions
breastfeeding persistence was higher among women receiving group prenatal care potentially due to motivational factors future studies should investigate how breastfeeding motivational factors could be effectively targeted in prenatal care to increase breastfeeding persistence"
utilizing digital media as a pedagogical tool for english instruction presents a multitude of benefits in the contemporary landscape influenced by technology it facilitates involvement and interactivity in the educational experience the purpose of this study is to examine the advantages connected with the use of digital media as an educational instrument for the training of the english language the primary focus of this study is to investigate how students interpret digital media within the framework of english language teaching elt the information is obtained using a questionnaire administered by the studies this study employed an empirical case study design the data obtained from the questionnaire is subjected to descriptive analysis the findings of this study indicate that the students exhibited a strong inclination toward digital media as a viable resource for english reading materials the questionnaire yielded a mean score of 349 indicating a classification of high when interpreted in addition to encountering challenges related to the utilization of digital media students and teachers have experienced several obstacles these include limited access to mobile phone facilities restricted online containers and issues with insufficient connectivity
"data retention plays a crucial role in the accreditation process of an oncampus program of study since 2012 polytechnic of media kreatif has been engaged in the implementation of accreditation procedures however the effectiveness of these procedures is hindered by the decentralized storage of data resulting in disruptions throughout the datagathering process consequently stakeholders have difficulties in their efforts to evaluate and accredit study programs as they are required to manually look through individual files and data this research anticipates that the deployment of centralized data storage for certification purposes will alleviate the challenges previously associated with this process the agile method was used to develop the systems in conclusion this study found that the applications that have been created to help with the management of accreditationrelated data were supported by a wellrunning blackbox test it is also backed by an average usability test of 74 
keywords cloud storage data management agile method"
the rapid development of social networks has a wide range of social effects which facilitates the study of social issues accurately forecasting the information propagation process within social networks is crucial for promptly understanding the event direction and effectively addressing social problems in a scientific manner the relationships between nonadjacent users and the attitudes of users significantly influence the information propagation process within social networks however existing research has ignored these two elements which poses challenges for accurately predicting the information propagation process this limitation significantly hinders the study of emotional contagion and influence maximization in social networks to address these issues by considering the relationships between nonadjacent users and the influence of user attitudes we propose a new information propagation model based on the independent cascade model experimental results obtained from six real weibo datasets validate the effectiveness of the proposed model which is reflected in increased prediction accuracy and reduced time complexity furthermore the information dissemination trend in social networks predicted by the proposed model closely resembles the actual information propagation process which demonstrates the superiority of the proposed model
jatsp 
the analysis of infrared video images is becoming one of the methods used to detect thermal hazards in many largescale engineering sites the fusion of infrared thermal imaging and visible image data in the target area can help people to identify and locate the fault points of thermal hazards among them a very important step is the registration of thermally visible images however the direct registration of images with largescale differences may lead to large registration errors or even failure this paper presents a novel twostage thermal–visibleimage registration strategy specifically designed for exceptional scenes such as a substation firstly the original image pairs that occur after binarization are quickly and roughly registered secondly the adaptive downsampling unit partialintensity invariant feature descriptor adupiifd algorithm is proposed to correct the smallscale differences in details and achieve finer registration experiments are conducted on 30 data sets containing complex power station scenes and compared with several other methods the results show that the proposed method exhibits an excellent and stable performance in thermal–visibleimage registration and the registration error on the entire data set is within five pixels especially for multimodal images with poor image quality and many detailed features the robustness of the proposed method is far better than that of other methods which provides a more reliable image registration scheme for the field of fire safety
federated learning fl is a collaborative learning paradigm enabling participants to collectively train a shared machine learning model while preserving the privacy of their sensitive data nevertheless the inherent decentralized and dataopaque characteristics of fl render its susceptibility to data poisoning attacks these attacks introduce malformed or malicious inputs during local model training subsequently influencing the global model and resulting in erroneous predictions current fl defense strategies against data poisoning attacks either involve a tradeoff between accuracy and robustness or necessitate the presence of a uniformly distributed root dataset at the server to overcome these limitations we present fedzz which harnesses a zonebased deviating update zbdu mechanism to effectively counter data poisoning attacks in fl the zbdu approach identifies the clusters of benign clients whose collective updates exhibit notable deviations from those of malicious clients engaged in data poisoning attack further we introduce a precisionguided methodology that actively characterizes these client clusters zones which in turn aids in recognizing and discarding malicious updates at the server our evaluation of fedzz across two widely recognized datasets cifar10 and emnist demonstrate its efficacy in mitigating data poisoning attacks surpassing the performance of prevailing stateoftheart methodologies in both single and multiclient attack scenarios and varying attack volumes notably fedzz also functions as a robust client selection strategy even in highly noniid and attackfree scenarios moreover in the face of escalating poisoning rates the model accuracy attained by fedzz displays superior resilience compared to existing techniques for instance when confronted with a 50 presence of malicious clients fedzz sustains an accuracy of 6743 while the accuracy of the secondbest solution fldefender diminishes to 4336
in a twocountry model of endogenous growth with international knowledge spillover corporate income tax competition reproduces the secondbest allocation attained by tax harmonization despite complex externalities this stems from the positive spillover eﬀect across the border and free trading by ricardian households in the global ﬁnancial market however such a neutrality result does not hold in the extended model which includes nonricardian households the equilibrium tax rate under the corporate income tax competition can be excessively high or low depending on the elasticity of the spillover eﬀect to the share of the ﬁrms’ locations
temporal link prediction tlp is a prominent problem in network analysis that focuses on predicting the existence of future connections or relationships between entities in a dynamic network over time the predictive capabilities of existing models of tlp are often constrained due to their difficulty in adapting to the changes in dynamic network structures over time in this article an improved tlp model denoted as tlpnegcn is introduced by leveraging network embedding graph convolutional networks gcns and bidirectional long shortterm memory bilstm this integration provides a robust model of tlp that leverages historical network structures and captures temporal dynamics leading to improved performances we employ graph embedding with selfclustering gemsec to create lower dimensional vector representations for all nodes of the network at the initial timestamps the node embeddings are fed into an iterative training process using gcns across timestamps in the dataset this process enhances the node embeddings by capturing the networks temporal dynamics and integrating neighborhood information we obtain edge embeddings by concatenating the node embeddings of the end nodes of each edge encapsulating the information about the relationships between nodes in the network subsequently these edge embeddings are processed through a bilstm architecture to forecast upcoming links in the network the performance of the proposed model is compared against several baselines and contemporary tlp models on various reallife temporal datasets the obtained results based on various evaluation metrics demonstrate the superiority of the proposed work
the steady increase in the aging population worldwide is expected to cause a shortage of doctors and therapists for older people this demographic shift requires more efficient and automated systems for rehabilitation and physical ability evaluations rehabilitation using mixed reality mr technology has attracted much attention in recent years mr displays virtual objects on a headmounted seethrough display that overlies the user’s field of vision and allows users to manipulate them as if they exist in reality however tasks in previous studies applying mr to rehabilitation have been limited to tasks in which the virtual objects are static and do not interact dynamically with the surrounding environment therefore in this study we developed an application to evaluate cognitive and motor functions with the aim of realizing a rehabilitation system that is dynamic and has interaction with the surrounding environment using mr technology the developed application enabled effective evaluation of the user’s spatial cognitive ability task skillfulness motor function and decisionmaking ability the results indicate the usefulness and feasibility of mr technology to quantify motor function and spatial cognition both for static and dynamic tasks in rehabilitation
the modern power grid faces rapid growth in load demand due to industrialization leading to an unregulated environment and increasing adoption of renewable energy sources which presents technical challenges particularly in terms of stability hydrogen conversion technology revolutionizes clean electricity storage with renewable energy and solar hydrogen is now available in autonomous solar systems the efficiency of solar photovoltaic systems is closely related to using digital electronic maximum peak power tracking mppt technology the internet of things iot is crucial for performance monitoring and realtime control of pv systems enhancing the understanding of realtime operating parameters iot and wireless sensor networks for distributed solar energy devices and joint building design are essential for developing the photovoltaic construction industry in this paper the monitoring system that has been proposed offers a potentially effective solution for the intelligent remote and realtime monitoring of solar photovoltaic pv systems it demonstrated a high level of accuracy reaching 9849 and can transmit graphical representations to a smartphone application within a time frame of 5234 seconds consequently the batterys longevity was extended energy consumption was diminished and the quality of service qos for realtime applications inside the internet of things iot was enhanced
we introduce iqsync a clock offset recovery method designed for implementation on lowlevel hardware such as fpgas or microcontrollers for quantum key distribution qkd iqsync requires minimal memory only a simple instruction set eg no floatingpoint operations and can be evaluated with sublinear time complexity typically involving no more than a few thousand iterations of a simple loop furthermore iqsync allows for a precise clock offset recovery within few seconds even for large offsets and is well suited for scenarios with high channel loss and low signaltonoise ratio irrespective of the prepareandmeasure qkd protocol used we implemented the method on our qkd platform demonstrating its performance and conformity with analytically derived success probabilities for channel attenuations exceeding 70 db
the human mandible’s cancellous bone which is characterized by its unique porosity and directional sensitivity to external forces is crucial for sustaining biting stress traditional computer aided design cad models fail to fully represent the bone’s anisotropic structure and thus depend on simple isotropic assumptions for our research we use the latest versions of ntop 4173 and creo parametric 80 software to make biomimetic voronoi lattice models that accurately reflect the complex geometry and mechanical properties of trabecular bone the porosity of human cancellous bone is accurately modeled in this work using biomimetic voronoi lattice models the porosities range from 70 to 95 which can be achieved by changing the pore sizes to 10 mm 15 mm 20 mm and 25 mm finite element analysis fea was used to examine the displacements stresses and strains acting on dental implants with a buttress thread abutment retaining screw and biting load surface the results show that the voronoi model accurately depicts the complex anatomy of the trabecular bone in the human jaw compared to standard solid block models the ideal pore size for biomimetic voronoi lattice trabecular bone models is 2 mm taking in to account both the von mises stress distribution over the dental implant screw retention cortical bone cancellous bone and micromotions this pore size displayed balanced performance by successfully matching natural bone’s mechanical characteristics advanced fea improves the biomechanical understanding of how bones and implants interact by creating more accurate models of biological problems and dynamic loading situations this makes biomechanical engineering better
in the application of system identification not only the output but also the input of the system may be corrupted by noise which is often characterized by the errorsinvariables eiv model to identify such systems a gradientdescent total leastsquares gdtls and a maximum total correntropy mtc algorithms were proposed in some scenarios the weight vector of the unknown system may be sparse eg the echo path in acoustic echo cancelation aec employing tls or mtc to estimate such systems may result in slow convergence rate since they assign the same gain to the update of each weight and therefore cannot make use of the sparsity feature of the system to accelerate convergence to address the above problem this article proposes a uniform optimization model for deriving proportionate total adaptive filtering algorithms and then two proportionate total adaptive filtering algorithms are developed namely the proportionate total normalized least mean square ptnlms algorithm for gaussian noise disturbance and the proportionate mtc pmtc algorithm for impulsive noise interference which are both derived by utilizing the method of lagrange multipliers moreover this article also makes a steadystate performance analysis of the two proposed algorithms simulations are performed to demonstrate the superior performance of the two proposed algorithms and to test the accuracy of the theory on the steadystate performance analysis
the large visionlanguage model lvlm field has seen significant advancements yet its progression has been hindered by challenges in comprehending finegrained visual content due to limited resolution recent efforts have aimed to enhance the highresolution understanding capabilities of lvlms yet they remain capped at approximately 1500 x 1500 pixels and constrained to a relatively narrow resolution range this paper represents internlmxcomposer24khd a groundbreaking exploration into elevating lvlm resolution capabilities up to 4k hd 3840 x 1600 and beyond concurrently considering the ultrahigh resolution may not be necessary in all scenarios it supports a wide range of diverse resolutions from 336 pixels to 4k standard significantly broadening its scope of applicability specifically this research advances the patch division paradigm by introducing a novel extension dynamic resolution with automatic patch configuration it maintains the training image aspect ratios while automatically varying patch counts and configuring layouts based on a pretrained vision transformer vit 336 x 336 leading to dynamic training resolution from 336 pixels to 4k standard our research demonstrates that scaling training resolution up to 4k hd leads to consistent performance enhancements without hitting the ceiling of potential improvements internlmxcomposer24khd shows superb capability that matches or even surpasses gpt4v and gemini pro in 10 of the 16 benchmarks the internlmxcomposer24khd model series with 7b parameters are publicly available at httpsgithubcominternlminternlmxcomposer
a layered metamaterial comprising periodic blue glass and fto mediums was investigated for gamma γ radiation dosimetry the device acts on the principle of absorption of the incidence radiation with sharp resonance absorption peaks which undergo shifts in the presence of γradiation the more the radiation dose is the more shift happens in the resonance absorption spectrum – the feature that can be exploited in the design of polarization insensitive γradiation dosimetry device
in the ongoing 5g and upcoming 6g eras the intelligent internet of things iot network will take increasingly important responsibility for industrial production daily life and so on the iot devices with limited battery size and computing ability cannot meet many applications brought out by the datadriven artificial intelligence technique the combination of wireless power transfer wpt and edge computing is regarded as an effective solution to this dilemma iot devices can collect radio frequency energy provided by hybrid access points haps to process data locally or offload data to the edge servers of haps however how to efficiently make offloading decisions and allocate resource is challenging especially for the networks with multiple haps in this paper we consider the sum computation rate maximization problem for a wpt empowered iot network with multiple haps and iot devices the problem is formulated as a mixedinteger nonlinear programming problem to solve this problem efficiently we decompose it into a topproblem of optimizing offloading decisions and a subproblem of optimizing time allocation under the given offloading decisions we propose a deep reinforcement learning drl based algorithm to output the nearoptimal offloading decision and design an efficient algorithm based on lagrangian duality method to obtain the consequent optimal time allocation simulations verified that the proposed drlbased algorithm can achieve more than 95 percent of the maximal computation rate with low complexity compared with the common actorcritic algorithm the proposed algorithm has the substantial advantage in convergence speed achieved computation rate and running time
deraining of images plays a pivotal role in computer vision by addressing the challenges posed by rain enhancing visibility and refining image quality by eliminating rain streaks traditional methods often fall short of effectively handling intricate rain patterns resulting in incomplete removal in this paper we propose an innovative deep learningbased deraining model leveraging a modified residual unet and a multiscale attentionguided convolutional neural network module as a discriminator within a conditional generative adversarial network framework the proposed approach introduces custom hyperparameters and a tailored loss function to facilitate the efficient removal of rain streaks from images evaluation on both synthetic and realworld datasets showcases superior performance as indicated by improved image evaluation metrics such as psnr ssim and niqe the effectiveness of our model extends to improving both rainy and foggy images we also conducted a comparative analysis of computational complexity in terms of running time gflops and no of parameters against other stateoftheart methods to demonstrate our model’s superiority
background this study aimed to explore gender differences in associations between cognitive symptoms and suicidal ideation si among patients with recurrent major depressive disorder mdd methods we recruited 1222 patients with recurrent mdd from the national survey on symptomatology of depression nssd a survey designed to investigate the symptoms experienced during current major depressive episodes in china a fourpoint likert questionnaire was used to assess the frequency of cognitive symptoms and si in the past two weeks results gender differences in clinical features and cognitive symptoms of participants with recurrent mdd were found specifically male patients had a higher prevalence of memory loss decreased verbal output indecisiveness and impaired interpersonal relationships while female patients exhibited a higher prevalence of impaired social and occupational functioning all p  005 no significant difference in si prevalence was found between male and female patients the logistic regression analysis revealed that in male patients si was associated with indecisiveness and impaired interpersonal relationships in female patients reduced verbal output and impaired social and professional functions were also associated with si in addition to the abovementioned variables conclusion the findings of gender differences in associations between cognitive symptoms and si highlight the need to carefully assess genderspecific cognitive predictors of si in patients with recurrent mdd this has further implications for more targeted prevention and treatment strategies for si based on gender supplementary information the online version contains supplementary material available at 101186s1288802405557x
given a hypergraph mathcalh we introduce a new class of evaluation toric codes called edge codes derived from mathcalh we analyze these codes focusing on determining their basic parameters we provide estimations for the minimum distance particularly in scenarios involving duniform clutters additionally we demonstrate that these codes exhibit selforthogonality furthermore we compute the minimum distances of edge codes for all graphs with five vertices
the dataset provided by the knowledge graph reasoning challenge is an eventcentric knowledge graph representation of avatar behavior in a virtual space therefore it contains information about the entire virtual space including parts of the virtual space that are hidden in the video however some information cannot be obtained by any means for example when sensor data from the real world are collected in this paper we report a method for predicting hidden parts using bert and the results of experiments using this method using the challenge dataset as complete data for training
desa bukit batu kabupaten bengkalis memiliki banyak daya tarik sebagai destinasi wisata penelitian ini bertujuan untuk menjelaskan model komunikasi pariwisata berbasis komunitas dalam pengembangan desa wisata bukit batu kajian ini menggunakan pendekatan komunikasi pariwisata dengan pendekatan pariwisata berbasis komunitas cbt metode penelitian yang digunakan adalah kualitatif dengan pendekatan analisis deskriptif hasil penelitian menunjukan bahwa komunikasi pariwisata dalam pengembangan desa wisata budaya bukit batu dilakukan oleh kelompok sadar wisata pokdarwis relawan wisata bukit batu kelompok sadar wisata ini berkontribusi dalam menyusun banyak program dan mengimplementasikan kegiatannya untuk memajukan desa wisata budaya bukit batu sehingga meraih banyak penghargaan komunikasi pokdarwis dalam pengembangan pariwisatanya dilakukan dengan bekerjasama dengan pemerintah daerah perusahaan dan komunitas lainnya seperti fotografer dan ekraf dan penggiat usaha mikro kecil menengah umkm model komunikasi pariwisata dalam pengembangan desa wisata budaya bukit batu dilakukan komunitas sebagai penggerak utama swasta pendukung pendanaan dan pemerintah sebagai fasilitator dan regulator
background dendrobium officinale is a medicinal plant with high commercial value the dendrobium officinale market in yunnan is affected by the standardization of medicinal material quality control and the increase in market demand mainly due to the inappropriate harvest time which puts it under increasing resource pressure in this study considering the high polysaccharide content of dendrobium leaves and its contribution to today’s medical industry fourier transform infrared spectrometer ftir combined with chemometrics was used to combine the yields of both stem and leaf parts of dendrobium officinale to identify the different harvesting periods and to predict the dry matter content for the selection of the optimal harvesting period results the threedimensional correlation spectroscopy 3dcos images of dendrobium stems to build a splitattention networks resnet model can identify different harvesting periods 100 which is 90 faster than support vector machine svm and provides a scientific basis for modeling a large number of samples the partial least squares regression plsr model based on msc preprocessing can predict the dry matter content of dendrobium stems with factor  7 rmse  047 r2  099 rpd  879 the plsr model based on sg preprocessing can predict the dry matter content of dendrobium leaves with factor  9 rmse  02 r2  099 rpd  955 conclusions these results show that the resnet model possesses a fast and accurate recognition ability and at the same time can provide a scientific basis for the processing of a large number of sample data the plsr model with msc and sg preprocessing can predict the dry matter content of dendrobium stems and leaves respectively the suitable harvesting period for d officinale is from november to april of the following year with the best harvesting period being december during this period it is necessary to ensure sufficient water supply between 700 and 1000 every day and to provide a certain degree of light blocking between 1400 and 1700 supplementary information the online version contains supplementary material available at 101186s13007024011729
the introduction of smart grids allows utility providers to collect detailed data about consumers which can be utilized to enhance grid efficiency and reliability however this data collection also raises privacy concerns to protect user privacy some studies suggest using batterybased load hiding nevertheless the impact of widespread adoption of this approach on utility providers remain unclear our paper seeks to evaluate the effects of batterybased load hiding on two critical operations user profiling and anomaly detection our findings reveal that the inclusion of battery users in datasets can diminish the quality of conclusions drawn from these data this can result in a decrease in the area under the curve auc by more than 10 when attempting to profile users within singleoccupant and multipleoccupant households furthermore our experiments demonstrate that batterybased load hiding not only conceals information about users employing the batteries but can also lead to an increased rate of false positives for other nonbattery users from 015 to 037 within the system to mitigate these adverse effects our study assessed various mitigation strategies in the context of user profiling our experiment demonstrated that identifying and removing battery users from the analytical dataset using unsupervised detection methods can effectively lessen the impact of battery users for anomaly detection our experiment revealed that creating separate classification models for battery and nonbattery users can significantly reduce the adverse influence of battery users on the detection performance
highpressure gaseous hydrogen storage is an important way of hydrogen energy storage and transport at present while highstrength steel material is one of the main materials used for hydrogen storage vessels however their internal doping elements and inherent defects often lead to a decrease in their mechanical properties which reduces the pressurebearing capacity and storage life of the vessel at present the influence mechanism of doping elements on the mechanical properties of highstrength steels is still unclear this paper therefore applies a firstprinciples approach to study the influence of elemental doping cr mn mo as sb bi sn pb on the mechanical properties of fe single crystals and fec systems results show that mn doping among the above elements increases the elastic modulus bulk modulus and shear modulus compared with that of pure fe while the remaining elements decrease them with the nontransition metal elements having a greater effect on the three moduli than the transition metal elements electronic structure analysis shows that the transition metal elements have better compatibility with the fe lattice molecular dynamics results further show that the injection of h atoms significantly disrupts the lattice ordering of the fe polycrystalline doped system with c cr and mn elements while the doping of cr elements can significantly enhance the dislocation density of the system in summary this paper explores the effects of doping elements on the mechanical properties of singlecrystal and polycrystalline fe which is of strong guiding significance for the mechanistic study of the effects of doping and defects on the strength of febased materials
in smart cities community pharmacies offer a range of health services including clinical data gathering medication dispensing and automated alerts for precautions our paper reviews smart pharmacies highlighting a shift from prescriptionbased treatment to patientcentered care this transformation involves promoting a culture of cooperation and shared decisionmaking among stakeholders for value cocreation through seamless integration of pharmaceutical services and technology through the lens of service dominant sd logic this conceptual paper sets out to examine the context of smart pharmacies in the contemporary healthcare landscape revealing their potential in advancing patientcentered care and driving collaborative value creation within a complex healthcare ecosystem
"the exact 3satisfiability problem x3sat is known to remain npcomplete when restricted to expressions where every variable has exactly three occurrences even in the absence of negated variables cubic monotone 1in3 sat problem 
the present paper shows that the cubic monotone 1in3 sat problem can be solved in polynomial time and therefore prove that the conjecture pnp holds"
in recent years the rise of antibioticresistant bacteria has posed a severe threat to global public health necessitating innovative and alternative approaches to combat this escalating crisis bacteriophages viruses that infect and replicate within bacteria have emerged as promising candidates for therapeutic intervention against antibioticresistant pathogens this study delves into the intricate landscape of bacteriophage research unraveling the trends and impact of research in the field the analysis considers the chronological evolution of research identifying key contributors collaborative networks and thematic trends that have shaped the trajectory of this rapidly growing field out of 101717 search results in the pubmed database 163 clinical trials were identified revealing a dynamic landscape of research activity between 1965 and 2024 the annual scientific publication analysis unveiled fluctuations in the number of publications indicating an overall increasing trend notably 2011 emerged as a peak year signifying heightened activity in bacteriophage research employing lotkas law the authors productivity analysis illustrated an inherent imbalance in author contributions with a majority contributing to a single clinical trial coauthorship analysis highlighted leading collaborators cooccurrence analysis of keywords unveiled thematic clusters providing insights into the diverse aspects of bacteriophage research a word cloud emphasized significant terms while a thematic map categorized themes into various developmental stages antimicrobial agents chemotherapy and poultry science were the most relevant journals based on the number of publications the analysis of countries contributions revealed the united states as a leading contributor with switzerland and china following suit collaboration patterns suggested predominantly independent research with potential for increased international partnerships in certain regions additionally temporal analysis of authors institutions sources and countries revealed productivity patterns historical context and research shifts by scrutinizing a vast array of scientific literature this investigation aims to provide a panoramic view of how the scientific community has explored the potential of bacteriophages in the context of antibiotic resistance
abstract in alzheimer’s disease reconfiguration and deterioration of tissue microstructure occur before substantial degeneration become evident we explored the diffusion properties of both water a ubiquitous marker measured by diffusion mri and nacetylaspartate a neuronal metabolite probed by diffusionweighted magnetic resonance spectroscopy for investigating cortical microstructural changes downstream of alzheimer’s disease pathology to this aim 50 participants from the swedish biofinder2 study were scanned on both 7 and 3 t mri systems we found that in cognitively impaired participants with evidence of both abnormal amyloidbeta csf amyloidbeta4240 and tau accumulation taupet the nacetylaspartate diffusion rate was significantly lower than in cognitively unimpaired participants p  005 this supports the hypothesis that intraneuronal tau accumulation hinders diffusion in the neuronal cytosol conversely water diffusivity was higher in cognitively impaired participants p  0001 and was positively associated with the concentration of myoinositol a preferentially astrocytic metabolite p  0001 suggesting that water diffusion is sensitive to alterations in the extracellular space and in glia in conclusion measuring the diffusion properties of both water and nacetylaspartate provides rich information on the cortical microstructure in alzheimer’s disease and can be used to develop new sensitive and specific markers to microstructural changes occurring during the disease course
in the exascale era in which application behavior has large powerenergy footprints perapplication joblevel awareness of such impression is crucial in taking steps towards achieving efficiency goals beyond performance such as energy efficiency and sustainability to achieve these goals we have developed a novel lowlatency job power profiling machine learning pipeline that can group joblevel power profiles based on their shapes as they complete this pipeline leverages a comprehensive feature extraction and clustering pipeline powered by a generative adversarial network gan model to handle the featurerich time series of joblevel power measurements the output is then used to train a classification model that can predict whether an incoming job power profile is similar to a known group of profiles or is completely new with extensive evaluations we demonstrate the effectiveness of each component in our pipeline also we provide a preliminary analysis of the resulting clusters that depict the power profile landscape of the summit supercomputer from more than 60k jobs sampled from the year 2021
this article proposes an efficient and accurate embedded motor imagerybased brain–computer interface mibci that meets the requirements for wearable and realtime applications to achieve a suitable accuracy considering hardware constraints we explore bci transducer algorithms among which infinite impulse response iir filter common spatial pattern and support vector machine are used to preprocess extract features and classify data respectively with our hardware implementation of these tasks we have achieved an accuracy of 77 our system is designed at register transfer level rtl targeting an asic implementation which significantly decreases power consumption latency and area compared to the stateoftheart soa architectures for embedded bci systems to this end we fold iir filters using timeshared and rambased techniques and use hardwarefriendly algorithms for the implementation of other tasks the rtl design is realized on 45 nm cmos technology consuming 4 mw power and 025 mm2 area which outperforms the soa platforms for embedded bci systems to further illustrate the outperformance of our design the proposed architecture is implemented on virtex7 field program gate array as a prototyping platform consuming 6 μj energy with 152 area utilization
the last decade has witnessed the rapid development of immersive virtual reality ivr and its application in various contexts however its application in supporting realtime virtual collaboration has been quite rare due to technical barriers and the lack of validated design principles to address this research gap this study designed and developed an ivr space to enable multiuser synchronous colocated collaboration to complete a fantasy game an evaluation study n  95 was conducted to explore its useful design considerations and the influencing factors for collaboration experience in the game the ivr space was enabled by the simultaneous localization and mapping slambased insideout tracking technique and was informed by four essential design considerations for promoting effective collaboration in ivr namely the role script learning task collaboration mechanism and communication design the study results revealed that students in general were satisfied with their collaboration experience in ivr with social presence and collaboration competency as significant predictors of collective efficacy and social experience based on both quantitative and qualitative results this study proposes four validated principles for designing effective ivr spaces to support synchronous colocated collaboration
major cities worldwide experience problems with the performance of their road transportation systems and the continuous increase in traffic demand presents a substantial challenge to the optimal operation of urban road networks and the efficiency of traffic control strategies the operation of transportation systems is widely considered to display fragile property ie the loss in performance increases exponentially with the linearly increasing magnitude of disruptions meanwhile the risk engineering community is embracing the novel concept of antifragility enabling systems to learn from historical disruptions and exhibit improved performance under black swan events in this study based on established traffic models namely fundamental diagrams and macroscopic fundamental diagrams we first conducted a rigorous mathematical analysis to prove the fragile nature of the systems theoretically subsequently we propose a skewnessbased indicator that can be readily applied to crosscompare the degree of fragility for different networks solely dependent on the mfdrelated parameters at last by taking realworld stochasticity into account we implemented a numerical simulation with realistic network data to bridge the gap between the theoretical proof and the realworld operations to reflect the potential impact of uncertainty on the fragility of the systems this work aims to demonstrate the fragile nature of road transportation systems and help researchers better comprehend the necessity to consider explicitly antifragile design for future traffic control strategies
traditional yard and warehouse management systems are often prone to human errors which result in misplaced materials and products operational disruptions and delays to address these challenges we propose a novel qrbased yard management system mobile application that leverages the ubiquity of qr codes to digitalize the process of material identification and placement verification upon receiving a material dedicated qr codes placed on the material and the bins are scanned to automatically keep track of which bin the material is being placed in along with a validation process that ensures that the material is being placed in the right bin this reduces the likelihood of misplacement of products minimizes inventory discrepancies and any associated disruptions additionally the system also provides realtime visibility into the product and bin statuses enabling the yard managers to effectively track material locations and ensure their timely availability by streamlining the material handling process the yard management system enhances overall yard operation efficiency thereby improving productivity and cost savings
prejudice toward people with a disability remains an unchallenged global problem this cross‐sectional study investigated online intergroup contact involving university students without disability n  107 participating within a social program aimed at fostering the social inclusion of people with a disability we tested two separate path models where positive and negative online contact were the predictors controlling for positive and negative offline contact sense of community was the mediator while social distance from and attitudes toward people with disability were the outcome variables results showed that greater positive but not negative online contact was associated with more positive evaluations and lower social distance toward people with a disability via heightened sense of community
